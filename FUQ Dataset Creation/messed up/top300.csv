,title,section,full_text,summ_text,questions,answers
0,machine learning,Summary,"Machine learning (ML) is the study of computer algorithms that can improve automatically through experience and by the use of data. It is seen as a part of artificial intelligence. Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.",Machine learning (ML) is the study of computer algorithms that can improve automatically through experience and by the use of data. It is seen as a part of artificial intelligence.,"[' What is the study of computer algorithms that can improve automatically through experience and by the use of data?', ' Machine learning is seen as what?']","['Machine learning (ML)', 'artificial intelligence']"
1,machine learning,Summary,"A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers; but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning. Some implementations of machine learning use data and neural networks in a way that mimics the working of a biological brain.  In its application across business problems, machine learning is also referred to as predictive analytics.
","A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers; but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning.","[' What is a subset of machine learning closely related to?', ' What focuses on making predictions using computers?', ' Not all machine learning is what?', ' The study of what delivers methods, theory and application domains?']","['computational statistics', 'computational statistics', 'statistical learning', 'mathematical optimization']"
2,machine learning,Overview,"Learning algorithms work on the basis that strategies, algorithms, and inferences that worked well in the past are likely to continue working well in the future. These inferences can be obvious, such as ""since the sun rose every morning for the last 10,000 days, it will probably rise tomorrow morning as well"". They can be nuanced, such as ""X% of families have geographically separate species with color variants, so there is a Y% chance that undiscovered black swans exist"".","Learning algorithms work on the basis that strategies, algorithms, and inferences that worked well in the past are likely to continue working well in the future. These inferences can be obvious, such as ""since the sun rose every morning for the last 10,000 days, it will probably rise tomorrow morning as well"".","[' Learning algorithms work on the basis that strategies, algorithms, and inferences that worked well in the past are likely to continue working well in what future?', ' How many days has the temperature risen in the last 10,000 days?', ' What day will it probably rise?']","['the future', 'every morning', 'tomorrow']"
3,machine learning,Overview,"Machine learning programs can perform tasks without being explicitly programmed to do so. It involves computers learning from data provided so that they carry out certain tasks. For simple tasks assigned to computers, it is possible to program algorithms telling the machine how to execute all steps required to solve the problem at hand; on the computer's part, no learning is needed. For more advanced tasks, it can be challenging for a human to manually create the needed algorithms. In practice, it can turn out to be more effective to help the machine develop its own algorithm, rather than having human programmers specify every needed step.",Machine learning programs can perform tasks without being explicitly programmed to do so. It involves computers learning from data provided so that they carry out certain tasks.,"[' What can machine learning programs do without being explicitly programmed to do?', ' Machine learning involves computers learning from data provided so that they carry out certain tasks?']","['tasks', 'Machine learning']"
4,machine learning,Overview,"The discipline of machine learning employs various approaches to teach computers to accomplish tasks where no fully satisfactory algorithm is available. In cases where vast numbers of potential answers exist, one approach is to label some of the correct answers as valid. This can then be used as training data for the computer to improve the algorithm(s) it uses to determine correct answers. For example, to train a system for the task of digital character recognition, the MNIST dataset of handwritten digits has often been used.","The discipline of machine learning employs various approaches to teach computers to accomplish tasks where no fully satisfactory algorithm is available. In cases where vast numbers of potential answers exist, one approach is to label some of the correct answers as valid.","[' What discipline uses various approaches to teach computers to accomplish tasks where no fully satisfactory algorithm is available?', ' What is one approach to label some of the correct answers as valid?']","['machine learning', 'machine learning']"
5,machine learning,History and relationships to other fields,"The term machine learning was coined in 1959 by Arthur Samuel, an American IBMer and pioneer in the field of computer gaming and artificial intelligence. Also the synonym self-teaching computers was used in this time period. A representative book of the machine learning research during the 1960s was the Nilsson's book on Learning Machines, dealing mostly with machine learning for pattern classification. Interest related to pattern recognition continued into the 1970s, as described by Duda and Hart in 1973. In 1981 a report was given on using teaching strategies so that a neural network learns to recognize 40 characters (26 letters, 10 digits, and 4 special symbols) from a computer terminal.","The term machine learning was coined in 1959 by Arthur Samuel, an American IBMer and pioneer in the field of computer gaming and artificial intelligence. Also the synonym self-teaching computers was used in this time period.","[' Who coined the term machine learning in 1959?', ' Who was a pioneer in the field of computer gaming and artificial intelligence?']","['Arthur Samuel', 'Arthur Samuel']"
6,machine learning,History and relationships to other fields,"Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: ""A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E."" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper ""Computing Machinery and Intelligence"", in which the question ""Can machines think?"" is replaced with the question ""Can machines do what we (as thinking entities) can do?"".","Tom M. Mitchell provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: ""A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P if its performance at tasks in T, as measured by P,  improves with experience E."" This definition of the tasks in which machine learning is concerned offers a fundamentally operational definition rather than defining the field in cognitive terms. This follows Alan Turing's proposal in his paper ""Computing Machinery and Intelligence"", in which the question ""Can machines think?""","[' Tom M. Mitchell provided a more formal definition of the algorithms studied in what field?', ' What does P do if its performance at tasks in T, as measured by P, improves with experience E?', ' What does Alan Turing\'s paper ""Computing"" offer?', ' What did Alan Turing propose in his paper ""Computing Machinery and Intelligence""?']","['machine learning', 'performance measure', 'the question ""Can machines think?""', '""Can machines think?""']"
7,machine learning,History and relationships to other fields,"Modern day machine learning has two objectives, one is to classify data based on models which have been developed, the other purpose is to make predictions for future outcomes based on these models. A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles. Where as, a machine learning algorithm for stock trading may inform the trader of future potential predictions.","Modern day machine learning has two objectives, one is to classify data based on models which have been developed, the other purpose is to make predictions for future outcomes based on these models. A hypothetical algorithm specific to classifying data may use computer vision of moles coupled with supervised learning in order to train it to classify the cancerous moles.","[' How many objectives does modern day machine learning have?', ' What is the other purpose of machine learning?', ' A hypothetical algorithm specific to classifying data may use what?', ' Computer vision of moles coupled with supervised learning may be used to classify what?']","['two', 'to make predictions for future outcomes based on these models', 'computer vision of moles', 'cancerous moles']"
8,machine learning,Theory,"A core objective of a learner is to generalize from its experience. Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.
","A core objective of a learner is to generalize from its experience. Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set.","[' What is a core objective of a learner?', ' What is generalization in this context?']","['to generalize from its experience', 'the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set']"
9,machine learning,Theory,"The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The bias–variance decomposition is one way to quantify generalization error.
","The computational analysis of machine learning algorithms and their performance is a branch of theoretical computer science known as computational learning theory. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms.","[' What is the branch of theoretical computer science known as?', ' What does computational analysis of machine learning algorithms and their performance do?', ' Training sets are finite and the future is uncertain, what does learning theory usually not yield?']","['computational learning theory', 'computational learning theory', 'guarantees of the performance of algorithms']"
10,machine learning,Theory,"For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to overfitting and generalization will be poorer.","For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has under fitted the data.","[' What should the complexity of the hypothesis match?', ' If the hypothesis is less complex than the function, then the model has under fitted what?']","['the complexity of the function underlying the data', 'the data']"
11,machine learning,Theory,"In addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time. There are two kinds of time complexity results: Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.
","In addition to performance bounds, learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in polynomial time.","[' What do learning theorists study in addition to performance bounds?', ' What is considered feasible in computational learning theory if it can be done in polynomial time?']","['the time complexity and feasibility of learning', 'a computation']"
12,machine learning,Applications,"In 2006, the media-services provider Netflix held the first ""Netflix Prize"" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%.  A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million. Shortly after the prize was awarded, Netflix realized that viewers' ratings were not the best indicators of their viewing patterns (""everything is a recommendation"") and they changed their recommendation engine accordingly. In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of machine learning to predict the financial crisis. In 2012, co-founder of Sun Microsystems, Vinod Khosla, predicted that 80% of medical doctors' jobs would be lost in the next two decades to automated machine learning medical diagnostic software. In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings and that it may have revealed previously unrecognized influences among artists. In 2019 Springer Nature published the first research book created using machine learning. In 2020, machine learning technology was used to help make diagnoses and aid researchers in developing a cure for COVID-19. Machine learning is recently applied to predict the green behavior of human-being. Recently, machine learning technology is also applied to optimise smartphone's performance and thermal behaviour based on the user's interaction with the phone.","In 2006, the media-services provider Netflix held the first ""Netflix Prize"" competition to find a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm by at least 10%. A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an ensemble model to win the Grand Prize in 2009 for $1 million.","[' In what year did Netflix hold the first ""Netflix Prize"" competition?', ' What was the goal of the first competition for a program to better predict user preferences and improve the accuracy of its existing Cinematch movie recommendation algorithm?', ' In 2006, what was the name of the competition held by the media-service provider Netflix?', ' Who built an ensemble model to win the Grand Prize in 2009?', ' What was the prize for the ensemble model?', ' How much did the model win?']","['2006', 'at least 10%.', 'Netflix Prize', 'A joint team made up of researchers from AT&T Labs-Research in collaboration with the teams Big Chaos and Pragmatic Theory', '$1\xa0million', '$1\xa0million']"
13,machine learning,Limitations,"Although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results. Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.","Although machine learning has been transformative in some fields, machine-learning programs often fail to deliver expected results. Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.","[' Machine learning has been transformative in what fields?', ' Why do machine learning programs often fail to deliver expected results?', ' What are some of the problems with algorithms?', ' What is one of the issues with evaluation?']","['some fields', 'lack of (suitable) data', 'badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems', 'lack of resources']"
14,machine learning,Limitations,"In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision. Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested.","In 2018, a self-driving car from Uber failed to detect a pedestrian, who was killed after a collision. Attempts to use machine learning in healthcare with the IBM Watson system failed to deliver even after years of time and billions of dollars invested.","[' In 2018, a self-driving car from what company failed to detect a pedestrian?', "" What was the cause of the pedestrian's death?"", ' In 2018, what system failed to deliver even after years of time and billions of dollars invested?']","['Uber', 'after a collision', 'IBM Watson']"
15,machine learning,Limitations,"Machine learning has been used as a strategy to update the evidence related to systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research themselves.","Machine learning has been used as a strategy to update the evidence related to systematic review and increased reviewer burden related to the growth of biomedical literature. While it has improved with training sets, it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research themselves.","[' Machine learning has been used to update the evidence related to what?', ' What has increased reviewer burden related to the growth of biomedical literature?', ' How has machine learning improved with training sets?', ' To reduce the workload without limiting the necessary sensitivity for the findings research themselves?']","['systematic review', 'Machine learning', 'it has not yet developed sufficiently to reduce the workload burden without limiting the necessary sensitivity for the findings research themselves', 'Machine learning']"
16,machine learning,Model assessments,"Classification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model. In addition to the holdout and cross-validation methods, bootstrap, which samples n instances with replacement from the dataset, can be used to assess model accuracy.","Classification of machine learning models can be validated by accuracy estimation techniques like the holdout method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the K-fold-cross-validation method randomly partitions the data into K subsets and then K experiments are performed each respectively considering 1 subset for evaluation and the remaining K-1 subsets for training the model.","[' What can be validated by accuracy estimation techniques like the holdout method?', ' What splits the data in a training and test set and evaluates the performance of the training model on the test set?', ' What method randomly partitions the data into K subsets?', ' What is performed each considering 1 subset for evaluation and the remaining K-1 subset to train the model?']","['Classification of machine learning models', 'holdout method', 'K-fold-cross-validation', 'K experiments']"
17,machine learning,Model assessments,"In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning True Positive Rate (TPR) and True Negative Rate (TNR) respectively. Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. The total operating characteristic (TOC) is an effective method to express a model's diagnostic ability. TOC shows the numerators and denominators of the previously mentioned rates, thus TOC provides more information than the commonly used receiver operating characteristic (ROC) and ROC's associated area under the curve (AUC).","In addition to overall accuracy, investigators frequently report sensitivity and specificity meaning True Positive Rate (TPR) and True Negative Rate (TNR) respectively. Similarly, investigators sometimes report the false positive rate (FPR) as well as the false negative rate (FNR).","[' What do investigators often report in addition to overall accuracy?', ' What are TPR and TNR?']","['sensitivity and specificity', 'True Positive Rate (TPR) and True Negative Rate']"
18,machine learning,Ethics,"Machine learning poses a host of ethical questions. Systems which are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices. For example, in 1988, the UK's Commission for Racial Equality found that St. George's Medical School had been using a computer program trained from data of previous admissions staff and this program had denied nearly 60 candidates who were found to be either women or had non-European sounding names. Using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants by similarity to previous successful applicants. Responsible collection of data and documentation of algorithmic rules used by a system thus is a critical part of machine learning.
","Machine learning poses a host of ethical questions. Systems which are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices.","[' Machine learning poses a host of ethical questions.', ' What may systems trained on datasets collected with biases exhibit upon use?']","['Machine learning poses a host of ethical questions. Systems which are trained on datasets collected with biases may exhibit these biases upon use (algorithmic bias), thus digitizing cultural prejudices.', 'digitizing cultural prejudices']"
19,machine learning,Ethics,"AI can be well-equipped to make decisions in technical fields, which rely heavily on data and historical information. These decisions rely on objectivity and logical reasoning. Because human languages contain biases, machines trained on language corpora will necessarily also learn these biases.","AI can be well-equipped to make decisions in technical fields, which rely heavily on data and historical information. These decisions rely on objectivity and logical reasoning.","[' What can AI be well-equipped to make in technical fields?', ' What rely heavily on data and historical information?', ' Decisions rely on what?']","['decisions', 'technical fields', 'objectivity and logical reasoning']"
20,machine learning,Ethics,"Other forms of ethical challenges, not related to personal biases, are seen in health care. There are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines. This is especially true in the United States where there is a long-standing ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes. There is  potential for machine learning in health care to provide professionals an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated.","Other forms of ethical challenges, not related to personal biases, are seen in health care. There are concerns among health care professionals that these systems might not be designed in the public's interest but as income-generating machines.","[' What are other forms of ethical challenges seen in health care?', ' There are concerns among health care professionals that these systems might not be designed in what?']","['not related to personal biases', ""the public's interest""]"
21,machine learning,Hardware,"Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of non-linear hidden units. By 2019, graphic processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware compute used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of compute required, with a doubling-time trendline of 3.4 months.","Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks (a particular narrow subdomain of machine learning) that contain many layers of non-linear hidden units. By 2019, graphic processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI.","[' In what decade did advances in machine learning algorithms and computer hardware lead to more efficient methods for training deep neural networks?', ' Deep neural networks contain many layers of what?', ' Graphic processing units (GPUs) had displaced what in AI-specific enhancements by 2019?', ' What was the dominant method of training large-scale commercial cloud AI?', ' Graphic processing units (GPUs) often had what type of enhancements?']","['2010s', 'non-linear hidden units', 'CPUs', 'CPUs', 'AI-specific']"
22,genetic algorithm,Summary,"In computer science and operations research, a genetic algorithm (GA) is a metaheuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms (EA). Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on biologically inspired operators such as mutation, crossover and selection. Some examples of GA applications include optimizing decision trees for better performance, automatically solve sudoku puzzles, hyperparameter optimization, etc.
","In computer science and operations research, a genetic algorithm (GA) is a metaheuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms (EA). Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on biologically inspired operators such as mutation, crossover and selection.","[' What is a genetic algorithm inspired by?', ' What is the larger class of evolutionary algorithms?', ' Genetic algorithms are commonly used to generate what?', ' What type of operators are used to solve optimization and search problems?']","['the process of natural selection', 'EA', 'high-quality solutions to optimization and search problems', 'biologically inspired']"
23,genetic algorithm,The building block hypothesis,"Genetic algorithms are simple to implement, but their behavior is difficult to understand. In particular, it is difficult to understand why these algorithms frequently succeed at generating solutions of high fitness when applied to practical problems. The building block hypothesis (BBH) consists of:
","Genetic algorithms are simple to implement, but their behavior is difficult to understand. In particular, it is difficult to understand why these algorithms frequently succeed at generating solutions of high fitness when applied to practical problems.","[' Genetic algorithms are simple to implement, but their behavior is difficult to understand what?', ' Genetic algorithms often succeed at generating solutions of high fitness when applied to what problem?']","['why these algorithms frequently succeed at generating solutions of high fitness when applied to practical problems', 'practical']"
24,genetic algorithm,The building block hypothesis,"Despite the lack of consensus regarding the validity of the building-block hypothesis, it has been consistently evaluated and used as reference throughout the years. Many estimation of distribution algorithms, for example, have been proposed in an attempt to provide an environment in which the hypothesis would hold. Although good results have been reported for some classes of problems, skepticism concerning the generality and/or practicality of the building-block hypothesis as an explanation for GAs efficiency still remains. Indeed, there is a reasonable amount of work that attempts to understand its limitations from the perspective of estimation of distribution algorithms.","Despite the lack of consensus regarding the validity of the building-block hypothesis, it has been consistently evaluated and used as reference throughout the years. Many estimation of distribution algorithms, for example, have been proposed in an attempt to provide an environment in which the hypothesis would hold.","[' What has been consistently evaluated and used as reference throughout the years?', ' Many estimation of distribution algorithms have been proposed in an attempt to provide an environment in which the hypothesis would hold?']","['building-block hypothesis', 'building-block hypothesis']"
25,genetic algorithm,Problem domains,"Problems which appear to be particularly appropriate for solution by genetic algorithms include timetabling and scheduling problems, and many scheduling software packages are based on GAs. GAs have also been applied to engineering. Genetic algorithms are often applied as an approach to solve global optimization problems.
","Problems which appear to be particularly appropriate for solution by genetic algorithms include timetabling and scheduling problems, and many scheduling software packages are based on GAs. GAs have also been applied to engineering.","[' What are two problems that are particularly suitable for genetic algorithms?', ' What are many scheduling software packages based on?', ' GAs have been applied to what?']","['timetabling and scheduling problems', 'GAs', 'engineering']"
26,genetic algorithm,Problem domains,"As a general rule of thumb genetic algorithms might be useful in problem domains that have a complex fitness landscape as mixing, i.e., mutation in combination with crossover, is designed to move the population away from local optima that a traditional hill climbing algorithm might get stuck in. Observe that commonly used crossover operators cannot change any uniform population. Mutation alone can provide ergodicity of the overall genetic algorithm process (seen as a Markov chain).
","As a general rule of thumb genetic algorithms might be useful in problem domains that have a complex fitness landscape as mixing, i.e., mutation in combination with crossover, is designed to move the population away from local optima that a traditional hill climbing algorithm might get stuck in. Observe that commonly used crossover operators cannot change any uniform population.","[' Genetic algorithms might be useful in problem domains that have a complex what?', ' Mutation in combination with crossover is designed to move the population away from local optima that a traditional hill climbing algorithm might get stuck in?', ' What kind of algorithm might get stuck in?', ' Commonly used crossover operators cannot change what?']","['fitness landscape', 'mixing', 'hill climbing', 'any uniform population']"
27,genetic algorithm,Problem domains,"[I]t is quite unnatural to model applications in terms of genetic operators like mutation and crossover on bit strings. The pseudobiology adds another level of complexity between you and your problem. Second, genetic algorithms take a very long time on nontrivial problems. [...] [T]he analogy with evolution—where significant progress require [sic] millions of years—can be quite appropriate.
",[I]t is quite unnatural to model applications in terms of genetic operators like mutation and crossover on bit strings. The pseudobiology adds another level of complexity between you and your problem.,"[' What is it unnatural to model applications in terms of genetic operators like mutation and crossover on bit strings?', ' What adds another level of complexity between you and your problem?']","['I]t is quite unnatural to model applications in terms of genetic operators like mutation and crossover on bit strings. The pseudobiology', 'The pseudobiology']"
28,genetic algorithm,Problem domains,"
I have never encountered any problem where genetic algorithms seemed to me the right way to attack it. Further, I have never seen any computational results reported using genetic algorithms that have favorably impressed me. Stick to simulated annealing for your heuristic search voodoo needs.","
I have never encountered any problem where genetic algorithms seemed to me the right way to attack it. Further, I have never seen any computational results reported using genetic algorithms that have favorably impressed me.","[' I have never encountered a problem where genetic algorithms seemed to me the right way to attack it?', ' What have I never seen reported using genetic algorithms that have favorably impressed me?']","['I have never encountered any problem where genetic algorithms seemed to me the right way to attack it.', 'computational results']"
29,genetic algorithm,History,"In 1950, Alan Turing proposed a ""learning machine"" which would parallel the principles of evolution. Computer simulation of evolution started as early as in 1954 with the work of Nils Aall Barricelli, who was using the computer at the Institute for Advanced Study in Princeton, New Jersey. His 1954 publication was not widely noticed. Starting in 1957, the Australian quantitative geneticist Alex Fraser published a series of papers on simulation of artificial selection of organisms with multiple loci controlling a measurable trait. From these beginnings, computer simulation of evolution by biologists became more common in the early 1960s, and the methods were described in books by Fraser and Burnell (1970) and Crosby (1973). Fraser's simulations included all of the essential elements of modern genetic algorithms. In addition, Hans-Joachim Bremermann published a series of papers in the 1960s that also adopted a population of solution to optimization problems, undergoing recombination, mutation, and selection. Bremermann's research also included the elements of modern genetic algorithms. Other noteworthy early pioneers include Richard Friedberg, George Friedman, and Michael Conrad. Many early papers are reprinted by Fogel (1998).","In 1950, Alan Turing proposed a ""learning machine"" which would parallel the principles of evolution. Computer simulation of evolution started as early as in 1954 with the work of Nils Aall Barricelli, who was using the computer at the Institute for Advanced Study in Princeton, New Jersey.","[' When did Alan Turing propose a ""learning machine""?', ' When did computer simulation of evolution start?', ' Who was using the computer at the Institute for Advanced Study?']","['1950', '1954', 'Nils Aall Barricelli']"
30,genetic algorithm,History,"Although Barricelli, in work he reported in 1963, had simulated the evolution of ability to play a simple game, artificial evolution only became a widely recognized optimization method as a result of the work of Ingo Rechenberg and Hans-Paul Schwefel in the 1960s and early 1970s – Rechenberg's group was able to solve complex engineering problems through evolution strategies. Another approach was the evolutionary programming technique of Lawrence J. Fogel, which was proposed for generating artificial intelligence. Evolutionary programming originally used finite state machines for predicting environments, and used variation and selection to optimize the predictive logics. Genetic algorithms in particular became popular through the work of John Holland in the early 1970s, and particularly his book Adaptation in Natural and Artificial Systems (1975). His work originated with studies of cellular automata, conducted by Holland and his students at the University of Michigan. Holland introduced a formalized framework for predicting the quality of the next generation, known as Holland's Schema Theorem. Research in GAs remained largely theoretical until the mid-1980s, when The First International Conference on Genetic Algorithms was held in Pittsburgh, Pennsylvania.
","Although Barricelli, in work he reported in 1963, had simulated the evolution of ability to play a simple game, artificial evolution only became a widely recognized optimization method as a result of the work of Ingo Rechenberg and Hans-Paul Schwefel in the 1960s and early 1970s – Rechenberg's group was able to solve complex engineering problems through evolution strategies. Another approach was the evolutionary programming technique of Lawrence J. Fogel, which was proposed for generating artificial intelligence.","[' In what year did Barricelli report on the evolution of ability to play a simple game?', ' What was the result of the work of Ingo Rechenberg and Hans-Paul Schwefel in the 1960s and early 1970s?', "" Rechenberg's group was able to solve complex engineering problems through what?"", ' What technique was proposed for generating artificial intelligence?']","['1963', 'artificial evolution only became a widely recognized optimization method', 'evolution strategies', 'evolutionary programming technique']"
31,classification,Summary,"Classification is a process related to categorization, the process in which ideas and objects are recognized, differentiated and understood. 
Classification is the grouping of related facts into classes. 
It may also refer to:
","Classification is a process related to categorization, the process in which ideas and objects are recognized, differentiated and understood. Classification is the grouping of related facts into classes.","[' What is classification related to?', ' What is the process in which ideas and objects are recognized, differentiated and understood?', ' Classification is the grouping of related facts into what?']","['categorization', 'Classification', 'classes']"
32,deep learning,Summary,"Deep learning  (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.","Deep learning  (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.","[' What is deep learning also known as?', ' Deep learning is part of a broader family of what?', ' Learning can be supervised, semi-supervised or what else?']","['deep structured learning', 'machine learning methods', 'unsupervised']"
33,deep learning,Summary,"Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains.  Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analogue.",Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains.,"[' Artificial neural networks were inspired by what?', ' What are ANNs different from?']","['information processing and distributed communication nodes in biological systems', 'biological brains']"
34,deep learning,Summary,"The adjective ""deep"" in deep learning refers to the use of multiple layers in the network. Early work showed that a linear perceptron cannot be a universal classifier, but that a network with a nonpolynomial activation function with one hidden layer of unbounded width can. Deep learning is a modern variation which is concerned with an unbounded number of layers of bounded size, which permits practical application and optimized implementation, while retaining theoretical universality under mild conditions. In deep learning the layers are also permitted to be heterogeneous and to deviate widely from biologically informed connectionist models, for the sake of efficiency, trainability and understandability, whence the ""structured"" part.
","The adjective ""deep"" in deep learning refers to the use of multiple layers in the network. Early work showed that a linear perceptron cannot be a universal classifier, but that a network with a nonpolynomial activation function with one hidden layer of unbounded width can.","[' What does the adjective ""deep"" in deep learning refer to?', ' Early work showed that a linear perceptron cannot be a universal classifier, but a network with a nonpolynomial activation function with one hidden layer of unbounded width can.']","['the use of multiple layers in the network', 'can']"
35,deep learning,Definition,"Deep learning is a class of machine learning algorithms that: 199–200  uses multiple layers to progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.
","Deep learning is a class of machine learning algorithms that: 199–200  uses multiple layers to progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits or letters or faces.","[' What class of machine learning algorithms uses multiple layers to extract higher-level features from raw input?', ' In image processing, what may lower layers identify?', ' What may higher layers identify the concepts relevant to a human?']","['Deep learning', 'edges', 'digits or letters or faces']"
36,deep learning,Overview,"In deep learning, each level learns to transform its input data into a slightly more abstract and composite representation. In an image recognition application, the raw input may be a matrix of pixels; the first representational layer may abstract the pixels and encode edges; the second layer may compose and encode arrangements of edges; the third layer may encode a nose and eyes; and the fourth layer may recognize that the image contains a face. Importantly, a deep learning process can learn which features to optimally place in which level on its own. This does not completely eliminate the need for hand-tuning; for example, varying numbers of layers and layer sizes can provide different degrees of abstraction.","In deep learning, each level learns to transform its input data into a slightly more abstract and composite representation. In an image recognition application, the raw input may be a matrix of pixels; the first representational layer may abstract the pixels and encode edges; the second layer may compose and encode arrangements of edges; the third layer may encode a nose and eyes; and the fourth layer may recognize that the image contains a face.","[' In deep learning, each level learns to transform its input data into what?', ' In an image recognition application, the raw input may be a matrix of pixels?', ' The first representational layer may abstract the pixels and encode edges.', ' The second layer may compose and what else?', ' The second layer may compose and encode arrangements of what?', ' The third layer may encode a nose and what else?']","['a slightly more abstract and composite representation', 'deep learning', 'image recognition application, the raw input may be a matrix of pixels', 'encode arrangements of edges', 'edges', 'eyes']"
37,deep learning,Overview,"The word ""deep"" in ""deep learning"" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth. The CAP is the chain of transformations from input to output. CAPs describe potentially causal connections between input and output. For a feedforward neural network, the depth of the CAPs is that of the network and is the number of hidden layers plus one (as the output layer is also parameterized). For recurrent neural networks, in which a signal may propagate through a layer more than once, the CAP depth is potentially unlimited. No universally agreed-upon threshold of depth divides shallow learning from deep learning, but most researchers agree that deep learning involves CAP depth higher than 2. CAP of depth 2 has been shown to be a universal approximator in the sense that it can emulate any function. Beyond that, more layers do not add to the function approximator ability of the network. Deep models (CAP > 2) are able to extract better features than shallow models and hence, extra layers help in learning the features effectively.
","The word ""deep"" in ""deep learning"" refers to the number of layers through which the data is transformed. More precisely, deep learning systems have a substantial credit assignment path (CAP) depth.","[' What does the word ""deep"" in ""deep learning"" refer to?', ' Deep learning systems have what kind of depth?']","['the number of layers through which the data is transformed', 'credit assignment path (CAP)']"
38,deep learning,Overview,Deep learning architectures can be constructed with a greedy layer-by-layer method. Deep learning helps to disentangle these abstractions and pick out which features improve performance.,Deep learning architectures can be constructed with a greedy layer-by-layer method. Deep learning helps to disentangle these abstractions and pick out which features improve performance.,"[' What can be constructed with a greedy layer-by-layer method?', ' Deep learning helps to disentangle these abstractions and pick which features improve performance?']","['Deep learning architectures', 'Deep learning architectures']"
39,deep learning,Overview,Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data are more abundant than the labeled data. Examples of deep structures that can be trained in an unsupervised manner are neural history compressors and deep belief networks.,Deep learning algorithms can be applied to unsupervised learning tasks. This is an important benefit because unlabeled data are more abundant than the labeled data.,"[' Deep learning algorithms can be applied to what kind of tasks?', ' Unlabeled data are more abundant than what type of data?']","['unsupervised learning', 'labeled']"
40,deep learning,Interpretations,"The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions. In 1989, the first proof was published by George Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik. Recent work also showed that universal approximation also holds for non-bounded activation functions such as the rectified linear unit.","The classic universal approximation theorem concerns the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions. In 1989, the first proof was published by George Cybenko for sigmoid activation functions and was generalised to feed-forward multi-layer architectures in 1991 by Kurt Hornik.","[' What is the classic universal approximation theorem concerned with?', ' Who published the first proof for sigmoid activation functions in 1989?', ' What was generalised to feed forward multi-layer architectures in 1991 by Kurt Hornik?']","['the capacity of feedforward neural networks with a single hidden layer of finite size to approximate continuous functions', 'George Cybenko', 'the first proof was published by George Cybenko for sigmoid activation functions']"
41,deep learning,Interpretations,"The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al. proved that if the width of a deep neural network with ReLU activation is strictly larger than the input dimension, then the network can approximate any Lebesgue integrable function; If the width is smaller or equal to the input dimension, then a deep neural network is not a universal approximator.
",The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow. Lu et al.,[' The universal approximation theorem for deep neural networks concerns the capacity of networks with bounded width but the depth is allowed to grow?'],['depth']
42,deep learning,Interpretations,"The probabilistic interpretation derives from the field of machine learning. It features inference, as well as the optimization concepts of training and testing, related to fitting and generalization, respectively. More specifically, the probabilistic interpretation considers the activation nonlinearity as a cumulative distribution function. The probabilistic interpretation led to the introduction of dropout as regularizer in neural networks. The probabilistic interpretation was introduced by researchers including Hopfield, Widrow and Narendra and popularized in surveys such as the one by Bishop.","The probabilistic interpretation derives from the field of machine learning. It features inference, as well as the optimization concepts of training and testing, related to fitting and generalization, respectively.","[' What field does the probabilistic interpretation derive from?', ' Inference, training and testing are related to what two concepts?']","['machine learning', 'fitting and generalization']"
43,deep learning,History,"Some sources point out that Frank Rosenblatt developed and explored all of the basic ingredients of the deep learning systems of today. He described it in his book ""Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms"", published by Cornell Aeronautical Laboratory, Inc., Cornell University in 1962.
","Some sources point out that Frank Rosenblatt developed and explored all of the basic ingredients of the deep learning systems of today. He described it in his book ""Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms"", published by Cornell Aeronautical Laboratory, Inc., Cornell University in 1962.","[' Who wrote ""Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms""?', ' In what year did Frank Rosenblatt publish his book?']","['Frank Rosenblatt', '1962']"
44,deep learning,History,"The first general, working learning algorithm for supervised, deep, feedforward, multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1967. A 1971 paper described a deep network with eight layers trained by the group method of data handling. Other deep learning working architectures, specifically those built for computer vision, began with the Neocognitron introduced by Kunihiko Fukushima in 1980.","The first general, working learning algorithm for supervised, deep, feedforward, multilayer perceptrons was published by Alexey Ivakhnenko and Lapa in 1967. A 1971 paper described a deep network with eight layers trained by the group method of data handling.","[' In what year was the first working learning algorithm published?', ' What was the name of the first general learning algorithm?', ' How many layers were in the deep network described in 1971?']","['1967', 'Alexey Ivakhnenko and Lapa', 'eight']"
45,deep learning,History,"In 1989, Yann LeCun et al. applied the standard backpropagation algorithm, which had been around as the reverse mode of automatic differentiation since 1970, to a deep neural network with the purpose of recognizing handwritten ZIP codes on mail. While the algorithm worked, training required 3 days.","In 1989, Yann LeCun et al. applied the standard backpropagation algorithm, which had been around as the reverse mode of automatic differentiation since 1970, to a deep neural network with the purpose of recognizing handwritten ZIP codes on mail.",[' When did Yann LeCun apply the standard backpropagation algorithm to a deep neural network?'],['1989']
46,deep learning,History,"In 1994, André de Carvalho, together with Mike Fairhurst and David Bisset, published experimental results of a multi-layer boolean neural network, also known as a weightless neural network, composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN), which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.","In 1994, André de Carvalho, together with Mike Fairhurst and David Bisset, published experimental results of a multi-layer boolean neural network, also known as a weightless neural network, composed of a 3-layers self-organising feature extraction neural network module (SOFT) followed by a multi-layer classification neural network module (GSN), which were independently trained. Each layer in the feature extraction module extracted features with growing complexity regarding the previous layer.","[' André de Carvalho, Mike Fairhurst and David Bisset published experimental results of what in 1994?', ' What is another name for a multi-layer boolean neural network?', ' How many layers does SOFT consist of?', ' What is a GSN?', ' What did each layer in the feature extraction module extract features with?']","['a multi-layer boolean neural network', 'weightless neural network', '3', 'multi-layer classification neural network module', 'growing complexity']"
47,deep learning,History,"In 1995, Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm, co-developed with Peter Dayan and Hinton. Many factors contribute to the slow speed, including the vanishing gradient problem analyzed in 1991 by Sepp Hochreiter.","In 1995, Brendan Frey demonstrated that it was possible to train (over two days) a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm, co-developed with Peter Dayan and Hinton. Many factors contribute to the slow speed, including the vanishing gradient problem analyzed in 1991 by Sepp Hochreiter.","[' In what year did Brendan Frey demonstrate that it was possible to train a network containing six fully connected layers and several hundred hidden units using the wake-sleep algorithm?', ' Along with Peter Dayan and Hinton, what co-developer of the wake - sleep algorithm worked with Frey?', ' What problem contributes to the slow speed?', ' What problem was analyzed in 1991 by Sepp Hochreiter?']","['1995', 'Brendan Frey', 'vanishing gradient problem', 'vanishing gradient']"
48,deep learning,History,"Both shallow and deep learning (e.g., recurrent nets) of ANNs have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively. Key difficulties have been analyzed, including gradient diminishing and weak temporal correlation structure in neural predictive models. Additional difficulties were the lack of training data and limited computing power.
","Both shallow and deep learning (e.g., recurrent nets) of ANNs have been explored for many years. These methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model (GMM-HMM) technology based on generative models of speech trained discriminatively.","[' How long have shallow and deep learning of ANNs been explored?', ' What type of learning methods never outperformed non-uniform internal-handcrafting Gaussian mixture model/Hidden Markov model technology?']","['many years', 'shallow and deep learning (e.g., recurrent nets) of ANNs']"
49,deep learning,History,"Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s. Funded by the US government's NSA and DARPA, SRI studied deep neural networks in speech and speaker recognition. The speaker recognition team led by Larry Heck reported significant success with deep neural networks in speech processing in the 1998 National Institute of Standards and Technology Speaker Recognition evaluation. The SRI deep neural network was then deployed in the Nuance Verifier, representing the first major industrial application of deep learning.",Most speech recognition researchers moved away from neural nets to pursue generative modeling. An exception was at SRI International in the late 1990s.,"[' Most speech recognition researchers moved away from neural nets to pursue what?', ' What was the exception at SRI International in the late 1990s?']","['generative modeling', 'speech recognition researchers moved away from neural nets to pursue generative modeling']"
50,deep learning,History,"The principle of elevating ""raw"" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the ""raw"" spectrogram or linear filter-bank features in the late 1990s, showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.","The principle of elevating ""raw"" features over hand-crafted optimization was first explored successfully in the architecture of deep autoencoder on the ""raw"" spectrogram or linear filter-bank features in the late 1990s, showing its superiority over the Mel-Cepstral features that contain stages of fixed transformation from spectrograms. The raw features of speech, waveforms, later produced excellent larger-scale results.","[' In what decade was the principle of elevating ""raw"" features over hand-crafted optimization first explored?', ' What features contain stages of fixed transformation from spectrograms?', ' In what year did deep autoencoder demonstrate its superiority over Mel-Cepstral features?', ' What are the stages of fixed transformation from spectrograms?', ' The raw features of speech, waveforms, later produced what?']","['1990s', 'Mel-Cepstral', '1990s', 'Mel-Cepstral features', 'excellent larger-scale results']"
51,deep learning,History,"Many aspects of speech recognition were taken over by a deep learning method called long short-term memory (LSTM), a recurrent neural network published by Hochreiter and Schmidhuber in 1997. LSTM RNNs avoid the vanishing gradient problem and can learn ""Very Deep Learning"" tasks that require memories of events that happened thousands of discrete time steps before, which is important for speech. In 2003, LSTM started to become competitive with traditional speech recognizers on certain tasks. Later it was combined with connectionist temporal classification (CTC) in stacks of LSTM RNNs. In 2015, Google's speech recognition reportedly experienced a dramatic performance jump of 49% through CTC-trained LSTM, which they made available through Google Voice Search.","Many aspects of speech recognition were taken over by a deep learning method called long short-term memory (LSTM), a recurrent neural network published by Hochreiter and Schmidhuber in 1997. LSTM RNNs avoid the vanishing gradient problem and can learn ""Very Deep Learning"" tasks that require memories of events that happened thousands of discrete time steps before, which is important for speech.","[' What is long short-term memory?', ' When was LSTM published?', ' What is a recurrent neural network called?', ' Deep Learning tasks require memories of events that happened thousands of discrete time steps before.', ' What is important for speech?']","['a recurrent neural network', '1997', 'long short-term memory', 'speech recognition', 'memories of events that happened thousands of discrete time steps before']"
52,deep learning,History,"In 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then fine-tuning it using supervised backpropagation. The papers referred to learning for deep belief nets.
","In 2006, publications by Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh showed how a many-layered feedforward neural network could be effectively pre-trained one layer at a time, treating each layer in turn as an unsupervised restricted Boltzmann machine, then fine-tuning it using supervised backpropagation. The papers referred to learning for deep belief nets.","[' In what year did Geoff Hinton, Ruslan Salakhutdinov, Osindero and Teh publish their papers?', ' How was a many-layered feedforward neural network effectively pre-trained?', ' What did the papers refer to?', ' How did the papers refer to learning for deep belief nets?']","['2006', 'one layer at a time', 'learning for deep belief nets', 'supervised backpropagation']"
53,deep learning,History,"Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved. Convolutional neural networks (CNNs) were superseded for ASR by CTC for LSTM. but are more successful in computer vision.
","Deep learning is part of state-of-the-art systems in various disciplines, particularly computer vision and automatic speech recognition (ASR). Results on commonly used evaluation sets such as TIMIT (ASR) and MNIST (image classification), as well as a range of large-vocabulary speech recognition tasks have steadily improved.","[' Deep learning is part of state-of-the-art systems in what disciplines?', ' Deep learning results on commonly used evaluation sets such as TIMIT and MNIST have steadily improved what?']","['computer vision and automatic speech recognition', 'image classification), as well as a range of large-vocabulary speech recognition tasks']"
54,deep learning,History,"The impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around 2010.
","The impact of deep learning in industry began in the early 2000s, when CNNs already processed an estimated 10% to 20% of all the checks written in the US, according to Yann LeCun. Industrial applications of deep learning to large-scale speech recognition started around 2010.","[' When did the impact of deep learning in industry begin?', ' How much of all checks written in the US were processed by CNNs?', ' What was the estimated percentage of checks that CNNs processed?']","['early 2000s', '10% to 20%', '10% to 20%']"
55,deep learning,History,"The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets (DNN) might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets. However, it was discovered that replacing pre-training with large amounts of training data for straightforward backpropagation when using DNNs with large, context-dependent output layers produced error rates dramatically lower than then-state-of-the-art Gaussian mixture model (GMM)/Hidden Markov Model (HMM) and also than more-advanced generative model-based systems. The nature of the recognition errors produced by the two types of systems was characteristically different, offering technical insights into how to integrate deep learning into the existing highly efficient, run-time speech decoding system deployed by all major speech recognition systems. Analysis around 2009–2010, contrasting the GMM (and other generative speech models) vs. DNN models, stimulated early industrial investment in deep learning for speech recognition, eventually leading to pervasive and dominant use in that industry. That analysis was done with comparable performance (less than 1.5% in error rate) between discriminative DNNs and generative models.","The 2009 NIPS Workshop on Deep Learning for Speech Recognition was motivated by the limitations of deep generative models of speech, and the possibility that given more capable hardware and large-scale data sets that deep neural nets (DNN) might become practical. It was believed that pre-training DNNs using generative models of deep belief nets (DBN) would overcome the main difficulties of neural nets.","[' What was the motivation for the 2009 NIPS Workshop on Deep Learning for Speech Recognition?', ' What was motivated by the limitations of deep generative models of speech and the possibility that more capable hardware and data sets might become practical?', ' Pre-training DNNs using generative models of deep belief nets would overcome the main difficulties of what?']","['the limitations of deep generative models of speech', 'The 2009 NIPS Workshop on Deep Learning for Speech Recognition', 'neural nets']"
56,deep learning,History,"Advances in hardware have driven renewed interest in deep learning. In 2009, Nvidia was involved in what was called the “big bang” of deep learning, “as deep-learning neural networks were trained with Nvidia graphics processing units (GPUs).” That year, Andrew Ng determined that GPUs could increase the speed of deep-learning systems by about 100 times. In particular, GPUs are well-suited for the matrix/vector computations involved in machine learning. GPUs speed up training algorithms by orders of magnitude, reducing running times from weeks to days. Further, specialized hardware and algorithm optimizations can be used for efficient processing of deep learning models.","Advances in hardware have driven renewed interest in deep learning. In 2009, Nvidia was involved in what was called the “big bang” of deep learning, “as deep-learning neural networks were trained with Nvidia graphics processing units (GPUs).” That year, Andrew Ng determined that GPUs could increase the speed of deep-learning systems by about 100 times.","[' What has driven renewed interest in deep learning?', ' What was Nvidia involved in in 2009?', ' Who determined that GPUs could increase the speed of deep-learning?', ' Ng determined that GPUs could increase the speed of deep-learning systems by how many times?']","['Advances in hardware', 'the “big bang” of deep learning', 'Andrew Ng', '100']"
57,deep learning,Hardware,"Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer. By 2019, graphic processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI. OpenAI estimated the hardware computation used in the largest deep learning projects from AlexNet (2012) to AlphaZero (2017), and found a 300,000-fold increase in the amount of computation required, with a doubling-time trendline of 3.4 months.","Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more efficient methods for training deep neural networks that contain many layers of non-linear hidden units and a very large output layer. By 2019, graphic processing units (GPUs), often with AI-specific enhancements, had displaced CPUs as the dominant method of training large-scale commercial cloud AI.","[' In what decade did advances in machine learning algorithms and computer hardware lead to more efficient methods for training deep neural networks?', ' By 2019, graphic processing units (GPUs) had displaced what CPUs?', ' What was the dominant method of training large-scale commercial cloud AI?', ' What were GPUs often with?']","['2010s', 'CPUs', 'CPUs', 'AI-specific enhancements']"
58,deep learning,Relation to human cognitive and brain development,"Deep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s. These developmental theories were instantiated in computational models, making them predecessors of deep learning systems. These developmental models share the property that various proposed learning dynamics in the brain (e.g., a wave of nerve growth factor) support the self-organization somewhat analogous to the neural networks utilized in deep learning models. Like the neocortex, neural networks employ a hierarchy of layered filters in which each layer considers information from a prior layer (or the operating environment), and then passes its output (and possibly the original input), to other layers. This process yields a self-organizing stack of transducers, well-tuned to their operating environment. A 1995 description stated, ""...the infant's brain seems to organize itself under the influence of waves of so-called trophic-factors ... different regions of the brain become connected sequentially, with one layer of tissue maturing before another and so on until the whole brain is mature.""","Deep learning is closely related to a class of theories of brain development (specifically, neocortical development) proposed by cognitive neuroscientists in the early 1990s. These developmental theories were instantiated in computational models, making them predecessors of deep learning systems.","[' Deep learning is closely related to what class of theories of brain development?', ' Deep learning theories were instantiated in what?']","['neocortical development', 'computational models']"
59,deep learning,Relation to human cognitive and brain development,"A variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism. Other researchers have argued that unsupervised forms of deep learning, such as those based on hierarchical generative models and deep belief networks, may be closer to biological reality. In this respect, generative neural network models have been related to neurobiological evidence about sampling-based processing in the cerebral cortex.","A variety of approaches have been used to investigate the plausibility of deep learning models from a neurobiological perspective. On the one hand, several variants of the backpropagation algorithm have been proposed in order to increase its processing realism.","[' What has been used to investigate the plausibility of deep learning models from a neurobiological perspective?', ' What have several variants of the backpropagation algorithm been proposed to increase its processing realism?']","['A variety of approaches', 'deep learning models']"
60,deep learning,Relation to human cognitive and brain development,"Although a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported. For example, the computations performed by deep learning units could be similar to those of actual neurons and neural populations. Similarly, the representations developed by deep learning models are similar to those measured in the primate visual system both at the single-unit and at the population levels.
","Although a systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established, several analogies have been reported. For example, the computations performed by deep learning units could be similar to those of actual neurons and neural populations.","[' A systematic comparison between the human brain organization and the neuronal encoding in deep networks has not yet been established what?', ' The computations performed by deep learning units could be similar to those of actual neurons and neural populations?']","['several analogies have been reported', 'neuronal encoding in deep networks']"
61,deep learning,Commercial activity,"Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player. Google Translate uses a neural network to translate between more than 100 languages.
","Google's DeepMind Technologies developed a system capable of learning how to play Atari video games using only pixels as data input. In 2015 they demonstrated their AlphaGo system, which learned the game of Go well enough to beat a professional Go player.","[' What company developed a system capable of learning how to play Atari video games?', ' In what year did DeepMind Technologies demonstrate their AlphaGo system?']","['DeepMind Technologies', '2015']"
62,deep learning,Commercial activity,"As of 2008, researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers. Deep TAMER used deep learning to provide a robot the ability to learn new tasks through observation. Using Deep TAMER, a robot learned a task with a human trainer, watching video streams or observing a human perform a task in-person. The robot later practiced the task with the help of some coaching from the trainer, who provided feedback such as “good job” and “bad job.”","As of 2008, researchers at The University of Texas at Austin (UT) developed a machine learning framework called Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER, a new algorithm called Deep TAMER was later introduced in 2018 during a collaboration between U.S. Army Research Laboratory (ARL) and UT researchers.","[' What was the name of the machine learning framework developed by researchers at the University of Texas at Austin?', ' What is TAMER?', ' How did robots learn to perform tasks?', ' What is Deep TAMER?', ' What was the first algorithm to perform tasks by interacting with a human instructor?']","['Training an Agent Manually via Evaluative Reinforcement', 'Training an Agent Manually via Evaluative Reinforcement', 'by interacting with a human instructor', 'a new algorithm', 'Training an Agent Manually via Evaluative Reinforcement, or TAMER, which proposed new methods for robots or computer programs to learn how to perform tasks by interacting with a human instructor. First developed as TAMER']"
63,data mining,Summary,"Data mining is a process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal of extracting information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use. Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD. Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.","Data mining is a process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal of extracting information (with intelligent methods) from a data set and transform the information into a comprehensible structure for further use.","[' What is a process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems?', ' Data mining is an interdisciplinary subfield of what two disciplines?', ' What is the goal of extracting information from a data set and transform it into a comprehensible structure?']","['Data mining', 'computer science and statistics', 'Data mining']"
64,data mining,Summary,"The term ""data mining"" is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself. It also is a buzzword and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java (which covers mostly machine learning material) was originally to be named just Practical machine learning, and the term data mining was only added for marketing reasons. Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate.
","The term ""data mining"" is a misnomer, because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself. It also is a buzzword and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence.","[' What does the term ""data mining"" mean?', ' What is the goal of data mining?', ' Data mining is often applied to what?', ' What is a form of large-scale data processing?', ' What is an example of a computer decision support system?']","['extraction of patterns and knowledge from large amounts of data', 'extraction of patterns and knowledge from large amounts of data', 'any form of large-scale data or information processing', 'collection, extraction, warehousing, analysis, and statistics', 'artificial intelligence']"
65,data mining,Summary,"The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, but do belong to the overall KDD process as additional steps.
","The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices.","[' What is the actual data mining task?', ' What is cluster analysis?', ' How does cluster analysis extract previously unknown patterns?', ' What is a common technique used for pattern mining?']","['the semi-automatic or automatic analysis of large quantities of data', 'groups of data records', 'groups of data records', 'spatial indices']"
66,data mining,Summary,"The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.
","The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.","[' What are the terms data dredging, data fishing, and data snooping?', ' What are data mining methods used to sample parts of a larger population data set?', ' How can new hypotheses be used to test against larger data populations?']","['data mining methods', 'data dredging', 'data mining methods']"
67,data mining,Etymology,"In the 1960s, statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. The term ""data mining"" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies in 1983. Lovell indicates that the practice ""masquerades under a variety of aliases, ranging from ""experimentation"" (positive) to ""fishing"" or ""snooping"" (negative).
","In the 1960s, statisticians and economists used terms like data fishing or data dredging to refer to what they considered the bad practice of analyzing data without an a-priori hypothesis. The term ""data mining"" was used in a similarly critical way by economist Michael Lovell in an article published in the Review of Economic Studies in 1983.","[' When did statisticians and economists use terms like data fishing and data dredging to refer to bad practice of analyzing data without an a-priori hypothesis?', ' What term was used in a similar way by economist Michael Lovell in an article published in the 1960s?', "" In what year was Michael Lovell's article published?"", ' Who wrote the article?', ' What was the name of the economist who wrote it?']","['1960s', 'data mining', '1983', 'Michael Lovell', 'Michael Lovell']"
68,data mining,Etymology,"The term data mining appeared around 1990 in the database community, generally with positive connotations. For a short time in 1980s, a phrase ""database mining""™, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation; researchers consequently turned to data mining. Other terms used include data archaeology, information harvesting, information discovery, knowledge extraction, etc. Gregory Piatetsky-Shapiro coined the term ""knowledge discovery in databases"" for the first workshop on the same topic (KDD-1989) and this term became more popular in AI and machine learning community. However, the term data mining became more popular in the business and press communities. Currently, the terms data mining and knowledge discovery are used interchangeably.
","The term data mining appeared around 1990 in the database community, generally with positive connotations. For a short time in 1980s, a phrase ""database mining""™, was used, but since it was trademarked by HNC, a San Diego-based company, to pitch their Database Mining Workstation; researchers consequently turned to data mining.","[' When did the term data mining appear in the database community?', ' What phrase was used for a short time in the 1980s?', ' Who trademarked the phrase ""database mining""TM?']","['around 1990', 'database mining', 'HNC']"
69,data mining,Etymology,"In the academic community, the major forums for research started in 1995 when the First International Conference on Data Mining and Knowledge Discovery (KDD-95) was started in Montreal under AAAI sponsorship. It was co-chaired by Usama Fayyad and Ramasamy Uthurusamy. A year later, in 1996, Usama Fayyad launched the journal by Kluwer called Data Mining and Knowledge Discovery as its founding editor-in-chief. Later he started the SIGKDD Newsletter SIGKDD Explorations. The KDD International conference became the primary highest quality conference in data mining with an acceptance rate of research paper submissions below 18%. The journal Data Mining and Knowledge Discovery is the primary research journal of the field.
","In the academic community, the major forums for research started in 1995 when the First International Conference on Data Mining and Knowledge Discovery (KDD-95) was started in Montreal under AAAI sponsorship. It was co-chaired by Usama Fayyad and Ramasamy Uthurusamy.","[' When did the major forums for research start in the academic community?', ' Where was the First International Conference on Data Mining and Knowledge Discovery (KDD-95) started?', ' Who co-chaired the first international conference on data mining and knowledge discovery?']","['1995', 'Montreal', 'Usama Fayyad and Ramasamy Uthurusamy']"
70,data mining,Background,"The manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s). The proliferation, ubiquity and increasing power of computer technology have dramatically increased data collection, storage, and manipulation ability. As data sets have grown in size and complexity, direct ""hands-on"" data analysis has increasingly been augmented with indirect, automated data processing, aided by other discoveries in computer science, specially in the field of machine learning, such as neural networks, cluster analysis, genetic algorithms (1950s), decision trees and decision rules (1960s), and support vector machines (1990s). Data mining is the process of applying these methods with the intention of uncovering hidden patterns. in large data sets. It bridges the gap from applied statistics and artificial intelligence (which usually provide the mathematical background) to database management by exploiting the way data is stored and indexed in databases to execute the actual learning and discovery algorithms more efficiently, allowing such methods to be applied to ever-larger data sets.
",The manual extraction of patterns from data has occurred for centuries. Early methods of identifying patterns in data include Bayes' theorem (1700s) and regression analysis (1800s).,"[' How long has the manual extraction of patterns from data occurred?', ' What are two early methods of identifying patterns in data?', "" When was Bayes' theorem developed?""]","['centuries', ""Bayes' theorem (1700s) and regression analysis"", '1700s']"
71,data mining,Process,"Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners. The only other data mining standard named in these polls was SEMMA. However, 3–4 times as many people reported using CRISP-DM. Several teams of researchers have published reviews of data mining process models, and Azevedo and Santos conducted a comparison of CRISP-DM and SEMMA in 2008.","Polls conducted in 2002, 2004, 2007 and 2014 show that the CRISP-DM methodology is the leading methodology used by data miners. The only other data mining standard named in these polls was SEMMA.","[' What is the leading methodology used by data miners?', ' What was the only other data mining standard named in the polls?']","['CRISP-DM', 'SEMMA']"
72,data mining,Research,"The premier professional body in the field is the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD). Since 1989, this ACM SIG has hosted an annual international conference and published its proceedings, and since 1999 it has published a biannual academic journal titled ""SIGKDD Explorations"".","The premier professional body in the field is the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining (SIGKDD). Since 1989, this ACM SIG has hosted an annual international conference and published its proceedings, and since 1999 it has published a biannual academic journal titled ""SIGKDD Explorations"".","[' What is the acronym for the ACM Special Interest Group on Knowledge Discovery and Data Mining?', ' Since what year has the SIG hosted an annual international conference?', ' What does SIGKDD stand for?', ' What is the name of the biannual academic journal published by SIGKDD Explorations?']","['SIGKDD', '1989', 'Special Interest Group (SIG) on Knowledge Discovery and Data Mining', '1999']"
73,data mining,Standards,"There have been some efforts to define standards for the data mining process, for example, the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006 but has stalled since. JDM 2.0 was withdrawn without reaching a final draft.
","There have been some efforts to define standards for the data mining process, for example, the 1999 European Cross Industry Standard Process for Data Mining (CRISP-DM 1.0) and the 2004 Java Data Mining standard (JDM 1.0). Development on successors to these processes (CRISP-DM 2.0 and JDM 2.0) was active in 2006 but has stalled since.","[' What was the European Cross Industry Standard Process for Data Mining called in 1999?', ' What is the Java Data Mining standard called in 2004?', ' When were CRISP-DM 2.0 and JDM 2.0 developed?', ' When were CRISP-DM 2.0 and JDM 2.0 active?']","['CRISP-DM 1.0', 'JDM 1.0', '2006', '2006']"
74,data mining,Standards,"For exchanging the extracted models—in particular for use in predictive analytics—the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications. However, extensions to cover (for example) subspace clustering have been proposed independently of the DMG.","For exchanging the extracted models—in particular for use in predictive analytics—the key standard is the Predictive Model Markup Language (PMML), which is an XML-based language developed by the Data Mining Group (DMG) and supported as exchange format by many data mining applications. As the name suggests, it only covers prediction models, a particular data mining task of high importance to business applications.","[' What is the key standard for exchanging extracted models?', ' Who developed PMML?', ' What language is PMML an XML-based language?', ' What does mining applications only cover?', ' What is a particular data mining task of high importance to business applications?']","['Predictive Model Markup Language', 'Data Mining Group', 'Predictive Model Markup Language', 'prediction models', 'prediction models']"
75,data mining,Notable uses,"Data mining is used wherever there is digital data available today. Notable examples of data mining can be found throughout business, medicine, science, and surveillance.
","Data mining is used wherever there is digital data available today. Notable examples of data mining can be found throughout business, medicine, science, and surveillance.","[' What is used where there is digital data available today?', ' What are some notable examples of data mining?']","['Data mining', 'business, medicine, science, and surveillance']"
76,data mining,Privacy concerns and ethics,"The ways in which data mining can be used can in some cases and contexts raise questions regarding privacy, legality, and ethics. In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.","The ways in which data mining can be used can in some cases and contexts raise questions regarding privacy, legality, and ethics. In particular, data mining government or commercial data sets for national security or law enforcement purposes, such as in the Total Information Awareness Program or in ADVISE, has raised privacy concerns.","[' What are some aspects of data mining?', ' What is one example of a government or commercial data mining program?', ' What program has raised privacy concerns?', ' What is the name of the program that raises concerns about privacy?']","['privacy, legality, and ethics', 'Total Information Awareness Program', 'Total Information Awareness Program', 'Total Information Awareness Program']"
77,data mining,Privacy concerns and ethics,"Data mining requires data preparation which uncovers information or patterns which compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation. Data aggregation involves combining data together (possibly from various sources) in a way that facilitates analysis (but that also might make identification of private, individual-level data deducible or otherwise apparent). This is not data mining per se, but a result of the preparation of data before—and for the purposes of—the analysis. The threat to an individual's privacy comes into play when the data, once compiled, cause the data miner, or anyone who has access to the newly compiled data set, to be able to identify specific individuals, especially when the data were originally anonymous.",Data mining requires data preparation which uncovers information or patterns which compromise confidentiality and privacy obligations. A common way for this to occur is through data aggregation.,"[' What is required to uncover information or patterns that compromise confidentiality and privacy obligations?', ' What is a common way for data mining to occur?']","['data preparation', 'data aggregation']"
78,data mining,Privacy concerns and ethics,"Data may also be modified so as to become anonymous, so that individuals may not readily be identified. However, even ""anonymized"" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.","Data may also be modified so as to become anonymous, so that individuals may not readily be identified. However, even ""anonymized"" data sets can potentially contain enough information to allow identification of individuals, as occurred when journalists were able to find several individuals based on a set of search histories that were inadvertently released by AOL.","[' What can be modified so as to become anonymous?', ' What can potentially contain enough information to allow identification of individuals?', ' How did journalists find several individuals based on a set of search histories?', ' AOL released a set of search histories inadvertently by what company?']","['Data', 'anonymized"" data sets', 'inadvertently released by AOL', 'AOL']"
79,data mining,Privacy concerns and ethics,"The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices.   This indiscretion can cause financial,
emotional, or bodily harm to the indicated individual.  In one instance of privacy violation, the patrons of Walgreens filed a lawsuit against the company in 2011 for selling
prescription information to data mining companies who in turn provided the data
to pharmaceutical companies.","The inadvertent revelation of personally identifiable information leading to the provider violates Fair Information Practices. This indiscretion can cause financial,
emotional, or bodily harm to the indicated individual.","[' What is a violation of Fair Information Practices?', ' What can the disclosure of personal information lead to?']","['The inadvertent revelation of personally identifiable information', 'the provider']"
80,cloud computing,Summary,"Cloud computing is the on-demand availability of computer system resources, especially data storage (cloud storage) and computing power, without direct active management by the user. Large clouds often have functions distributed over multiple locations, each location being a data center. Cloud computing relies on sharing of resources to achieve coherence and economies of scale, typically using a ""pay-as-you-go"" model which can help in reducing capital expenses but may also lead to unexpected operating expenses for unaware users.","Cloud computing is the on-demand availability of computer system resources, especially data storage (cloud storage) and computing power, without direct active management by the user. Large clouds often have functions distributed over multiple locations, each location being a data center.","[' What is the term for the availability of computer system resources without direct active management by the user?', ' Large clouds often have functions distributed over multiple locations, each location being what?']","['Cloud computing', 'a data center']"
81,cloud computing,Value proposition,"Advocates of public and hybrid clouds note that cloud computing allows companies to avoid or minimize up-front IT infrastructure costs. Proponents also claim that cloud computing allows enterprises to get their applications up and running faster, with improved manageability and less maintenance, and that it enables IT teams to more rapidly adjust resources to meet fluctuating and unpredictable demand, providing the burst computing capability: high computing power at certain periods of peak demand.","Advocates of public and hybrid clouds note that cloud computing allows companies to avoid or minimize up-front IT infrastructure costs. Proponents also claim that cloud computing allows enterprises to get their applications up and running faster, with improved manageability and less maintenance, and that it enables IT teams to more rapidly adjust resources to meet fluctuating and unpredictable demand, providing the burst computing capability: high computing power at certain periods of peak demand.","[' What do advocates of public and hybrid clouds note that cloud computing allows companies to avoid or minimize up-front IT infrastructure costs?', ' Cloud computing allows enterprises to get their applications up and running faster, with improved manageability and less what?', ' What does less maintenance allow IT teams to do?', ' What is the burst computing capability?']","['cloud computing allows enterprises to get their applications up and running faster, with improved manageability and less maintenance', 'maintenance', 'more rapidly adjust resources to meet fluctuating and unpredictable demand', 'high computing power at certain periods of peak demand']"
82,cloud computing,History,"The cloud symbol was used to represent networks of computing equipment in the original ARPANET by as early as 1977, and the CSNET by 1981—both predecessors to the Internet itself. The word cloud was used as a metaphor for the Internet and a standardized cloud-like shape was used to denote a network on telephony schematics. With this simplification, the implication is that the specifics of how the endpoints of a network are connected are not relevant to understanding the diagram.","The cloud symbol was used to represent networks of computing equipment in the original ARPANET by as early as 1977, and the CSNET by 1981—both predecessors to the Internet itself. The word cloud was used as a metaphor for the Internet and a standardized cloud-like shape was used to denote a network on telephony schematics.","[' What was used to represent networks of computing equipment in the original ARPANET?', ' When was the CSNET created?', ' What word was used as a metaphor for the Internet?', ' What was used to denote a network on telephony schematics?', ' Internet and a standardized cloud-like shape were used on what?']","['The cloud symbol', '1981', 'cloud', 'a standardized cloud-like shape', 'telephony schematics']"
83,cloud computing,History,"The term cloud was used to refer to platforms for distributed computing as early as 1993, when Apple spin-off General Magic and AT&T used it in describing their (paired) Telescript and PersonaLink technologies.  In Wired's April 1994 feature ""Bill and Andy's Excellent Adventure II"", Andy Hertzfeld commented on Telescript, General Magic's distributed programming language:
","The term cloud was used to refer to platforms for distributed computing as early as 1993, when Apple spin-off General Magic and AT&T used it in describing their (paired) Telescript and PersonaLink technologies. In Wired's April 1994 feature ""Bill and Andy's Excellent Adventure II"", Andy Hertzfeld commented on Telescript, General Magic's distributed programming language:","[' When was the term cloud first used to refer to platforms for distributed computing?', "" What was the name of AT&T's (paired) Telescript and PersonaLink technologies?"", "" Who commented on Telescript, General Magic, and General Magic in Wired's April 1994 feature?"", ' In what year was ""Bill and Andy\'s Excellent Adventure II"" published?', "" Andy Hertzfeld commented on Telescript, General Magic's distributed programming language.""]","['1993', 'General Magic', 'Andy Hertzfeld', '1994', 'Wired\'s April 1994 feature ""Bill and Andy\'s Excellent Adventure II']"
84,cloud computing,History,"""The beauty of Telescript ... is that now, instead of just having a device to program, we now have the entire Cloud out there, where a single program can go and travel to many different sources of information and create a sort of a virtual service. No one had conceived that before. The example Jim White [the designer of Telescript, X.400 and ASN.1] uses now is a date-arranging service where a software agent goes to the flower store and orders flowers and then goes to the ticket shop and gets the tickets for the show, and everything is communicated to both parties.""","""The beauty of Telescript ... is that now, instead of just having a device to program, we now have the entire Cloud out there, where a single program can go and travel to many different sources of information and create a sort of a virtual service. No one had conceived that before.","[' What is the beauty of Telescript?', ' What can a single program go and travel to many different sources of information and create a sort of virtual service?', ' What was the name of the virtual service?', ' Who had never conceived a virtual service before?']","['we now have the entire Cloud out there', 'the entire Cloud', 'Telescript', 'No one']"
85,cloud computing,Similar concepts,"The goal of cloud computing is to allow users to take benefit from all of these technologies, without the need for deep knowledge about or expertise with each one of them. The cloud aims to cut costs and helps the users focus on their core business instead of being impeded by IT obstacles. The main enabling technology for cloud computing is virtualization. Virtualization software separates a physical computing device into one or more ""virtual"" devices, each of which can be easily used and managed to perform computing tasks. With operating system–level virtualization essentially creating a scalable system of multiple independent computing devices, idle computing resources can be allocated and used more efficiently. Virtualization provides the agility required to speed up IT operations and reduces cost by increasing infrastructure utilization. Autonomic computing automates the process through which the user can provision resources on-demand. By minimizing user involvement, automation speeds up the process, reduces labor costs and reduces the possibility of human errors.","The goal of cloud computing is to allow users to take benefit from all of these technologies, without the need for deep knowledge about or expertise with each one of them. The cloud aims to cut costs and helps the users focus on their core business instead of being impeded by IT obstacles.","[' What is the goal of cloud computing?', ' What does the cloud aim to cut costs and help users focus on?', ' What do users focus on instead of being impeded by IT obstacles?']","['to allow users to take benefit from all of these technologies', 'their core business', 'their core business']"
86,cloud computing,Similar concepts,Cloud computing uses concepts from utility computing to provide metrics for the services used. Cloud computing attempts to address QoS (quality of service) and reliability problems of other grid computing models.,Cloud computing uses concepts from utility computing to provide metrics for the services used. Cloud computing attempts to address QoS (quality of service) and reliability problems of other grid computing models.,"[' What does cloud computing use concepts from?', ' What does QoS stand for?', ' Cloud computing attempts to address what problems of other grid computing models?']","['utility computing', 'quality of service', 'QoS (quality of service) and reliability']"
87,cloud computing,Characteristics,"On-demand self-service. A consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with each service provider.
","On-demand self-service. A consumer can unilaterally provision computing capabilities, such as server time and network storage, as needed automatically without requiring human interaction with each service provider.","[' What is on-demand self-service?', ' A consumer can provision computing capabilities as needed automatically without human interaction with each service provider?']","['A consumer can unilaterally provision computing capabilities', 'On-demand self-service']"
88,cloud computing,Characteristics,"Broad network access. Capabilities are available over the network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms (e.g., mobile phones, tablets, laptops, and workstations).
","Broad network access. Capabilities are available over the network and accessed through standard mechanisms that promote use by heterogeneous thin or thick client platforms (e.g., mobile phones, tablets, laptops, and workstations).","[' What type of access is available over the network?', ' What are some examples of thin or thick client platforms?']","['Broad network access. Capabilities', 'mobile phones, tablets, laptops, and workstations']"
89,cloud computing,Characteristics,"Resource pooling. The provider's computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned according to consumer demand. 
","Resource pooling. The provider's computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned according to consumer demand.","[' What is resource pooling?', ' What is a multi-tenant model?']","[""The provider's computing resources are pooled to serve multiple consumers using a multi-tenant model"", ""Resource pooling. The provider's computing resources are pooled to serve multiple consumers using a multi-tenant model, with different physical and virtual resources dynamically assigned and reassigned according to consumer demand""]"
90,cloud computing,Characteristics,"Rapid elasticity. Capabilities can be elastically provisioned and released, in some cases automatically, to scale rapidly outward and inward commensurate with demand. To the consumer, the capabilities available for provisioning often appear unlimited and can be appropriated in any quantity at any time.
","Rapid elasticity. Capabilities can be elastically provisioned and released, in some cases automatically, to scale rapidly outward and inward commensurate with demand.",[' What can be elastically provisioned and released to scale rapidly outward and inward commensurate with demand?'],['Rapid elasticity. Capabilities']
91,cloud computing,Characteristics,"Measured service. Cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction appropriate to the type of service (e.g., storage, processing, bandwidth, and active user accounts). Resource usage can be monitored, controlled, and reported, providing transparency for both the provider and consumer of the utilized service.","Measured service. Cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of abstraction appropriate to the type of service (e.g., storage, processing, bandwidth, and active user accounts).",[' Cloud systems automatically control and optimize resource use by leveraging a metering capability at some level of what?'],['abstraction']
92,cloud computing,Service models,"Though service-oriented architecture advocates ""Everything as a service"" (with the acronyms EaaS or XaaS, or simply aas), cloud-computing providers offer their ""services"" according to different models, of which the three standard models per NIST are Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS). These models offer increasing abstraction; they are thus often portrayed as layers in a stack: infrastructure-, platform- and software-as-a-service, but these need not be related. For example, one can provide SaaS implemented on physical machines (bare metal), without using underlying PaaS or IaaS layers, and conversely one can run a program on IaaS and access it directly, without wrapping it as SaaS.
","Though service-oriented architecture advocates ""Everything as a service"" (with the acronyms EaaS or XaaS, or simply aas), cloud-computing providers offer their ""services"" according to different models, of which the three standard models per NIST are Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS). These models offer increasing abstraction; they are thus often portrayed as layers in a stack: infrastructure-, platform- and software-as-a-service, but these need not be related.","[' What does service-oriented architecture advocate?', ' What are the three standard models per NIST?', ' What are PaaS and SaaS models?', ' What offer increasing abstraction?', ' Infrastructure-, platform- and software-as-a-service are often portrayed as what?']","['Everything as a service', 'Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS).', 'Platform as a Service', 'cloud-computing providers offer their ""services"" according to different models, of which the three standard models per NIST are Infrastructure as a Service (IaaS), Platform as a Service (PaaS), and Software as a Service (SaaS).', 'layers in a stack']"
93,cloud computing,Architecture,"Cloud architecture, the systems architecture of the software systems involved in the delivery of cloud computing, typically involves multiple cloud components communicating with each other over a loose coupling mechanism such as a messaging queue. Elastic provision implies intelligence in the use of tight or loose coupling as applied to mechanisms such as these and others.
","Cloud architecture, the systems architecture of the software systems involved in the delivery of cloud computing, typically involves multiple cloud components communicating with each other over a loose coupling mechanism such as a messaging queue. Elastic provision implies intelligence in the use of tight or loose coupling as applied to mechanisms such as these and others.","[' What is the system architecture of the software systems involved in the delivery of cloud computing?', ' Cloud architecture typically involves multiple cloud components communicating with each other over what mechanism?', ' What implies intelligence in the use of tight or loose coupling?', ' The use of tight or loose coupling as applied to mechanisms such as these and others?']","['Cloud architecture', 'loose coupling', 'Elastic provision', 'Elastic provision implies intelligence']"
94,cloud computing,Security and privacy,"Cloud computing poses privacy concerns because the service provider can access the data that is in the cloud at any time.  It could accidentally or deliberately alter or delete information. Many cloud providers can share information with third parties if necessary for purposes of law and order without a warrant. That is permitted in their privacy policies, which users must agree to before they start using cloud services. Solutions to privacy include policy and legislation as well as end-users' choices for how data is stored. Users can encrypt data that is processed or stored within the cloud to prevent unauthorized access. Identity management systems can also provide practical solutions to privacy concerns in cloud computing. These systems distinguish between authorized and unauthorized users and determine the amount of data that is accessible to each entity. The systems work by creating and describing identities, recording activities, and getting rid of unused identities.
",Cloud computing poses privacy concerns because the service provider can access the data that is in the cloud at any time. It could accidentally or deliberately alter or delete information.,"[' Cloud computing poses privacy concerns because who can access the data that is in the cloud at any time?', ' What could accidentally or deliberately alter or delete information?']","['the service provider', 'Cloud computing']"
95,cloud computing,Security and privacy,"According to the Cloud Security Alliance, the top three threats in the cloud are Insecure Interfaces and APIs, Data Loss & Leakage, and Hardware Failure—which accounted for 29%, 25% and 10% of all cloud security outages respectively. Together, these form shared technology vulnerabilities. In a cloud provider platform being shared by different users, there may be a possibility that information belonging to different customers resides on the same data server. Additionally, Eugene Schultz, chief technology officer at Emagined Security, said that hackers are spending substantial time and effort looking for ways to penetrate the cloud. ""There are some real Achilles' heels in the cloud infrastructure that are making big holes for the bad guys to get into"". Because data from hundreds or thousands of companies can be stored on large cloud servers, hackers can theoretically gain control of huge stores of information through a single attack—a process he called ""hyperjacking"". Some examples of this include the Dropbox security breach, and iCloud 2014 leak. Dropbox had been breached in October 2014, having over 7 million of its users passwords stolen by hackers in an effort to get monetary value from it by Bitcoins (BTC). By having these passwords, they are able to read private data as well as have this data be indexed by search engines (making the information public).","According to the Cloud Security Alliance, the top three threats in the cloud are Insecure Interfaces and APIs, Data Loss & Leakage, and Hardware Failure—which accounted for 29%, 25% and 10% of all cloud security outages respectively. Together, these form shared technology vulnerabilities.","[' According to the Cloud Security Alliance, what are the top three threats in the cloud?', ' Insecure Interfaces and APIs, Data Loss & Leakage, and Hardware Failure were the top threats in what?', ' Hardware Failure accounted for what percentage of all cloud security outages?']","['Insecure Interfaces and APIs, Data Loss & Leakage, and Hardware Failure', 'Cloud Security Alliance, the top three threats in the cloud', '10%']"
96,cloud computing,Security and privacy,"There is the problem of legal ownership of the data (If a user stores some data in the cloud, can the cloud provider profit from it?). Many Terms of Service agreements are silent on the question of ownership. Physical control of the computer equipment (private cloud) is more secure than having the equipment off-site and under someone else's control (public cloud). This delivers great incentive to public cloud computing service providers to prioritize building and maintaining strong management of secure services. Some small businesses that don't have expertise in IT security could find that it's more secure for them to use a public cloud. There is the risk that end users do not understand the issues involved when signing on to a cloud service (persons sometimes don't read the many pages of the terms of service agreement, and just click ""Accept"" without reading). This is important now that cloud computing is becoming popular and required for some services to work, for example for an intelligent personal assistant (Apple's Siri or Google Now). Fundamentally, private cloud is seen as more secure with higher levels of control for the owner, however public cloud is seen to be more flexible and requires less time and money investment from the user.","There is the problem of legal ownership of the data (If a user stores some data in the cloud, can the cloud provider profit from it?). Many Terms of Service agreements are silent on the question of ownership.","[' What is the problem with storing data in the cloud?', ' What are many Terms of Service agreements silent on?']","['legal ownership', 'the question of ownership']"
97,cloud computing,Limitations and disadvantages,"According to Bruce Schneier, ""The downside is that you will have limited customization options. Cloud computing is cheaper because of economics of scale, and—like any outsourced task—you tend to get what you want. A restaurant with a limited menu is cheaper than a personal chef who can cook anything you want. Fewer options at a much cheaper price: it's a feature, not a bug."" He also suggests that ""the cloud provider might not meet your legal needs"" and that businesses need to weigh the benefits of cloud computing against the risks.
In cloud computing, the control of the back end infrastructure is limited to the cloud vendor only. Cloud providers often decide on the management policies, which moderates what the cloud users are able to do with their deployment. Cloud users are also limited to the control and management of their applications, data and services. This includes data caps, which are placed on cloud users by the cloud vendor allocating a certain amount of bandwidth for each customer and are often shared among other cloud users.","According to Bruce Schneier, ""The downside is that you will have limited customization options. Cloud computing is cheaper because of economics of scale, and—like any outsourced task—you tend to get what you want.","[' According to Bruce Schneier, what is the downside of cloud computing?', ' What is cheaper because of economics of scale?']","['you will have limited customization options', 'Cloud computing']"
98,cloud computing,Limitations and disadvantages,"Privacy and confidentiality are big concerns in some activities. For instance, sworn translators working under the stipulations of an NDA, might face problems regarding sensitive data that are not encrypted. Due to the use of the internet, confidential information such as employee data and user data can be easily available to third-party organisations and people in Cloud Computing.","Privacy and confidentiality are big concerns in some activities. For instance, sworn translators working under the stipulations of an NDA, might face problems regarding sensitive data that are not encrypted.","[' What are big concerns in some activities?', ' Sworn translators working under the stipulations of an NDA might face problems with what?']","['Privacy and confidentiality', 'sensitive data that are not encrypted']"
99,cloud computing,Limitations and disadvantages,"Cloud computing is beneficial to many enterprises; it lowers costs and allows them to focus on competence instead of on matters of IT and infrastructure. Nevertheless, cloud computing has proven to have some limitations and disadvantages, especially for smaller business operations, particularly regarding security and downtime. Technical outages are inevitable and occur sometimes when cloud service providers (CSPs) become overwhelmed in the process of serving their clients. This may result in temporary business suspension. Since this technology's systems rely on the Internet, an individual cannot access their applications, server, or data from the cloud during an outage.","Cloud computing is beneficial to many enterprises; it lowers costs and allows them to focus on competence instead of on matters of IT and infrastructure. Nevertheless, cloud computing has proven to have some limitations and disadvantages, especially for smaller business operations, particularly regarding security and downtime.","[' What is cloud computing beneficial to?', ' What does cloud computing lower costs and allow enterprises to focus on instead of IT and infrastructure?']","['many enterprises', 'competence']"
100,cloud computing,Emerging trends,"Cloud computing is still a subject of research. A driving factor in the evolution of cloud computing has been chief technology officers seeking to minimize risk of internal outages and mitigate the complexity of housing network and computing hardware in-house. They are also looking to share information to workers located in diverse areas in near and real-time, to enable teams to work seamlessly, no matter where they are located. Since the global pandemic of 2020, it is said that cloud technology jumped ahead in popularity due to the level of security of data and the flexibility of working options for all employees, notably remote workers. For example, Zoom grew over 160% in 2020 alone.",Cloud computing is still a subject of research. A driving factor in the evolution of cloud computing has been chief technology officers seeking to minimize risk of internal outages and mitigate the complexity of housing network and computing hardware in-house.,"[' What is still a subject of research?', ' What has been a driving factor in the evolution of cloud computing?', ' Chief technology officers sought to minimize what?']","['Cloud computing', 'chief technology officers', 'risk of internal outages']"
101,cloud computing,Digital forensics in the cloud,The issue of carrying out investigations where the cloud storage devices cannot be physically accessed has generated a number of changes to the way that digital evidence is located and collected. New process models have been developed to formalize collection.,The issue of carrying out investigations where the cloud storage devices cannot be physically accessed has generated a number of changes to the way that digital evidence is located and collected. New process models have been developed to formalize collection.,"[' What has caused a number of changes to the way that digital evidence is located and collected?', ' New process models have been developed to formalize what?']","['The issue of carrying out investigations where the cloud storage devices cannot be physically accessed', 'collection']"
102,support vector machine,Summary,"In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995,  Vapnik et al., 1997) SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.
","In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995,  Vapnik et al., 1997) SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974).","[' What are support-vector machines also known as?', ' What are SVMs?', ' Where were SVM developed?', ' What is one of the most robust prediction methods?', ' What is based on statistical learning frameworks or VC theory?']","['SVMs', 'support-vector machines', 'AT&T Bell Laboratories', 'SVMs', 'SVMs']"
103,support vector machine,Summary,"When data are unlabelled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The support-vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data.","When data are unlabelled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The support-vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data.","[' When data are unlabelled, what is not possible?', ' What is required when data are not labelled?', ' Who created the support-vector clustering algorithm?', ' Who created the support vector machines algorithm?', ' What is the name of the algorithm that uses support vectors to categorize unlabeled data?']","['supervised learning', 'unsupervised learning approach', 'Hava Siegelmann and Vladimir Vapnik', 'Hava Siegelmann and Vladimir Vapnik', 'support-vector clustering algorithm']"
104,support vector machine,Motivation,"Classifying data is a common task in machine learning.
Suppose some given data points each belong to one of two classes, and the goal is to decide which class a new data point will be in. In the case of support-vector machines, a data point is viewed as a 



p


{\displaystyle p}
-dimensional vector (a list of 



p


{\displaystyle p}
 numbers), and we want to know whether we can separate such points with a 



(
p
−
1
)


{\displaystyle (p-1)}
-dimensional hyperplane. This is called a linear classifier. There are many hyperplanes that might classify the data. One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two classes. So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it defines is known as a maximum-margin classifier; or equivalently, the perceptron of optimal stability.","Classifying data is a common task in machine learning. Suppose some given data points each belong to one of two classes, and the goal is to decide which class a new data point will be in.","[' What is a common task in machine learning?', ' What is the goal of classifying data?']","['Classifying data', 'to decide which class']"
105,support vector machine,Definition,"More formally, a support-vector machine constructs a hyperplane or set of hyperplanes in a high- or infinite-dimensional space, which can be used for classification, regression, or other tasks like outliers detection. Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class (so-called functional margin), since in general the larger the margin, the lower the generalization error of the classifier.","More formally, a support-vector machine constructs a hyperplane or set of hyperplanes in a high- or infinite-dimensional space, which can be used for classification, regression, or other tasks like outliers detection. Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class (so-called functional margin), since in general the larger the margin, the lower the generalization error of the classifier.","[' What does a support-vector machine construct in a high- or infinite-dimensional space?', ' What can be used for classification, regression, or other tasks like outliers detection?', ' What is the largest distance to the nearest training-data point of any class?', ' The larger the margin, the lower the generalization error of the classifier?']","['a hyperplane or set of hyperplanes', 'a support-vector machine constructs a hyperplane or set of hyperplanes in a high- or infinite-dimensional space', 'functional margin', 'larger']"
106,support vector machine,Definition,"Whereas the original problem may be stated in a finite-dimensional space, it often happens that the sets to discriminate are not linearly separable in that space. For this reason, it was proposed that the original finite-dimensional space be mapped into a much higher-dimensional space, presumably making the separation easier in that space. To keep the computational load reasonable, the mappings used by SVM schemes are designed to ensure that dot products of pairs of input data vectors may be computed easily in terms of the variables in the original space, by defining them in terms of a kernel function 



k
(
x
,
y
)


{\displaystyle k(x,y)}
 selected to suit the problem. The hyperplanes in the higher-dimensional space are defined as the set of points whose dot product with a vector in that space is constant, where such a set of vectors is an orthogonal (and thus minimal) set of vectors that defines a hyperplane. The vectors defining the hyperplanes can be chosen to be linear combinations with parameters 




α

i




{\displaystyle \alpha _{i}}
 of images of feature vectors 




x

i




{\displaystyle x_{i}}
 that occur in the data base. With this choice of a hyperplane, the points 



x


{\displaystyle x}
 in the feature space that are mapped into the hyperplane are defined by the relation 





∑

i



α

i


k
(

x

i


,
x
)
=

constant

.



{\displaystyle \textstyle \sum _{i}\alpha _{i}k(x_{i},x)={\text{constant}}.}
  Note that if 



k
(
x
,
y
)


{\displaystyle k(x,y)}
 becomes small as 



y


{\displaystyle y}
 grows further away from 



x


{\displaystyle x}
, each term in the sum measures the degree of closeness of the test point 



x


{\displaystyle x}
 to the corresponding data base point 




x

i




{\displaystyle x_{i}}
. In this way, the sum of kernels above can be used to measure the relative nearness of each test point to the data points originating in one or the other of the sets to be discriminated. Note the fact that the set of points 



x


{\displaystyle x}
 mapped into any hyperplane can be quite convoluted as a result, allowing much more complex discrimination between sets that are not convex at all in the original space.
","Whereas the original problem may be stated in a finite-dimensional space, it often happens that the sets to discriminate are not linearly separable in that space. For this reason, it was proposed that the original finite-dimensional space be mapped into a much higher-dimensional space, presumably making the separation easier in that space.","[' Where is the original problem stated?', ' Why was it proposed that the original finite-dimensional space be mapped into a much higher dimensional space?', ' What does a much higher-dimensional space make separation easier in?']","['finite-dimensional space', 'making the separation easier in that space', 'finite-dimensional space, it often happens that the sets to discriminate are not linearly separable in that space. For this reason, it was proposed that the original finite-dimensional space be mapped into a much higher-dimensional space, presumably making the separation easier in that space']"
107,support vector machine,History,"The original SVM algorithm was invented by Vladimir N. Vapnik and Alexey Ya. Chervonenkis in 1963. In 1992, Bernhard Boser, Isabelle Guyon and Vladimir Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick to maximum-margin hyperplanes. The ""soft margin"" incarnation, as is commonly used software packages, was proposed by Corinna Cortes and Vapnik in 1993 and published in 1995.",The original SVM algorithm was invented by Vladimir N. Vapnik and Alexey Ya. Chervonenkis in 1963.,"[' Who invented the original SVM algorithm?', ' In what year was the original algorithm invented?']","['Vladimir N. Vapnik and Alexey Ya. Chervonenkis', '1963']"
108,support vector machine,Linear SVM,"where the 




y

i




{\displaystyle y_{i}}
 are either 1 or −1, each indicating the class to which the point 





x


i




{\displaystyle \mathbf {x} _{i}}
 belongs. Each 





x


i




{\displaystyle \mathbf {x} _{i}}
 is a 



p


{\displaystyle p}
-dimensional real vector. We want to find the ""maximum-margin hyperplane"" that divides the group of points 





x


i




{\displaystyle \mathbf {x} _{i}}
 for which 




y

i


=
1


{\displaystyle y_{i}=1}
 from the group of points for which 




y

i


=
−
1


{\displaystyle y_{i}=-1}
, which is defined so that the distance between the hyperplane and the nearest point 





x


i




{\displaystyle \mathbf {x} _{i}}
 from either group is maximized.
","where the 




y

i




{\displaystyle y_{i}}
 are either 1 or −1, each indicating the class to which the point 





x


i




{\displaystyle \mathbf {x} _{i}}
 belongs. Each 





x


i




{\displaystyle \mathbf {x} _{i}}
 is a 



p


{\displaystyle p}
-dimensional real vector.",[' What is a p <unk>displaystyle p<unk> -dimensional real vector?'],['x\n\n\ni']
109,support vector machine,Linear SVM,"where 




w



{\displaystyle \mathbf {w} }
 is the (not necessarily normalized) normal vector to the hyperplane. This is much like Hesse normal form, except that 




w



{\displaystyle \mathbf {w} }
 is not necessarily a unit vector. The parameter 






b

‖

w

‖






{\displaystyle {\tfrac {b}{\|\mathbf {w} \|}}}
 determines the offset of the hyperplane from the origin along the normal vector 




w



{\displaystyle \mathbf {w} }
.
","where 




w



{\displaystyle \mathbf {w} }
 is the (not necessarily normalized) normal vector to the hyperplane. This is much like Hesse normal form, except that 




w



{\displaystyle \mathbf {w} }
 is not necessarily a unit vector.","[' Where w <unk>displaystyle <unk>mathbf <unk>w<unk> <unk> is the (not necessarily normalized) normal vector to the hyperplane?', ' How is this similar to Hesse normal form?', ' What is not necessarily a unit vector?']","['w', 'w\n\n\n\n{\\displaystyle \\mathbf {w} }\n is the (not necessarily normalized) normal vector to the hyperplane', 'w\n\n\n\n{\\displaystyle \\mathbf {w} }\n is the (not necessarily normalized) normal vector to the hyperplane']"
110,support vector machine,Nonlinear Kernels,"The original maximum-margin hyperplane algorithm proposed by Vapnik in 1963 constructed a linear classifier. However, in 1992, Bernhard Boser, Isabelle Guyon and Vladimir Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick (originally proposed by Aizerman et al.) to maximum-margin hyperplanes. The resulting algorithm is formally similar, except that every dot product is replaced by a nonlinear kernel function. This allows the algorithm to fit the maximum-margin hyperplane in a transformed feature space. The transformation may be nonlinear and the transformed space high-dimensional; although the classifier is a hyperplane in the transformed feature space, it may be nonlinear in the original input space.
","The original maximum-margin hyperplane algorithm proposed by Vapnik in 1963 constructed a linear classifier. However, in 1992, Bernhard Boser, Isabelle Guyon and Vladimir Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick (originally proposed by Aizerman et al.)","[' In what year was the original maximum-margin hyperplane algorithm proposed by Vapnik?', ' Who proposed a way to create nonlinear classifiers by applying the kernel trick?']","['1963', 'Bernhard Boser, Isabelle Guyon and Vladimir Vapnik']"
111,support vector machine,Nonlinear Kernels,"The kernel is related to the transform 



φ
(




x

i


→



)


{\displaystyle \varphi ({\vec {x_{i}}})}
 by the equation 



k
(




x

i


→



,




x

j


→



)
=
φ
(




x

i


→



)
⋅
φ
(




x

j


→



)


{\displaystyle k({\vec {x_{i}}},{\vec {x_{j}}})=\varphi ({\vec {x_{i}}})\cdot \varphi ({\vec {x_{j}}})}
. The value w is also in the transformed space, with 







w
→



=

∑

i



α

i



y

i


φ
(




x
→




i


)



{\displaystyle \textstyle {\vec {w}}=\sum _{i}\alpha _{i}y_{i}\varphi ({\vec {x}}_{i})}
. Dot products with w for classification can again be computed by the kernel trick, i.e. 







w
→



⋅
φ
(



x
→



)
=

∑

i



α

i



y

i


k
(




x
→




i


,



x
→



)



{\displaystyle \textstyle {\vec {w}}\cdot \varphi ({\vec {x}})=\sum _{i}\alpha _{i}y_{i}k({\vec {x}}_{i},{\vec {x}})}
.
","The kernel is related to the transform 



φ
(




x

i


→



)


{\displaystyle \varphi ({\vec {x_{i}}})}
 by the equation 



k
(




x

i


→



,




x

j


→



)
=
φ
(




x

i


→



)
⋅
φ
(




x

j


→



)


{\displaystyle k({\vec {x_{i}}},{\vec {x_{j}}})=\varphi ({\vec {x_{i}}})\cdot \varphi ({\vec {x_{j}}})}
. The value w is also in the transformed space, with 







w
→



=

∑

i



α

i



y

i


φ
(




x
→




i


)



{\displaystyle \textstyle {\vec {w}}=\sum _{i}\alpha _{i}y_{i}\varphi ({\vec {x}}_{i})}
.","[' The kernel is related to the transform what?', ' The value w is also in the transformed space?']","['φ\n(\n\n\n\n\nx\n\ni\n\n\n→\n\n\n\n)\n⋅\nφ\n(\n\n\n\n\nx\n\nj\n\n\n→', 'w']"
112,support vector machine,Computing the SVM classifier,"We focus on the soft-margin classifier since, as noted above, choosing a sufficiently small value for 



λ


{\displaystyle \lambda }
 yields the hard-margin classifier for linearly classifiable input data. The classical approach, which involves reducing (2) to a quadratic programming problem, is detailed below. Then, more recent approaches such as sub-gradient descent and coordinate descent will be discussed.
","We focus on the soft-margin classifier since, as noted above, choosing a sufficiently small value for 



λ


{\displaystyle \lambda }
 yields the hard-margin classifier for linearly classifiable input data. The classical approach, which involves reducing (2) to a quadratic programming problem, is detailed below.","[' What is the hard-margin classifier for linearly classifiable input data?', ' What approach involves reducing (2) to a quadratic programming problem?']","['soft-margin classifier', 'classical']"
113,support vector machine,Empirical risk minimization,"The soft-margin support vector machine described above is an example of an empirical risk minimization (ERM) algorithm for the hinge loss. Seen this way, support vector machines belong to a natural class of algorithms for statistical inference, and many of its unique features are due to the behavior of the hinge loss. This perspective can provide further insight into how and why SVMs work, and allow us to better analyze their statistical properties.
","The soft-margin support vector machine described above is an example of an empirical risk minimization (ERM) algorithm for the hinge loss. Seen this way, support vector machines belong to a natural class of algorithms for statistical inference, and many of its unique features are due to the behavior of the hinge loss.","[' What is an example of an empirical risk minimization (ERM) algorithm for the hinge loss?', ' Support vector machines belong to what class of algorithms for statistical inference?', ' The unique features of the hinge are due to the behavior of what?']","['soft-margin support vector machine', 'natural', 'loss']"
114,support vector machine,Properties,"SVMs belong to a family of generalized linear classifiers and can be interpreted as an extension of the perceptron. They can also be considered a special case of Tikhonov regularization. A special property is that they simultaneously minimize the empirical classification error and maximize the geometric margin; hence they are also known as maximum margin classifiers.
",SVMs belong to a family of generalized linear classifiers and can be interpreted as an extension of the perceptron. They can also be considered a special case of Tikhonov regularization.,"[' What type of classifiers do SVMs belong to?', ' What can be interpreted as an extension of the perceptron?']","['generalized linear classifiers', 'SVMs']"
115,support vector machine,Implementation,"The parameters of the maximum-margin hyperplane are derived by solving the optimization. There exist several specialized algorithms for quickly solving the quadratic programming (QP) problem that arises from SVMs, mostly relying on heuristics for breaking the problem down into smaller, more manageable chunks.
","The parameters of the maximum-margin hyperplane are derived by solving the optimization. There exist several specialized algorithms for quickly solving the quadratic programming (QP) problem that arises from SVMs, mostly relying on heuristics for breaking the problem down into smaller, more manageable chunks.","[' The parameters of the maximum-margin hyperplane are derived by solving what?', ' There exist several specialized algorithms for quickly solving the quadratic programming (QP) problem that arises from SVMs.']","['the optimization', 'The parameters of the maximum-margin hyperplane are derived by solving the optimization']"
116,support vector machine,Implementation,"Another approach is to use an interior-point method that uses Newton-like iterations to find a solution of the Karush–Kuhn–Tucker conditions of the primal and dual problems.
Instead of solving a sequence of broken-down problems, this approach directly solves the problem altogether. To avoid solving a linear system involving the large kernel matrix, a low-rank approximation to the matrix is often used in the kernel trick.
","Another approach is to use an interior-point method that uses Newton-like iterations to find a solution of the Karush–Kuhn–Tucker conditions of the primal and dual problems. Instead of solving a sequence of broken-down problems, this approach directly solves the problem altogether.","[' What method uses Newton-like iterations to find a solution of the Karush–Kuhn–Tucker conditions of the primal and dual problems?', ' Instead of solving a sequence of broken-down problems, what solves the problem altogether?']","['interior-point method', 'an interior-point method']"
117,support vector machine,Implementation,"Another common method is Platt's sequential minimal optimization (SMO) algorithm, which breaks the problem down into 2-dimensional sub-problems that are solved analytically, eliminating the need for a numerical optimization algorithm and matrix storage. This algorithm is conceptually simple, easy to implement, generally faster, and has better scaling properties for difficult SVM problems.","Another common method is Platt's sequential minimal optimization (SMO) algorithm, which breaks the problem down into 2-dimensional sub-problems that are solved analytically, eliminating the need for a numerical optimization algorithm and matrix storage. This algorithm is conceptually simple, easy to implement, generally faster, and has better scaling properties for difficult SVM problems.","["" What is Platt's sequential minimal optimization (SMO) algorithm?"", ' What does SMO eliminate the need for?', ' How is SMO implemented?', ' What kind of problem does implement generally have better scaling properties for?']","['breaks the problem down into 2-dimensional sub-problems that are solved analytically', 'a numerical optimization algorithm and matrix storage', 'easy to implement', 'difficult SVM']"
118,support vector machine,Implementation,"The special case of linear support-vector machines can be solved more efficiently by the same kind of algorithms used to optimize its close cousin, logistic regression; this class of algorithms includes sub-gradient descent (e.g., PEGASOS) and coordinate descent (e.g., LIBLINEAR). LIBLINEAR has some attractive training-time properties. Each convergence iteration takes time linear in the time taken to read the train data, and the iterations also have a Q-linear convergence property, making the algorithm extremely fast.
","The special case of linear support-vector machines can be solved more efficiently by the same kind of algorithms used to optimize its close cousin, logistic regression; this class of algorithms includes sub-gradient descent (e.g., PEGASOS) and coordinate descent (e.g., LIBLINEAR). LIBLINEAR has some attractive training-time properties.","[' What type of machines can be solved more efficiently by the same kind of algorithms used to optimize its close cousin, logistic regression?', ' What class of algorithms includes sub-gradient descent (e.g., PEGASOS)?']","['linear support-vector', 'logistic regression']"
119,support vector machine,Implementation,"The general kernel SVMs can also be solved more efficiently using sub-gradient descent (e.g. P-packSVM), especially when parallelization is allowed.
","The general kernel SVMs can also be solved more efficiently using sub-gradient descent (e.g. P-packSVM), especially when parallelization is allowed.",[' What type of SVMs can be solved more efficiently using sub-gradient descent?'],['general kernel']
120,support vector machine,Implementation,"Preprocessing of data (standardization) is highly recommended to enhance accuracy of classification. There are a few methods of standardization, such as min-max, normalization by decimal scaling, Z-score. Subtraction of mean and division by variance of each feature is usually used for SVM.","Preprocessing of data (standardization) is highly recommended to enhance accuracy of classification. There are a few methods of standardization, such as min-max, normalization by decimal scaling, Z-score.","[' What is highly recommended to enhance accuracy of classification?', ' What are a few methods of standardization?']","['Preprocessing of data', 'min-max, normalization by decimal scaling, Z-score']"
121,security,Summary,"Security is protection from, or resilience against, potential harm (or other unwanted coercive change) caused by others, by restraining the freedom of others to act. Beneficiaries (technically referents) of security may be of persons and social groups, objects and institutions, ecosystems or any other entity or phenomenon vulnerable to unwanted change. 
","Security is protection from, or resilience against, potential harm (or other unwanted coercive change) caused by others, by restraining the freedom of others to act. Beneficiaries (technically referents) of security may be of persons and social groups, objects and institutions, ecosystems or any other entity or phenomenon vulnerable to unwanted change.","[' What is security?', ' What is the term for protection against potential harm caused by others?', ' How is security restraining the freedom of others to act?', ' What are ecosystems vulnerable to?', ' What is the name of the entity or phenomenon vulnerable to unwanted change?']","['protection from, or resilience against, potential harm (or other unwanted coercive change) caused by others', 'Security', 'protection from, or resilience against, potential harm (or other unwanted coercive change) caused by others', 'unwanted change', 'Beneficiaries']"
122,security,Summary,"Security mostly refers to protection from hostile forces, but it has a wide range of other senses: for example, as the absence of harm (e.g. freedom from want); as the presence of an essential good (e.g. food security); as resilience against potential damage or harm (e.g. secure foundations); as secrecy (e.g. a secure telephone line); as containment (e.g. a secure room or cell); and as a state of mind (e.g. emotional security).
","Security mostly refers to protection from hostile forces, but it has a wide range of other senses: for example, as the absence of harm (e.g. freedom from want); as the presence of an essential good (e.g.","[' What does security refer to?', ' What is the absence of harm referred to as?']","['protection from hostile forces', 'freedom from want']"
123,security,Summary,"The term is also used to refer to acts and systems whose purpose may be to provide security (e.g.: security companies, security forces, security guard, cyber security systems, security cameras, remote guarding).
","The term is also used to refer to acts and systems whose purpose may be to provide security (e.g. : security companies, security forces, security guard, cyber security systems, security cameras, remote guarding).",[' What is another term used to refer to acts and systems whose purpose may be to provide security?'],"['cyber security systems, security cameras, remote guarding']"
124,security,Etymology,"The word 'secure' entered the English language in the 16th century. It is derived from Latin securus, meaning freedom from anxiety: se (without) + cura (care, anxiety).","The word 'secure' entered the English language in the 16th century. It is derived from Latin securus, meaning freedom from anxiety: se (without) + cura (care, anxiety).","["" When did the word'secure' enter the English language?"", ' What Latin word means freedom from anxiety?']","['16th century', 'securus']"
125,security,Perceptions of security,"Since it is not possible to know with precision the extent to which something is 'secure' (and a measure of vulnerability is unavoidable), perceptions of security vary, often greatly. For example, a fear of death by earthquake is common in the United States (US), but slipping on the bathroom floor kills more people; and in France, the United Kingdom and the US there are far fewer deaths caused by terrorism than there are women killed by their partners in the home.","Since it is not possible to know with precision the extent to which something is 'secure' (and a measure of vulnerability is unavoidable), perceptions of security vary, often greatly. For example, a fear of death by earthquake is common in the United States (US), but slipping on the bathroom floor kills more people; and in France, the United Kingdom and the US there are far fewer deaths caused by terrorism than there are women killed by their partners in the home.","["" What is not possible to know with precision the extent to which something is'secure'?"", ' What is common in the United States?', ' What does slipping on the bathroom floor kill more people in the US?', ' In France, the United Kingdom and the US, what causes far fewer deaths than women killed by their partners in the home?']","['secure', 'a fear of death by earthquake', 'a fear of death by earthquake', 'terrorism']"
126,security,Perceptions of security,"Another problem of perception is the common assumption that the mere presence of a security system (such as armed forces, or antivirus software) implies security. For example, two computer security programs installed on the same device can prevent each other from working properly, while the user assumes that he or she benefits from twice the protection that only one program would afford.
","Another problem of perception is the common assumption that the mere presence of a security system (such as armed forces, or antivirus software) implies security. For example, two computer security programs installed on the same device can prevent each other from working properly, while the user assumes that he or she benefits from twice the protection that only one program would afford.","[' What is another problem of perception?', ' What is the common assumption that the mere presence of a security system implies?', ' How many computer security programs can prevent each other from working properly?', ' What does the user assume he or she benefits from twice the protection that only one program would afford?']","['the common assumption that the mere presence of a security system (such as armed forces, or antivirus software) implies security', 'security', 'two', 'two computer security programs installed on the same device']"
127,security,Perceptions of security,"Security theater is a critical term for measures that change perceptions of security without necessarily affecting security itself. For example, visual signs of security protections, such as a home that advertises its alarm system, may deter an intruder, whether or not the system functions properly. Similarly, the increased presence of military personnel on the streets of a city after a terrorist attack may help to reassure the public, whether or not it diminishes the risk of further attacks.
","Security theater is a critical term for measures that change perceptions of security without necessarily affecting security itself. For example, visual signs of security protections, such as a home that advertises its alarm system, may deter an intruder, whether or not the system functions properly.","[' What is a critical term for measures that change perceptions of security without necessarily affecting security itself?', ' What may deter an intruder if a home advertises its alarm system?']","['Security theater', 'visual signs of security protections']"
128,optimization,Summary,"Mathematical optimization (alternatively spelled optimisation) or mathematical programming is the selection of a best element, with regard to some criterion, from some set of available alternatives. Optimization problems of sorts arise in all quantitative disciplines from computer science and engineering to operations research and economics, and the development of solution methods has been of interest in mathematics for centuries.","Mathematical optimization (alternatively spelled optimisation) or mathematical programming is the selection of a best element, with regard to some criterion, from some set of available alternatives. Optimization problems of sorts arise in all quantitative disciplines from computer science and engineering to operations research and economics, and the development of solution methods has been of interest in mathematics for centuries.","[' Mathematical optimization or mathematical programming is what?', ' What is the selection of a best element with regard to some criterion, from some set of available alternatives?', ' Optimization problems arise in all what disciplines?', ' What has been of interest to mathematics for centuries?']","['the selection of a best element, with regard to some criterion, from some set of available alternatives', 'Mathematical optimization', 'quantitative', 'the development of solution methods']"
129,optimization,Summary,"In the simplest case, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function. The generalization of optimization theory and techniques to other formulations constitutes a large area of applied mathematics. More generally, optimization includes finding ""best available"" values of some objective function given a defined domain (or input), including a variety of different types of objective functions and different types of domains.
","In the simplest case, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function. The generalization of optimization theory and techniques to other formulations constitutes a large area of applied mathematics.","[' In the simplest case, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function?', ' The generalization of optimization theory and techniques constitutes a large area of what?']","['optimization theory and techniques to other formulations constitutes a large area of applied mathematics', 'applied mathematics']"
130,optimization,Optimization problems,"Such a formulation is called an optimization problem or a mathematical programming problem (a term not directly related to computer programming, but still in use for example in linear programming – see History below). Many real-world and theoretical problems may be modeled in this general framework.
","Such a formulation is called an optimization problem or a mathematical programming problem (a term not directly related to computer programming, but still in use for example in linear programming – see History below). Many real-world and theoretical problems may be modeled in this general framework.","[' What is another name for a mathematical programming problem?', ' What is a term not directly related to computer programming but still in use?']","['optimization problem', 'mathematical programming problem']"
131,optimization,Optimization problems,"it is more convenient to solve minimization problems. However, the opposite perspective would be valid, too.
","it is more convenient to solve minimization problems. However, the opposite perspective would be valid, too.","[' Is it more convenient to solve minimization problems?', ' Is the opposite perspective valid?']","['it is more convenient', 'the opposite perspective would be valid']"
132,optimization,Optimization problems,"Problems formulated using this technique in the fields of physics may refer to the technique as energy minimization, speaking of the value of the function f as representing the energy of the system being modeled. In machine learning, it is always necessary to continuously evaluate the quality of a data model by using a cost function where a minimum implies a set of possibly optimal parameters with an optimal (lowest) error.
","Problems formulated using this technique in the fields of physics may refer to the technique as energy minimization, speaking of the value of the function f as representing the energy of the system being modeled. In machine learning, it is always necessary to continuously evaluate the quality of a data model by using a cost function where a minimum implies a set of possibly optimal parameters with an optimal (lowest) error.","[' What can problems formulated using this technique in the fields of physics refer to the technique as?', ' What is always necessary to continuously evaluate the quality of a data?', ' What is always necessary to continuously evaluate the quality of a data model?', ' What implies a set of possibly optimal parameters with an optimal (lowest) error?']","['energy minimization', 'machine learning', 'machine learning', 'a minimum']"
133,optimization,Optimization problems,"Typically, A is some subset of the Euclidean space ℝn, often specified by a set of constraints, equalities or inequalities that the members of A have to satisfy.  The domain A of f is called the search space or the choice set, while the elements of A are called candidate solutions or feasible solutions.
","Typically, A is some subset of the Euclidean space ℝn, often specified by a set of constraints, equalities or inequalities that the members of A have to satisfy. The domain A of f is called the search space or the choice set, while the elements of A are called candidate solutions or feasible solutions.","[' What is a subset of the Euclidean space Rn?', ' What is the domain A of f called?', ' The elements of A are called what?', ' What are the elements of A called?']","['A', 'the search space or the choice set', 'candidate solutions or feasible solutions', 'candidate solutions or feasible solutions']"
134,optimization,Optimization problems,"The function f is called, variously, an objective function, a loss function or cost function (minimization),  a utility function or fitness function (maximization), or, in certain fields, an energy function or energy functional. A feasible solution that minimizes (or maximizes, if that is the goal) the objective function is called an optimal solution.
","The function f is called, variously, an objective function, a loss function or cost function (minimization),  a utility function or fitness function (maximization), or, in certain fields, an energy function or energy functional. A feasible solution that minimizes (or maximizes, if that is the goal) the objective function is called an optimal solution.","[' What is the function f called?', ' What is a feasible solution that minimizes or maximizes the objective function?', ' What is the objective function called?', ' What is an optimal solution?']","['an objective function', 'optimal solution', 'an optimal solution', 'A feasible solution that minimizes (or maximizes, if that is the goal) the objective function']"
135,optimization,Optimization problems,"that is to say, on some region around x* all of the function values are greater than or equal to the value at that element. 
Local maxima are defined similarly.
","that is to say, on some region around x* all of the function values are greater than or equal to the value at that element. Local maxima are defined similarly.","[' Where are all function values greater than or equal to the value at that element?', ' Where are local maxima defined similarly?']","['on some region around x*', 'on some region around x*']"
136,optimization,Optimization problems,"While a local minimum is at least as good as any nearby elements, a global minimum is at least as good as every feasible element.
Generally, unless the objective function is convex in a minimization problem, there may be several local minima.
In a convex problem, if there is a local minimum that is interior (not on the edge of the set of feasible elements), it is also the global minimum, but a nonconvex problem may have more than one local minimum not all of which need be global minima.
","While a local minimum is at least as good as any nearby elements, a global minimum is at least as good as every feasible element. Generally, unless the objective function is convex in a minimization problem, there may be several local minima.","[' What is at least as good as any nearby elements as a local minimum?', ' A global minimum is at best as every feasible element?', ' Unless the objective function is convex in a minimization problem, there may be how many local minima?']","['global minimum', 'local minimum is at least as good as any nearby elements', 'several']"
137,optimization,Optimization problems,"A large number of algorithms proposed for solving the nonconvex problems – including the majority of commercially available solvers – are  not capable of making a distinction between locally optimal solutions and globally optimal solutions, and will treat the former as actual solutions to the original problem. Global optimization is the branch of applied mathematics and numerical analysis that is concerned with the development of deterministic algorithms that are capable of guaranteeing convergence in finite time to the actual optimal solution of a nonconvex problem.
","A large number of algorithms proposed for solving the nonconvex problems – including the majority of commercially available solvers – are  not capable of making a distinction between locally optimal solutions and globally optimal solutions, and will treat the former as actual solutions to the original problem. Global optimization is the branch of applied mathematics and numerical analysis that is concerned with the development of deterministic algorithms that are capable of guaranteeing convergence in finite time to the actual optimal solution of a nonconvex problem.","[' How many algorithms are not capable of making a distinction between locally optimal solutions and globally optimal solutions?', ' What will treat the latter as actual solutions to the original problem?', ' What is the branch of applied mathematics and numerical analysis that is concerned with the development of deterministic algorithms that are capable of guaranteeing convergence in finite time?']","['A large number', 'globally optimal solutions', 'Global optimization']"
138,optimization,Notation,"Optimization problems are often expressed with special notation. Here are some examples:
",Optimization problems are often expressed with special notation. Here are some examples:,[' What type of notation is used to express optimization problems?'],['special']
139,optimization,History,"The term ""linear programming"" for certain optimization cases was due to George B. Dantzig, although much of the theory had been introduced by Leonid Kantorovich in 1939. (Programming in this context does not refer to computer programming, but comes from the use of program by the United States military to refer to proposed training and logistics schedules, which were the problems Dantzig studied at that time.) Dantzig published the Simplex algorithm in 1947, and John von Neumann developed the theory of duality in the same year.","The term ""linear programming"" for certain optimization cases was due to George B. Dantzig, although much of the theory had been introduced by Leonid Kantorovich in 1939. (Programming in this context does not refer to computer programming, but comes from the use of program by the United States military to refer to proposed training and logistics schedules, which were the problems Dantzig studied at that time.)","[' Who invented the term ""linear programming"" for certain optimization cases?', ' Who introduced the theory of linear programming in 1939?', ' What does programming in this context not refer to?', ' What were the problems Dantzig studied at that time?']","['George\xa0B. Dantzig', 'Leonid Kantorovich', 'computer programming', 'proposed training and logistics schedules']"
140,feature selection,Summary,"In machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for several reasons:
","In machine learning and statistics, feature selection, also known as variable selection, attribute selection or variable subset selection, is the process of selecting a subset of relevant features (variables, predictors) for use in model construction. Feature selection techniques are used for several reasons:","[' In machine learning and statistics, what is the process of selecting a subset of relevant features for use in model construction?']",['feature selection']
141,feature selection,Summary,"The central premise when using a feature selection technique is that the data contains some features that are either redundant or irrelevant, and can thus be removed without incurring much loss of information. Redundant and irrelevant are two distinct notions, since one relevant feature may be redundant in the presence of another relevant feature with which it is strongly correlated.","The central premise when using a feature selection technique is that the data contains some features that are either redundant or irrelevant, and can thus be removed without incurring much loss of information. Redundant and irrelevant are two distinct notions, since one relevant feature may be redundant in the presence of another relevant feature with which it is strongly correlated.","[' What is the central premise when using a feature selection technique?', ' What can be removed without incurring much loss of information?', ' Redundant and irrelevant are two distinct notions?', ' What may be redundant in the presence of another relevant feature with which it is strongly correlated?']","['the data contains some features that are either redundant or irrelevant, and can thus be removed without incurring much loss of information', 'the data contains some features that are either redundant or irrelevant', 'redundant in the presence of another relevant feature with which it is strongly correlated', 'one relevant feature']"
142,feature selection,Summary,"Feature selection techniques should be distinguished from feature extraction. Feature extraction creates new features from functions of the original features, whereas feature selection returns a subset of the features. Feature selection techniques are often used in domains where there are many features and comparatively few samples (or data points). Archetypal cases for the application of feature selection include the analysis of written texts and DNA microarray data, where there are many thousands of features, and a few tens to hundreds of samples.
","Feature selection techniques should be distinguished from feature extraction. Feature extraction creates new features from functions of the original features, whereas feature selection returns a subset of the features.","[' What should be distinguished from feature extraction techniques?', ' Feature extraction creates new features from functions of what?', ' What returns a subset of features?']","['Feature selection techniques', 'the original features', 'feature selection']"
143,feature selection,Introduction,"A feature selection algorithm can be seen as the combination of a search technique for proposing new feature subsets, along with an evaluation measure which scores the different feature subsets. The simplest algorithm is to test each possible subset of features finding the one which minimizes the error rate. This is an exhaustive search of the space, and is computationally intractable for all but the smallest of feature sets. The choice of evaluation metric heavily influences the algorithm, and it is these evaluation metrics which distinguish between the three main categories of feature selection algorithms: wrappers, filters and embedded methods.","A feature selection algorithm can be seen as the combination of a search technique for proposing new feature subsets, along with an evaluation measure which scores the different feature subsets. The simplest algorithm is to test each possible subset of features finding the one which minimizes the error rate.","[' What is a feature selection algorithm a combination of?', ' What is the simplest algorithm to test?']","['a search technique for proposing new feature subsets, along with an evaluation measure which scores the different feature subsets', 'each possible subset of features finding the one which minimizes the error rate']"
144,feature selection,Introduction,"In traditional regression analysis, the most popular form of feature selection is stepwise regression, which is a wrapper technique.  It is a greedy algorithm that adds the best feature (or deletes the worst feature) at each round.  The main control issue is deciding when to stop the algorithm.  In machine learning, this is typically done by cross-validation.  In statistics, some criteria are optimized.  This leads to the inherent problem of nesting. More robust methods have been explored, such as branch and bound and piecewise linear network.
","In traditional regression analysis, the most popular form of feature selection is stepwise regression, which is a wrapper technique. It is a greedy algorithm that adds the best feature (or deletes the worst feature) at each round.","[' What is the most popular form of feature selection in traditional regression analysis?', ' What is stepwise regression a wrapper technique?', ' Stepwise regression is a greedy algorithm that adds what at each round?']","['stepwise regression', 'a greedy algorithm that adds the best feature (or deletes the worst feature) at each round', 'the best feature']"
145,feature selection,Subset selection,"Subset selection evaluates a subset of features as a group for suitability. Subset selection algorithms can be broken up into wrappers, filters, and embedded methods. Wrappers use a search algorithm to search through the space of possible features and evaluate each subset by running a model on the subset. Wrappers can be computationally expensive and have a risk of over fitting to the model. Filters are similar to wrappers in the search approach, but instead of evaluating against a model, a simpler filter is evaluated. Embedded techniques are embedded in, and specific to, a model.
","Subset selection evaluates a subset of features as a group for suitability. Subset selection algorithms can be broken up into wrappers, filters, and embedded methods.","[' Subset selection evaluates a subset of features as a group for suitability for what purpose?', ' What can subset selection algorithms be broken up into?']","['suitability', 'wrappers, filters, and embedded methods']"
146,feature selection,Subset selection,"Many popular search approaches use greedy hill climbing, which iteratively evaluates a candidate subset of features, then modifies the subset and evaluates if the new subset is an improvement over the old. Evaluation of the subsets requires a scoring metric that grades a subset of features.  Exhaustive search is generally impractical, so at some implementor (or operator) defined stopping point, the subset of features with the highest score discovered up to that point is selected as the satisfactory feature subset.  The stopping criterion varies by algorithm; possible criteria include: a subset score exceeds a threshold, a program's maximum allowed run time has been surpassed, etc.
","Many popular search approaches use greedy hill climbing, which iteratively evaluates a candidate subset of features, then modifies the subset and evaluates if the new subset is an improvement over the old. Evaluation of the subsets requires a scoring metric that grades a subset of features.","[' What does greedy hill climbing do?', ' What is a scoring metric that grades a subset of features?']","['iteratively evaluates a candidate subset of features', 'Evaluation of the subsets']"
147,feature selection,Subset selection,"Two popular filter metrics for classification problems are correlation and mutual information, although neither are true metrics or 'distance measures' in the mathematical sense, since they fail to obey the triangle inequality and thus do not compute any actual 'distance' – they should rather be regarded as 'scores'.  These scores are computed between a candidate feature (or set of features) and the desired output category.  There are, however, true metrics that are a simple function of the mutual information; see here.
","Two popular filter metrics for classification problems are correlation and mutual information, although neither are true metrics or 'distance measures' in the mathematical sense, since they fail to obey the triangle inequality and thus do not compute any actual 'distance' – they should rather be regarded as 'scores'. These scores are computed between a candidate feature (or set of features) and the desired output category.","[' What are two popular filter metrics for classification problems?', ' What do correlation and mutual information fail to obey?', ' What should scores be regarded as?', ' What are scores computed between?']","['correlation and mutual information', 'the triangle inequality', 'scores', 'a candidate feature (or set of features) and the desired output category']"
148,feature selection,Optimality criteria,"The choice of optimality criteria is difficult as there are multiple objectives in a feature selection task. Many common criteria incorporate a measure of accuracy, penalised by the number of features selected. Examples include Akaike information criterion (AIC) and Mallows's Cp, which have a penalty of 2 for each added feature. AIC is based on information theory, and is effectively derived via the maximum entropy principle.","The choice of optimality criteria is difficult as there are multiple objectives in a feature selection task. Many common criteria incorporate a measure of accuracy, penalised by the number of features selected.","[' The choice of optimality criteria is difficult because there are multiple objectives in a feature selection task.', ' Many common criteria incorporate what measure of accuracy?', ' The number of features selected penalises the measure of what?']","['Many common criteria incorporate a measure of accuracy, penalised by the number of features selected.', 'optimality criteria is difficult as there are multiple objectives in a feature selection task. Many common criteria incorporate a measure of accuracy, penalised by the number of features selected', 'accuracy']"
149,feature selection,Optimality criteria,"Other criteria are Bayesian information criterion (BIC), which uses a penalty of 





log
⁡

n





{\displaystyle {\sqrt {\log {n}}}}
 for each added feature, minimum description length (MDL) which asymptotically uses 





log
⁡

n





{\displaystyle {\sqrt {\log {n}}}}
, Bonferroni / RIC which use 





2
log
⁡

p





{\displaystyle {\sqrt {2\log {p}}}}
, maximum dependency feature selection, and a variety of new criteria that are motivated by false discovery rate (FDR), which use something close to 





2
log
⁡


p
q






{\displaystyle {\sqrt {2\log {\frac {p}{q}}}}}
. A maximum entropy rate criterion may also be used to select the most relevant subset of features.","Other criteria are Bayesian information criterion (BIC), which uses a penalty of 





log
⁡

n





{\displaystyle {\sqrt {\log {n}}}}
 for each added feature, minimum description length (MDL) which asymptotically uses 





log
⁡

n





{\displaystyle {\sqrt {\log {n}}}}
, Bonferroni / RIC which use 





2
log
⁡

p





{\displaystyle {\sqrt {2\log {p}}}}
, maximum dependency feature selection, and a variety of new criteria that are motivated by false discovery rate (FDR), which use something close to 





2
log
⁡


p
q






{\displaystyle {\sqrt {2\log {\frac {p}{q}}}}}
. A maximum entropy rate criterion may also be used to select the most relevant subset of features.","[' What is another term for Bayesian information criterion?', ' What does MDL stand for?', ' Bonferroni / RIC use how many logs?', ' What motivates new criteria that are motivated by false discovery rate (FDR)?', ' What may be used to select the most relevant subset of features?']","['log\n\u2061\n\nn', 'minimum description length', '2\nlog\n\u2061\n\np', 'maximum dependency feature selection', 'maximum entropy rate criterion']"
150,feature selection,Structure learning,"Filter feature selection is a specific case of a more general paradigm called structure learning. Feature selection finds the relevant feature set for a specific target variable whereas structure learning finds the relationships between all the variables, usually by expressing these relationships as a graph. The most common structure learning algorithms assume the data is generated by a Bayesian Network, and so the structure is a directed graphical model. The optimal solution to the filter feature selection problem is the Markov blanket of the target node, and in a Bayesian Network, there is a unique Markov Blanket for each node.","Filter feature selection is a specific case of a more general paradigm called structure learning. Feature selection finds the relevant feature set for a specific target variable whereas structure learning finds the relationships between all the variables, usually by expressing these relationships as a graph.","[' What is a specific case of a more general paradigm called structure learning?', ' Feature selection finds the relevant feature set for what target variable?', ' Structure learning finds the relationships between all the variables by expressing the relationships as what?']","['Filter feature selection', 'specific', 'a graph']"
151,feature selection,Information Theory Based Feature Selection Mechanisms,"There are different Feature Selection mechanisms around that utilize mutual information for scoring the different features. They usually use all the same algorithm:
",There are different Feature Selection mechanisms around that utilize mutual information for scoring the different features. They usually use all the same algorithm:,"[' What do Feature Selection mechanisms use for scoring the different features?', ' What is the name of the mechanism that uses mutual information for scoring?']","['mutual information', 'Feature Selection']"
152,feature selection,Hilbert-Schmidt Independence Criterion Lasso based feature selection,"For high-dimensional and small sample data (e.g., dimensionality > 105 and the number of samples < 103), the Hilbert-Schmidt Independence Criterion Lasso (HSIC Lasso) is useful. HSIC Lasso optimization problem is given as
","For high-dimensional and small sample data (e.g., dimensionality > 105 and the number of samples < 103), the Hilbert-Schmidt Independence Criterion Lasso (HSIC Lasso) is useful. HSIC Lasso optimization problem is given as","[' What is useful for high-dimensional and small sample data?', ' What is the Hilbert-Schmidt Independence Criterion Lasso?']","['Hilbert-Schmidt Independence Criterion Lasso', 'HSIC Lasso']"
153,feature selection,Hilbert-Schmidt Independence Criterion Lasso based feature selection,"where 





HSIC


(

f

k


,
c
)
=


tr


(





K

¯




(
k
)






L

¯



)


{\displaystyle {\mbox{HSIC}}(f_{k},c)={\mbox{tr}}({\bar {\mathbf {K} }}^{(k)}{\bar {\mathbf {L} }})}
 is a kernel-based independence measure called the (empirical) Hilbert-Schmidt independence criterion (HSIC), 





tr


(
⋅
)


{\displaystyle {\mbox{tr}}(\cdot )}
 denotes the trace, 



λ


{\displaystyle \lambda }
 is the regularization parameter, 








K

¯




(
k
)


=

Γ



K


(
k
)



Γ



{\displaystyle {\bar {\mathbf {K} }}^{(k)}=\mathbf {\Gamma } \mathbf {K} ^{(k)}\mathbf {\Gamma } }
 and 







L

¯



=

Γ


L


Γ



{\displaystyle {\bar {\mathbf {L} }}=\mathbf {\Gamma } \mathbf {L} \mathbf {\Gamma } }
 are input and output centered Gram matrices, 




K

i
,
j


(
k
)


=
K
(

u

k
,
i


,

u

k
,
j


)


{\displaystyle K_{i,j}^{(k)}=K(u_{k,i},u_{k,j})}
 and 




L

i
,
j


=
L
(

c

i


,

c

j


)


{\displaystyle L_{i,j}=L(c_{i},c_{j})}
 are Gram matrices, 



K
(
u
,

u
′

)


{\displaystyle K(u,u')}
 and 



L
(
c
,

c
′

)


{\displaystyle L(c,c')}
 are kernel functions, 




Γ

=


I


m


−


1
m




1


m




1


m


T




{\displaystyle \mathbf {\Gamma } =\mathbf {I} _{m}-{\frac {1}{m}}\mathbf {1} _{m}\mathbf {1} _{m}^{T}}
 is the centering matrix, 





I


m




{\displaystyle \mathbf {I} _{m}}
 is the m-dimensional identity matrix (m: the number of samples), 





1


m




{\displaystyle \mathbf {1} _{m}}
 is the m-dimensional vector with all ones, and  



‖
⋅

‖

1




{\displaystyle \|\cdot \|_{1}}
 is the 




ℓ

1




{\displaystyle \ell _{1}}
-norm.  HSIC always takes a non-negative value, and is zero if and only if two random variables are statistically independent when a universal reproducing kernel such as the Gaussian kernel is used.
","where 





HSIC


(

f

k


,
c
)
=


tr


(





K

¯




(
k
)






L

¯



)


{\displaystyle {\mbox{HSIC}}(f_{k},c)={\mbox{tr}}({\bar {\mathbf {K} }}^{(k)}{\bar {\mathbf {L} }})}
 is a kernel-based independence measure called the (empirical) Hilbert-Schmidt independence criterion (HSIC), 





tr


(
⋅
)


{\displaystyle {\mbox{tr}}(\cdot )}
 denotes the trace, 



λ


{\displaystyle \lambda }
 is the regularization parameter, 








K

¯




(
k
)


=

Γ



K


(
k
)



Γ



{\displaystyle {\bar {\mathbf {K} }}^{(k)}=\mathbf {\Gamma } \mathbf {K} ^{(k)}\mathbf {\Gamma } }
 and 







L

¯



=

Γ


L


Γ



{\displaystyle {\bar {\mathbf {L} }}=\mathbf {\Gamma } \mathbf {L} \mathbf {\Gamma } }
 are input and output centered Gram matrices, 




K

i
,
j


(
k
)


=
K
(

u

k
,
i


,

u

k
,
j


)


{\displaystyle K_{i,j}^{(k)}=K(u_{k,i},u_{k,j})}
 and 




L

i
,
j


=
L
(

c

i


,

c

j


)


{\displaystyle L_{i,j}=L(c_{i},c_{j})}
 are Gram matrices, 



K
(
u
,

u
′

)


{\displaystyle K(u,u')}
 and 



L
(
c
,

c
′

)


{\displaystyle L(c,c')}
 are kernel functions, 




Γ

=


I


m


−


1
m




1


m




1


m


T




{\displaystyle \mathbf {\Gamma } =\mathbf {I} _{m}-{\frac {1}{m}}\mathbf {1} _{m}\mathbf {1} _{m}^{T}}
 is the centering matrix, 





I


m




{\displaystyle \mathbf {I} _{m}}
 is the m-dimensional identity matrix (m: the number of samples), 





1


m




{\displaystyle \mathbf {1} _{m}}
 is the m-dimensional vector with all ones, and  



‖
⋅

‖

1




{\displaystyle \|\cdot \|_{1}}
 is the 




ℓ

1




{\displaystyle \ell _{1}}
-norm. HSIC always takes a non-negative value, and is zero if and only if two random variables are statistically independent when a universal reproducing kernel such as the Gaussian kernel is used.","[' What is the kernel-based independence measure called?', ' What denotes the trace?', ' <unk> <unk>displaystyle <unk>lambda <unk> is what?', ' What are input and output centered Gram matrices?', ' What are kernel functions?', ' What is the centering matrix?', ' I m <unk>displaystyle <unk>mathbf <unk>I<unk> _<unk>m<unk> is the m-dimensional identity matrix (m: the number of samples).', ' What is the l 1 <unk>displaystyle <unk>ell _<unk>1<unk> -norm?', ' What always takes a non-negative value?']","['Hilbert-Schmidt independence criterion', 'Schmidt independence criterion', 'regularization parameter', 'K\n\ni\n,\nj', 'Gram matrices', '{1}{m}}\\mathbf {1} _{m}\\mathbf {1} _{m}^{T}}', 'tr', 'mbox{tr}}(\\cdot )}\n denotes the trace, \n\n\n\nλ\n\n\n{\\displaystyle \\lambda }\n is the regularization parameter, \n\n\n\n\n\n\n\n\nK\n\n¯\n\n\n\n\n(\nk\n)\n\n\n=\n\nΓ\n\n\n\nK\n\n\n(\nk\n)\n\n\n\nΓ\n\n\n\n{\\displaystyle {\\bar {\\mathbf {K} }}^{(k)}=\\mathbf {\\Gamma } \\mathbf {K} ^{(k)}\\mathbf {\\Gamma } }\n and \n\n\n\n\n\n\n\nL\n\n¯\n\n\n\n=\n\nΓ\n\n\nL\n\n\nΓ', 'HSIC']"
154,feature selection,Hilbert-Schmidt Independence Criterion Lasso based feature selection,"where 



‖
⋅

‖

F




{\displaystyle \|\cdot \|_{F}}
 is the Frobenius norm. The optimization problem is a Lasso problem, and thus it can be efficiently solved with a state-of-the-art Lasso solver such as the dual augmented Lagrangian method.
","where 



‖
⋅

‖

F




{\displaystyle \|\cdot \|_{F}}
 is the Frobenius norm. The optimization problem is a Lasso problem, and thus it can be efficiently solved with a state-of-the-art Lasso solver such as the dual augmented Lagrangian method.","[' What is the Frobenius norm?', ' The optimization problem is a Lasso problem, which can be efficiently solved with a state-of-the-art what?', ' What solver can be used to solve the optimization problem?']","['‖\n⋅\n\n‖\n\nF', 'Lasso solver', 'Lasso solver']"
155,feature selection,Correlation feature selection,"The correlation feature selection (CFS) measure evaluates subsets of features on the basis of the following hypothesis: ""Good feature subsets contain features highly correlated with the classification, yet uncorrelated to each other"". The following equation gives the merit of a feature subset S consisting of k features:
","The correlation feature selection (CFS) measure evaluates subsets of features on the basis of the following hypothesis: ""Good feature subsets contain features highly correlated with the classification, yet uncorrelated to each other"". The following equation gives the merit of a feature subset S consisting of k features:","[' What measure evaluates subsets of features on the basis of the following hypothesis?', ' What equation gives the merit of a feature subset S consisting of k features?']","['correlation feature selection (CFS)', 'The following']"
156,neural network,Summary,"A neural network is a network or circuit of biological neurons, or in a modern sense, an artificial neural network, composed of artificial neurons or nodes. Thus a neural network is either a biological neural network, made up of biological neurons, or an artificial neural network, for solving artificial intelligence (AI) problems. The connections of the biological neuron are modeled in artificial neural networks as weights between nodes. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred to as a linear combination. Finally, an activation function controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be −1 and 1.
","A neural network is a network or circuit of biological neurons, or in a modern sense, an artificial neural network, composed of artificial neurons or nodes. Thus a neural network is either a biological neural network, made up of biological neurons, or an artificial neural network, for solving artificial intelligence (AI) problems.","[' What is a network or circuit of biological neurons called?', ' What is an artificial neural network composed of?', ' What is another name for an artificial neural network?', ' What does AI stand for?']","['A neural network', 'artificial neurons or nodes', 'A neural network', 'artificial intelligence']"
157,neural network,Summary,"These artificial networks may be used for predictive modeling, adaptive control and applications where they can be trained via a dataset. Self-learning resulting from experience can occur within networks, which can derive conclusions from a complex and seemingly unrelated set of information.","These artificial networks may be used for predictive modeling, adaptive control and applications where they can be trained via a dataset. Self-learning resulting from experience can occur within networks, which can derive conclusions from a complex and seemingly unrelated set of information.","[' Artificial networks can be used for predictive modeling, adaptive control and applications where they can be trained via what?', ' Self-learning resulting from experience can occur within networks, which can derive conclusions from a complex and seemingly unrelated set of information?']","['a dataset', 'artificial networks']"
158,neural network,Overview,"A biological neural network is composed of a groups of chemically connected or functionally associated neurons. A single neuron may be connected to many other neurons and the total number of neurons and connections in a network may be extensive. Connections, called synapses, are usually formed from axons to dendrites, though dendrodendritic synapses and other connections are possible. Apart from the electrical signaling, there are other forms of signaling that arise from neurotransmitter diffusion.
",A biological neural network is composed of a groups of chemically connected or functionally associated neurons. A single neuron may be connected to many other neurons and the total number of neurons and connections in a network may be extensive.,"[' What is a biological neural network composed of?', ' A single neuron may be connected to many other neurons and the total number of neurons and connections in a network may be what?']","['a groups of chemically connected or functionally associated neurons', 'extensive']"
159,neural network,Overview,"Artificial intelligence, cognitive modeling, and neural networks are information processing paradigms inspired by the way biological neural systems process data. Artificial intelligence and cognitive modeling try to simulate some properties of biological neural networks. In the artificial intelligence field, artificial neural networks have been applied successfully to speech recognition, image analysis and adaptive control, in order to construct software agents (in computer and video games) or autonomous robots.
","Artificial intelligence, cognitive modeling, and neural networks are information processing paradigms inspired by the way biological neural systems process data. Artificial intelligence and cognitive modeling try to simulate some properties of biological neural networks.","[' Artificial intelligence, cognitive modeling, and neural networks are information processing paradigms inspired by what?', ' Artificial intelligence and cognitive modeling try to simulate what properties of biological neural networks?']","['the way biological neural systems process data', 'some properties']"
160,neural network,Overview,"Historically, digital computers evolved from the von Neumann model, and operate via the execution of explicit instructions via access to memory by a number of processors. On the other hand, the origins of neural networks are based on efforts to model information processing in biological systems. Unlike the von Neumann model, neural network computing does not separate memory and processing.
","Historically, digital computers evolved from the von Neumann model, and operate via the execution of explicit instructions via access to memory by a number of processors. On the other hand, the origins of neural networks are based on efforts to model information processing in biological systems.","[' Digital computers evolved from what model?', ' The origins of neural networks are based on efforts to model what in biological systems?']","['von Neumann model', 'information processing']"
161,neural network,History,"The preliminary theoretical base for contemporary neural networks was independently proposed by Alexander Bain (1873) and William James (1890). In their work, both thoughts and body activity resulted from interactions among neurons within the brain.
","The preliminary theoretical base for contemporary neural networks was independently proposed by Alexander Bain (1873) and William James (1890). In their work, both thoughts and body activity resulted from interactions among neurons within the brain.","[' Who independently proposed the preliminary theoretical base for contemporary neural networks?', ' What did Alexander Bain and William James work on?']","['Alexander Bain (1873) and William James', 'neural networks']"
162,neural network,History,"For Bain, every activity led to the firing of a certain set of neurons. When activities were repeated, the connections between those neurons strengthened. According to his theory, this repetition was what led to the formation of memory. The general scientific community at the time was skeptical of Bain's theory because it required what appeared to be an inordinate number of neural connections within the brain. It is now apparent that the brain is exceedingly complex and that the same brain “wiring” can handle multiple problems and inputs.
","For Bain, every activity led to the firing of a certain set of neurons. When activities were repeated, the connections between those neurons strengthened.","[' What did every activity for Bain lead to?', ' What did Bain believe was strengthened by repeated activities?']","['the firing of a certain set of neurons', 'the connections between those neurons']"
163,neural network,History,"James's theory was similar to Bain's, however, he suggested that memories and actions resulted from electrical currents flowing among the neurons in the brain. His model, by focusing on the flow of electrical currents, did not require individual neural connections for each memory or action.
","James's theory was similar to Bain's, however, he suggested that memories and actions resulted from electrical currents flowing among the neurons in the brain. His model, by focusing on the flow of electrical currents, did not require individual neural connections for each memory or action.","["" James's theory was similar to what Bain's?"", "" James' theory suggested that memories and actions resulted from electrical currents flowing among the neurons in what?"", "" What did James' model focus on?""]","['memories', 'the brain', 'the flow of electrical currents']"
164,neural network,History,"C. S. Sherrington (1898) conducted experiments to test James's theory. He ran electrical currents down the spinal cords of rats. However, instead of demonstrating an increase in electrical current as projected by James, Sherrington found that the electrical current strength decreased as the testing continued over time. Importantly, this work led to the discovery of the concept of habituation. 
",C. S. Sherrington (1898) conducted experiments to test James's theory. He ran electrical currents down the spinal cords of rats.,"["" Who conducted experiments to test James's theory?"", ' Who ran electrical currents down the spinal cords of rats?']","['C. S. Sherrington', 'C. S. Sherrington']"
165,neural network,History,"McCulloch and Pitts  (1943) created a computational model for neural networks based on mathematics and algorithms. They called this model threshold logic. The model paved the way for neural network research to split into two distinct approaches. One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence.
",McCulloch and Pitts  (1943) created a computational model for neural networks based on mathematics and algorithms. They called this model threshold logic.,"[' Who created a computational model for neural networks?', ' What did McCulloch and Pitts call their model threshold logic?']","['McCulloch and Pitts', 'computational model for neural networks']"
166,neural network,History,"In the late 1940s psychologist Donald Hebb  created a hypothesis of learning based on the mechanism of neural plasticity that is now known as Hebbian learning. Hebbian learning is considered to be a 'typical' unsupervised learning rule and its later variants were early models for long term potentiation. These ideas started being applied to computational models in 1948 with Turing's B-type machines.
",In the late 1940s psychologist Donald Hebb  created a hypothesis of learning based on the mechanism of neural plasticity that is now known as Hebbian learning. Hebbian learning is considered to be a 'typical' unsupervised learning rule and its later variants were early models for long term potentiation.,"[' In what decade did Donald Hebb create a hypothesis of learning based on neural plasticity?', ' What is Hebbian learning considered to be?']","['1940s', ""a 'typical' unsupervised learning rule""]"
167,neural network,History,"Farley and Clark (1954) first used computational machines, then called calculators, to simulate a Hebbian network at MIT. Other neural network computational machines were created by Rochester, Holland, Habit, and Duda (1956).
","Farley and Clark (1954) first used computational machines, then called calculators, to simulate a Hebbian network at MIT. Other neural network computational machines were created by Rochester, Holland, Habit, and Duda (1956).","[' When did Farley and Clark first use computational machines to simulate a Hebbian network at MIT?', ' When did Rochester, Holland, Habit, and Duda create other neural network computational machines?']","['1954', '1956']"
168,neural network,History,"Rosenblatt (1958) created the perceptron, an algorithm for pattern recognition based on a two-layer learning computer network using simple addition and subtraction. With mathematical notation, Rosenblatt also described circuitry not in the basic perceptron, such as the exclusive-or circuit, a circuit whose mathematical computation could not be processed until after the backpropagation algorithm was created by Werbos (1975).
","Rosenblatt (1958) created the perceptron, an algorithm for pattern recognition based on a two-layer learning computer network using simple addition and subtraction. With mathematical notation, Rosenblatt also described circuitry not in the basic perceptron, such as the exclusive-or circuit, a circuit whose mathematical computation could not be processed until after the backpropagation algorithm was created by Werbos (1975).","[' Who created the perceptron?', ' What was the algorithm for pattern recognition based on?', ' How did Rosenblatt describe the exclusive-or circuit?', ' Who created the backpropagation algorithm?']","['Rosenblatt', 'two-layer learning computer network', 'With mathematical notation', 'Werbos']"
169,neural network,History,Neural network research stagnated after the publication of machine learning research by Marvin Minsky and Seymour Papert (1969). They discovered two key issues with the computational machines that processed neural networks. The first issue was that single-layer neural networks were incapable of processing the exclusive-or circuit. The second significant issue was that computers were not sophisticated enough to effectively handle the long run time required by large neural networks. Neural network research slowed until computers achieved greater processing power. Also key in later advances was the backpropagation algorithm which effectively solved the exclusive-or problem (Werbos 1975).,Neural network research stagnated after the publication of machine learning research by Marvin Minsky and Seymour Papert (1969). They discovered two key issues with the computational machines that processed neural networks.,"[' When did machine learning research by Marvin Minsky and Seymour Papert begin?', ' What two key issues were discovered with the computational machines that processed neural networks?']","['1969', 'Marvin Minsky and Seymour Papert']"
170,neural network,History,"The parallel distributed processing of the mid-1980s became popular under the name connectionism. The text by Rumelhart and McClelland (1986) provided a full exposition on the use of connectionism in computers to simulate neural processes.
",The parallel distributed processing of the mid-1980s became popular under the name connectionism. The text by Rumelhart and McClelland (1986) provided a full exposition on the use of connectionism in computers to simulate neural processes.,"[' What was parallel distributed processing known as in the mid-1980s?', ' What was the name of the text written by Rumelhart and McClelland in 1986?']","['connectionism', 'connectionism']"
171,neural network,Artificial intelligence,"A neural network (NN), in the case of artificial neurons called artificial neural network (ANN) or simulated neural network (SNN), is an interconnected group of natural or artificial neurons that uses a mathematical or computational model for information processing based on a connectionistic approach to computation. In most cases an ANN is an adaptive system that changes its structure based on external or internal information that flows through the network.
","A neural network (NN), in the case of artificial neurons called artificial neural network (ANN) or simulated neural network (SNN), is an interconnected group of natural or artificial neurons that uses a mathematical or computational model for information processing based on a connectionistic approach to computation. In most cases an ANN is an adaptive system that changes its structure based on external or internal information that flows through the network.","[' What is a simulated neural network?', ' What is an interconnected group of natural or artificial neurons called?', ' What is an ANN an adaptive system that changes its structure based on external or internal information?']","['an interconnected group of natural or artificial neurons', 'A neural network', 'neural network']"
172,neural network,Artificial intelligence,"In more practical terms neural networks are non-linear statistical data modeling or decision making tools. They can be used to model complex relationships between inputs and outputs or to find patterns in data.
",In more practical terms neural networks are non-linear statistical data modeling or decision making tools. They can be used to model complex relationships between inputs and outputs or to find patterns in data.,"[' What are neural networks?', ' What can neural networks be used for?']","['non-linear statistical data modeling or decision making tools', 'to model complex relationships between inputs and outputs or to find patterns in data']"
173,neural network,Artificial intelligence,"An artificial neural network involves a network of simple processing elements (artificial neurons) which can exhibit complex global behavior, determined by the connections between the processing elements and element parameters. Artificial neurons were first proposed in 1943 by Warren McCulloch, a neurophysiologist, and Walter Pitts, a logician, who first collaborated at the University of Chicago.","An artificial neural network involves a network of simple processing elements (artificial neurons) which can exhibit complex global behavior, determined by the connections between the processing elements and element parameters. Artificial neurons were first proposed in 1943 by Warren McCulloch, a neurophysiologist, and Walter Pitts, a logician, who first collaborated at the University of Chicago.","[' What is a network of simple processing elements called?', ' What can exhibit complex global behavior?', ' When were artificial neurons first proposed?', ' Who proposed artificial neurons?', ' Who was a neurophysiologist?', ' Where did Walter Pitts work?', "" What was Pitts' profession?""]","['artificial neurons', 'An artificial neural network involves a network of simple processing elements (artificial neurons', '1943', 'Warren McCulloch, a neurophysiologist, and Walter Pitts', 'Warren McCulloch', 'University of Chicago', 'logician']"
174,neural network,Artificial intelligence,"The utility of artificial neural network models lies in the fact that they can be used to infer a function from observations and also to use it. Unsupervised neural networks can also be used to learn representations of the input that capture the salient characteristics of the input distribution, e.g., see the Boltzmann machine (1983), and more recently, deep learning algorithms, which can implicitly learn the distribution function of the observed data. Learning in neural networks is particularly useful in applications where the complexity of the data or task makes the design of such functions by hand impractical.
","The utility of artificial neural network models lies in the fact that they can be used to infer a function from observations and also to use it. Unsupervised neural networks can also be used to learn representations of the input that capture the salient characteristics of the input distribution, e.g., see the Boltzmann machine (1983), and more recently, deep learning algorithms, which can implicitly learn the distribution function of the observed data.","[' What is the utility of artificial neural network models?', ' What can unsupervised neural networks learn representations of?', ' What machine captures salient characteristics of the input distribution?', ' What can implicitly learn the distribution function of the observed data?']","['they can be used to infer a function from observations and also to use it', 'the input that capture the salient characteristics of the input distribution', 'Boltzmann machine', 'deep learning algorithms']"
175,neural network,Applications,"Neural networks can be used in different fields. The tasks to which artificial neural networks are applied tend to fall within the following broad categories:
",Neural networks can be used in different fields. The tasks to which artificial neural networks are applied tend to fall within the following broad categories:,"[' What can be used in different fields?', ' What are the tasks to which artificial neural networks are applied?']","['Neural networks', 'broad categories']"
176,neural network,Applications,"Application areas of ANNs include nonlinear system identification and control (vehicle control, process control), game-playing and decision making (backgammon, chess, racing), pattern recognition (radar systems, face identification, object recognition), sequence recognition (gesture, speech, handwritten text recognition), medical diagnosis, financial applications, data mining (or knowledge discovery in databases, ""KDD""), visualization and e-mail spam filtering. For example, it is possible to create a semantic profile of user's interests emerging from pictures trained for object recognition.","Application areas of ANNs include nonlinear system identification and control (vehicle control, process control), game-playing and decision making (backgammon, chess, racing), pattern recognition (radar systems, face identification, object recognition), sequence recognition (gesture, speech, handwritten text recognition), medical diagnosis, financial applications, data mining (or knowledge discovery in databases, ""KDD""), visualization and e-mail spam filtering. For example, it is possible to create a semantic profile of user's interests emerging from pictures trained for object recognition.","[' What are nonlinear system identification and control?', ' What are game-playing and decision making applications of ANNs?', ' Data mining, visualization and e-mail spam filtering are examples of what?', ' What is the term for knowledge discovery in databases?']","['vehicle control, process control', 'backgammon, chess, racing', 'ANNs', 'KDD']"
177,neural network,Neuroscience,"Theoretical and computational neuroscience is the field concerned with the analysis and computational modeling of biological neural systems.
Since neural systems are intimately related to cognitive processes and behaviour, the field is closely related to cognitive and behavioural modeling.
","Theoretical and computational neuroscience is the field concerned with the analysis and computational modeling of biological neural systems. Since neural systems are intimately related to cognitive processes and behaviour, the field is closely related to cognitive and behavioural modeling.","[' What is the field concerned with the analysis and computational modeling of biological neural systems?', ' Theoretical and computational neuroscience is closely related to what?']","['Theoretical and computational neuroscience', 'cognitive and behavioural modeling']"
178,neural network,Neuroscience,"The aim of the field is to create models of biological neural systems in order to understand how biological systems work. To gain this understanding, neuroscientists strive to make a link between observed biological processes (data), biologically plausible mechanisms for neural processing and learning (biological neural network models) and theory (statistical learning theory and information theory).
","The aim of the field is to create models of biological neural systems in order to understand how biological systems work. To gain this understanding, neuroscientists strive to make a link between observed biological processes (data), biologically plausible mechanisms for neural processing and learning (biological neural network models) and theory (statistical learning theory and information theory).","[' What is the aim of the field?', ' What do neuroscientists strive to make a link between?', ' What are neural processing and learning?', ' What are statistical learning theory and information theory?']","['to create models of biological neural systems', 'observed biological processes', 'biological neural network models', 'theory']"
179,neural network,Criticism,"A common criticism of neural networks, particularly in robotics, is that they require a large diversity of training samples for real-world operation. This is not surprising, since any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Dean Pomerleau, in his research presented in the paper ""Knowledge-based Training of Artificial Neural Networks for Autonomous Robot Driving,"" uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane, multi-lane, dirt, etc.). A large amount of his research is devoted to (1) extrapolating multiple training scenarios from a single training experience, and (2) preserving past training diversity so that the system does not become overtrained (if, for example, it is presented with a series of right turns—it should not learn to always turn right). These issues are common in neural networks that must decide from amongst a wide variety of responses, but can be dealt with in several ways, for example by randomly shuffling the training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, or by grouping examples in so-called mini-batches.
","A common criticism of neural networks, particularly in robotics, is that they require a large diversity of training samples for real-world operation. This is not surprising, since any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases.","[' What is a common criticism of neural networks?', ' What does a learning machine need to capture to allow it to generalize to new cases?']","['they require a large diversity of training samples for real-world operation', 'the underlying structure']"
180,neural network,Criticism,"Arguments for Dewdney's position are that to implement large and effective software neural networks, much processing and storage resources need to be committed. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a most simplified form on Von Neumann technology may compel a neural network designer to fill many millions of database rows for its connections—which can consume vast amounts of computer memory and data storage capacity. Furthermore, the designer of neural network systems will often need to simulate the transmission of signals through many of these connections and their associated neurons—which must often be matched with incredible amounts of CPU processing power and time. While neural networks often yield effective programs, they too often do so at the cost of efficiency (they tend to consume considerable amounts of time and money).
","Arguments for Dewdney's position are that to implement large and effective software neural networks, much processing and storage resources need to be committed. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a most simplified form on Von Neumann technology may compel a neural network designer to fill many millions of database rows for its connections—which can consume vast amounts of computer memory and data storage capacity.","["" Dewdney's argument is that to implement large and effective software neural networks, how much processing and storage resources need to be committed?"", ' The brain has hardware tailored to the task of processing signals through what?', ' How many database rows does a neural network designer have to fill for its connections?', ' What can consume vast amounts of computer memory?']","['much', 'a graph of neurons', 'millions', 'database rows']"
181,neural network,Criticism,"Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be ""an opaque, unreadable table...valueless as a scientific resource"".
","Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be ""an opaque, unreadable table...valueless as a scientific resource"".","[' Why are neural networks in the dock?', ' What could you create a successful net without understanding how it worked?', ' What would its behavior be in all probability?', ' What would it be?']","[""they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked"", 'Neural networks', '""an opaque, unreadable table...valueless as a scientific resource"".', 'an opaque, unreadable table...valueless as a scientific resource"".']"
182,neural network,Criticism,"In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.","In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.","[' What does Dewdney think of neural nets as?', ' What is an unreadable table that a useful machine could read?']","['bad science', 'neural nets']"
183,neural network,Criticism,"Although it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover, recent emphasis on the explainability of AI has contributed towards the development of methods, notably those based on attention mechanisms, for visualizing and explaining learned neural networks. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful. For example, Bengio and LeCun (2007) wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture.","Although it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover, recent emphasis on the explainability of AI has contributed towards the development of methods, notably those based on attention mechanisms, for visualizing and explaining learned neural networks.","[' What is much easier to do than to analyze what has been learned by a biological neural network?', ' Recent emphasis on the explainability of AI has contributed towards the development of what?', ' What has contributed to the development of methods for visualizing and explaining learned neural networks?']","['analyzing what has been learned by an artificial neural network', 'methods', 'recent emphasis on the explainability of AI']"
184,neural network,Criticism,"Some other criticisms came from believers of hybrid models (combining neural networks and symbolic approaches). They advocate the intermix of these two approaches and believe that hybrid models can better capture the mechanisms of the human mind (Sun and Bookman, 1990).","Some other criticisms came from believers of hybrid models (combining neural networks and symbolic approaches). They advocate the intermix of these two approaches and believe that hybrid models can better capture the mechanisms of the human mind (Sun and Bookman, 1990).","[' What is a hybrid model?', ' What do hybrid models combine to better capture the human mind?']","['combining neural networks and symbolic approaches', 'neural networks and symbolic approaches']"
185,neural network,Recent improvements,"Biophysical models, such as BCM theory, have been important in understanding mechanisms for synaptic plasticity, and have had applications in both computer science and neuroscience. Research is ongoing in understanding the computational algorithms used in the brain, with some recent biological evidence for radial basis networks and neural backpropagation as mechanisms for processing data.","Biophysical models, such as BCM theory, have been important in understanding mechanisms for synaptic plasticity, and have had applications in both computer science and neuroscience. Research is ongoing in understanding the computational algorithms used in the brain, with some recent biological evidence for radial basis networks and neural backpropagation as mechanisms for processing data.","[' What has been important in understanding mechanisms for synaptic plasticity?', ' What has BCM theory had applications in both computer science and neuroscience?', ' Research is ongoing in understanding what?', ' What does biological evidence show about neural backpropagation and radial basis networks?']","['Biophysical models', 'understanding mechanisms for synaptic plasticity', 'computational algorithms used in the brain', 'mechanisms for processing data']"
186,neural network,Recent improvements,"Computational devices have been created in CMOS for both biophysical simulation and neuromorphic computing. More recent efforts show promise for creating nanodevices for very large scale principal components analyses and convolution. If successful, these efforts could usher in a new era of neural computing that is a step beyond digital computing, because it depends on learning rather than programming and because it is fundamentally analog rather than digital even though the first instantiations may in fact be with CMOS digital devices.
",Computational devices have been created in CMOS for both biophysical simulation and neuromorphic computing. More recent efforts show promise for creating nanodevices for very large scale principal components analyses and convolution.,"[' What has been created in CMOS for both biophysical simulation and neuromorphic computing?', ' Recent efforts show promise for creating nanodevices for what?']","['Computational devices', 'very large scale principal components analyses and convolution']"
187,neural network,Recent improvements,"Between 2009 and 2012, the recurrent neural networks and deep feedforward neural networks developed in the research group of Jürgen Schmidhuber at the Swiss AI Lab IDSIA have won eight international competitions in pattern recognition and machine learning. For example, multi-dimensional long short term memory (LSTM) won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three different languages to be learned.
","Between 2009 and 2012, the recurrent neural networks and deep feedforward neural networks developed in the research group of Jürgen Schmidhuber at the Swiss AI Lab IDSIA have won eight international competitions in pattern recognition and machine learning. For example, multi-dimensional long short term memory (LSTM) won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three different languages to be learned.","[' How many international competitions have the recurrent neural networks and deep feedforward neural networks won between 2009 and 2012?', ' What did LSTM win three competitions in?', ' Where is Jürgen Schmidhuber located?', ' How many competitions did LSTM win in connected handwriting recognition?', ' What was the name of the 2009 International Conference on Document Analysis and Recognition (ICDAR)?']","['eight', 'connected handwriting recognition', 'Swiss', 'three', 'multi-dimensional long short term memory (LSTM) won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three different languages to be learned']"
188,neural network,Recent improvements,Radial basis function and wavelet networks have also been introduced. These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications.,Radial basis function and wavelet networks have also been introduced. These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications.,"[' What have been introduced to offer best approximation properties?', ' What are wavelet networks used for?']","['Radial basis function and wavelet networks', 'nonlinear system identification and classification applications']"
189,neural network,Recent improvements,"Deep learning feedforward networks alternate convolutional layers and max-pooling layers, topped by several pure classification layers. Fast GPU-based implementations of this approach have won several pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition and the  ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge. Such neural networks also were the first artificial pattern recognizers to achieve human-competitive or even superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012), or the MNIST handwritten digits problem of Yann LeCun and colleagues at NYU.
","Deep learning feedforward networks alternate convolutional layers and max-pooling layers, topped by several pure classification layers. Fast GPU-based implementations of this approach have won several pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition and the  ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge.","[' Deep learning feedforward networks alternate what layers?', ' Fast GPU-based implementations of what approach have won several pattern recognition contests?', ' What was the IJCNN 2011 Traffic Sign Recognition Competition?']","['convolutional layers and max-pooling layers', 'Deep learning feedforward networks', 'Fast GPU-based implementations of this approach have won several pattern recognition contests']"
190,multiagent system,Summary,"A multi-agent system (MAS or ""self-organized system"") is a computerized system composed of multiple interacting intelligent agents. Multi-agent systems can solve problems that are difficult or impossible for an individual agent or a monolithic system to solve. Intelligence may include methodic, functional, procedural approaches, algorithmic search or reinforcement learning.","A multi-agent system (MAS or ""self-organized system"") is a computerized system composed of multiple interacting intelligent agents. Multi-agent systems can solve problems that are difficult or impossible for an individual agent or a monolithic system to solve.","[' What does MAS stand for?', ' What is a multi-agent system composed of?']","['multi-agent system', 'multiple interacting intelligent agents']"
191,multiagent system,Summary,"Despite considerable overlap, a multi-agent system is not always the same as an agent-based model (ABM).  The goal of an ABM is to search for explanatory insight into the collective behavior of agents (which don't necessarily need to be ""intelligent"") obeying simple rules, typically in natural systems, rather than in solving specific practical or engineering problems. The terminology of ABM tends to be used more often in the science, and MAS in engineering and technology. Applications where multi-agent systems research may deliver an appropriate approach include online trading, disaster response, target surveillance   and social structure modelling.","Despite considerable overlap, a multi-agent system is not always the same as an agent-based model (ABM). The goal of an ABM is to search for explanatory insight into the collective behavior of agents (which don't necessarily need to be ""intelligent"") obeying simple rules, typically in natural systems, rather than in solving specific practical or engineering problems.","[' A multi-agent system is not always the same as what?', ' What is the goal of an ABM?', ' ABM is to search for explanatory insight into the collective behavior of agents.', ' What is the most common way to obey simple rules?', ' What is more common in natural systems?']","['an agent-based model', 'to search for explanatory insight into the collective behavior of agents', 'agent-based model', 'natural systems', 'simple rules']"
192,multiagent system,Concept,"Multi-agent systems consist of agents and their environment. Typically multi-agent systems research refers to software agents. However, the agents in a multi-agent system could equally well be robots, humans or human teams. A multi-agent system may contain combined human-agent teams.
",Multi-agent systems consist of agents and their environment. Typically multi-agent systems research refers to software agents.,"[' What do multi-agent systems consist of?', ' What does research refer to?']","['agents and their environment', 'software agents']"
193,multiagent system,Concept,"Agents can be divided into types spanning simple to complex. Categories include:
",Agents can be divided into types spanning simple to complex. Categories include:,"[' What can agents be divided into?', ' What types of agents are there?']","['simple to complex', 'simple to complex']"
194,multiagent system,Concept,"Agent environments can also be organized according to properties such as accessibility (whether it is possible to gather complete information about the environment), determinism (whether an action causes a definite effect), dynamics (how many entities influence the environment in the moment), discreteness (whether the number of possible actions in the environment is finite), episodicity (whether agent actions in certain time periods influence other periods), and dimensionality (whether spatial characteristics are important factors of the environment and the agent considers space in its decision making). Agent actions are typically mediated via an appropriate middleware. This middleware offers a first-class design abstraction for multi-agent systems, providing means to govern resource access and agent coordination.","Agent environments can also be organized according to properties such as accessibility (whether it is possible to gather complete information about the environment), determinism (whether an action causes a definite effect), dynamics (how many entities influence the environment in the moment), discreteness (whether the number of possible actions in the environment is finite), episodicity (whether agent actions in certain time periods influence other periods), and dimensionality (whether spatial characteristics are important factors of the environment and the agent considers space in its decision making). Agent actions are typically mediated via an appropriate middleware.","[' What is accessibility?', ' What is determinism?', ' How many entities influence the environment in the moment?', ' What is discreteness?', ' What is episodicity?', ' How are agent actions typically mediated?', ' What type of middleware is used?']","['whether it is possible to gather complete information about the environment', 'whether an action causes a definite effect', 'dynamics (how many', 'whether the number of possible actions in the environment is finite', 'whether agent actions in certain time periods influence other periods', 'via an appropriate middleware', 'appropriate']"
195,multiagent system,Research,"The study of multi-agent systems is ""concerned with the development and analysis of sophisticated AI problem-solving and control architectures for both single-agent and multiple-agent systems."" Research topics include:
","The study of multi-agent systems is ""concerned with the development and analysis of sophisticated AI problem-solving and control architectures for both single-agent and multiple-agent systems."" Research topics include:","[' What is the study of multi-agent systems concerned with?', ' What is one of the research topics?']","['the development and analysis of sophisticated AI problem-solving and control architectures for both single-agent and multiple-agent systems', 'multi-agent systems']"
196,multiagent system,Frameworks,"Frameworks have emerged that implement common standards (such as the FIPA and OMG MASIF standards). These frameworks e.g. JADE, save time and aid in the standardization of MAS development.",Frameworks have emerged that implement common standards (such as the FIPA and OMG MASIF standards). These frameworks e.g.,"[' What are examples of common standards?', ' What are two common standards implemented by frameworks?']","['FIPA and OMG MASIF standards', 'FIPA and OMG MASIF standards']"
197,multiagent system,Frameworks,"Currently though, no standard is actively maintained from FIPA or OMG. Efforts for further development of software agents in industrial context are carried out in IEEE IES technical committee on Industrial Agents.","Currently though, no standard is actively maintained from FIPA or OMG. Efforts for further development of software agents in industrial context are carried out in IEEE IES technical committee on Industrial Agents.","[' What is the current status of FIPA or OMG?', ' Who is responsible for further development of software agents in industrial context?']","['no standard is actively maintained', 'IEEE IES technical committee on Industrial Agents']"
198,multiagent system,Applications,"MAS have not only been applied in academic research, but also in industry. MAS are applied in the real world to graphical applications such as computer games. Agent systems have been used in films. It is widely advocated for use in networking and mobile technologies, to achieve automatic and dynamic load balancing, high scalability and self-healing networks. They are being used for coordinated defence systems.
","MAS have not only been applied in academic research, but also in industry. MAS are applied in the real world to graphical applications such as computer games.","[' MAS have not only been applied in academic research but also in what industry?', ' MAS are applied in the real world to what kind of applications?']","['industry', 'graphical']"
199,multiagent system,Applications,"Also, Multi-agent Systems Artificial Intelligence (MAAI) are used for simulating societies, the purpose thereof being helpful in the fields of climate, energy, epidemiology, conflict management, child abuse, .... Some organisations working on using multi-agent system models include Center for Modelling Social Systems, Centre for Research in Social Simulation, Centre for Policy Modelling, Society for Modelling and Simulation International. Hallerbach et al. discussed the application of agent-based approaches for the development and validation of automated driving systems via a digital twin of the vehicle-under-test and microscopic traffic simulation based on independent agents. Waymo has created a multi-agent simulation environment Carcraft to test algorithms for self-driving cars. It simulates traffic interactions between human drivers, pedestrians and automated vehicles. People's behavior is imitated by artificial agents based on data of real human behavior.
","Also, Multi-agent Systems Artificial Intelligence (MAAI) are used for simulating societies, the purpose thereof being helpful in the fields of climate, energy, epidemiology, conflict management, child abuse, .... Some organisations working on using multi-agent system models include Center for Modelling Social Systems, Centre for Research in Social Simulation, Centre for Policy Modelling, Society for Modelling and Simulation International.","[' What are Multi-agent Systems Artificial Intelligence (MAAI) used for?', ' What is the purpose of MAAI?', ' Centre for Research in Social Simulation, Centre for Policy Modelling, Society for Modelling and Simulation International.']","['simulating societies', 'simulating societies', 'multi-agent system models']"
200,simulation,Summary,"A simulation is the imitation of the operation of a real-world process or system over time. Simulations require the use of models; the model represents the key characteristics or behaviors of the selected system or process, whereas the simulation represents the evolution of the model over time. Often, computers are used to execute the simulation.
","A simulation is the imitation of the operation of a real-world process or system over time. Simulations require the use of models; the model represents the key characteristics or behaviors of the selected system or process, whereas the simulation represents the evolution of the model over time.","[' What is the term for the imitation of the operation of a real-world process or system over time?', ' Simulations require the use of what?', ' What represents the key characteristics or behaviors of the selected system or process?', ' The simulation represents the evolution of the model over time.']","['simulation', 'models', 'the model', 'simulation']"
201,simulation,Summary,"Simulation is used in many contexts, such as simulation of technology for performance tuning or optimizing, safety engineering, testing, training, education, and video games. Simulation is also used with scientific modelling of natural systems or human systems to gain insight into their functioning, as in economics. Simulation can be used to show the eventual real effects of alternative conditions and courses of action. Simulation is also used when the real system cannot be engaged, because it may not be accessible, or it may be dangerous or unacceptable to engage, or it is being designed but not yet built, or it may simply not exist.","Simulation is used in many contexts, such as simulation of technology for performance tuning or optimizing, safety engineering, testing, training, education, and video games. Simulation is also used with scientific modelling of natural systems or human systems to gain insight into their functioning, as in economics.","[' What is a common use of simulation?', ' What is an example of a context in which simulation is used?', ' How is simulation used in economics?']","['simulation of technology for performance tuning or optimizing, safety engineering, testing, training, education, and video games', 'simulation of technology for performance tuning or optimizing, safety engineering, testing, training, education, and video games', 'scientific modelling of natural systems or human systems to gain insight into their functioning']"
202,simulation,Summary,"Key issues in modeling and simulation include the acquisition of valid sources of information about the relevant selection of key characteristics and behaviors used to build the model, the use of simplifying approximations and assumptions within the model, and fidelity and validity of the simulation outcomes. Procedures and protocols for model verification and validation are an ongoing field of academic study, refinement, research and development in simulations technology or practice, particularly in the work of computer simulation.
","Key issues in modeling and simulation include the acquisition of valid sources of information about the relevant selection of key characteristics and behaviors used to build the model, the use of simplifying approximations and assumptions within the model, and fidelity and validity of the simulation outcomes. Procedures and protocols for model verification and validation are an ongoing field of academic study, refinement, research and development in simulations technology or practice, particularly in the work of computer simulation.","[' What are some of the key issues in modeling and simulation?', ' What is one way to acquire valid sources of information about the relevant selection of key characteristics and behaviors?', ' What is an ongoing field of academic study, refinement, research and development in simulations technology or practice?']","['the acquisition of valid sources of information about the relevant selection of key characteristics and behaviors used to build the model, the use of simplifying approximations and assumptions within the model, and fidelity and validity of the simulation outcomes', 'build the model', 'Procedures and protocols for model verification and validation']"
203,simulation,Classification and terminology,"Physical simulation refers to simulation in which physical objects are substituted for the real thing (some circles use the term for computer simulations modelling selected lawsphysics, but this article does not). These physical objects are often chosen because they are smaller or cheaper than the actual object or system.
","Physical simulation refers to simulation in which physical objects are substituted for the real thing (some circles use the term for computer simulations modelling selected lawsphysics, but this article does not). These physical objects are often chosen because they are smaller or cheaper than the actual object or system.","[' What is a simulation in which physical objects are substituted for the real thing?', ' Some circles use the term for what?', ' Why are physical objects chosen?']","['Physical simulation', 'computer simulations', 'they are smaller or cheaper than the actual object or system']"
204,simulation,Classification and terminology,"Discrete-event simulation studies systems whose states change their values only at discrete times. For example, a simulation of an epidemic could change the number of infected people at time instants when susceptible individuals get infected or when infected individuals recover.
","Discrete-event simulation studies systems whose states change their values only at discrete times. For example, a simulation of an epidemic could change the number of infected people at time instants when susceptible individuals get infected or when infected individuals recover.","[' Discrete-event simulation studies systems whose states change their values only at discrete times?', ' A simulation of an epidemic could change the number of infected people at time instants when what happens?']","['Discrete-event simulation studies systems whose states change their values only at discrete times.', 'susceptible individuals get infected or when infected individuals recover']"
205,simulation,Classification and terminology,Stochastic simulation is a simulation where some variable or process is subject to random variations and is projected using Monte Carlo techniques using pseudo-random numbers. Thus replicated runs with the same boundary conditions will each produce different results within a specific confidence band.,Stochastic simulation is a simulation where some variable or process is subject to random variations and is projected using Monte Carlo techniques using pseudo-random numbers. Thus replicated runs with the same boundary conditions will each produce different results within a specific confidence band.,"[' What is a stochastic simulation where some variable or process is subject to random variations and is projected using Monte Carlo techniques using pseudo-random numbers?', ' Replicated runs with the same boundary conditions will each produce different results within what?']","['Stochastic simulation', 'a specific confidence band']"
206,simulation,Classification and terminology,"Deterministic simulation is a simulation which is not stochastic: thus the variables are regulated by deterministic algorithms. So replicated runs from the same boundary conditions always produce identical results.
",Deterministic simulation is a simulation which is not stochastic: thus the variables are regulated by deterministic algorithms. So replicated runs from the same boundary conditions always produce identical results.,"[' What is a simulation that is not stochastic?', ' The variables are regulated by what?', ' Replicated runs from the same boundary conditions always produce identical results?']","['Deterministic simulation', 'deterministic algorithms', 'Deterministic simulation']"
207,simulation,Classification and terminology,"A distributed simulation is one which uses more than one computer simultaneously, to guarantee access from/to different resources (e.g. multi-users operating different systems, or distributed data sets); a classical example is Distributed Interactive Simulation (DIS).","A distributed simulation is one which uses more than one computer simultaneously, to guarantee access from/to different resources (e.g. multi-users operating different systems, or distributed data sets); a classical example is Distributed Interactive Simulation (DIS).","[' What is a distributed simulation?', ' What does DIS stand for?']","['one which uses more than one computer simultaneously', 'Distributed Interactive Simulation']"
208,simulation,Classification and terminology,"Modeling, interoperable simulation and serious games is where serious game approaches (e.g. game engines and engagement methods) are integrated with interoperable simulation.","Modeling, interoperable simulation and serious games is where serious game approaches (e.g. game engines and engagement methods) are integrated with interoperable simulation.","[' Modeling, interoperable simulation and serious games are where serious game approaches (e.g. game engines and engagement methods) are integrated with what?']",['interoperable simulation']
209,simulation,Classification and terminology,"Simulation fidelity is used to describe the accuracy of a simulation and how closely it imitates the real-life counterpart. Fidelity is broadly classified as one of three categories: low, medium, and high. Specific descriptions of fidelity levels are subject to interpretation, but the following generalizations can be made:
","Simulation fidelity is used to describe the accuracy of a simulation and how closely it imitates the real-life counterpart. Fidelity is broadly classified as one of three categories: low, medium, and high.","[' What is used to describe the accuracy of a simulation and how closely it imitates the real-life counterpart?', ' What are the three categories of fidelity?']","['Simulation fidelity', 'low, medium, and high']"
210,simulation,Classification and terminology,"Simulation in failure analysis refers to simulation in which we create environment/conditions to identify the cause of equipment failure. This can be the best and fastest method to identify the failure cause.
",Simulation in failure analysis refers to simulation in which we create environment/conditions to identify the cause of equipment failure. This can be the best and fastest method to identify the failure cause.,"[' What does simulation in failure analysis refer to?', ' What can be the best and fastest method to identify the failure cause?']","['we create environment/conditions to identify the cause of equipment failure', 'Simulation']"
211,simulation,Computer simulation,"A computer simulation (or ""sim"") is an attempt to model a real-life or hypothetical situation on a computer so that it can be studied to see how the system works. By changing variables in the simulation, predictions may be made about the behaviour of the system. It is a tool to virtually investigate the behaviour of the system under study.","A computer simulation (or ""sim"") is an attempt to model a real-life or hypothetical situation on a computer so that it can be studied to see how the system works. By changing variables in the simulation, predictions may be made about the behaviour of the system.","[' What is an attempt to model a real-life or hypothetical situation on a computer so that it can be studied?', ' By changing variables in the simulation, what can be made about the behaviour of the system?']","['A computer simulation', 'predictions']"
212,simulation,Computer simulation,"Computer simulation has become a useful part of modeling many natural systems in physics, chemistry and biology, and human systems in economics and social science (e.g., computational sociology) as well as in engineering to gain insight into the operation of those systems. A good example of the usefulness of using computers to simulate can be found in the field of network traffic simulation. In such simulations, the model behaviour will change each simulation according to the set of initial parameters assumed for the environment.
","Computer simulation has become a useful part of modeling many natural systems in physics, chemistry and biology, and human systems in economics and social science (e.g., computational sociology) as well as in engineering to gain insight into the operation of those systems. A good example of the usefulness of using computers to simulate can be found in the field of network traffic simulation.","[' What has become a useful part of modeling many natural systems?', ' What is a good example of the usefulness of using computer simulation?', ' What is a good example of the usefulness of using computers to simulate?']","['Computer simulation', 'network traffic simulation', 'network traffic simulation']"
213,simulation,Computer simulation,"Traditionally, the formal modeling of systems has been via a mathematical model, which attempts to find analytical solutions enabling the prediction of the behaviour of the system from a set of parameters and initial conditions. Computer simulation is often used as an adjunct to, or substitution for, modeling systems for which simple closed form analytic solutions are not possible. There are many different types of computer simulation, the common feature they all share is the attempt to generate a sample of representative scenarios for a model in which a complete enumeration of all possible states would be prohibitive or impossible.
","Traditionally, the formal modeling of systems has been via a mathematical model, which attempts to find analytical solutions enabling the prediction of the behaviour of the system from a set of parameters and initial conditions. Computer simulation is often used as an adjunct to, or substitution for, modeling systems for which simple closed form analytic solutions are not possible.","[' What has traditionally been the formal modeling of systems via?', ' What attempts to find analytical solutions enabling the prediction of the behaviour of the system from a set of parameters and initial conditions?', ' Computer simulation is often used as what?', ' As an adjunct to, or substitution for, modeling systems for which closed form analytic solutions are not possible?']","['a mathematical model', 'mathematical model', 'an adjunct to, or substitution for, modeling systems', 'Computer simulation']"
214,simulation,Computer simulation,"Several software packages exist for running computer-based simulation modeling (e.g. Monte Carlo simulation, stochastic modeling, multimethod modeling) that makes all the modeling almost effortless.
","Several software packages exist for running computer-based simulation modeling (e.g. Monte Carlo simulation, stochastic modeling, multimethod modeling) that makes all the modeling almost effortless.","[' Several software packages exist for running what kind of modeling?', ' What makes all the modeling almost effortless?']","['computer-based simulation', 'Several software packages exist for running computer-based simulation modeling']"
215,simulation,Simulation in education and training,"Simulation is extensively used for educational purposes. It is used for cases where it is prohibitively expensive or simply too dangerous to allow trainees to use the real equipment in the real world. In such situations they will spend time learning valuable lessons in a ""safe"" virtual environment yet living a lifelike experience (or at least it is the goal). Often the convenience is to permit mistakes during training for a safety-critical system.
",Simulation is extensively used for educational purposes. It is used for cases where it is prohibitively expensive or simply too dangerous to allow trainees to use the real equipment in the real world.,"[' What is extensively used for educational purposes?', ' What is used when it is prohibitively expensive or simply too dangerous to allow trainees to use real equipment in the real world?']","['Simulation', 'Simulation']"
216,simulation,Simulation in education and training,"Simulations in education are somewhat like training simulations. They focus on specific tasks. The term 'microworld' is used to refer to educational simulations which model some abstract concept rather than simulating a realistic object or environment, or in some cases model a real-world environment in a simplistic way so as to help a learner develop an understanding of the key concepts. Normally, a user can create some sort of construction within the microworld that will behave in a way consistent with the concepts being modeled. Seymour Papert was one of the first to advocate the value of microworlds, and the Logo programming environment developed by Papert is one of the most well-known microworlds.
",Simulations in education are somewhat like training simulations. They focus on specific tasks.,"[' What are simulations in education similar to?', ' What do simulations focus on?']","['training simulations', 'specific tasks']"
217,simulation,Simulation in education and training,Project management simulation is increasingly used to train students and professionals in the art and science of project management. Using simulation for project management training improves learning retention and enhances the learning process.,Project management simulation is increasingly used to train students and professionals in the art and science of project management. Using simulation for project management training improves learning retention and enhances the learning process.,"[' What is increasingly used to train students and professionals in the art and science of project management?', ' Using simulation for project management training improves learning retention and enhances the learning process?']","['Project management simulation', 'Project management simulation is increasingly used to train students and professionals in the art and science of project management']"
218,simulation,Simulation in education and training,"Social simulations may be used in social science classrooms to illustrate social and political processes in anthropology, economics, history, political science, or sociology courses, typically at the high school or university level. These may, for example, take the form of civics simulations, in which participants assume roles in a simulated society, or international relations simulations in which participants engage in negotiations, alliance formation, trade, diplomacy, and the use of force. Such simulations might be based on fictitious political systems, or be based on current or historical events. An example of the latter would be Barnard College's Reacting to the Past series of historical educational games. The National Science Foundation has also supported the creation of reacting games that address science and math education. In social media simulations, participants train communication with critics and other stakeholders in a private environment.
","Social simulations may be used in social science classrooms to illustrate social and political processes in anthropology, economics, history, political science, or sociology courses, typically at the high school or university level. These may, for example, take the form of civics simulations, in which participants assume roles in a simulated society, or international relations simulations in which participants engage in negotiations, alliance formation, trade, diplomacy, and the use of force.","[' What type of simulations can be used in social science classrooms to illustrate social and political processes in anthropology, economics, history, political science, or sociology courses?', ' Social simulations may take the form of what?', ' What do participants assume in a simulated civics simulation?', ' What are civics simulations?', ' What are international relations simulations called?']","['Social simulations', 'civics simulations', 'roles in a simulated society', 'participants assume roles in a simulated society', 'civics simulations']"
219,simulation,Simulation in education and training,"In recent years, there has been increasing use of social simulations for staff training in aid and development agencies. The Carana simulation, for example, was first developed by the United Nations Development Programme, and is now used in a very revised form by the World Bank for training staff to deal with fragile and conflict-affected countries.","In recent years, there has been increasing use of social simulations for staff training in aid and development agencies. The Carana simulation, for example, was first developed by the United Nations Development Programme, and is now used in a very revised form by the World Bank for training staff to deal with fragile and conflict-affected countries.","[' What has been increasing in recent years for staff training in aid and development agencies?', ' Who first developed the Carana simulation?', ' What is now used in a revised form by the World Bank for training staff?', ' Who revised the form for training staff to deal with fragile and conflict-affected countries?']","['social simulations', 'United Nations Development Programme', 'The Carana simulation', 'World Bank']"
220,simulation,Simulation in education and training,"Military uses for simulation often involve aircraft or armoured fighting vehicles, but can also target small arms and other weapon systems training. Specifically, virtual firearms ranges have become the norm in most military training processes and there is a significant amount of data to suggest this is a useful tool for armed professionals.","Military uses for simulation often involve aircraft or armoured fighting vehicles, but can also target small arms and other weapon systems training. Specifically, virtual firearms ranges have become the norm in most military training processes and there is a significant amount of data to suggest this is a useful tool for armed professionals.","[' Military uses for simulation often involve what?', ' Virtual firearms ranges have become the norm in most military training processes?', ' How much data do you have to suggest this tool is useful for?']","['aircraft or armoured fighting vehicles', 'there is a significant amount of data to suggest this is a useful tool for armed professionals', 'armed professionals']"
221,simulation,Virtual simulation,"A virtual simulation is a category of simulation that uses simulation equipment to create a simulated world for the user. Virtual simulations allow users to interact with a virtual world. Virtual worlds operate on platforms of integrated software and hardware components. In this manner, the system can accept input from the user (e.g., body tracking, voice/sound recognition, physical controllers) and produce output to the user (e.g., visual display, aural display, haptic display) . Virtual simulations use the aforementioned modes of interaction to produce a sense of immersion for the user.
",A virtual simulation is a category of simulation that uses simulation equipment to create a simulated world for the user. Virtual simulations allow users to interact with a virtual world.,"[' What is a category of simulation that uses simulation equipment to create a simulated world for the user?', ' Virtual simulations allow users to interact with what?']","['virtual simulation', 'a virtual world']"
222,simulation,Clinical healthcare simulators,"Clinical healthcare simulators are increasingly being developed and deployed to teach therapeutic and diagnostic procedures as well as medical concepts and decision making to personnel in the health professions. Simulators have been developed for training procedures ranging from the basics such as blood draw, to laparoscopic surgery and trauma care. They are also important to help on prototyping new devices for biomedical engineering problems. Currently, simulators are applied to research and develop tools for new therapies, treatments and early diagnosis in medicine.
","Clinical healthcare simulators are increasingly being developed and deployed to teach therapeutic and diagnostic procedures as well as medical concepts and decision making to personnel in the health professions. Simulators have been developed for training procedures ranging from the basics such as blood draw, to laparoscopic surgery and trauma care.","[' What are clinical healthcare simulators increasingly being used for?', ' What are simulators being used to teach to health professionals?']","['teach therapeutic and diagnostic procedures as well as medical concepts and decision making to personnel in the health professions', 'therapeutic and diagnostic procedures as well as medical concepts and decision making']"
223,simulation,Clinical healthcare simulators,"Many medical simulators involve a computer connected to a plastic simulation of the relevant anatomy. Sophisticated simulators of this type employ a life-size mannequin that responds to injected drugs and can be programmed to create simulations of life-threatening emergencies.
",Many medical simulators involve a computer connected to a plastic simulation of the relevant anatomy. Sophisticated simulators of this type employ a life-size mannequin that responds to injected drugs and can be programmed to create simulations of life-threatening emergencies.,[' What type of simulator uses a life-size mannequin that responds to injected drugs and can be programmed to create simulations of life-threatening emergencies?'],['Sophisticated']
224,simulation,Clinical healthcare simulators,"In other simulations, visual components of the procedure are reproduced by computer graphics techniques, while touch-based components are reproduced by haptic feedback devices combined with physical simulation routines computed in response to the user's actions. Medical simulations of this sort will often use 3D CT or MRI scans of patient data to enhance realism. Some medical simulations are developed to be widely distributed (such as web-enabled simulations and procedural simulations that can be viewed via standard web browsers) and can be interacted with using standard computer interfaces, such as the keyboard and mouse.
","In other simulations, visual components of the procedure are reproduced by computer graphics techniques, while touch-based components are reproduced by haptic feedback devices combined with physical simulation routines computed in response to the user's actions. Medical simulations of this sort will often use 3D CT or MRI scans of patient data to enhance realism.","[' What is used to reproduce the visual components of a procedure in a medical simulation?', ' What are haptic feedback devices combined with?', ' How are 3D CT or MRI scans used in medical simulations?', ' What type of scans are often used to enhance realism?']","['computer graphics techniques', 'physical simulation routines', 'to enhance realism', '3D CT or MRI']"
225,simulation,Simulation in entertainment,"Simulation in entertainment encompasses many large and popular industries such as film, television, video games (including serious games) and rides in theme parks. Although modern simulation is thought to have its roots in training and the military, in the 20th century it also became a conduit for enterprises which were more hedonistic in nature.
","Simulation in entertainment encompasses many large and popular industries such as film, television, video games (including serious games) and rides in theme parks. Although modern simulation is thought to have its roots in training and the military, in the 20th century it also became a conduit for enterprises which were more hedonistic in nature.","[' What industries does simulation in entertainment encompass?', ' What are some of the industries in which simulation has its roots?', ' In what century did simulation become a conduit for enterprises?', ' What type of enterprises did it serve as a conduit for?', ' What was more hedonistic?']","['film, television, video games (including serious games) and rides in theme parks', 'training and the military', '20th', 'hedonistic', 'enterprises']"
226,simulation,Simulation and manufacturing,"Manufacturing simulation represents one of the most important applications of simulation. This technique represents a valuable tool used by engineers when evaluating the effect of capital investment in equipment and physical facilities like factory plants, warehouses, and distribution centers. Simulation can be used to predict the performance of an existing or planned system and to compare alternative solutions for a particular design problem.","Manufacturing simulation represents one of the most important applications of simulation. This technique represents a valuable tool used by engineers when evaluating the effect of capital investment in equipment and physical facilities like factory plants, warehouses, and distribution centers.","[' What is one of the most important applications of simulation?', ' What is a valuable tool used by engineers when evaluating the effect of capital investment in equipment and physical facilities?']","['Manufacturing simulation', 'Manufacturing simulation']"
227,simulation,Simulation and manufacturing,Another important goal of simulation in manufacturing systems is to quantify system performance. Common measures of system performance include the following:,Another important goal of simulation in manufacturing systems is to quantify system performance. Common measures of system performance include the following:,"[' What is another important goal of simulation in manufacturing systems?', ' What are common measures of system performance?']","['to quantify system performance', 'the following:']"
228,simulation,Simulation games,"Many other video games are simulators of some kind. Such games can simulate various aspects of reality, from business, to government, to construction, to piloting vehicles (see above).
","Many other video games are simulators of some kind. Such games can simulate various aspects of reality, from business, to government, to construction, to piloting vehicles (see above).","[' What are many other video games called?', ' What can simulation games simulate?']","['simulators', 'various aspects of reality']"
229,neural networks,Summary,"A neural network is a network or circuit of biological neurons, or in a modern sense, an artificial neural network, composed of artificial neurons or nodes. Thus a neural network is either a biological neural network, made up of biological neurons, or an artificial neural network, for solving artificial intelligence (AI) problems. The connections of the biological neuron are modeled in artificial neural networks as weights between nodes. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred to as a linear combination. Finally, an activation function controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be −1 and 1.
","A neural network is a network or circuit of biological neurons, or in a modern sense, an artificial neural network, composed of artificial neurons or nodes. Thus a neural network is either a biological neural network, made up of biological neurons, or an artificial neural network, for solving artificial intelligence (AI) problems.","[' What is a network or circuit of biological neurons called?', ' What is an artificial neural network composed of?', ' What is another name for an artificial neural network?', ' What does AI stand for?']","['A neural network', 'artificial neurons or nodes', 'A neural network', 'artificial intelligence']"
230,neural networks,Summary,"These artificial networks may be used for predictive modeling, adaptive control and applications where they can be trained via a dataset. Self-learning resulting from experience can occur within networks, which can derive conclusions from a complex and seemingly unrelated set of information.","These artificial networks may be used for predictive modeling, adaptive control and applications where they can be trained via a dataset. Self-learning resulting from experience can occur within networks, which can derive conclusions from a complex and seemingly unrelated set of information.","[' Artificial networks can be used for predictive modeling, adaptive control and applications where they can be trained via what?', ' Self-learning resulting from experience can occur within networks, which can derive conclusions from a complex and seemingly unrelated set of information?']","['a dataset', 'artificial networks']"
231,neural networks,Overview,"A biological neural network is composed of a groups of chemically connected or functionally associated neurons. A single neuron may be connected to many other neurons and the total number of neurons and connections in a network may be extensive. Connections, called synapses, are usually formed from axons to dendrites, though dendrodendritic synapses and other connections are possible. Apart from the electrical signaling, there are other forms of signaling that arise from neurotransmitter diffusion.
",A biological neural network is composed of a groups of chemically connected or functionally associated neurons. A single neuron may be connected to many other neurons and the total number of neurons and connections in a network may be extensive.,"[' What is a biological neural network composed of?', ' A single neuron may be connected to many other neurons and the total number of neurons and connections in a network may be what?']","['a groups of chemically connected or functionally associated neurons', 'extensive']"
232,neural networks,Overview,"Artificial intelligence, cognitive modeling, and neural networks are information processing paradigms inspired by the way biological neural systems process data. Artificial intelligence and cognitive modeling try to simulate some properties of biological neural networks. In the artificial intelligence field, artificial neural networks have been applied successfully to speech recognition, image analysis and adaptive control, in order to construct software agents (in computer and video games) or autonomous robots.
","Artificial intelligence, cognitive modeling, and neural networks are information processing paradigms inspired by the way biological neural systems process data. Artificial intelligence and cognitive modeling try to simulate some properties of biological neural networks.","[' Artificial intelligence, cognitive modeling, and neural networks are information processing paradigms inspired by what?', ' Artificial intelligence and cognitive modeling try to simulate what properties of biological neural networks?']","['the way biological neural systems process data', 'some properties']"
233,neural networks,Overview,"Historically, digital computers evolved from the von Neumann model, and operate via the execution of explicit instructions via access to memory by a number of processors. On the other hand, the origins of neural networks are based on efforts to model information processing in biological systems. Unlike the von Neumann model, neural network computing does not separate memory and processing.
","Historically, digital computers evolved from the von Neumann model, and operate via the execution of explicit instructions via access to memory by a number of processors. On the other hand, the origins of neural networks are based on efforts to model information processing in biological systems.","[' Digital computers evolved from what model?', ' The origins of neural networks are based on efforts to model what in biological systems?']","['von Neumann model', 'information processing']"
234,neural networks,History,"The preliminary theoretical base for contemporary neural networks was independently proposed by Alexander Bain (1873) and William James (1890). In their work, both thoughts and body activity resulted from interactions among neurons within the brain.
","The preliminary theoretical base for contemporary neural networks was independently proposed by Alexander Bain (1873) and William James (1890). In their work, both thoughts and body activity resulted from interactions among neurons within the brain.","[' Who independently proposed the preliminary theoretical base for contemporary neural networks?', ' What did Alexander Bain and William James work on?']","['Alexander Bain (1873) and William James', 'neural networks']"
235,neural networks,History,"For Bain, every activity led to the firing of a certain set of neurons. When activities were repeated, the connections between those neurons strengthened. According to his theory, this repetition was what led to the formation of memory. The general scientific community at the time was skeptical of Bain's theory because it required what appeared to be an inordinate number of neural connections within the brain. It is now apparent that the brain is exceedingly complex and that the same brain “wiring” can handle multiple problems and inputs.
","For Bain, every activity led to the firing of a certain set of neurons. When activities were repeated, the connections between those neurons strengthened.","[' What did every activity for Bain lead to?', ' What did Bain believe was strengthened by repeated activities?']","['the firing of a certain set of neurons', 'the connections between those neurons']"
236,neural networks,History,"James's theory was similar to Bain's, however, he suggested that memories and actions resulted from electrical currents flowing among the neurons in the brain. His model, by focusing on the flow of electrical currents, did not require individual neural connections for each memory or action.
","James's theory was similar to Bain's, however, he suggested that memories and actions resulted from electrical currents flowing among the neurons in the brain. His model, by focusing on the flow of electrical currents, did not require individual neural connections for each memory or action.","["" James's theory was similar to what Bain's?"", "" James' theory suggested that memories and actions resulted from electrical currents flowing among the neurons in what?"", "" What did James' model focus on?""]","['memories', 'the brain', 'the flow of electrical currents']"
237,neural networks,History,"C. S. Sherrington (1898) conducted experiments to test James's theory. He ran electrical currents down the spinal cords of rats. However, instead of demonstrating an increase in electrical current as projected by James, Sherrington found that the electrical current strength decreased as the testing continued over time. Importantly, this work led to the discovery of the concept of habituation. 
",C. S. Sherrington (1898) conducted experiments to test James's theory. He ran electrical currents down the spinal cords of rats.,"["" Who conducted experiments to test James's theory?"", ' Who ran electrical currents down the spinal cords of rats?']","['C. S. Sherrington', 'C. S. Sherrington']"
238,neural networks,History,"McCulloch and Pitts  (1943) created a computational model for neural networks based on mathematics and algorithms. They called this model threshold logic. The model paved the way for neural network research to split into two distinct approaches. One approach focused on biological processes in the brain and the other focused on the application of neural networks to artificial intelligence.
",McCulloch and Pitts  (1943) created a computational model for neural networks based on mathematics and algorithms. They called this model threshold logic.,"[' Who created a computational model for neural networks?', ' What did McCulloch and Pitts call their model threshold logic?']","['McCulloch and Pitts', 'computational model for neural networks']"
239,neural networks,History,"In the late 1940s psychologist Donald Hebb  created a hypothesis of learning based on the mechanism of neural plasticity that is now known as Hebbian learning. Hebbian learning is considered to be a 'typical' unsupervised learning rule and its later variants were early models for long term potentiation. These ideas started being applied to computational models in 1948 with Turing's B-type machines.
",In the late 1940s psychologist Donald Hebb  created a hypothesis of learning based on the mechanism of neural plasticity that is now known as Hebbian learning. Hebbian learning is considered to be a 'typical' unsupervised learning rule and its later variants were early models for long term potentiation.,"[' In what decade did Donald Hebb create a hypothesis of learning based on neural plasticity?', ' What is Hebbian learning considered to be?']","['1940s', ""a 'typical' unsupervised learning rule""]"
240,neural networks,History,"Farley and Clark (1954) first used computational machines, then called calculators, to simulate a Hebbian network at MIT. Other neural network computational machines were created by Rochester, Holland, Habit, and Duda (1956).
","Farley and Clark (1954) first used computational machines, then called calculators, to simulate a Hebbian network at MIT. Other neural network computational machines were created by Rochester, Holland, Habit, and Duda (1956).","[' When did Farley and Clark first use computational machines to simulate a Hebbian network at MIT?', ' When did Rochester, Holland, Habit, and Duda create other neural network computational machines?']","['1954', '1956']"
241,neural networks,History,"Rosenblatt (1958) created the perceptron, an algorithm for pattern recognition based on a two-layer learning computer network using simple addition and subtraction. With mathematical notation, Rosenblatt also described circuitry not in the basic perceptron, such as the exclusive-or circuit, a circuit whose mathematical computation could not be processed until after the backpropagation algorithm was created by Werbos (1975).
","Rosenblatt (1958) created the perceptron, an algorithm for pattern recognition based on a two-layer learning computer network using simple addition and subtraction. With mathematical notation, Rosenblatt also described circuitry not in the basic perceptron, such as the exclusive-or circuit, a circuit whose mathematical computation could not be processed until after the backpropagation algorithm was created by Werbos (1975).","[' Who created the perceptron?', ' What was the algorithm for pattern recognition based on?', ' How did Rosenblatt describe the exclusive-or circuit?', ' Who created the backpropagation algorithm?']","['Rosenblatt', 'two-layer learning computer network', 'With mathematical notation', 'Werbos']"
242,neural networks,History,Neural network research stagnated after the publication of machine learning research by Marvin Minsky and Seymour Papert (1969). They discovered two key issues with the computational machines that processed neural networks. The first issue was that single-layer neural networks were incapable of processing the exclusive-or circuit. The second significant issue was that computers were not sophisticated enough to effectively handle the long run time required by large neural networks. Neural network research slowed until computers achieved greater processing power. Also key in later advances was the backpropagation algorithm which effectively solved the exclusive-or problem (Werbos 1975).,Neural network research stagnated after the publication of machine learning research by Marvin Minsky and Seymour Papert (1969). They discovered two key issues with the computational machines that processed neural networks.,"[' When did machine learning research by Marvin Minsky and Seymour Papert begin?', ' What two key issues were discovered with the computational machines that processed neural networks?']","['1969', 'Marvin Minsky and Seymour Papert']"
243,neural networks,History,"The parallel distributed processing of the mid-1980s became popular under the name connectionism. The text by Rumelhart and McClelland (1986) provided a full exposition on the use of connectionism in computers to simulate neural processes.
",The parallel distributed processing of the mid-1980s became popular under the name connectionism. The text by Rumelhart and McClelland (1986) provided a full exposition on the use of connectionism in computers to simulate neural processes.,"[' What was parallel distributed processing known as in the mid-1980s?', ' What was the name of the text written by Rumelhart and McClelland in 1986?']","['connectionism', 'connectionism']"
244,neural networks,Artificial intelligence,"A neural network (NN), in the case of artificial neurons called artificial neural network (ANN) or simulated neural network (SNN), is an interconnected group of natural or artificial neurons that uses a mathematical or computational model for information processing based on a connectionistic approach to computation. In most cases an ANN is an adaptive system that changes its structure based on external or internal information that flows through the network.
","A neural network (NN), in the case of artificial neurons called artificial neural network (ANN) or simulated neural network (SNN), is an interconnected group of natural or artificial neurons that uses a mathematical or computational model for information processing based on a connectionistic approach to computation. In most cases an ANN is an adaptive system that changes its structure based on external or internal information that flows through the network.","[' What is a simulated neural network?', ' What is an interconnected group of natural or artificial neurons called?', ' What is an ANN an adaptive system that changes its structure based on external or internal information?']","['an interconnected group of natural or artificial neurons', 'A neural network', 'neural network']"
245,neural networks,Artificial intelligence,"In more practical terms neural networks are non-linear statistical data modeling or decision making tools. They can be used to model complex relationships between inputs and outputs or to find patterns in data.
",In more practical terms neural networks are non-linear statistical data modeling or decision making tools. They can be used to model complex relationships between inputs and outputs or to find patterns in data.,"[' What are neural networks?', ' What can neural networks be used for?']","['non-linear statistical data modeling or decision making tools', 'to model complex relationships between inputs and outputs or to find patterns in data']"
246,neural networks,Artificial intelligence,"An artificial neural network involves a network of simple processing elements (artificial neurons) which can exhibit complex global behavior, determined by the connections between the processing elements and element parameters. Artificial neurons were first proposed in 1943 by Warren McCulloch, a neurophysiologist, and Walter Pitts, a logician, who first collaborated at the University of Chicago.","An artificial neural network involves a network of simple processing elements (artificial neurons) which can exhibit complex global behavior, determined by the connections between the processing elements and element parameters. Artificial neurons were first proposed in 1943 by Warren McCulloch, a neurophysiologist, and Walter Pitts, a logician, who first collaborated at the University of Chicago.","[' What is a network of simple processing elements called?', ' What can exhibit complex global behavior?', ' When were artificial neurons first proposed?', ' Who proposed artificial neurons?', ' Who was a neurophysiologist?', ' Where did Walter Pitts work?', "" What was Pitts' profession?""]","['artificial neurons', 'An artificial neural network involves a network of simple processing elements (artificial neurons', '1943', 'Warren McCulloch, a neurophysiologist, and Walter Pitts', 'Warren McCulloch', 'University of Chicago', 'logician']"
247,neural networks,Artificial intelligence,"The utility of artificial neural network models lies in the fact that they can be used to infer a function from observations and also to use it. Unsupervised neural networks can also be used to learn representations of the input that capture the salient characteristics of the input distribution, e.g., see the Boltzmann machine (1983), and more recently, deep learning algorithms, which can implicitly learn the distribution function of the observed data. Learning in neural networks is particularly useful in applications where the complexity of the data or task makes the design of such functions by hand impractical.
","The utility of artificial neural network models lies in the fact that they can be used to infer a function from observations and also to use it. Unsupervised neural networks can also be used to learn representations of the input that capture the salient characteristics of the input distribution, e.g., see the Boltzmann machine (1983), and more recently, deep learning algorithms, which can implicitly learn the distribution function of the observed data.","[' What is the utility of artificial neural network models?', ' What can unsupervised neural networks learn representations of?', ' What machine captures salient characteristics of the input distribution?', ' What can implicitly learn the distribution function of the observed data?']","['they can be used to infer a function from observations and also to use it', 'the input that capture the salient characteristics of the input distribution', 'Boltzmann machine', 'deep learning algorithms']"
248,neural networks,Applications,"Neural networks can be used in different fields. The tasks to which artificial neural networks are applied tend to fall within the following broad categories:
",Neural networks can be used in different fields. The tasks to which artificial neural networks are applied tend to fall within the following broad categories:,"[' What can be used in different fields?', ' What are the tasks to which artificial neural networks are applied?']","['Neural networks', 'broad categories']"
249,neural networks,Applications,"Application areas of ANNs include nonlinear system identification and control (vehicle control, process control), game-playing and decision making (backgammon, chess, racing), pattern recognition (radar systems, face identification, object recognition), sequence recognition (gesture, speech, handwritten text recognition), medical diagnosis, financial applications, data mining (or knowledge discovery in databases, ""KDD""), visualization and e-mail spam filtering. For example, it is possible to create a semantic profile of user's interests emerging from pictures trained for object recognition.","Application areas of ANNs include nonlinear system identification and control (vehicle control, process control), game-playing and decision making (backgammon, chess, racing), pattern recognition (radar systems, face identification, object recognition), sequence recognition (gesture, speech, handwritten text recognition), medical diagnosis, financial applications, data mining (or knowledge discovery in databases, ""KDD""), visualization and e-mail spam filtering. For example, it is possible to create a semantic profile of user's interests emerging from pictures trained for object recognition.","[' What are nonlinear system identification and control?', ' What are game-playing and decision making applications of ANNs?', ' Data mining, visualization and e-mail spam filtering are examples of what?', ' What is the term for knowledge discovery in databases?']","['vehicle control, process control', 'backgammon, chess, racing', 'ANNs', 'KDD']"
250,neural networks,Neuroscience,"Theoretical and computational neuroscience is the field concerned with the analysis and computational modeling of biological neural systems.
Since neural systems are intimately related to cognitive processes and behaviour, the field is closely related to cognitive and behavioural modeling.
","Theoretical and computational neuroscience is the field concerned with the analysis and computational modeling of biological neural systems. Since neural systems are intimately related to cognitive processes and behaviour, the field is closely related to cognitive and behavioural modeling.","[' What is the field concerned with the analysis and computational modeling of biological neural systems?', ' Theoretical and computational neuroscience is closely related to what?']","['Theoretical and computational neuroscience', 'cognitive and behavioural modeling']"
251,neural networks,Neuroscience,"The aim of the field is to create models of biological neural systems in order to understand how biological systems work. To gain this understanding, neuroscientists strive to make a link between observed biological processes (data), biologically plausible mechanisms for neural processing and learning (biological neural network models) and theory (statistical learning theory and information theory).
","The aim of the field is to create models of biological neural systems in order to understand how biological systems work. To gain this understanding, neuroscientists strive to make a link between observed biological processes (data), biologically plausible mechanisms for neural processing and learning (biological neural network models) and theory (statistical learning theory and information theory).","[' What is the aim of the field?', ' What do neuroscientists strive to make a link between?', ' What are neural processing and learning?', ' What are statistical learning theory and information theory?']","['to create models of biological neural systems', 'observed biological processes', 'biological neural network models', 'theory']"
252,neural networks,Criticism,"A common criticism of neural networks, particularly in robotics, is that they require a large diversity of training samples for real-world operation. This is not surprising, since any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases. Dean Pomerleau, in his research presented in the paper ""Knowledge-based Training of Artificial Neural Networks for Autonomous Robot Driving,"" uses a neural network to train a robotic vehicle to drive on multiple types of roads (single lane, multi-lane, dirt, etc.). A large amount of his research is devoted to (1) extrapolating multiple training scenarios from a single training experience, and (2) preserving past training diversity so that the system does not become overtrained (if, for example, it is presented with a series of right turns—it should not learn to always turn right). These issues are common in neural networks that must decide from amongst a wide variety of responses, but can be dealt with in several ways, for example by randomly shuffling the training examples, by using a numerical optimization algorithm that does not take too large steps when changing the network connections following an example, or by grouping examples in so-called mini-batches.
","A common criticism of neural networks, particularly in robotics, is that they require a large diversity of training samples for real-world operation. This is not surprising, since any learning machine needs sufficient representative examples in order to capture the underlying structure that allows it to generalize to new cases.","[' What is a common criticism of neural networks?', ' What does a learning machine need to capture to allow it to generalize to new cases?']","['they require a large diversity of training samples for real-world operation', 'the underlying structure']"
253,neural networks,Criticism,"Arguments for Dewdney's position are that to implement large and effective software neural networks, much processing and storage resources need to be committed. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a most simplified form on Von Neumann technology may compel a neural network designer to fill many millions of database rows for its connections—which can consume vast amounts of computer memory and data storage capacity. Furthermore, the designer of neural network systems will often need to simulate the transmission of signals through many of these connections and their associated neurons—which must often be matched with incredible amounts of CPU processing power and time. While neural networks often yield effective programs, they too often do so at the cost of efficiency (they tend to consume considerable amounts of time and money).
","Arguments for Dewdney's position are that to implement large and effective software neural networks, much processing and storage resources need to be committed. While the brain has hardware tailored to the task of processing signals through a graph of neurons, simulating even a most simplified form on Von Neumann technology may compel a neural network designer to fill many millions of database rows for its connections—which can consume vast amounts of computer memory and data storage capacity.","["" Dewdney's argument is that to implement large and effective software neural networks, how much processing and storage resources need to be committed?"", ' The brain has hardware tailored to the task of processing signals through what?', ' How many database rows does a neural network designer have to fill for its connections?', ' What can consume vast amounts of computer memory?']","['much', 'a graph of neurons', 'millions', 'database rows']"
254,neural networks,Criticism,"Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be ""an opaque, unreadable table...valueless as a scientific resource"".
","Neural networks, for instance, are in the dock not only because they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked: the bunch of numbers that captures its behaviour would in all probability be ""an opaque, unreadable table...valueless as a scientific resource"".","[' Why are neural networks in the dock?', ' What could you create a successful net without understanding how it worked?', ' What would its behavior be in all probability?', ' What would it be?']","[""they have been hyped to high heaven, (what hasn't?) but also because you could create a successful net without understanding how it worked"", 'Neural networks', '""an opaque, unreadable table...valueless as a scientific resource"".', 'an opaque, unreadable table...valueless as a scientific resource"".']"
255,neural networks,Criticism,"In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.","In spite of his emphatic declaration that science is not technology, Dewdney seems here to pillory neural nets as bad science when most of those devising them are just trying to be good engineers. An unreadable table that a useful machine could read would still be well worth having.","[' What does Dewdney think of neural nets as?', ' What is an unreadable table that a useful machine could read?']","['bad science', 'neural nets']"
256,neural networks,Criticism,"Although it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover, recent emphasis on the explainability of AI has contributed towards the development of methods, notably those based on attention mechanisms, for visualizing and explaining learned neural networks. Furthermore, researchers involved in exploring learning algorithms for neural networks are gradually uncovering generic principles that allow a learning machine to be successful. For example, Bengio and LeCun (2007) wrote an article regarding local vs non-local learning, as well as shallow vs deep architecture.","Although it is true that analyzing what has been learned by an artificial neural network is difficult, it is much easier to do so than to analyze what has been learned by a biological neural network. Moreover, recent emphasis on the explainability of AI has contributed towards the development of methods, notably those based on attention mechanisms, for visualizing and explaining learned neural networks.","[' What is much easier to do than to analyze what has been learned by a biological neural network?', ' Recent emphasis on the explainability of AI has contributed towards the development of what?', ' What has contributed to the development of methods for visualizing and explaining learned neural networks?']","['analyzing what has been learned by an artificial neural network', 'methods', 'recent emphasis on the explainability of AI']"
257,neural networks,Criticism,"Some other criticisms came from believers of hybrid models (combining neural networks and symbolic approaches). They advocate the intermix of these two approaches and believe that hybrid models can better capture the mechanisms of the human mind (Sun and Bookman, 1990).","Some other criticisms came from believers of hybrid models (combining neural networks and symbolic approaches). They advocate the intermix of these two approaches and believe that hybrid models can better capture the mechanisms of the human mind (Sun and Bookman, 1990).","[' What is a hybrid model?', ' What do hybrid models combine to better capture the human mind?']","['combining neural networks and symbolic approaches', 'neural networks and symbolic approaches']"
258,neural networks,Recent improvements,"Biophysical models, such as BCM theory, have been important in understanding mechanisms for synaptic plasticity, and have had applications in both computer science and neuroscience. Research is ongoing in understanding the computational algorithms used in the brain, with some recent biological evidence for radial basis networks and neural backpropagation as mechanisms for processing data.","Biophysical models, such as BCM theory, have been important in understanding mechanisms for synaptic plasticity, and have had applications in both computer science and neuroscience. Research is ongoing in understanding the computational algorithms used in the brain, with some recent biological evidence for radial basis networks and neural backpropagation as mechanisms for processing data.","[' What has been important in understanding mechanisms for synaptic plasticity?', ' What has BCM theory had applications in both computer science and neuroscience?', ' Research is ongoing in understanding what?', ' What does biological evidence show about neural backpropagation and radial basis networks?']","['Biophysical models', 'understanding mechanisms for synaptic plasticity', 'computational algorithms used in the brain', 'mechanisms for processing data']"
259,neural networks,Recent improvements,"Computational devices have been created in CMOS for both biophysical simulation and neuromorphic computing. More recent efforts show promise for creating nanodevices for very large scale principal components analyses and convolution. If successful, these efforts could usher in a new era of neural computing that is a step beyond digital computing, because it depends on learning rather than programming and because it is fundamentally analog rather than digital even though the first instantiations may in fact be with CMOS digital devices.
",Computational devices have been created in CMOS for both biophysical simulation and neuromorphic computing. More recent efforts show promise for creating nanodevices for very large scale principal components analyses and convolution.,"[' What has been created in CMOS for both biophysical simulation and neuromorphic computing?', ' Recent efforts show promise for creating nanodevices for what?']","['Computational devices', 'very large scale principal components analyses and convolution']"
260,neural networks,Recent improvements,"Between 2009 and 2012, the recurrent neural networks and deep feedforward neural networks developed in the research group of Jürgen Schmidhuber at the Swiss AI Lab IDSIA have won eight international competitions in pattern recognition and machine learning. For example, multi-dimensional long short term memory (LSTM) won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three different languages to be learned.
","Between 2009 and 2012, the recurrent neural networks and deep feedforward neural networks developed in the research group of Jürgen Schmidhuber at the Swiss AI Lab IDSIA have won eight international competitions in pattern recognition and machine learning. For example, multi-dimensional long short term memory (LSTM) won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three different languages to be learned.","[' How many international competitions have the recurrent neural networks and deep feedforward neural networks won between 2009 and 2012?', ' What did LSTM win three competitions in?', ' Where is Jürgen Schmidhuber located?', ' How many competitions did LSTM win in connected handwriting recognition?', ' What was the name of the 2009 International Conference on Document Analysis and Recognition (ICDAR)?']","['eight', 'connected handwriting recognition', 'Swiss', 'three', 'multi-dimensional long short term memory (LSTM) won three competitions in connected handwriting recognition at the 2009 International Conference on Document Analysis and Recognition (ICDAR), without any prior knowledge about the three different languages to be learned']"
261,neural networks,Recent improvements,Radial basis function and wavelet networks have also been introduced. These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications.,Radial basis function and wavelet networks have also been introduced. These can be shown to offer best approximation properties and have been applied in nonlinear system identification and classification applications.,"[' What have been introduced to offer best approximation properties?', ' What are wavelet networks used for?']","['Radial basis function and wavelet networks', 'nonlinear system identification and classification applications']"
262,neural networks,Recent improvements,"Deep learning feedforward networks alternate convolutional layers and max-pooling layers, topped by several pure classification layers. Fast GPU-based implementations of this approach have won several pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition and the  ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge. Such neural networks also were the first artificial pattern recognizers to achieve human-competitive or even superhuman performance on benchmarks such as traffic sign recognition (IJCNN 2012), or the MNIST handwritten digits problem of Yann LeCun and colleagues at NYU.
","Deep learning feedforward networks alternate convolutional layers and max-pooling layers, topped by several pure classification layers. Fast GPU-based implementations of this approach have won several pattern recognition contests, including the IJCNN 2011 Traffic Sign Recognition Competition and the  ISBI 2012 Segmentation of Neuronal Structures in Electron Microscopy Stacks challenge.","[' Deep learning feedforward networks alternate what layers?', ' Fast GPU-based implementations of what approach have won several pattern recognition contests?', ' What was the IJCNN 2011 Traffic Sign Recognition Competition?']","['convolutional layers and max-pooling layers', 'Deep learning feedforward networks', 'Fast GPU-based implementations of this approach have won several pattern recognition contests']"
263,artificial intelligence,Summary,"Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to natural intelligence displayed by animals including humans. Leading AI textbooks define the field as the study of ""intelligent agents"": any system that perceives its environment and takes actions that maximize its chance of achieving its goals.","Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to natural intelligence displayed by animals including humans. Leading AI textbooks define the field as the study of ""intelligent agents"": any system that perceives its environment and takes actions that maximize its chance of achieving its goals.","[' What does AI stand for?', ' What is the difference between AI and natural intelligence?', ' Which AI textbooks define AI?']","['Artificial intelligence', 'intelligence demonstrated by machines, as opposed to natural intelligence displayed by animals including humans', 'Leading AI textbooks define the field as the study of ""intelligent agents']"
264,artificial intelligence,Summary,"AI applications include advanced web search engines (e.g., Google), recommendation systems (used by YouTube, Amazon and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Tesla), automated decision-making and competing at the highest level in strategic game systems (such as chess and Go).
As machines become increasingly capable, tasks considered to require ""intelligence"" are often removed from the definition of AI, a phenomenon known as the AI effect.  For instance, optical character recognition is frequently excluded from things considered to be AI, having become a routine technology.","AI applications include advanced web search engines (e.g., Google), recommendation systems (used by YouTube, Amazon and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Tesla), automated decision-making and competing at the highest level in strategic game systems (such as chess and Go). As machines become increasingly capable, tasks considered to require ""intelligence"" are often removed from the definition of AI, a phenomenon known as the AI effect.","[' What are some examples of AI applications?', ' What are two examples of self-driving cars?', ' How do AI applications compare to humans?', ' What are two examples of systems?', ' As machines become more capable, tasks considered to require what are often removed from the definition of AI?']","['advanced web search engines (e.g., Google), recommendation systems (used by YouTube, Amazon and Netflix', 'Tesla', 'understanding human speech', 'YouTube, Amazon and Netflix', 'intelligence""']"
265,artificial intelligence,Summary,"Artificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an ""AI winter""), followed by new approaches, success and renewed funding. AI research has tried and discarded many different approaches since its founding, including simulating the brain, modeling human problem solving, formal logic, large databases of knowledge and imitating animal behavior. In the first decades of the 21st century, highly mathematical statistical machine learning has dominated the field, and this technique has proved highly successful, helping to solve many challenging problems throughout industry and academia.","Artificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an ""AI winter""), followed by new approaches, success and renewed funding. AI research has tried and discarded many different approaches since its founding, including simulating the brain, modeling human problem solving, formal logic, large databases of knowledge and imitating animal behavior.","[' When was AI founded as an academic discipline?', ' What is the name of the wave of optimism that has followed AI research?', "" How has AI research tried and discarded many different approaches since it's founding?"", ' What has AI tried simulating the brain tried?']","['1956', 'AI winter', 'simulating the brain, modeling human problem solving, formal logic, large databases of knowledge and imitating animal behavior', 'modeling human problem solving']"
266,artificial intelligence,Summary,"The various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and the ability to move and manipulate objects. General intelligence (the ability to solve an arbitrary problem) is among the field's long-term goals. To solve these problems, AI researchers have adapted and integrated a wide range of problem-solving techniques—including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, probability and economics. AI also draws upon computer science, psychology, linguistics, philosophy, and many other fields.
","The various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and the ability to move and manipulate objects.","[' What are the various sub-fields of AI research centered around?', ' What are some of the traditional goals of AI?']","['particular goals and the use of particular tools', 'reasoning, knowledge representation, planning, learning, natural language processing, perception, and the ability to move and manipulate objects']"
267,artificial intelligence,Summary,"The field was founded on the assumption that human intelligence ""can be so precisely described that a machine can be made to simulate it"".
This raises philosophical arguments about the mind and the ethics of creating artificial beings endowed with human-like intelligence. These issues have been explored by myth, fiction, and philosophy since antiquity.Science fiction and futurology have also suggested that, with its enormous potential and power, AI may become an existential risk to humanity.","The field was founded on the assumption that human intelligence ""can be so precisely described that a machine can be made to simulate it"". This raises philosophical arguments about the mind and the ethics of creating artificial beings endowed with human-like intelligence.","[' On what assumption was the field of artificial intelligence founded?', ' What does the assumption raise about the ethics of creating artificial beings with human-like intelligence?']","['human intelligence ""can be so precisely described that a machine can be made to simulate it"".', 'philosophical arguments']"
268,artificial intelligence,History,"Artificial beings with intelligence appeared as storytelling devices in antiquity,
and have been common in fiction, as in Mary Shelley's Frankenstein or Karel Čapek's R.U.R. These characters and their fates raised many of the same issues now discussed in the ethics of artificial intelligence.","Artificial beings with intelligence appeared as storytelling devices in antiquity,
and have been common in fiction, as in Mary Shelley's Frankenstein or Karel Čapek's R.U.R. These characters and their fates raised many of the same issues now discussed in the ethics of artificial intelligence.","[' Who wrote Frankenstein?', ' Who wrote R.U.R?']","['Mary Shelley', 'Karel Čapek']"
269,artificial intelligence,History,"The study of mechanical or ""formal"" reasoning began with philosophers and mathematicians in antiquity. The study of mathematical logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as ""0"" and ""1"", could simulate any conceivable act of mathematical deduction. This insight that digital computers can simulate any process of formal reasoning is known as the Church–Turing thesis.","The study of mechanical or ""formal"" reasoning began with philosophers and mathematicians in antiquity. The study of mathematical logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as ""0"" and ""1"", could simulate any conceivable act of mathematical deduction.","[' When did the study of mechanical reasoning begin?', "" What led directly to Alan Turing's theory of computation?"", ' How could a machine simulate any conceivable act of mathematical deduction?']","['antiquity', 'The study of mathematical logic', 'by shuffling symbols as simple as ""0"" and ""1']"
270,artificial intelligence,History,"The Church-Turing thesis, along with concurrent discoveries in neurobiology, information theory and cybernetics, led researchers to consider the possibility of building an electronic brain.
The first work that is now generally recognized as AI was McCullouch and Pitts' 1943 formal design for Turing-complete ""artificial neurons"".","The Church-Turing thesis, along with concurrent discoveries in neurobiology, information theory and cybernetics, led researchers to consider the possibility of building an electronic brain. The first work that is now generally recognized as AI was McCullouch and Pitts' 1943 formal design for Turing-complete ""artificial neurons"".","[' What thesis led researchers to consider the possibility of building an electronic brain?', ' What was the first work that is now generally recognized as AI?', ' When was McCullouch and Pitts\' formal design for Turing-complete ""artificial neurons""?']","['Church-Turing', 'McCullouch and Pitts\' 1943 formal design for Turing-complete ""artificial neurons"".', '1943']"
271,artificial intelligence,History,"When access to digital computers became possible in the mid-1950s, AI research began to explore the possibility that human intelligence could be reduced to step-by-step symbol manipulation, known as Symbolic AI or GOFAI. Approaches based on cybernetics or artificial neural networks were abandoned or pushed into the background.
","When access to digital computers became possible in the mid-1950s, AI research began to explore the possibility that human intelligence could be reduced to step-by-step symbol manipulation, known as Symbolic AI or GOFAI. Approaches based on cybernetics or artificial neural networks were abandoned or pushed into the background.","[' When did access to digital computers become possible?', ' What was GOFAI?', ' How was human intelligence reduced?']","['mid-1950s', 'Symbolic AI', 'step-by-step symbol manipulation']"
272,artificial intelligence,History,"The field of AI research was born at a workshop at Dartmouth College in 1956.
The attendees became the founders and leaders of AI research.
They and their students produced programs that the press described as ""astonishing"":
computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English.
By the middle of the 1960s, research in the U.S. was heavily funded by the Department of Defense
and laboratories had been established around the world.",The field of AI research was born at a workshop at Dartmouth College in 1956. The attendees became the founders and leaders of AI research.,"[' When was the field of AI research born?', ' Where was the workshop held?', ' Who became the founders and leaders of AI Research?']","['1956', 'Dartmouth College', 'The attendees']"
273,artificial intelligence,History,"They failed to recognize the difficulty of some of the remaining tasks. Progress slowed and in 1974, in response to the criticism of Sir James Lighthill
and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off exploratory research in AI. The next few years would later be called an ""AI winter"", a period when obtaining funding for AI projects was difficult.
","They failed to recognize the difficulty of some of the remaining tasks. Progress slowed and in 1974, in response to the criticism of Sir James Lighthill
and ongoing pressure from the US Congress to fund more productive projects, both the U.S. and British governments cut off exploratory research in AI.",[' When did the U.S. and British governments cut off exploratory research in AI?'],['1974']
274,artificial intelligence,History,"In the early 1980s, AI research was revived by the commercial success of expert systems,
a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S and British governments to restore funding for academic research.
However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.","In the early 1980s, AI research was revived by the commercial success of expert systems,
a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars.","[' In what decade was AI research revived?', ' Expert systems simulated the knowledge and analytical skills of what?', ' By 1985, the market for AI had reached how much?']","['1980s', 'human experts', 'over a billion dollars']"
275,artificial intelligence,History,"Many researchers began to doubt that the symbolic approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. A number of researchers began to look into ""sub-symbolic"" approaches to specific AI problems. Robotics researchers, such as Rodney Brooks, rejected symbolic AI and focused on the basic engineering problems that would allow robots to move, survive, and learn their environment.
Interest in neural networks and ""connectionism"" was revived by Geoffrey Hinton, David Rumelhart and others in the middle of the 1980s.Soft computing tools were developed in the 80s, such as neural networks, fuzzy systems, Grey system theory, evolutionary computation and many tools drawn from statistics or mathematical optimization.
","Many researchers began to doubt that the symbolic approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. A number of researchers began to look into ""sub-symbolic"" approaches to specific AI problems.","[' Many researchers began to doubt that the symbolic approach would be able to imitate all the processes of human cognition, especially what?', ' A number of researchers began looking into what approach to specific AI problems?']","['perception, robotics, learning and pattern recognition', 'sub-symbolic']"
276,artificial intelligence,History,"AI gradually restored its reputation in the late 1990s and early 21st century by finding specific solutions to specific problems. The narrow focus allowed researchers to produce verifiable results, exploit more  mathematical methods, and collaborate with other fields (such as statistics, economics and mathematics).
By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as ""artificial intelligence"".","AI gradually restored its reputation in the late 1990s and early 21st century by finding specific solutions to specific problems. The narrow focus allowed researchers to produce verifiable results, exploit more  mathematical methods, and collaborate with other fields (such as statistics, economics and mathematics).","[' When did AI gradually restore its reputation?', ' What allowed researchers to produce verifiable results, exploit more mathematical methods and collaborate with other fields?']","['late 1990s and early 21st century', 'The narrow focus']"
277,artificial intelligence,History,"Faster computers, algorithmic improvements, and access to large amounts of data enabled advances in machine learning and perception; data-hungry deep learning methods started to dominate accuracy benchmarks around 2012.
According to Bloomberg's Jack Clark, 2015 was a landmark year for artificial intelligence, with the number of software projects that use AI within Google increased from a ""sporadic usage"" in 2012 to more than 2,700 projects. He attributes this to an increase in affordable neural networks, due to a rise in cloud computing infrastructure and to an increase in research tools and datasets. In a 2017 survey, one in five companies reported they had ""incorporated AI in some offerings or processes"". The amount of research into AI (measured by total publications) increased by 50% in the years 2015–2019.","Faster computers, algorithmic improvements, and access to large amounts of data enabled advances in machine learning and perception; data-hungry deep learning methods started to dominate accuracy benchmarks around 2012. According to Bloomberg's Jack Clark, 2015 was a landmark year for artificial intelligence, with the number of software projects that use AI within Google increased from a ""sporadic usage"" in 2012 to more than 2,700 projects.","[' What enabled advances in machine learning and perception?', ' When did data-hungry deep learning methods start to dominate accuracy benchmarks?', ' According to Bloomberg, 2015 was a landmark year for what?', ' How many software projects use AI within Google?', ' What was the number of software projects that use AI in 2012?']","['Faster computers, algorithmic improvements, and access to large amounts of data', '2012', 'artificial intelligence', '2,700', '2,700']"
278,artificial intelligence,History,"Numerous academic researchers became concerned that AI was no longer pursuing the original goal of creating versatile, fully intelligent machines. Much of current research involves statistical AI, which is overwhelmingly used to solve specific problems, even highly successful techniques such as deep learning. This concern has led to the subfield artificial general intelligence (or ""AGI""), which had several well-funded institutions by the 2010s.","Numerous academic researchers became concerned that AI was no longer pursuing the original goal of creating versatile, fully intelligent machines. Much of current research involves statistical AI, which is overwhelmingly used to solve specific problems, even highly successful techniques such as deep learning.","[' What did many academic researchers become concerned that AI was no longer pursuing the original goal of creating?', ' Much of current research involves what?', ' What is overwhelmingly used to solve specific problems?']","['versatile, fully intelligent machines', 'statistical AI', 'statistical AI']"
279,artificial intelligence,Goals,The general problem of simulating (or creating) intelligence has been broken down into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention.,The general problem of simulating (or creating) intelligence has been broken down into sub-problems. These consist of particular traits or capabilities that researchers expect an intelligent system to display.,"[' The problem of simulating intelligence has been broken down into what?', ' What are the sub-problems?']","['sub-problems', 'particular traits or capabilities that researchers expect an intelligent system to display']"
280,artificial intelligence,Applications,"AI is relevant to any intellectual task.
Modern artificial intelligence techniques are pervasive and are too numerous to list here.
Frequently, when a technique reaches mainstream use, it is no longer considered artificial intelligence; this phenomenon is described as the AI effect.",AI is relevant to any intellectual task. Modern artificial intelligence techniques are pervasive and are too numerous to list here.,"[' What is relevant to any intellectual task?', ' Modern artificial intelligence techniques are pervasive and are too numerous to list here?']","['AI', 'AI']"
281,artificial intelligence,Applications,"In the 2010s, AI applications were at the heart of the most commercially successful areas of computing, and have become a ubiquitous feature of daily life. AI is used in search engines (such as Google Search),
targeting online advertisements,recommendation systems (offered by Netflix, YouTube or Amazon),
driving internet traffic,targeted advertising (AdSense, Facebook),
virtual assistants (such as Siri or Alexa),autonomous vehicles (including drones and self-driving cars),
automatic language translation (Microsoft Translator, Google Translate),
facial recognition (Apple's Face ID or Microsoft's DeepFace),
image labeling (used by Facebook, Apple's iPhoto and TikTok)
and spam filtering.
","In the 2010s, AI applications were at the heart of the most commercially successful areas of computing, and have become a ubiquitous feature of daily life. AI is used in search engines (such as Google Search),
targeting online advertisements,recommendation systems (offered by Netflix, YouTube or Amazon),
driving internet traffic,targeted advertising (AdSense, Facebook),
virtual assistants (such as Siri or Alexa),autonomous vehicles (including drones and self-driving cars),
automatic language translation (Microsoft Translator, Google Translate),
facial recognition (Apple's Face ID or Microsoft's DeepFace),
image labeling (used by Facebook, Apple's iPhoto and TikTok)
and spam filtering.","[' What was at the heart of the most commercially successful areas of computing in the 2010s?', ' What has become a ubiquitous feature of daily life?', ' How is AI used in search engines?', ' What are some examples of virtual assistants?', ' What is an example of a virtual assistant?']","['AI applications', 'AI applications', 'targeting online advertisements', 'Siri or Alexa', 'Siri or Alexa']"
282,artificial intelligence,Applications,"There are also thousands of successful AI applications used to solve problems for specific industries or institutions. A few examples are: 
energy storage,deepfakes,
medical diagnosis, 
military logistics, or 
supply chain management.
","There are also thousands of successful AI applications used to solve problems for specific industries or institutions. A few examples are: 
energy storage,deepfakes,
medical diagnosis, 
military logistics, or 
supply chain management.","[' How many successful AI applications are there?', ' What are a few examples of AI applications?']","['thousands', 'energy storage,deepfakes,\nmedical diagnosis, \nmilitary logistics, or \nsupply chain management']"
283,artificial intelligence,Applications,"Game playing has been a test of AI's strength since the 1950s. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. 
In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin. 
In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps.
Other programs handle imperfect-information games; such as for poker at a superhuman level, Pluribus
and Cepheus.DeepMind in the 2010s developed a ""generalized artificial intelligence"" that could learn many diverse Atari games on its own.","Game playing has been a test of AI's strength since the 1950s. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997.","["" What has been a test of AI's strength since the 1950s?"", ' What was the first computer chess-playing system to beat Garry Kasparov?', ' On what date did Deep Blue beat the reigning world chess champion?']","['Game playing', 'Deep Blue', '11 May 1997']"
284,artificial intelligence,Applications,"By 2020, Natural Language Processing systems such as the enormous GPT-3 (then by far the largest artificial neural network) were matching human performance on pre-existing benchmarks, albeit without the system attaining commonsense understanding of the contents of the benchmarks.
DeepMind's AlphaFold 2 (2020) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein.
Other applications predict the result of judicial decisions, create art (such as poetry or painting) and prove mathematical theorems.
","By 2020, Natural Language Processing systems such as the enormous GPT-3 (then by far the largest artificial neural network) were matching human performance on pre-existing benchmarks, albeit without the system attaining commonsense understanding of the contents of the benchmarks. DeepMind's AlphaFold 2 (2020) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein.","[' What was the largest artificial neural network?', "" What was DeepMind's AlphaFold 2?"", ' AlphaFold 2 demonstrated the ability to approximate, in hours rather than months, the 3D structure of what?']","['GPT-3', 'demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein', 'a protein']"
285,artificial intelligence,In fiction,"A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.","A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999).","[' Who wrote Frankenstein?', ' Who wrote 2001: A Space Odyssey?', ' What was the name of HAL 9000?', ' What was the name of the computer in charge of the Discovery One spaceship?', ' What was 9000?', ' When was The Terminator released?']","['Mary Shelley', 'Arthur C. Clarke', 'the murderous computer in charge of the Discovery One spaceship', 'HAL 9000', 'the murderous computer in charge of the Discovery One spaceship', '1984']"
286,artificial intelligence,In fiction,"Isaac Asimov introduced the Three Laws of Robotics in many books and stories, most notably the ""Multivac"" series about a super-intelligent computer of the same name. Asimov's laws are often brought up during lay discussions of machine ethics;
while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.","Isaac Asimov introduced the Three Laws of Robotics in many books and stories, most notably the ""Multivac"" series about a super-intelligent computer of the same name. Asimov's laws are often brought up during lay discussions of machine ethics;
while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.","[' Who introduced the Three Laws of Robotics in many books and stories?', ' What is the Multivac series about?', "" When are Asimov's laws often brought up during lay discussions of machine ethics?"", "" What do artificial intelligence researchers know about Asimov's laws?"", ' What do researchers generally consider the laws useless for?']","['Isaac Asimov', 'a super-intelligent computer of the same name', ""almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless"", 'through popular culture', 'ambiguity']"
287,artificial intelligence,In fiction,"Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.","Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I.","["" What does Karel <unk>apek's R.U.R. show us?""]","['artificial beings that have the ability to feel, and thus to suffer']"
288,ontology,Summary,"Ontology is the branch of philosophy that studies concepts such as existence, being, becoming, and reality. It includes the questions of how entities are grouped into basic categories and which of these entities exist on the most fundamental level. Ontology is sometimes referred to as the science of being and belongs to the major branch of philosophy known as metaphysics.
","Ontology is the branch of philosophy that studies concepts such as existence, being, becoming, and reality. It includes the questions of how entities are grouped into basic categories and which of these entities exist on the most fundamental level.","[' What is the branch of philosophy that studies concepts such as existence, being, becoming, and reality?', ' Ontology includes the questions of how entities are grouped into what basic categories?']","['Ontology', 'which of these entities exist on the most fundamental level']"
289,ontology,Summary,"Ontologists often try to determine what the categories or highest kinds are and how they form a system of categories that provides an encompassing classification of all entities. Commonly proposed categories include substances, properties, relations, states of affairs and events. These categories are characterized by fundamental ontological concepts, like particularity and universality, abstractness and concreteness, or possibility and necessity. Of special interest is the concept of ontological dependence, which determines whether the entities of a category exist on the most fundamental level. Disagreements within ontology are often about whether entities belonging to a certain category exist and, if so, how they are related to other entities.","Ontologists often try to determine what the categories or highest kinds are and how they form a system of categories that provides an encompassing classification of all entities. Commonly proposed categories include substances, properties, relations, states of affairs and events.","[' What do ontologists often try to determine?', ' What are the most common proposed categories?']","['what the categories or highest kinds are', 'substances, properties, relations, states of affairs and events']"
290,ontology,Summary,"When used as a countable noun, the terms ""ontology"" and ""ontologies"" refer not to the science of being but to theories within the science of being. Ontological theories can be divided into various types according to their theoretical commitments. Monocategorical ontologies hold that there is only one basic category, which is rejected by polycategorical ontologies. Hierarchical ontologies assert that some entities exist on a more fundamental level and that other entities depend on them. Flat ontologies, on the other hand, deny such a privileged status to any entity.
","When used as a countable noun, the terms ""ontology"" and ""ontologies"" refer not to the science of being but to theories within the science of being. Ontological theories can be divided into various types according to their theoretical commitments.","[' What do the terms ""ontology"" and ""ontologies"" refer to when used as a countable noun?', ' What can ontological theories be divided into?']","['theories within the science of being', 'various types']"
291,ontology,Overview,"Ontology is closely associated with Aristotle's question of 'being qua being': the question of what all entities in the widest sense have in common. The Eleatic principle is one answer to this question: it states that being is inextricably tied to causation, that ""Power is the mark of Being"". One problem with this answer is that it excludes abstract objects. Another explicit but little accepted answer can be found in Berkeley's slogan that ""to be is to be perceived"". Intimately related but not identical to the question of 'being qua being' is the problem of categories. Categories are usually seen as the highest kinds or genera. A system of categories provides a classification of entities that is exclusive and exhaustive: every entity belongs to exactly one category. Various such classifications have been proposed, they often include categories for substances, properties, relations, states of affairs or events. At the core of the differentiation between categories are various fundamental ontological concepts and distinctions, for example, the concepts of particularity and universality, of abstractness and concreteness, of ontological dependence, of identity and of modality. These concepts are sometimes treated as categories themselves, are used to explain the difference between categories or play other central roles for characterizing different ontological theories. Within ontology, there is a lack of general consensus concerning how the different categories are to be defined. Different ontologists often disagree on whether a certain category has any members at all or whether a given category is fundamental.","Ontology is closely associated with Aristotle's question of 'being qua being': the question of what all entities in the widest sense have in common. The Eleatic principle is one answer to this question: it states that being is inextricably tied to causation, that ""Power is the mark of Being"".","["" Ontology is closely associated with Aristotle's question of what all entities in the widest sense have in common?"", ' The Eleatic principle states that being is inextricably tied to what?']","['being qua being', 'causation']"
292,ontology,Types of ontologies,"Ontological theories can be divided into various types according to their theoretical commitments. Particular ontological theories or types of theories are often referred to as ""ontologies"" (singular or plural). This usage contrasts with the meaning of ""ontology"" (only singular) as a branch of philosophy: the science of being in general.","Ontological theories can be divided into various types according to their theoretical commitments. Particular ontological theories or types of theories are often referred to as ""ontologies"" (singular or plural).","[' Ontological theories can be divided into various types according to what?', ' What are particular ontological theories referred to as?']","['their theoretical commitments', 'ontologies']"
293,wireless sensor network,Summary,"Wireless sensor networks (WSNs) refer to networks of spatially dispersed and dedicated sensors that monitor and record the physical conditions of the environment and forward the collected data to a central location. WSNs can measure environmental conditions such as temperature, sound, pollution levels, humidity and wind.","Wireless sensor networks (WSNs) refer to networks of spatially dispersed and dedicated sensors that monitor and record the physical conditions of the environment and forward the collected data to a central location. WSNs can measure environmental conditions such as temperature, sound, pollution levels, humidity and wind.","[' What does WSN stand for?', ' What do WSNs monitor and record?', ' Where can WSN data be sent?']","['Wireless sensor networks', 'the physical conditions of the environment', 'central location']"
294,wireless sensor network,Summary,"These are similar to wireless ad hoc networks in the sense that they rely on wireless connectivity and spontaneous formation of networks so that sensor data can be transported wirelessly. WSNs monitor physical or environmental conditions, such as temperature, sound, and pressure. Modern networks are bi-directional, both collecting data and enabling control of sensor activity.  The development of these networks was motivated by military applications such as battlefield surveillance. Such networks are used in industrial and consumer applications, such as industrial process monitoring and control and machine health monitoring.
","These are similar to wireless ad hoc networks in the sense that they rely on wireless connectivity and spontaneous formation of networks so that sensor data can be transported wirelessly. WSNs monitor physical or environmental conditions, such as temperature, sound, and pressure.","[' What type of networks are similar to wireless ad hoc networks in the sense that they rely on wireless connectivity and spontaneous formation of networks?', ' WSNs monitor physical or environmental conditions, such as temperature, sound, and pressure.']","['WSNs', 'wireless ad hoc networks']"
295,wireless sensor network,Summary,"A WSN is built of ""nodes"" – from a few to hundreds or thousands, where each node is connected to other sensors. Each such node typically has several parts: a radio transceiver with an internal antenna or connection to an external antenna, a microcontroller, an electronic circuit for interfacing with the sensors and an energy source, usually a battery or an embedded form of energy harvesting. A sensor node might vary in size from a shoebox to (theoretically) a grain of dust, although microscopic dimensions have yet to be realized. Sensor node cost is similarly variable, ranging from a few to hundreds of dollars, depending on node sophistication. Size and cost constraints constrain resources such as energy, memory, computational speed and communications bandwidth. The topology of a WSN can vary from a simple star network to an advanced multi-hop wireless mesh network. Propagation can employ routing or flooding.","A WSN is built of ""nodes"" – from a few to hundreds or thousands, where each node is connected to other sensors. Each such node typically has several parts: a radio transceiver with an internal antenna or connection to an external antenna, a microcontroller, an electronic circuit for interfacing with the sensors and an energy source, usually a battery or an embedded form of energy harvesting.","[' What is a WSN built of?', ' How many sensors are connected to each node?', ' What is the name of the electronic circuit for interfacing with other sensors?', ' What is a microcontroller?', ' What is an energy source?']","['nodes', 'hundreds or thousands', 'microcontroller', 'an electronic circuit for interfacing with the sensors', 'a battery']"
296,wireless sensor network,Summary,"In computer science and telecommunications, wireless sensor networks are an active research area supporting many workshops and conferences, including International Workshop on Embedded Networked Sensors (EmNetS), IPSN, SenSys, MobiCom and EWSN. As of 2010, wireless sensor networks had deployed approximately 120 million remote units worldwide.","In computer science and telecommunications, wireless sensor networks are an active research area supporting many workshops and conferences, including International Workshop on Embedded Networked Sensors (EmNetS), IPSN, SenSys, MobiCom and EWSN. As of 2010, wireless sensor networks had deployed approximately 120 million remote units worldwide.","[' In what fields are wireless sensor networks an active research area?', ' What is the International Workshop on Embedded Networked Sensors?', ' As of 2010, how many remote units were deployed worldwide?']","['computer science and telecommunications', 'EmNetS', '120\xa0million']"
297,wireless sensor network,Characteristics,"Cross-layer is becoming an important studying area for wireless communications. In addition, the traditional layered approach presents three main problems:
","Cross-layer is becoming an important studying area for wireless communications. In addition, the traditional layered approach presents three main problems:","[' What is becoming an important studying area for wireless communications?', ' How many main problems does the traditional layered approach present?']","['Cross-layer', 'three']"
298,wireless sensor network,Characteristics,"So the cross-layer can be used to make the optimal modulation to improve the transmission performance, such as data rate, energy efficiency, quality of service (QoS), etc. Sensor nodes can be imagined as small computers which are extremely basic in terms of their interfaces and their components. They usually consist of a processing unit with limited computational power and limited memory, sensors or MEMS (including specific conditioning circuitry), a communication device (usually radio transceivers or alternatively optical), and a power source usually in the form of a battery. Other possible inclusions are energy harvesting modules, secondary ASICs, and possibly secondary communication interface (e.g. RS-232 or USB).
","So the cross-layer can be used to make the optimal modulation to improve the transmission performance, such as data rate, energy efficiency, quality of service (QoS), etc. Sensor nodes can be imagined as small computers which are extremely basic in terms of their interfaces and their components.","[' What can be used to make the optimal modulation to improve the transmission performance?', ' Sensor nodes can be imagined as small computers which are what?']","['the cross-layer', 'extremely basic']"
299,wireless sensor network,Characteristics,"The base stations are one or more components of the WSN with much more computational, energy and communication resources. They act as a gateway between sensor nodes and the end user as they typically forward data from the WSN on to a server. Other special components in routing based networks are routers, designed to compute, calculate and distribute the routing tables.","The base stations are one or more components of the WSN with much more computational, energy and communication resources. They act as a gateway between sensor nodes and the end user as they typically forward data from the WSN on to a server.","[' What are base stations?', ' What do base stations act as between sensor nodes and end user?']","['one or more components of the WSN with much more computational, energy and communication resources', 'gateway']"
300,wireless sensor network,Simulation,"At present, agent-based modeling and simulation is the only paradigm which allows the simulation of complex behavior in the environments of wireless sensors (such as flocking). Agent-based simulation of wireless sensor and ad hoc networks is a relatively new paradigm. Agent-based modelling was originally based on social simulation.
","At present, agent-based modeling and simulation is the only paradigm which allows the simulation of complex behavior in the environments of wireless sensors (such as flocking). Agent-based simulation of wireless sensor and ad hoc networks is a relatively new paradigm.","[' What is the only paradigm that allows the simulation of complex behavior in the environments of wireless sensors?', ' What is a relatively new paradigm?']","['agent-based modeling and simulation', 'Agent-based simulation of wireless sensor and ad hoc networks']"
301,logic program,Summary,"Logic programming is a programming paradigm which is largely based on formal logic. Any program written in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain.  Major logic programming language families include Prolog, answer set programming (ASP) and Datalog. In all of these languages, rules are written in the form of clauses:
","Logic programming is a programming paradigm which is largely based on formal logic. Any program written in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain.","[' What is a programming paradigm which is largely based on formal logic?', ' Any program written in a logic programming language is what?', ' A set of sentences in logical form expresses facts and rules about some problem domain?']","['Logic programming', 'a set of sentences in logical form', 'Logic programming']"
302,logic program,Summary,"H is called the head of the rule and B1, ..., Bn is called the body. Facts are rules that have no body, and are written in the simplified form:
","H is called the head of the rule and B1, ..., Bn is called the body. Facts are rules that have no body, and are written in the simplified form:","[' What is the head of a rule?', ' What is B1 called?', ' Where are rules written that have no body?']","['H', 'the body', 'Facts']"
303,logic program,Summary,"In the simplest case in which H, B1, ..., Bn are all atomic formulae, these clauses are called definite clauses or Horn clauses. However, there are many extensions of this simple case, the most important one being the case in which conditions in the body of a clause can also be negations of atomic formulas. Logic programming languages that include this extension have the knowledge representation capabilities of a non-monotonic logic.
","In the simplest case in which H, B1, ..., Bn are all atomic formulae, these clauses are called definite clauses or Horn clauses. However, there are many extensions of this simple case, the most important one being the case in which conditions in the body of a clause can also be negations of atomic formulas.","[' What are definite clauses called in the simplest case?', ' What are Horn clauses?', ' Which conditions in the body of a clause can also be negations of atomic formulas?']","['Horn clauses', 'definite clauses', 'the case in which conditions']"
304,logic program,Summary,"In ASP and Datalog, logic programs have only a declarative reading, and their execution is performed by means of a proof procedure or model generator whose behaviour is not meant to be controlled by the programmer. However, in the Prolog family of languages, logic programs also have a procedural interpretation as goal-reduction procedures:
","In ASP and Datalog, logic programs have only a declarative reading, and their execution is performed by means of a proof procedure or model generator whose behaviour is not meant to be controlled by the programmer. However, in the Prolog family of languages, logic programs also have a procedural interpretation as goal-reduction procedures:","[' In ASP and Datalog, what type of reading do logic programs have?', ' What type of program executes logic programs by means of a proof procedure or model generator?', ' In the Prolog family of languages, logic programs also have what kind of interpretation?', ' What type of interpretation do logic programs have?']","['declarative', 'ASP and Datalog', 'procedural', 'procedural']"
305,logic program,Summary,"based on an example used by Terry Winograd to illustrate the programming language Planner. As a clause in a logic program, it can be used both as a procedure to test whether X is fallible by testing whether X is human, and as a procedure to find an X which is fallible by finding an X which is human. Even facts have a procedural interpretation. For example, the clause:
","based on an example used by Terry Winograd to illustrate the programming language Planner. As a clause in a logic program, it can be used both as a procedure to test whether X is fallible by testing whether X is human, and as a procedure to find an X which is fallible by finding an X which is human.","[' Who used an example to illustrate the programming language Planner?', ' What can be used as a clause in a logic program?', ' What is a procedure to find an X which is fallible?']","['Terry Winograd', 'it can be used both as a procedure to test whether X is fallible', 'by finding an X which is human']"
306,logic program,Summary,"The declarative reading of logic programs can be used by a programmer to verify their correctness. Moreover, logic-based program transformation techniques can also be used to transform logic programs into logically equivalent programs that are more efficient. In the Prolog family of logic programming languages, the programmer can also use the known problem-solving behaviour of the execution mechanism to improve the efficiency of programs.
","The declarative reading of logic programs can be used by a programmer to verify their correctness. Moreover, logic-based program transformation techniques can also be used to transform logic programs into logically equivalent programs that are more efficient.","[' What can be used by a programmer to verify their correctness?', ' What can also be used to transform logic programs into logically equivalent programs that are more efficient?']","['declarative reading of logic programs', 'logic-based program transformation techniques']"
307,logic program,History,"The use of mathematical logic to represent and execute computer programs is also a feature of the lambda calculus, developed by Alonzo Church in the 1930s. However, the first proposal to use the clausal form of logic for representing computer programs was made by Cordell Green. This used an axiomatization of a subset of LISP, together with a representation of an input-output relation, to compute the relation by simulating the execution of the program in LISP. Foster and Elcock's Absys, on the other hand, employed a combination of equations and lambda calculus in an assertional programming language which places no constraints on the order in which operations are performed.","The use of mathematical logic to represent and execute computer programs is also a feature of the lambda calculus, developed by Alonzo Church in the 1930s. However, the first proposal to use the clausal form of logic for representing computer programs was made by Cordell Green.","[' Who developed lambda calculus in the 1930s?', ' Who made the first proposal to use the clausal form of logic for computer programs?']","['Alonzo Church', 'Cordell Green']"
308,logic program,History,"Logic programming in its present form can be traced back to debates in the late 1960s and early 1970s about declarative versus procedural representations of knowledge in artificial intelligence. Advocates of declarative representations were notably working at Stanford, associated with John McCarthy, Bertram Raphael and Cordell Green, and in Edinburgh, with John Alan Robinson (an academic visitor from Syracuse University), Pat Hayes, and Robert Kowalski. Advocates of procedural representations were mainly centered at MIT, under the leadership of Marvin Minsky and Seymour Papert.","Logic programming in its present form can be traced back to debates in the late 1960s and early 1970s about declarative versus procedural representations of knowledge in artificial intelligence. Advocates of declarative representations were notably working at Stanford, associated with John McCarthy, Bertram Raphael and Cordell Green, and in Edinburgh, with John Alan Robinson (an academic visitor from Syracuse University), Pat Hayes, and Robert Kowalski.","[' Logic programming in its present form can be traced back to the debates in the late 1960s and early 1970s about what?', ' Advocates of declarative representations were notably working at what university?', ' John McCarthy, Bertram Raphael, and Cordell Green were in Edinburgh with whom?', ' John Alan Robinson was an academic visitor from what university?', ' Pat Hays and Robert Kowalski were in what city?']","['declarative versus procedural representations of knowledge in artificial intelligence', 'Stanford', 'John Alan Robinson (an academic visitor from Syracuse University), Pat Hayes, and Robert Kowalski', 'Syracuse University', 'Edinburgh']"
309,logic program,History,"Although it was based on the proof methods of logic, Planner, developed at MIT, was the first language to emerge within this proceduralist paradigm. Planner featured pattern-directed invocation of procedural plans from goals (i.e. goal-reduction or backward chaining) and from assertions (i.e. forward chaining). The most influential implementation of Planner was the subset of Planner, called Micro-Planner, implemented by Gerry Sussman, Eugene Charniak and Terry Winograd. It was used to implement Winograd's natural-language understanding program SHRDLU, which was a landmark at that time. To cope with the very limited memory systems at the time, Planner used a backtracking control structure so that only one possible computation path had to be stored at a time. Planner gave rise to the programming languages QA-4, Popler, Conniver, QLISP, and the concurrent language Ether.","Although it was based on the proof methods of logic, Planner, developed at MIT, was the first language to emerge within this proceduralist paradigm. Planner featured pattern-directed invocation of procedural plans from goals (i.e.","[' Where was Planner developed?', ' What was the first language to emerge within the proceduralist paradigm?', ' Planner featured pattern-directed invocation of what?']","['MIT', 'Planner', 'procedural plans from goals']"
310,logic program,History,"Hayes and Kowalski in Edinburgh tried to reconcile the logic-based declarative approach to knowledge representation with Planner's procedural approach. Hayes (1973) developed an equational language, Golux, in which different procedures could be obtained by altering the behavior of the theorem prover. Kowalski, on the other hand, developed SLD resolution, a variant of SL-resolution, and showed how it treats implications as goal-reduction procedures. Kowalski collaborated with Colmerauer in Marseille, who developed these ideas in the design and implementation of the programming language Prolog.
","Hayes and Kowalski in Edinburgh tried to reconcile the logic-based declarative approach to knowledge representation with Planner's procedural approach. Hayes (1973) developed an equational language, Golux, in which different procedures could be obtained by altering the behavior of the theorem prover.","["" What did Hayes and Kowalski try to reconcile with Planner's procedural approach to knowledge representation?"", "" What was the name of the equational language Haye's developed in 1973?"", ' How could different procedures be obtained by altering the behavior of the theorem prover?']","['logic-based declarative approach', 'Golux', 'Golux']"
311,virtual reality,Summary,"Virtual reality (VR) is a simulated experience that can be similar to or completely different from the real world. Applications of virtual reality include entertainment (particularly video games), education (such as medical or military training) and business (such as virtual meetings). Other distinct types of VR-style technology include augmented reality and mixed reality, sometimes referred to as extended reality or XR.","Virtual reality (VR) is a simulated experience that can be similar to or completely different from the real world. Applications of virtual reality include entertainment (particularly video games), education (such as medical or military training) and business (such as virtual meetings).","[' What does VR stand for?', ' What are some examples of applications of virtual reality?']","['Virtual reality', 'entertainment (particularly video games), education (such as medical or military training) and business']"
312,virtual reality,Summary,"Currently, standard virtual reality systems use either virtual reality headsets or multi-projected environments to generate realistic images, sounds and other sensations that simulate a user's physical presence in a virtual environment. A person using virtual reality equipment is able to look around the artificial world, move around in it, and interact with virtual features or items. The effect is commonly created by VR headsets consisting of a head-mounted display with a small screen in front of the eyes, but can also be created through specially designed rooms with multiple large screens. Virtual reality typically incorporates auditory and video feedback, but may also allow other types of sensory and force feedback through haptic technology.
","Currently, standard virtual reality systems use either virtual reality headsets or multi-projected environments to generate realistic images, sounds and other sensations that simulate a user's physical presence in a virtual environment. A person using virtual reality equipment is able to look around the artificial world, move around in it, and interact with virtual features or items.","[' What do standard virtual reality systems use to generate realistic images, sounds and other sensations?', ' What does a person using virtual reality equipment look around the artificial world, move around in it, and do?', ' How do you look around the artificial world?', ' What do you interact with virtual features or items?']","['virtual reality headsets or multi-projected environments', 'interact with virtual features or items', 'move around in it', 'A person using virtual reality equipment']"
313,virtual reality,Etymology,"""Virtual"" has had the meaning of ""being something in essence or effect, though not actually or in fact"" since the mid-1400s. The term ""virtual"" has been used in the computer sense of ""not physically existing but made to appear by software"" since 1959.","""Virtual"" has had the meaning of ""being something in essence or effect, though not actually or in fact"" since the mid-1400s. The term ""virtual"" has been used in the computer sense of ""not physically existing but made to appear by software"" since 1959.","[' Since when has the term ""virtual"" had the meaning of being something in essence or effect, though not actually or fact?', ' Since 1959, what has been used in the computer sense of ""not physically existing but made to appear by software?""']","['mid-1400s', 'virtual']"
314,virtual reality,Etymology,"In 1938, French avant-garde playwright Antonin Artaud described the illusory nature of characters and objects in the theatre as ""la réalité virtuelle"" in a collection of essays, Le Théâtre et son double. The English translation of this book, published in 1958 as The Theater and its Double, is the earliest published use of the term ""virtual reality"". The term ""artificial reality"", coined by Myron Krueger, has been in use since the 1970s. The term ""virtual reality"" was first used in a science fiction context in The Judas Mandala, a 1982 novel by Damien Broderick.
","In 1938, French avant-garde playwright Antonin Artaud described the illusory nature of characters and objects in the theatre as ""la réalité virtuelle"" in a collection of essays, Le Théâtre et son double. The English translation of this book, published in 1958 as The Theater and its Double, is the earliest published use of the term ""virtual reality"".","[' Who described the illusory nature of characters and objects in the theatre as ""la réalité virtuelle""?', ' What was the name of the collection of essays by Antonin Artaud?', ' When was the English translation of Le Théâtre et son double published?', ' When was The Theater and its Double published?', ' What is the earliest published use of the term ""virtual reality""?']","['Antonin Artaud', 'Le Théâtre et son double', '1958', '1958', 'The Theater and its Double']"
315,virtual reality,Forms and methods,"One method by which virtual reality can be realized is simulation-based virtual reality. Driving simulators, for example, give the driver on board the impression of actually driving an actual vehicle by predicting vehicular motion caused by driver input and feeding back corresponding visual, motion and audio cues to the driver.
","One method by which virtual reality can be realized is simulation-based virtual reality. Driving simulators, for example, give the driver on board the impression of actually driving an actual vehicle by predicting vehicular motion caused by driver input and feeding back corresponding visual, motion and audio cues to the driver.","[' What is one method by which virtual reality can be realized?', ' What gives the driver on board the impression of actually driving an actual vehicle?']","['simulation-based virtual reality', 'Driving simulators']"
316,virtual reality,Forms and methods,"With avatar image-based virtual reality, people can join the virtual environment in the form of real video as well as an avatar. One can participate in the 3D distributed virtual environment as form of either a conventional avatar or a real video. Users can select their own type of participation based on the system capability.
","With avatar image-based virtual reality, people can join the virtual environment in the form of real video as well as an avatar. One can participate in the 3D distributed virtual environment as form of either a conventional avatar or a real video.","[' What can people join the virtual environment in the form of?', ' What can one participate in the 3D distributed virtual environment as form of a conventional avatar or real video?']","['real video', 'avatar']"
317,virtual reality,Forms and methods,"In projector-based virtual reality, modeling of the real environment plays a vital role in various virtual reality applications, such as robot navigation, construction modeling, and airplane simulation. Image-based virtual reality systems have been gaining popularity in computer graphics and computer vision communities. In generating realistic models, it is essential to accurately register acquired 3D data; usually, a camera is used for modeling small objects at a short distance.
","In projector-based virtual reality, modeling of the real environment plays a vital role in various virtual reality applications, such as robot navigation, construction modeling, and airplane simulation. Image-based virtual reality systems have been gaining popularity in computer graphics and computer vision communities.","[' Projector-based virtual reality plays a vital role in what kind of applications?', ' What are some of the applications of projector based VR?', ' Image-based VR systems have been gaining popularity in what areas?']","['virtual reality', 'robot navigation, construction modeling, and airplane simulation', 'computer graphics and computer vision']"
318,virtual reality,Forms and methods,"Desktop-based virtual reality involves displaying a 3D virtual world on a regular desktop display without use of any specialized VR positional tracking equipment. Many modern first-person video games can be used as an example, using various triggers, responsive characters, and other such interactive devices to make the user feel as though they are in a virtual world. A common criticism of this form of immersion is that there is no sense of peripheral vision, limiting the user's ability to know what is happening around them.
","Desktop-based virtual reality involves displaying a 3D virtual world on a regular desktop display without use of any specialized VR positional tracking equipment. Many modern first-person video games can be used as an example, using various triggers, responsive characters, and other such interactive devices to make the user feel as though they are in a virtual world.","[' What type of virtual reality involves displaying a 3D virtual world on a regular desktop display without use of any specialized VR positional tracking equipment?', ' Many modern first-person video games can be used as an example of what?', ' What do interactive devices make the user feel like they are in?']","['Desktop-based', 'Desktop-based virtual reality', 'a virtual world']"
319,virtual reality,Forms and methods,"A head-mounted display (HMD) more fully immerses the user in a virtual world. A virtual reality headset typically includes two small high resolution OLED or LCD monitors which provide separate images for each eye for stereoscopic graphics rendering a 3D virtual world, a binaural audio system, positional and rotational real-time head tracking for six degrees of movement. Options include motion controls with haptic feedback for physically interacting within the virtual world in an intuitive way with little to no abstraction and an omnidirectional treadmill for more freedom of physical movement allowing the user to perform locomotive motion in any direction.
","A head-mounted display (HMD) more fully immerses the user in a virtual world. A virtual reality headset typically includes two small high resolution OLED or LCD monitors which provide separate images for each eye for stereoscopic graphics rendering a 3D virtual world, a binaural audio system, positional and rotational real-time head tracking for six degrees of movement.","[' What is another name for a head-mounted display?', ' What does HMD stand for?', ' How many monitors does a virtual reality headset typically have?', ' How many degrees of movement does the head tracker have?', ' What kind of audio system is used?']","['HMD', 'head-mounted display', 'two', 'six', 'binaural']"
320,virtual reality,Forms and methods,"Augmented reality (AR) is a type of virtual reality technology that blends what the user sees in their real surroundings with digital content generated by computer software. The additional software-generated images with the virtual scene typically enhance how the real surroundings look in some way. AR systems layer virtual information over a camera live feed into a headset or smartglasses or through a mobile device giving the user the ability to view three-dimensional images.
",Augmented reality (AR) is a type of virtual reality technology that blends what the user sees in their real surroundings with digital content generated by computer software. The additional software-generated images with the virtual scene typically enhance how the real surroundings look in some way.,"[' What is augmented reality?', ' What does AR combine with real surroundings?', ' How does the software-generated images with the virtual scene look?']","['a type of virtual reality technology', 'digital content generated by computer software', 'enhance how the real surroundings look']"
321,virtual reality,History,"The exact origins of virtual reality are disputed, partly because of how difficult it has been to formulate a definition for the concept of an alternative existence. The development of perspective in Renaissance Europe created convincing depictions of spaces that did not exist, in what has been referred to as the ""multiplying of artificial worlds"". Other elements of virtual reality appeared as early as the 1860s. Antonin Artaud took the view that illusion was not distinct from reality, advocating that spectators at a play should suspend disbelief and regard the drama on stage as reality. The first references to the more modern concept of virtual reality came from science fiction.
","The exact origins of virtual reality are disputed, partly because of how difficult it has been to formulate a definition for the concept of an alternative existence. The development of perspective in Renaissance Europe created convincing depictions of spaces that did not exist, in what has been referred to as the ""multiplying of artificial worlds"".","[' The origins of virtual reality are disputed partly because of how difficult it has been to formulate a definition for the concept of what?', ' The development of perspective in Renaissance Europe created convincing depictions of spaces that did not exist, what is referred to as?', ' What has been referred to as the ""multiplying of artificial worlds""?']","['an alternative existence', 'multiplying of artificial worlds', 'The development of perspective in Renaissance Europe created convincing depictions of spaces that did not exist']"
322,virtual reality,Applications,"Virtual reality is most commonly used in entertainment applications such as video games, 3D cinema, and social virtual worlds. Consumer virtual reality headsets were first released by video game companies in the early-mid 1990s. Beginning in the 2010s, next-generation commercial tethered headsets were released by Oculus (Rift), HTC (Vive) and Sony (PlayStation VR), setting off a new wave of application development. 3D cinema has been used for sporting events, pornography, fine art, music videos and short films. Since 2015, roller coasters and theme parks have incorporated virtual reality to match visual effects with haptic feedback.","Virtual reality is most commonly used in entertainment applications such as video games, 3D cinema, and social virtual worlds. Consumer virtual reality headsets were first released by video game companies in the early-mid 1990s.","[' What is the most common use of virtual reality?', ' When were consumer virtual reality headsets first released?']","['entertainment applications', 'early-mid 1990s']"
323,virtual reality,Applications,"In social sciences and psychology, virtual reality offers a cost-effective tool to study and replicate interactions in a controlled environment. It can be used as a form of therapeutic intervention. For instance, there is the case of the virtual reality exposure therapy (VRET), a form of exposure therapy for treating anxiety disorders such as post traumatic stress disorder (PTSD) and phobias.","In social sciences and psychology, virtual reality offers a cost-effective tool to study and replicate interactions in a controlled environment. It can be used as a form of therapeutic intervention.","[' What can virtual reality be used for in social sciences and psychology?', ' What is a cost-effective way to study interactions in a controlled environment?']","['therapeutic intervention', 'virtual reality']"
324,virtual reality,Applications,Virtual reality programs are being used in the rehabilitation processes with elderly individuals that have been diagnosed with Alzheimer's disease. This gives these elderly patients the opportunity to simulate real experiences that they would not otherwise be able to experience due to their current state. 17 recent studies with randomized controlled trials have shown that virtual reality applications are effective in treating cognitive deficits with neurological diagnoses. Loss of mobility in elderly patients can lead to a sense of loneliness and depression. Virtual reality is able to assist in making aging in place a lifeline to an outside world that they cannot easily navigate. Virtual reality allows exposure therapy to take place in a safe environment.,Virtual reality programs are being used in the rehabilitation processes with elderly individuals that have been diagnosed with Alzheimer's disease. This gives these elderly patients the opportunity to simulate real experiences that they would not otherwise be able to experience due to their current state.,"["" What is being used in the rehabilitation processes with elderly individuals that have been diagnosed with Alzheimer's disease?"", ' What gives elderly patients the opportunity to simulate real experiences that they would not otherwise be able to experience due to their current state?']","['Virtual reality programs', 'Virtual reality programs']"
325,virtual reality,Applications,"In medicine, simulated VR surgical environments were first developed in the 1990s.  Under the supervision of experts, VR can provide effective and repeatable training at a low cost, allowing trainees to recognize and amend errors as they occur.","In medicine, simulated VR surgical environments were first developed in the 1990s. Under the supervision of experts, VR can provide effective and repeatable training at a low cost, allowing trainees to recognize and amend errors as they occur.","[' When were simulated VR surgical environments first developed?', ' Under the supervision of experts, what can VR provide effective and repeatable training at a low cost?']","['1990s', 'allowing trainees to recognize and amend errors as they occur']"
326,virtual reality,Applications,"Virtual reality has been used in physical rehabilitation since the 2000s. Despite numerous studies conducted, good quality evidence of its efficacy compared to other rehabilitation methods without sophisticated and expensive equipment is lacking for the treatment of Parkinson's disease. A 2018 review on the effectiveness of mirror therapy by virtual reality and robotics for any type of pathology concluded in a similar way. Another study was conducted that showed the potential for VR to promote mimicry and revealed the difference between neurotypical and autism spectrum disorder individuals in their response to a two-dimensional avatar.","Virtual reality has been used in physical rehabilitation since the 2000s. Despite numerous studies conducted, good quality evidence of its efficacy compared to other rehabilitation methods without sophisticated and expensive equipment is lacking for the treatment of Parkinson's disease.","[' Since when has virtual reality been used in physical rehabilitation?', "" What is lacking for the treatment of Parkinson's disease?""]","['since the 2000s', 'good quality evidence of its efficacy compared to other rehabilitation methods without sophisticated and expensive equipment']"
327,virtual reality,Applications,Immersive virtual reality technology with myoelectric and motion tracking control may represent a possible therapy option for treatment-resistant phantom limb pain. Pain scale measurements were taken into account and an interactive 3-D kitchen environment was developed bases on the principles of mirror therapy to allow for control of virtual hands while wearing a motion-tracked VR headset. A systematic search in Pubmed and Embase was performed to determine results that were pooled in two meta-analysis. Meta-analysis showed a significant result in favor of VRT for balance.,Immersive virtual reality technology with myoelectric and motion tracking control may represent a possible therapy option for treatment-resistant phantom limb pain. Pain scale measurements were taken into account and an interactive 3-D kitchen environment was developed bases on the principles of mirror therapy to allow for control of virtual hands while wearing a motion-tracked VR headset.,"[' What is a possible therapy option for treatment resistant phantom limb pain?', ' What was developed based on the principles of mirror therapy?', ' Mirror therapy allows for control of virtual hands while wearing what?']","['Immersive virtual reality technology', 'an interactive 3-D kitchen environment', 'motion-tracked VR headset']"
328,virtual reality,Applications,"
In the fast-paced and globalised business world meetings in VR are used to create an environment in which interactions with other people (e.g. colleagues, customers, partners) can feel more natural than a phone call or video chat. In the customisable meeting rooms all parties can join using the VR headset and interact as if they are in the same physical room. Presentations, videos or 3D models (of e.g. products or prototypes) can be uploaded and interacted with.","
In the fast-paced and globalised business world meetings in VR are used to create an environment in which interactions with other people (e.g. colleagues, customers, partners) can feel more natural than a phone call or video chat.",[' Meetings in VR are used to create an environment in which interactions with other people can feel more natural than a phone call or what?'],['video chat']
329,virtual reality,Applications,"VR can simulate real workspaces for workplace occupational safety and health purposes, educational purposes, and training purposes. It can be used to provide learners with a virtual environment where they can develop their skills without the real-world consequences of failing. It has been used and studied in primary education, anatomy teaching, military, astronaut training, flight simulators, miner training, medical education, architectural design, driver training and bridge inspection. Immersive VR engineering systems enable engineers to see virtual prototypes prior to the availability of any physical prototypes. Supplementing training with virtual training environments has been claimed to offer avenues of realism in military and healthcare training while minimizing cost. It also has been claimed to reduce military training costs by minimizing the amounts of ammunition expended during training periods. VR can also be used for the healthcare training and education for medical practitioners.","VR can simulate real workspaces for workplace occupational safety and health purposes, educational purposes, and training purposes. It can be used to provide learners with a virtual environment where they can develop their skills without the real-world consequences of failing.","[' What can VR simulate for workplace occupational safety and health purposes, educational purposes, and training purposes?', ' What does VR provide learners with?']","['real workspaces', 'a virtual environment where they can develop their skills without the real-world consequences of failing']"
330,virtual reality,Applications,"In the engineering field, VR has proved very useful for both engineering educators and the students. A previously expensive cost in the educational department now being much more accessible due to lowered overall costs, has proven to be a very useful tool in educating future engineers. The most significant element lies in the ability for the students to be able to interact with 3-D models that accurately respond based on real world possibilities. This added tool of education provides many the immersion needed to grasp complex topics and be able to apply them. As noted, the future architects and engineers benefit greatly by being able to form understandings between spatial relationships and providing solutions based on real-world future applications.","In the engineering field, VR has proved very useful for both engineering educators and the students. A previously expensive cost in the educational department now being much more accessible due to lowered overall costs, has proven to be a very useful tool in educating future engineers.","[' What has VR proved very useful for in the engineering field?', ' What was previously expensive in the educational department now being much more accessible?']","['engineering educators and the students', 'VR']"
331,virtual reality,Applications,"The first fine art virtual world was created in the 1970s. As the technology developed, more artistic programs were produced throughout the 1990s, including feature films. When commercially available technology became more widespread, VR festivals began to emerge in the mid-2010s. The first uses of VR in museum settings began in the 1990s, seeing a significant increase in the mid-2010s. Additionally, museums have begun making some of their content virtual reality accessible.","The first fine art virtual world was created in the 1970s. As the technology developed, more artistic programs were produced throughout the 1990s, including feature films.","[' When was the first fine art virtual world created?', ' When were more artistic programs produced?']","['1970s', 'throughout the 1990s']"
332,virtual reality,Applications,"Virtual reality's growing market presents an opportunity and an alternative channel for digital marketing. It is also seen as a new platform for e-commerce, particularly in the bid to challenge traditional ""brick and mortar"" retailers. However, a 2018 study revealed that the majority of goods are still purchased in physical stores.","Virtual reality's growing market presents an opportunity and an alternative channel for digital marketing. It is also seen as a new platform for e-commerce, particularly in the bid to challenge traditional ""brick and mortar"" retailers.","[' What is a new platform for e-commerce?', "" What is virtual reality's growing market?""]","['Virtual reality', 'presents an opportunity and an alternative channel for digital marketing']"
333,virtual reality,Applications,"A case has also been made for including virtual reality technology in the context of public libraries. This would give library users access to cutting-edge technology and unique educational experiences. This could include giving users access to virtual, interactive copies of rare texts and artifacts and to tours of famous landmarks and archeological digs (as in the case with the Virtual Ganjali Khan Project).",A case has also been made for including virtual reality technology in the context of public libraries. This would give library users access to cutting-edge technology and unique educational experiences.,"[' What type of technology has been advocated for in public libraries?', ' What would virtual reality technology give library users access to?']","['virtual reality', 'cutting-edge technology and unique educational experiences']"
334,virtual reality,Applications,"Starting in the early 2020s, virtual reality has also been discussed as a technological setting that may support people's griefing process, based on digital recreations of deceased individuals. In 2021, this practice received substantial media attention following a South Korean TV documentary, which invited a griefing mother to interact with a virtual replica of her deceased daughter. Subsequently, scientists have summarized several potential implications of such endeavours, including its potential to facilitate adaptive mourning, but also many ethical challenges.  ","Starting in the early 2020s, virtual reality has also been discussed as a technological setting that may support people's griefing process, based on digital recreations of deceased individuals. In 2021, this practice received substantial media attention following a South Korean TV documentary, which invited a griefing mother to interact with a virtual replica of her deceased daughter.","["" In what decade has virtual reality been discussed as a technological setting that may support people's griefing process?"", ' What is based on digital recreations of deceased individuals?', ' In what year did virtual reality receive media attention?', ' Who was invited to interact with a griefing mother in a South Korean TV documentary?', ' What type of documentary invited a griefing mother to interact with a virtual replica of her deceased daughter?']","['2020s', 'virtual reality', '2021', 'her deceased daughter', 'South Korean TV']"
335,virtual reality,Concerts,"On October 24th, 2021, Billie Eilish performed on Oculus Venues. Pop group Imagine Dragons will perform on June 15, 2022.
","On October 24th, 2021, Billie Eilish performed on Oculus Venues. Pop group Imagine Dragons will perform on June 15, 2022.","[' On what date did Billie Eilish perform on Oculus Venues?', ' What pop group will perform on June 15, 2022?']","['October 24th, 2021', 'Imagine Dragons']"
336,particle swarm optimization,Summary,"In computational science, particle swarm optimization (PSO) is a computational method that optimizes a problem by iteratively trying to improve a candidate solution with regard to a given measure of quality. It solves a problem by having a population of candidate solutions, here dubbed particles, and moving these particles around in the search-space according to simple mathematical formula over the particle's position and velocity. Each particle's movement is influenced by its local best known position, but is also guided toward the best known positions in the search-space, which are updated as better positions are found by other particles. This is expected to move the swarm toward the best solutions.
","In computational science, particle swarm optimization (PSO) is a computational method that optimizes a problem by iteratively trying to improve a candidate solution with regard to a given measure of quality. It solves a problem by having a population of candidate solutions, here dubbed particles, and moving these particles around in the search-space according to simple mathematical formula over the particle's position and velocity.","[' What is particle swarm optimization?', ' What is PSO?', ' How does PSO solve a problem?', ' What are candidate solutions dubbed?', ' What is a simple mathematical formula for moving particles around in the search-space?']","['a computational method that optimizes a problem by iteratively trying to improve a candidate solution with regard to a given measure of quality', 'particle swarm optimization', 'by having a population of candidate solutions', 'particles', ""over the particle's position and velocity""]"
337,particle swarm optimization,Summary,"PSO is originally attributed to Kennedy, Eberhart and Shi and was first intended for simulating social behaviour, as a stylized representation of the movement of organisms in a bird flock or fish school. The algorithm was simplified and it was observed to be performing optimization. The book by Kennedy and Eberhart describes many philosophical aspects of PSO and swarm intelligence. An extensive  survey of PSO applications is made by Poli. Recently, a comprehensive review on theoretical and experimental works on PSO has been published by Bonyadi and Michalewicz.","PSO is originally attributed to Kennedy, Eberhart and Shi and was first intended for simulating social behaviour, as a stylized representation of the movement of organisms in a bird flock or fish school. The algorithm was simplified and it was observed to be performing optimization.","[' Who are the original attributions of PSO?', ' What was PSO originally intended to simulate?', ' How was the PSO algorithm simplified?']","['Kennedy, Eberhart and Shi', 'social behaviour', 'it was observed to be performing optimization']"
338,particle swarm optimization,Summary,"PSO is a metaheuristic as it makes few or no assumptions about the problem being optimized and can search very large spaces of candidate solutions. Also, PSO does not use the gradient of the problem being optimized, which means PSO does not require that the optimization problem be differentiable as is required by classic optimization methods such as gradient descent and quasi-newton methods. However, metaheuristics such as PSO do not guarantee an optimal solution is ever found. 
","PSO is a metaheuristic as it makes few or no assumptions about the problem being optimized and can search very large spaces of candidate solutions. Also, PSO does not use the gradient of the problem being optimized, which means PSO does not require that the optimization problem be differentiable as is required by classic optimization methods such as gradient descent and quasi-newton methods.","[' What is a metaheuristic?', ' What can PSO search very large spaces of?', ' PSO does not use what?', ' Does not require that the optimization problem be differentiable as is required by classic methods such as gradient descent and quasi-newton methods?']","['PSO', 'candidate solutions', 'the gradient of the problem being optimized', 'PSO']"
339,particle swarm optimization,Algorithm,"A basic variant of the PSO algorithm works by having a population (called a swarm) of candidate solutions (called particles). These particles are moved around in the search-space according to a few simple formulae. The movements of the particles are guided by their own best-known position in the search-space as well as the entire swarm's best-known position. When improved positions are being discovered these will then come to guide the movements of the swarm. The process is repeated and by doing so it is hoped, but not guaranteed, that a satisfactory solution will eventually be discovered.
",A basic variant of the PSO algorithm works by having a population (called a swarm) of candidate solutions (called particles). These particles are moved around in the search-space according to a few simple formulae.,"[' What is the basic variant of the PSO algorithm called?', ' What is a swarm of candidate solutions?', ' How are particles moved around in the search-space?']","['a swarm', 'a population', 'according to a few simple formulae']"
340,particle swarm optimization,Algorithm,"Formally, let f: ℝn → ℝ be the cost function which must be minimized. The function takes a candidate solution as an argument in the form of a vector of real numbers and produces a real number as output which indicates the objective function value of the given candidate solution. The gradient of f is not known. The goal is to find a solution a for which f(a) ≤ f(b) for all b in the search-space, which would mean a is the global minimum.
","Formally, let f: ℝn → ℝ be the cost function which must be minimized. The function takes a candidate solution as an argument in the form of a vector of real numbers and produces a real number as output which indicates the objective function value of the given candidate solution.","[' Let f: Rn <unk> R be the cost function which must be minimized?', ' The function takes a candidate solution as an argument in the form of what?']","['ℝn\xa0→ ℝ', 'a vector of real numbers']"
341,particle swarm optimization,Algorithm,"Let S be the number of particles in the swarm, each having a position xi ∈ ℝn in the search-space and a velocity vi ∈ ℝn. Let pi be the best known position of particle i and let g be the best known position of the entire swarm. A basic PSO algorithm is then:","Let S be the number of particles in the swarm, each having a position xi ∈ ℝn in the search-space and a velocity vi ∈ ℝn. Let pi be the best known position of particle i and let g be the best known position of the entire swarm.","[' Let S be the number of particles in the swarm, each having a position xi <unk> Rn in what space?', ' Let pi be the best known position of particle i and let g be what?']","['search-space', 'the best known position of the entire swarm']"
342,particle swarm optimization,Algorithm,"The values blo and bup represent the lower and upper boundaries of the search-space respectively. The w parameter is the inertia weight.  The parameters φp  and  φg are often called cognitive coefficient and social coefficient.
",The values blo and bup represent the lower and upper boundaries of the search-space respectively. The w parameter is the inertia weight.,"[' What represent the lower and upper boundaries of the search-space?', ' What are the values blo and bup?', ' The w parameter is what?']","['blo and bup', 'lower and upper boundaries of the search-space', 'inertia weight']"
343,particle swarm optimization,Algorithm,"The termination criterion can be the number of iterations performed, or a solution where the adequate objective function value is found. The parameters w, φp, and φg are selected by the practitioner and control the behaviour and efficacy of the PSO method (below). 
","The termination criterion can be the number of iterations performed, or a solution where the adequate objective function value is found. The parameters w, φp, and φg are selected by the practitioner and control the behaviour and efficacy of the PSO method (below).","[' What is the termination criterion?', ' What are the parameters selected by the practitioner and control the behaviour and efficacy of the PSO method?']","['the number of iterations performed', 'w, φp, and φg']"
344,particle swarm optimization,Parameter selection,The choice of PSO parameters can have a large impact on optimization performance. Selecting PSO parameters that yield good performance has therefore been the subject of much research.,The choice of PSO parameters can have a large impact on optimization performance. Selecting PSO parameters that yield good performance has therefore been the subject of much research.,"[' What can have a large impact on optimization performance?', ' What has been the subject of much research?']","['The choice of PSO parameters', 'Selecting PSO parameters that yield good performance']"
345,particle swarm optimization,Parameter selection,"To prevent divergence (""explosion"") the inertia weight must be smaller than 1. The two other parameters  can be then derived thanks to the constriction approach, or freely selected, but the analyses suggest convergence domains to constrain them. Typical values are in 



[
1
,
3
]


{\displaystyle [1,3]}
.
","To prevent divergence (""explosion"") the inertia weight must be smaller than 1. The two other parameters  can be then derived thanks to the constriction approach, or freely selected, but the analyses suggest convergence domains to constrain them.","[' To prevent divergence, what must the inertia weight be?', ' How many other parameters can be derived thanks to the constriction approach?', ' The analyses suggest what to constrain them?']","['smaller than 1', 'two', 'convergence domains']"
346,particle swarm optimization,Neighbourhoods and topologies,"The topology of the swarm defines the subset of particles with which each particle can exchange information. The basic version of the algorithm uses the global topology as the swarm communication structure. This topology allows all particles to communicate with all the other particles, thus the whole swarm share the same best position g from a single particle. However, this approach might lead the swarm to be trapped into a local minimum, thus different topologies have been used to control the flow of information among particles. For instance, in local topologies, particles only share information with a subset of particles. This subset can be a geometrical one – for example ""the m nearest particles"" – or, more often, a social one, i.e. a set of particles that is not depending on any distance. In such cases, the PSO variant is said to be local best (vs global best for the basic PSO).
",The topology of the swarm defines the subset of particles with which each particle can exchange information. The basic version of the algorithm uses the global topology as the swarm communication structure.,"[' What defines the subset of particles with which each particle can exchange information?', ' The basic version of the algorithm uses the global topology as what?']","['The topology of the swarm', 'the swarm communication structure']"
347,particle swarm optimization,Neighbourhoods and topologies,"A commonly used swarm topology is the ring, in which each particle has just two neighbours, but there are many others. The topology is not necessarily static. In fact, since the topology is related to the diversity of communication of the particles, some efforts have been done to create adaptive topologies (SPSO, APSO, stochastic star, TRIBES, Cyber Swarm, and C-PSO).
","A commonly used swarm topology is the ring, in which each particle has just two neighbours, but there are many others. The topology is not necessarily static.","[' What is a common swarm topology?', ' How many neighbours does each particle have?']","['the ring', 'two']"
348,particle swarm optimization,Inner workings,"A common belief amongst researchers is that the swarm behaviour varies between exploratory behaviour, that is, searching a broader region of the search-space, and exploitative behaviour, that is, a locally oriented search so as to get closer to a (possibly local) optimum. This school of thought has been prevalent since the inception of PSO. This school of thought contends that the PSO algorithm and its parameters must be chosen so as to properly balance between exploration and exploitation to avoid premature convergence to a local optimum yet still ensure a good rate of convergence to the optimum. This belief is the precursor of many PSO variants, see below.
","A common belief amongst researchers is that the swarm behaviour varies between exploratory behaviour, that is, searching a broader region of the search-space, and exploitative behaviour, that is, a locally oriented search so as to get closer to a (possibly local) optimum. This school of thought has been prevalent since the inception of PSO.","[' What is exploratory behaviour?', ' Exploitative behaviour is a local oriented search to get closer to what?', ' What school of thought has been prevalent since the inception of PSO?']","['searching a broader region of the search-space', 'a (possibly local) optimum', 'exploitative behaviour']"
349,particle swarm optimization,Inner workings,"Another school of thought is that the behaviour of a PSO swarm is not well understood in terms of how it affects actual optimization performance, especially for higher-dimensional search-spaces and optimization problems that may be discontinuous, noisy, and time-varying. This school of thought merely tries to find PSO algorithms and parameters that cause good performance regardless of how the swarm behaviour can be interpreted in relation to e.g. exploration and exploitation. Such studies have led to the simplification of the PSO algorithm, see below.
","Another school of thought is that the behaviour of a PSO swarm is not well understood in terms of how it affects actual optimization performance, especially for higher-dimensional search-spaces and optimization problems that may be discontinuous, noisy, and time-varying. This school of thought merely tries to find PSO algorithms and parameters that cause good performance regardless of how the swarm behaviour can be interpreted in relation to e.g.","[' What is not well understood in terms of actual optimization performance?', ' What school of thought tries to find PSO algorithms?', ' School of thought tries to find what?']","['the behaviour of a PSO swarm', 'PSO swarm', 'PSO algorithms and parameters']"
350,particle swarm optimization,Variants,"Numerous variants of even a basic PSO algorithm are possible. For example, there are different ways to initialize the particles and velocities (e.g. start with zero velocities instead), how to dampen the velocity, only update pi and g after the entire swarm has been updated, etc. Some of these choices and their possible performance impact have been discussed in the literature.","Numerous variants of even a basic PSO algorithm are possible. For example, there are different ways to initialize the particles and velocities (e.g.","[' How many variants of even a basic PSO algorithm are possible?', ' What are there different ways to initialize?']","['Numerous', 'particles and velocities']"
351,particle swarm optimization,Variants,"A series of standard implementations have been created by leading researchers, ""intended for use both as a baseline for performance testing of improvements to the technique, as well as to represent PSO to the wider optimization community. Having a well-known, strictly-defined standard algorithm provides a valuable point of comparison which can be used throughout the field of research to better test new advances."" The latest is Standard PSO 2011 (SPSO-2011).","A series of standard implementations have been created by leading researchers, ""intended for use both as a baseline for performance testing of improvements to the technique, as well as to represent PSO to the wider optimization community. Having a well-known, strictly-defined standard algorithm provides a valuable point of comparison which can be used throughout the field of research to better test new advances.""","[' What has been created by leading researchers?', ' What are standard implementations intended for use as a baseline for?', ' What provides a valuable point of comparison that can be used throughout the field of research to better test new advances?']","['standard implementations', 'performance testing of improvements to the technique', 'Having a well-known, strictly-defined standard algorithm']"
352,privacy,Summary,"When something is private to a person, it usually means that something is inherently special or sensitive to them. The domain of privacy partially overlaps with security, which can include the concepts of appropriate use and protection of information. Privacy may also take the form of bodily integrity. The right not to be subjected to unsanctioned invasions of privacy by the government, corporations, or individuals is part of many countries' privacy laws, and in some cases, constitutions.
","When something is private to a person, it usually means that something is inherently special or sensitive to them. The domain of privacy partially overlaps with security, which can include the concepts of appropriate use and protection of information.","[' When something is private to a person, it usually means that something is inherently special or what to them?', ' The domain of privacy overlaps with what?', ' What can include the concepts of appropriate use and protection of information?']","['sensitive', 'security', 'security']"
353,privacy,Summary,"The concept of universal individual privacy is a modern concept primarily associated with Western culture, particularly British and North American, and remained virtually unknown in some cultures until recent times. Now, most cultures recognize the ability of individuals to withhold certain parts of personal information from wider society. With the rise of technology, the debate regarding privacy has shifted from a bodily sense to a digital sense. As the world has become digital, there have been conflicts regarding the legal right to privacy and where it is applicable. In most countries, the right to a reasonable expectation to digital privacy has been extended from the original right to privacy, and many countries, notably the US, under its agency, the Federal Trade Commission, and those within the European Union (EU), have passed acts that further protect digital privacy from public and private entities and grant additional rights to users of technology.    
","The concept of universal individual privacy is a modern concept primarily associated with Western culture, particularly British and North American, and remained virtually unknown in some cultures until recent times. Now, most cultures recognize the ability of individuals to withhold certain parts of personal information from wider society.","[' What is a modern concept associated with Western culture?', ' What was virtually unknown in some cultures until recent times?', ' Most cultures recognize the ability of individuals to withhold certain parts of personal information from what?']","['universal individual privacy', 'universal individual privacy', 'wider society']"
354,privacy,Summary,"With the rise of the Internet, there has been an increase in the prevalence of social bots, causing political polarization and harassment. Online harassment has also spiked, particularly with teenagers, which has consequently resulted in multiple privacy breaches. Selfie culture, the prominence of networks like Facebook and Instagram, location technology, and the use of advertisements and their tracking methods also pose threats to digital privacy. 
","With the rise of the Internet, there has been an increase in the prevalence of social bots, causing political polarization and harassment. Online harassment has also spiked, particularly with teenagers, which has consequently resulted in multiple privacy breaches.","[' What has increased with the rise of the internet?', ' What has caused political polarization and harassment?', ' Has online harassment spiked with teenagers?']","['the prevalence of social bots', 'social bots', 'Online harassment has also spiked']"
355,privacy,Summary,"Through the rise of technology and immensity of the debate regarding privacy, there have been various conceptions of privacy, which include the right to be let alone as defined in ""The Right to Privacy"", the first U.S. publication discussing privacy as a legal right, to the theory of the privacy paradox, which describes the notion that users' online may say they are concerned about their privacy, but in reality, are not. Along with various understandings of privacy, there are actions that reduce privacy, the most recent classification includes processing of information, sharing information, and invading personal space to get private information, as defined by Daniel J. Solove. Conversely, in order to protect a users's privacy, multiple steps can be taken, specifically through practicing encryption, anonymity, and taking further measures to bolster the security of their data.
","Through the rise of technology and immensity of the debate regarding privacy, there have been various conceptions of privacy, which include the right to be let alone as defined in ""The Right to Privacy"", the first U.S. publication discussing privacy as a legal right, to the theory of the privacy paradox, which describes the notion that users' online may say they are concerned about their privacy, but in reality, are not. Along with various understandings of privacy, there are actions that reduce privacy, the most recent classification includes processing of information, sharing information, and invading personal space to get private information, as defined by Daniel J. Solove.","[' What was the first U.S. publication discussing privacy as a legal right?', ' What is the theory of privacy?', ' What theory describes the notion that users online may say they are concerned about their privacy but are not?', ' Along with various understandings of privacy, what actions reduce privacy?', ' What is the most recent classification?', ' What is the most recent classification that reduces privacy?', ' Who defined the classification that includes processing of information, sharing information, and invading personal space to get private information?']","['The Right to Privacy', 'privacy paradox', 'privacy paradox', 'processing of information, sharing information, and invading personal space to get private information', 'processing of information, sharing information, and invading personal space to get private information', 'processing of information, sharing information, and invading personal space to get private information', 'Daniel J. Solove']"
356,privacy,History,"Privacy has historical roots in ancient Greek philosophical discussions. The most well-known of these was Aristotle's distinction between two spheres of life: the public sphere of the polis, associated with political life, and the private sphere of the oikos, associated with domestic life. In the United States, more systematic treatises of privacy did not appear until the 1890s, with the development of privacy law in America.","Privacy has historical roots in ancient Greek philosophical discussions. The most well-known of these was Aristotle's distinction between two spheres of life: the public sphere of the polis, associated with political life, and the private sphere of the oikos, associated with domestic life.","[' Whose distinction between two spheres of life was the most well-known?', ' The public sphere of the polis is associated with what kind of life?', ' What type of life is the oikos associated with?']","['Aristotle', 'political life', 'domestic life']"
357,privacy,Actions which reduce privacy,"As with other conceptions of privacy, there are various ways to discuss what kinds of processes or actions remove, challenge, lessen, or attack privacy. In 1960 legal scholar William Prosser created the following list of activities which can be remedied with privacy protection:","As with other conceptions of privacy, there are various ways to discuss what kinds of processes or actions remove, challenge, lessen, or attack privacy. In 1960 legal scholar William Prosser created the following list of activities which can be remedied with privacy protection:",[' In what year did William Prosser create a list of activities that can be remedied with privacy protection?'],['1960']
358,privacy,Techniques to improve privacy,"Similarly to actions which reduce privacy, there are multiple angles of privacy and multiple techniques to improve them to varying extents. When actions are done at an organizational level, they may be referred to as cybersecurity. 
","Similarly to actions which reduce privacy, there are multiple angles of privacy and multiple techniques to improve them to varying extents. When actions are done at an organizational level, they may be referred to as cybersecurity.","[' What are actions that reduce privacy referred to as?', ' What are multiple angles of privacy and multiple techniques to improve them to varying degrees?']","['cybersecurity', 'actions which reduce privacy']"
359,scheduling,Summary,"A schedule or a timetable, as a basic time-management tool, consists of a list of times at which possible tasks, events, or actions are intended to take place, or of a sequence of events in the chronological order in which such things are intended to take place. The process of creating a schedule — deciding how to order these tasks and how to commit resources between the variety of possible tasks — is called scheduling, and a person responsible for making a particular schedule may be called a scheduler. Making and following schedules is an ancient human activity.","A schedule or a timetable, as a basic time-management tool, consists of a list of times at which possible tasks, events, or actions are intended to take place, or of a sequence of events in the chronological order in which such things are intended to take place. The process of creating a schedule — deciding how to order these tasks and how to commit resources between the variety of possible tasks — is called scheduling, and a person responsible for making a particular schedule may be called a scheduler.","[' What is a basic time-management tool?', ' What is the process of creating a timetable?', ' What is the process of creating a schedule?', ' What is a person responsible for making a particular schedule called?']","['a list of times at which possible tasks, events, or actions are intended to take place', 'scheduling', 'scheduling', 'scheduler']"
360,scheduling,Summary,"Some scenarios associate this kind of planning with learning life skills.
Schedules are necessary, or at least useful, in situations where individuals need to know what time they must be at a specific location to receive a specific service, and where people need to accomplish a set of goals within a set time period.
","Some scenarios associate this kind of planning with learning life skills. Schedules are necessary, or at least useful, in situations where individuals need to know what time they must be at a specific location to receive a specific service, and where people need to accomplish a set of goals within a set time period.","[' What do some scenarios associate this kind of planning with?', ' What are schedules useful in?', ' Where people need to accomplish a set of goals within a certain time period?']","['learning life skills', 'situations where individuals need to know what time they must be at a specific location to receive a specific service, and where people need to accomplish a set of goals within a set time period', 'Schedules']"
361,scheduling,Summary,"Schedules can usefully span both short periods, such as a daily or weekly schedule, and long-term planning with respect to periods of several months or years. They are often made using a calendar, where the person making the schedule can note the dates and times at which various events are planned to occur. Schedules that do not set forth specific times for events to occur may instead list algorithmically an expected order in which events either can or must take place.
","Schedules can usefully span both short periods, such as a daily or weekly schedule, and long-term planning with respect to periods of several months or years. They are often made using a calendar, where the person making the schedule can note the dates and times at which various events are planned to occur.","[' How are schedules often made?', ' What can be used for short periods of time?', ' How can a person make a schedule?', ' Note the dates and times at which various events are planned to occur?']","['using a calendar', 'Schedules', 'a calendar', 'Schedules can usefully span both short periods, such as a daily or weekly schedule, and long-term planning with respect to periods of several months or years. They are often made using a calendar']"
362,scheduling,Summary,"In some situations, schedules can be uncertain, such as where the conduct of daily life relies on environmental factors outside human control. People who are vacationing or otherwise seeking to reduce stress and achieve relaxation may intentionally avoid having a schedule for a certain period of time.","In some situations, schedules can be uncertain, such as where the conduct of daily life relies on environmental factors outside human control. People who are vacationing or otherwise seeking to reduce stress and achieve relaxation may intentionally avoid having a schedule for a certain period of time.","[' In some situations, schedules can be uncertain, such as when the conduct of daily life relies on what?', ' People who are vacationing or otherwise seeking to reduce stress and achieve relaxation may intentionally avoid having a schedule for what period of time?']","['environmental factors outside human control', 'a certain']"
363,face recognition,Summary,"Development began on similar systems in the 1960s, beginning as a form of computer application. Since their inception, facial recognition systems have seen wider uses in recent times on smartphones and in other forms of technology, such as robotics. Because computerized facial recognition involves the measurement of a human's physiological characteristics, facial recognition systems are categorized as biometrics. Although the accuracy of facial recognition systems as a biometric technology is lower than iris recognition and fingerprint recognition, it is widely adopted due to its contactless process. Facial recognition systems have been deployed in advanced human–computer interaction, video surveillance and automatic indexing of images.","Development began on similar systems in the 1960s, beginning as a form of computer application. Since their inception, facial recognition systems have seen wider uses in recent times on smartphones and in other forms of technology, such as robotics.","[' In what decade did facial recognition systems begin to be developed?', ' What was the first use for facial recognition in the 1960s?', ' In addition to smartphones and robotics, what other form of technology has facial recognition seen wider use?']","['1960s', 'computer application', 'robotics']"
364,face recognition,Summary,"Facial recognition systems are employed throughout the world today by governments and private companies. Their effectiveness varies, and some systems have previously been scrapped because of their ineffectiveness. The use of facial recognition systems has also raised controversy, with claims that the systems violate citizens' privacy, commonly make incorrect identifications, encourage gender norms and racial profiling, and do not protect important biometric data. These claims have led to the ban of facial recognition systems in several cities in the United States. As a result of growing societal concerns, Meta announced that it plans to shut down Facebook facial recognition system, deleting the face scan data of more than one billion users. This change will represent one of the largest shifts in facial recognition usage in the technology's history.
","Facial recognition systems are employed throughout the world today by governments and private companies. Their effectiveness varies, and some systems have previously been scrapped because of their ineffectiveness.","[' What are facial recognition systems employed by governments and private companies?', ' Why have some facial recognition system been scrapped?']","['effectiveness', 'ineffectiveness']"
365,face recognition,History of facial recognition technology,"Automated facial recognition was pioneered in the 1960s. Woody Bledsoe, Helen Chan Wolf, and Charles Bisson worked on using the computer to recognize human faces. Their early facial recognition project was dubbed ""man-machine"" because the coordinates of the facial features in a photograph had to be established by a human before they could be used by the computer for recognition. On a graphics tablet a human had to pinpoint the coordinates of facial features such as the pupil centers, the inside and outside corner of eyes, and the widows peak in the hairline. The coordinates were used to calculate 20 distances, including the width of the mouth and of the eyes. A human could process about 40 pictures an hour in this manner and so build a database of the computed distances. A computer would then automatically compare the distances for each photograph, calculate the difference between the distances and return the closed records as a possible match.","Automated facial recognition was pioneered in the 1960s. Woody Bledsoe, Helen Chan Wolf, and Charles Bisson worked on using the computer to recognize human faces.","[' When was automated facial recognition pioneered?', ' Who worked on using the computer to recognize human faces?']","['1960s', 'Woody Bledsoe, Helen Chan Wolf, and Charles Bisson']"
366,face recognition,History of facial recognition technology,"In 1970, Takeo Kanade publicly demonstrated a face matching system that located anatomical features such as the chin and calculated the distance ratio between facial features without human intervention. Later tests revealed that the system could not always reliably identify facial features. Nonetheless, interest in the subject grew and in 1977 Kanade published the first detailed book on facial recognition technology.","In 1970, Takeo Kanade publicly demonstrated a face matching system that located anatomical features such as the chin and calculated the distance ratio between facial features without human intervention. Later tests revealed that the system could not always reliably identify facial features.","[' When did Takeo Kanade demonstrate a face matching system?', ' Who demonstrated a system that located anatomical features such as the chin and calculated the distance ratio between facial features without human intervention?']","['1970', 'Takeo Kanade']"
367,face recognition,History of facial recognition technology,"In 1993, the Defense Advanced Research Project Agency (DARPA) and the Army Research Laboratory (ARL) established the face recognition technology program FERET to develop ""automatic face recognition capabilities"" that could be employed in a productive real life environment ""to assist security, intelligence, and law enforcement personnel in the performance of their duties."" Face recognition systems that had been trialed in research labs were evaluated and the FERET tests found that while the performance of existing automated facial recognition systems varied, a handful of existing methods could viably be used to recognize faces in still images taken in a controlled environment. The FERET tests spawned three US companies that sold automated facial recognition systems. Vision Corporation and Miros Inc were both founded in 1994, by researchers who used the results of the FERET tests as a selling point. Viisage Technology was established by a identification card defense contractor in 1996 to commercially exploit the rights to the facial recognition algorithm developed by Alex Pentland at MIT.","In 1993, the Defense Advanced Research Project Agency (DARPA) and the Army Research Laboratory (ARL) established the face recognition technology program FERET to develop ""automatic face recognition capabilities"" that could be employed in a productive real life environment ""to assist security, intelligence, and law enforcement personnel in the performance of their duties."" Face recognition systems that had been trialed in research labs were evaluated and the FERET tests found that while the performance of existing automated facial recognition systems varied, a handful of existing methods could viably be used to recognize faces in still images taken in a controlled environment.","[' What did the Defense Advanced Research Project Agency and the Army Research Laboratory establish in 1993?', ' What was the goal of the face recognition technology program FERET?', ' What did the FERET tests find about existing automated facial recognition systems?', ' What were security, intelligence, and law enforcement personnel in the performance of their duties?', ' What could be used to recognize faces in still images taken in a controlled environment?']","['FERET', 'to develop ""automatic face recognition capabilities"" that could be employed in a productive real life environment ""to assist security, intelligence, and law enforcement personnel in the performance of their duties.""', 'varied', 'automatic face recognition capabilities', 'a handful of existing methods']"
368,face recognition,History of facial recognition technology,"Following the 1993 FERET face recognition vendor test the Department of Motor Vehicles (DMV) offices in West Virginia and New Mexico were the first DMV offices to use automated facial recognition systems as a way to prevent and detect people obtaining multiple driving licenses under different names. Driver's licenses in the United States were at that point a commonly accepted form of photo identification. DMV offices across the United States were undergoing a technological upgrade and were in the process of establishing databases of digital ID photographs. This enabled DMV offices to deploy the facial recognition systems on the market to search photographs for new driving licenses against the existing DMV database. DMV offices became one of the first major markets for automated facial recognition technology and introduced US citizens to facial recognition as a standard method of identification. The increase of the US prison population in the 1990s prompted U.S. states to established connected and automated identification systems that incorporated digital biometric databases, in some instances this included facial recognition. In 1999 Minnesota incorporated the facial recognition system FaceIT by Visionics into a mug shot booking system that allowed police, judges and court officers to track criminals across the state.",Following the 1993 FERET face recognition vendor test the Department of Motor Vehicles (DMV) offices in West Virginia and New Mexico were the first DMV offices to use automated facial recognition systems as a way to prevent and detect people obtaining multiple driving licenses under different names. Driver's licenses in the United States were at that point a commonly accepted form of photo identification.,"[' What was the first DMV office to use automated facial recognition systems?', ' What did the DMV offices in West Virginia and New Mexico use to prevent and detect people obtaining multiple driving licenses?', ' What was a common form of photo identification in the United States?', ' How many driving licenses were under different names?']","['West Virginia and New Mexico', 'automated facial recognition systems', ""Driver's licenses"", 'multiple']"
369,face recognition,History of facial recognition technology,"Until the 1990s facial recognition systems were developed primarily by using photographic portraits of human faces. Research on face recognition to reliably locate a face in an image that contains other objects gained traction in the early 1990s with the principle component analysis (PCA). The PCA method of face detection is also known as Eigenface and was developed by Matthew Turk and Alex Pentland. Turk and Pentland combined the conceptual approach of the Karhunen–Loève theorem and factor analysis, to develop a linear model. Eigenfaces are determined based on global and orthogonal features in human faces. A human face is calculated as a weighted combination of a number of Eigenfaces. Because few Eigenfaces were used to encode human faces of a given population, Turk and Pentland's PCA face detection method greatly reduced the amount of data that had to be processed to detect a face. Pentland in 1994 defined Eigenface features, including eigen eyes, eigen mouths and eigen noses, to advance the use of PCA in facial recognition. In 1997 the PCA Eigenface method of face recognition was improved upon using linear discriminant analysis (LDA) to produce Fisherfaces. LDA Fisherfaces became dominantly used in PCA feature based face recognition. While Eigenfaces were also used for face reconstruction. In these approaches no global structure of the face is calculated which links the facial features or parts.",Until the 1990s facial recognition systems were developed primarily by using photographic portraits of human faces. Research on face recognition to reliably locate a face in an image that contains other objects gained traction in the early 1990s with the principle component analysis (PCA).,"[' When were facial recognition systems developed primarily by using photographic portraits of human faces?', ' What gained traction in the early 1990s with the principle component analysis?']","['Until the 1990s', 'Research on face recognition']"
370,face recognition,History of facial recognition technology,"Purely feature based approaches to facial recognition were overtaken in the late 1990s by the Bochum system, which used Gabor filter to record the face features and computed a grid of the face structure to link the features. Christoph von der Malsburg and his research team at the University of Bochum developed Elastic Bunch Graph Matching in the mid 1990s to extract a face out of an image using skin segmentation. By 1997 the face detection method developed by Malsburg outperformed most other facial detection systems on the market. The so-called ""Bochum system"" of face detection was sold commercially on the market as ZN-Face to operators of airports and other busy locations. The software was ""robust enough to make identifications from less-than-perfect face views. It can also often see through such impediments to identification as mustaches, beards, changed hairstyles and glasses—even sunglasses"".","Purely feature based approaches to facial recognition were overtaken in the late 1990s by the Bochum system, which used Gabor filter to record the face features and computed a grid of the face structure to link the features. Christoph von der Malsburg and his research team at the University of Bochum developed Elastic Bunch Graph Matching in the mid 1990s to extract a face out of an image using skin segmentation.","[' What system overtook feature based approaches to facial recognition in the late 1990s?', ' What system used Gabor filter to record the face features and computed a grid of the face structure to link the features?', ' Christoph von der Malsburg and his research team at what university did their research?', ' Who developed Elastic Bunch Graph Matching in the mid 1990s?', ' What did der Malsburg and his team use to extract a face out of an image?']","['Bochum system', 'Bochum system', 'University of Bochum', 'Christoph von der Malsburg and his research team at the University of Bochum', 'skin segmentation']"
371,face recognition,History of facial recognition technology,"Real-time face detection in video footage became possible in 2001 with the Viola–Jones object detection framework for faces. Paul Viola and Michael Jones combined their face detection method with the Haar-like feature approach to object recognition in digital images to launch AdaBoost, the first real-time frontal-view face detector. By 2015 the Viola-Jones algorithm had been implemented using small low power detectors on handheld devices and embedded systems. Therefore, the Viola-Jones algorithm has not only broadened the practical application of face recognition systems but has also been used to support new features in user interfaces and teleconferencing.","Real-time face detection in video footage became possible in 2001 with the Viola–Jones object detection framework for faces. Paul Viola and Michael Jones combined their face detection method with the Haar-like feature approach to object recognition in digital images to launch AdaBoost, the first real-time frontal-view face detector.","[' When did real-time face detection in video footage become possible?', ' What did Paul Viola and Michael Jones combine their face detection method with the Haar-like feature approach to object recognition in digital images?', ' AdaBoost is the first what?']","['2001', 'AdaBoost', 'real-time frontal-view face detector']"
372,face recognition,Techniques for face recognition,"While humans can recognize faces without much effort, facial recognition is a challenging pattern recognition problem in computing. Facial recognition systems attempt to identify a human face, which is three-dimensional and changes in appearance with lighting and facial expression, based on its two-dimensional image. To accomplish this computational task, facial recognition systems perform four steps. First face detection is used to segment the face from the image background. In the second step the segmented face image is aligned to account for face pose, image size and photographic properties, such as illumination and grayscale. The purpose of the alignment process is to enable the accurate localization of facial features in the third step, the facial feature extraction. Features such as eyes, nose and mouth are pinpointed and measured in the image to represent the face. The so established feature vector of the face is then, in the fourth step, matched against a database of faces.","While humans can recognize faces without much effort, facial recognition is a challenging pattern recognition problem in computing. Facial recognition systems attempt to identify a human face, which is three-dimensional and changes in appearance with lighting and facial expression, based on its two-dimensional image.","[' How can humans recognize faces without much effort?', ' What is a challenging pattern recognition problem in computing?', ' Facial recognition systems attempt to identify a human face based on what?']","['facial recognition is a challenging pattern recognition problem in computing', 'facial recognition', 'two-dimensional image']"
373,face recognition,Bans on the use of facial recognition technology,"In May 2019, San Francisco, California became the first major United States city to ban the use of facial recognition software for police and other local government agencies' usage. San Francisco Supervisor, Aaron Peskin, introduced regulations that will require agencies to gain approval from the San Francisco Board of Supervisors to purchase surveillance technology. The regulations also require that agencies publicly disclose the intended use for new surveillance technology. In June 2019, Somerville, Massachusetts became the first city on the East Coast to ban face surveillance software for government use, specifically in police investigations and municipal surveillance. In July 2019, Oakland, California banned the usage of facial recognition technology by city departments.","In May 2019, San Francisco, California became the first major United States city to ban the use of facial recognition software for police and other local government agencies' usage. San Francisco Supervisor, Aaron Peskin, introduced regulations that will require agencies to gain approval from the San Francisco Board of Supervisors to purchase surveillance technology.","[' In what month and year did San Francisco ban facial recognition software?', ' Who is the San Francisco Supervisor?', ' What is the name of the city that banned facial recognition in May 2019?', ' To gain approval from the San Francisco Board of Supervisors to purchase what?']","['May 2019', 'Aaron Peskin', 'San Francisco', 'surveillance technology']"
374,face recognition,Bans on the use of facial recognition technology,"The American Civil Liberties Union (""ACLU"") has campaigned across the United States for transparency in surveillance technology and has supported both San Francisco and Somerville's ban on facial recognition software. The ACLU works to challenge the secrecy and surveillance with this technology.","The American Civil Liberties Union (""ACLU"") has campaigned across the United States for transparency in surveillance technology and has supported both San Francisco and Somerville's ban on facial recognition software. The ACLU works to challenge the secrecy and surveillance with this technology.","[' What does the ACLU stand for?', "" What city's facial recognition software is banned in somerville?""]","['American Civil Liberties Union', 'San Francisco']"
375,face recognition,Bans on the use of facial recognition technology,"During the George Floyd protests, use of facial recognition by city government was banned in Boston, Massachusetts. As of June 10, 2020, municipal use has been banned in:","During the George Floyd protests, use of facial recognition by city government was banned in Boston, Massachusetts. As of June 10, 2020, municipal use has been banned in:","[' In what city was facial recognition banned during the George Floyd protests?', ' As of June 10, 2020, municipal use of facial recognition has been banned in what city?']","['Boston', 'Boston']"
376,face recognition,Bans on the use of facial recognition technology,"On October 27, 2020, 22 human rights groups called upon the University Of Miami to ban facial recognition technology. This came after the students accused the school of using the software to identify student protesters. The allegations were, however, denied by the university.","On October 27, 2020, 22 human rights groups called upon the University Of Miami to ban facial recognition technology. This came after the students accused the school of using the software to identify student protesters.","[' On what date did 22 human rights groups call upon the University of Miami to ban facial recognition technology?', ' What did the students accuse the University Of Miami of using to identify protesters?']","['October 27, 2020', 'facial recognition technology']"
377,face recognition,Bans on the use of facial recognition technology,"The European ""Reclaim Your Face"" coalition launched in October 2020. The coalition calls for a ban on facial recognition and launched a European Citizens' Initiative in February 2021. More than 60 organizations call on the European Commission to strictly regulate the use of biometric surveillance technologies.","The European ""Reclaim Your Face"" coalition launched in October 2020. The coalition calls for a ban on facial recognition and launched a European Citizens' Initiative in February 2021.","[' When did the European ""Reclaim Your Face"" coalition launch?', ' What does the ""reclaim your face"" coalition call for?', ' When was the ""European Citizens\' Initiative"" launched?']","['October 2020', 'a ban on facial recognition', 'February 2021']"
378,face recognition,Bans on the use of facial recognition technology,"A state police reform law in Massachusetts will take effect in July 2021; a ban passed by the legislature was rejected by governor Charlie Baker. Instead, the law requires a judicial warrant, limit the personnel who can perform the search, record data about how the technology is used, and create a commission to make recommendations about future regulations.","A state police reform law in Massachusetts will take effect in July 2021; a ban passed by the legislature was rejected by governor Charlie Baker. Instead, the law requires a judicial warrant, limit the personnel who can perform the search, record data about how the technology is used, and create a commission to make recommendations about future regulations.","[' When will a state police reform law in Massachusetts take effect?', ' Who rejected a ban passed by the legislature?', ' What does the law require?', ' What is a commission created to make recommendations about future regulations?']","['July 2021', 'governor Charlie Baker', 'a judicial warrant', 'state police reform law in Massachusetts']"
379,face recognition,Emotion recognition,"In the 18th and 19th century the belief that facial expressions revealed the moral worth or true inner state of a human was widespread and physiognomy was a respected science in the Western world. From the early 19th century onwards photography was used in the physiognomic analysis of facial features and facial expression to detect insanity and dementia. In the 1960s and 1970s the study of human emotions and its expressions was reinvented by psychologists, who tried to define a normal range of emotional responses to events. The research on automated emotion recognition has since the 1970s focused on facial expressions and speech, which are regarded as the two most important ways in which humans communicate emotions to other humans. In the 1970s the Facial Action Coding System (FACS) categorization for the physical expression of emotions was established. Its developer Paul Ekman maintains that there are six emotions that are universal to all human beings and that these can be coded in facial expressions. Research into automatic emotion specific expression recognition has in the past decades focused on frontal view images of human faces.",In the 18th and 19th century the belief that facial expressions revealed the moral worth or true inner state of a human was widespread and physiognomy was a respected science in the Western world. From the early 19th century onwards photography was used in the physiognomic analysis of facial features and facial expression to detect insanity and dementia.,"[' In what century was the belief that facial expressions revealed the true inner state of a human widespread?', ' What was a respected science in the Western world in the 18th and 19th centuries?', ' From the early 19th century onwards what was used in the physiognomic analysis of facial features?', ' What was used in the physiognomic analysis of facial features and facial expression to detect insanity and dementia?']","['18th and 19th', 'physiognomy', 'photography', 'photography']"
380,face recognition,Emotion recognition,"In 2016 facial feature emotion recognition algorithms were among the new technologies, alongside high-definition CCTV, high resolution 3D face recognition and iris recognition, that found their way out of university research labs. In 2016 Facebook acquired FacioMetrics, a facial feature emotion recognition corporate spin-off by Carnegie Mellon University. In the same year Apple Inc. acquired the facial feature emotion recognition start-up Emotient. By the end of 2016 commercial vendors of facial recognition systems offered to integrate and deploy emotion recognition algorithms for facial features. The MIT's Media Lab spin-off Affectiva by late 2019 offered a facial expression emotion detection product that can recognize emotions in humans while driving.
","In 2016 facial feature emotion recognition algorithms were among the new technologies, alongside high-definition CCTV, high resolution 3D face recognition and iris recognition, that found their way out of university research labs. In 2016 Facebook acquired FacioMetrics, a facial feature emotion recognition corporate spin-off by Carnegie Mellon University.","[' In what year did Facebook acquire FacioMetrics?', ' What was the name of the facial feature emotion recognition corporate spin-off by Carnegie Mellon University?']","['2016', 'FacioMetrics']"
381,face recognition,Anti-facial recognition systems,"In January 2013 Japanese researchers from the National Institute of Informatics created 'privacy visor' glasses that use nearly infrared light to make the face underneath it unrecognizable to face recognition software. The latest version uses a titanium frame, light-reflective material and a mask which uses angles and patterns to disrupt facial recognition technology through both absorbing and bouncing back light sources. Some projects use adversarial machine learning to come up with new printed patterns that confuse existing face recognition software.","In January 2013 Japanese researchers from the National Institute of Informatics created 'privacy visor' glasses that use nearly infrared light to make the face underneath it unrecognizable to face recognition software. The latest version uses a titanium frame, light-reflective material and a mask which uses angles and patterns to disrupt facial recognition technology through both absorbing and bouncing back light sources.","["" In what year did Japanese researchers create 'privacy visor' glasses?"", ' What did the privacy visor use to make the face underneath it unrecognizable to face recognition software?', ' The latest version of the Privacy visor uses a titanium frame, light-reflecting material and what else?', ' What uses angles and patterns to disrupt facial recognition technology?', ' What does a mask use to disrupt?']","['2013', 'nearly infrared light', 'a mask', 'mask', 'angles and patterns']"
382,face recognition,Anti-facial recognition systems,"Another method to protect from facial recognition systems are specific haircuts and make-up patterns that prevent the used algorithms to detect a face, known as computer vision dazzle. Incidentally, the makeup styles popular with Juggalos can also protect against facial recognition.","Another method to protect from facial recognition systems are specific haircuts and make-up patterns that prevent the used algorithms to detect a face, known as computer vision dazzle. Incidentally, the makeup styles popular with Juggalos can also protect against facial recognition.","[' What is another method to protect against facial recognition systems?', ' What are specific haircuts and make-up patterns that prevent algorithms to detect a face known as?', ' The makeup styles popular with Juggalos can also protect against what?']","['specific haircuts and make-up patterns', 'computer vision dazzle', 'facial recognition']"
383,face recognition,Anti-facial recognition systems,"Facial masks that are worn to protect from contagious viruses can reduce the accuracy of facial recognition systems. A 2020 NIST study tested popular one-to-one matching systems and found a failure rate between five and fifty percent on masked individuals. The Verge speculated that the accuracy rate of mass surveillance systems, which were not included in the study, would be even less accurate than the accuracy of one-to-one matching systems. The facial recognition of Apple Pay can work through many barriers, including heavy makeup, thick beards and even sunglasses, but fails with masks.",Facial masks that are worn to protect from contagious viruses can reduce the accuracy of facial recognition systems. A 2020 NIST study tested popular one-to-one matching systems and found a failure rate between five and fifty percent on masked individuals.,"[' What is worn to protect against contagious viruses?', ' What can reduce the accuracy of facial recognition systems?', ' A 2020 NIST study tested popular one-to-one matching systems and found a failure rate between five and fifty percent on masked individuals?']","['Facial masks', 'Facial masks', 'Facial masks']"
384,logic programming,Summary,"Logic programming is a programming paradigm which is largely based on formal logic. Any program written in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain.  Major logic programming language families include Prolog, answer set programming (ASP) and Datalog. In all of these languages, rules are written in the form of clauses:
","Logic programming is a programming paradigm which is largely based on formal logic. Any program written in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain.","[' What is a programming paradigm which is largely based on formal logic?', ' Any program written in a logic programming language is what?', ' A set of sentences in logical form expresses facts and rules about some problem domain?']","['Logic programming', 'a set of sentences in logical form', 'Logic programming']"
385,logic programming,Summary,"H is called the head of the rule and B1, ..., Bn is called the body. Facts are rules that have no body, and are written in the simplified form:
","H is called the head of the rule and B1, ..., Bn is called the body. Facts are rules that have no body, and are written in the simplified form:","[' What is the head of a rule?', ' What is B1 called?', ' Where are rules written that have no body?']","['H', 'the body', 'Facts']"
386,logic programming,Summary,"In the simplest case in which H, B1, ..., Bn are all atomic formulae, these clauses are called definite clauses or Horn clauses. However, there are many extensions of this simple case, the most important one being the case in which conditions in the body of a clause can also be negations of atomic formulas. Logic programming languages that include this extension have the knowledge representation capabilities of a non-monotonic logic.
","In the simplest case in which H, B1, ..., Bn are all atomic formulae, these clauses are called definite clauses or Horn clauses. However, there are many extensions of this simple case, the most important one being the case in which conditions in the body of a clause can also be negations of atomic formulas.","[' What are definite clauses called in the simplest case?', ' What are Horn clauses?', ' Which conditions in the body of a clause can also be negations of atomic formulas?']","['Horn clauses', 'definite clauses', 'the case in which conditions']"
387,logic programming,Summary,"In ASP and Datalog, logic programs have only a declarative reading, and their execution is performed by means of a proof procedure or model generator whose behaviour is not meant to be controlled by the programmer. However, in the Prolog family of languages, logic programs also have a procedural interpretation as goal-reduction procedures:
","In ASP and Datalog, logic programs have only a declarative reading, and their execution is performed by means of a proof procedure or model generator whose behaviour is not meant to be controlled by the programmer. However, in the Prolog family of languages, logic programs also have a procedural interpretation as goal-reduction procedures:","[' In ASP and Datalog, what type of reading do logic programs have?', ' What type of program executes logic programs by means of a proof procedure or model generator?', ' In the Prolog family of languages, logic programs also have what kind of interpretation?', ' What type of interpretation do logic programs have?']","['declarative', 'ASP and Datalog', 'procedural', 'procedural']"
388,logic programming,Summary,"based on an example used by Terry Winograd to illustrate the programming language Planner. As a clause in a logic program, it can be used both as a procedure to test whether X is fallible by testing whether X is human, and as a procedure to find an X which is fallible by finding an X which is human. Even facts have a procedural interpretation. For example, the clause:
","based on an example used by Terry Winograd to illustrate the programming language Planner. As a clause in a logic program, it can be used both as a procedure to test whether X is fallible by testing whether X is human, and as a procedure to find an X which is fallible by finding an X which is human.","[' Who used an example to illustrate the programming language Planner?', ' What can be used as a clause in a logic program?', ' What is a procedure to find an X which is fallible?']","['Terry Winograd', 'it can be used both as a procedure to test whether X is fallible', 'by finding an X which is human']"
389,logic programming,Summary,"The declarative reading of logic programs can be used by a programmer to verify their correctness. Moreover, logic-based program transformation techniques can also be used to transform logic programs into logically equivalent programs that are more efficient. In the Prolog family of logic programming languages, the programmer can also use the known problem-solving behaviour of the execution mechanism to improve the efficiency of programs.
","The declarative reading of logic programs can be used by a programmer to verify their correctness. Moreover, logic-based program transformation techniques can also be used to transform logic programs into logically equivalent programs that are more efficient.","[' What can be used by a programmer to verify their correctness?', ' What can also be used to transform logic programs into logically equivalent programs that are more efficient?']","['declarative reading of logic programs', 'logic-based program transformation techniques']"
390,logic programming,History,"The use of mathematical logic to represent and execute computer programs is also a feature of the lambda calculus, developed by Alonzo Church in the 1930s. However, the first proposal to use the clausal form of logic for representing computer programs was made by Cordell Green. This used an axiomatization of a subset of LISP, together with a representation of an input-output relation, to compute the relation by simulating the execution of the program in LISP. Foster and Elcock's Absys, on the other hand, employed a combination of equations and lambda calculus in an assertional programming language which places no constraints on the order in which operations are performed.","The use of mathematical logic to represent and execute computer programs is also a feature of the lambda calculus, developed by Alonzo Church in the 1930s. However, the first proposal to use the clausal form of logic for representing computer programs was made by Cordell Green.","[' Who developed lambda calculus in the 1930s?', ' Who made the first proposal to use the clausal form of logic for computer programs?']","['Alonzo Church', 'Cordell Green']"
391,logic programming,History,"Logic programming in its present form can be traced back to debates in the late 1960s and early 1970s about declarative versus procedural representations of knowledge in artificial intelligence. Advocates of declarative representations were notably working at Stanford, associated with John McCarthy, Bertram Raphael and Cordell Green, and in Edinburgh, with John Alan Robinson (an academic visitor from Syracuse University), Pat Hayes, and Robert Kowalski. Advocates of procedural representations were mainly centered at MIT, under the leadership of Marvin Minsky and Seymour Papert.","Logic programming in its present form can be traced back to debates in the late 1960s and early 1970s about declarative versus procedural representations of knowledge in artificial intelligence. Advocates of declarative representations were notably working at Stanford, associated with John McCarthy, Bertram Raphael and Cordell Green, and in Edinburgh, with John Alan Robinson (an academic visitor from Syracuse University), Pat Hayes, and Robert Kowalski.","[' Logic programming in its present form can be traced back to the debates in the late 1960s and early 1970s about what?', ' Advocates of declarative representations were notably working at what university?', ' John McCarthy, Bertram Raphael, and Cordell Green were in Edinburgh with whom?', ' John Alan Robinson was an academic visitor from what university?', ' Pat Hays and Robert Kowalski were in what city?']","['declarative versus procedural representations of knowledge in artificial intelligence', 'Stanford', 'John Alan Robinson (an academic visitor from Syracuse University), Pat Hayes, and Robert Kowalski', 'Syracuse University', 'Edinburgh']"
392,logic programming,History,"Although it was based on the proof methods of logic, Planner, developed at MIT, was the first language to emerge within this proceduralist paradigm. Planner featured pattern-directed invocation of procedural plans from goals (i.e. goal-reduction or backward chaining) and from assertions (i.e. forward chaining). The most influential implementation of Planner was the subset of Planner, called Micro-Planner, implemented by Gerry Sussman, Eugene Charniak and Terry Winograd. It was used to implement Winograd's natural-language understanding program SHRDLU, which was a landmark at that time. To cope with the very limited memory systems at the time, Planner used a backtracking control structure so that only one possible computation path had to be stored at a time. Planner gave rise to the programming languages QA-4, Popler, Conniver, QLISP, and the concurrent language Ether.","Although it was based on the proof methods of logic, Planner, developed at MIT, was the first language to emerge within this proceduralist paradigm. Planner featured pattern-directed invocation of procedural plans from goals (i.e.","[' Where was Planner developed?', ' What was the first language to emerge within the proceduralist paradigm?', ' Planner featured pattern-directed invocation of what?']","['MIT', 'Planner', 'procedural plans from goals']"
393,logic programming,History,"Hayes and Kowalski in Edinburgh tried to reconcile the logic-based declarative approach to knowledge representation with Planner's procedural approach. Hayes (1973) developed an equational language, Golux, in which different procedures could be obtained by altering the behavior of the theorem prover. Kowalski, on the other hand, developed SLD resolution, a variant of SL-resolution, and showed how it treats implications as goal-reduction procedures. Kowalski collaborated with Colmerauer in Marseille, who developed these ideas in the design and implementation of the programming language Prolog.
","Hayes and Kowalski in Edinburgh tried to reconcile the logic-based declarative approach to knowledge representation with Planner's procedural approach. Hayes (1973) developed an equational language, Golux, in which different procedures could be obtained by altering the behavior of the theorem prover.","["" What did Hayes and Kowalski try to reconcile with Planner's procedural approach to knowledge representation?"", "" What was the name of the equational language Haye's developed in 1973?"", ' How could different procedures be obtained by altering the behavior of the theorem prover?']","['logic-based declarative approach', 'Golux', 'Golux']"
394,augmented reality,Summary,"Augmented reality (AR) is an interactive experience of a real-world environment where the objects that reside in the real world are enhanced by computer-generated perceptual information, sometimes across multiple sensory modalities, including visual, auditory, haptic, somatosensory and olfactory. AR can be defined as a system that incorporates three basic features: a combination of real and virtual worlds, real-time interaction, and accurate 3D registration of virtual and real objects. The overlaid sensory information can be constructive (i.e. additive to the natural environment), or destructive (i.e. masking of the natural environment). This experience is seamlessly interwoven with the physical world such that it is perceived as an immersive aspect of the real environment. In this way, augmented reality alters one's ongoing perception of a real-world environment, whereas virtual reality completely replaces the user's real-world environment with a simulated one. Augmented reality is related to two largely synonymous terms: mixed reality and computer-mediated reality.
","Augmented reality (AR) is an interactive experience of a real-world environment where the objects that reside in the real world are enhanced by computer-generated perceptual information, sometimes across multiple sensory modalities, including visual, auditory, haptic, somatosensory and olfactory. AR can be defined as a system that incorporates three basic features: a combination of real and virtual worlds, real-time interaction, and accurate 3D registration of virtual and real objects.","[' What does AR stand for?', ' What are the three basic features of AR?', ' How many basic features does a system have?', ' What is the combination of real and virtual worlds, real-time interaction and accurate 3D registration of?']","['Augmented reality', 'a combination of real and virtual worlds, real-time interaction, and accurate 3D registration of virtual and real objects', 'three', 'virtual and real objects']"
395,augmented reality,Summary,"The primary value of augmented reality is the manner in which components of the digital world blend into a person's perception of the real world, not as a simple display of data, but through the integration of immersive sensations, which are perceived as natural parts of an environment. The earliest functional AR systems that provided immersive mixed reality experiences for users were invented in the early 1990s, starting with the Virtual Fixtures system developed at the U.S. Air Force's Armstrong Laboratory in 1992. Commercial augmented reality experiences were first introduced in entertainment and gaming businesses. Subsequently, augmented reality applications have spanned commercial industries such as education, communications, medicine, and entertainment. In education, content may be accessed by scanning or viewing an image with a mobile device or by using markerless AR techniques.","The primary value of augmented reality is the manner in which components of the digital world blend into a person's perception of the real world, not as a simple display of data, but through the integration of immersive sensations, which are perceived as natural parts of an environment. The earliest functional AR systems that provided immersive mixed reality experiences for users were invented in the early 1990s, starting with the Virtual Fixtures system developed at the U.S. Air Force's Armstrong Laboratory in 1992.","[' What is the primary value of augmented reality?', ' What are perceived as natural parts of an environment?', ' When were the earliest functional AR systems invented?', ' Where was the Virtual Fixtures system developed?']","[""the manner in which components of the digital world blend into a person's perception of the real world"", 'immersive sensations', 'early 1990s', ""U.S. Air Force's Armstrong Laboratory""]"
396,augmented reality,Summary,"Augmented reality is used to enhance natural environments or situations and offer perceptually enriched experiences. With the help of advanced AR technologies (e.g. adding computer vision, incorporating AR cameras into smartphone applications and object recognition) the information about the surrounding real world of the user becomes interactive and digitally manipulated. Information about the environment and its objects is overlaid on the real world. This information can be virtual. Augmented Reality is any experience which is artificial and which adds to the already existing reality. or real, e.g. seeing other real sensed or measured information such as electromagnetic radio waves overlaid in exact alignment with where they actually are in space. Augmented reality also has a lot of potential in the gathering and sharing of tacit knowledge. Augmentation techniques are typically performed in real time and in semantic contexts with environmental elements. Immersive perceptual information is sometimes combined with supplemental information like scores over a live video feed of a sporting event. This combines the benefits of both augmented reality technology and heads up display technology (HUD).
",Augmented reality is used to enhance natural environments or situations and offer perceptually enriched experiences. With the help of advanced AR technologies (e.g.,"[' What is used to enhance natural environments or situations?', ' What are advanced AR technologies?']","['Augmented reality', 'With the help of advanced AR technologies']"
397,augmented reality,Comparison with virtual reality,"In virtual reality (VR), the users' perception of reality is completely based on virtual information. In augmented reality (AR) the user is provided with additional computer generated information within the data collected from real life that enhances their perception of reality. For example, in architecture, VR can be used to create a walk-through simulation of the inside of a new building; and AR can be used to show a building's structures and systems super-imposed on a real-life view. Another example is through the use of utility applications. Some AR applications, such as Augment, enable users to apply digital objects into real environments, allowing businesses to use augmented reality devices as a way to preview their products in the real world. Similarly, it can also be used to demo what products may look like in an environment for customers, as demonstrated by companies such as Mountain Equipment Co-op or Lowe's who use augmented reality to allow customers to preview what their products might look like at home through the use of 3D models.","In virtual reality (VR), the users' perception of reality is completely based on virtual information. In augmented reality (AR) the user is provided with additional computer generated information within the data collected from real life that enhances their perception of reality.","[' What is VR?', ' What does AR stand for?', "" In VR, what is the user's perception of reality based on?""]","['virtual reality', 'augmented reality', 'virtual information']"
398,augmented reality,Comparison with virtual reality,"Augmented reality (AR) differs from virtual reality (VR) in the sense that in AR part of the surrounding environment is 'real' and just adding layers of virtual objects to the real environment. On the other hand, in VR the surrounding environment is completely virtual and computer generated. A demonstration of how AR layers objects onto the real world can be seen with augmented reality games. WallaMe is an augmented reality game application that allows users to hide messages in real environments, utilizing geolocation technology in order to enable users to hide messages wherever they may wish in the world. Such applications have many uses in the world, including in activism and artistic expression.","Augmented reality (AR) differs from virtual reality (VR) in the sense that in AR part of the surrounding environment is 'real' and just adding layers of virtual objects to the real environment. On the other hand, in VR the surrounding environment is completely virtual and computer generated.","[' What is the difference between AR and VR?', ' What does AR stand for?', ' In VR, the surrounding environment is what?']","[""part of the surrounding environment is 'real' and just adding layers of virtual objects to the real environment"", 'Augmented reality', 'completely virtual and computer generated']"
399,augmented reality,Possible applications,"Augmented reality has been explored for many applications, from gaming and entertainment to medicine, education and business. Example application areas described below include archaeology, architecture, commerce and education. Some of the earliest cited examples include augmented reality used to support surgery by providing virtual overlays to guide medical practitioners, to AR content for astronomy and welding.","Augmented reality has been explored for many applications, from gaming and entertainment to medicine, education and business. Example application areas described below include archaeology, architecture, commerce and education.","[' What has been explored for many applications?', ' Archaeology, architecture, commerce and education are examples of what?']","['Augmented reality', 'application areas']"
400,big data,Summary,"Big data is a field that treats ways to analyze, systematically extract information from, or otherwise deal with data sets that are too large or complex to be dealt with by traditional data-processing application software. Data with many fields (columns) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate. Big data analysis challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy, and data source. Big data was originally associated with three key concepts: volume, variety, and velocity. The analysis of big data presents challenges in sampling, and thus previously allowing for only observations and sampling. Therefore, big data often includes data with sizes that exceed the capacity of traditional software to process within an acceptable time and value.
","Big data is a field that treats ways to analyze, systematically extract information from, or otherwise deal with data sets that are too large or complex to be dealt with by traditional data-processing application software. Data with many fields (columns) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate.","[' Big data is a field that treats ways to analyze, extract information from, or deal with data sets that are too large or complex to be dealt with by what?', ' Data with many fields (columns) offer greater statistical power, while data with higher complexity (more complex) offer more statistical power?', ' What may lead to a higher false discovery rate?']","['traditional data-processing application software', 'Big data', 'data with higher complexity']"
401,big data,Summary,"Current usage of the term big data tends to refer to the use of predictive analytics, user behavior analytics, or certain other advanced data analytics methods that extract value from big data, and seldom to a particular size of data set. ""There is little doubt that the quantities of data now available are indeed large, but that's not the most relevant characteristic of this new data ecosystem.""
Analysis of data sets can find new correlations to ""spot business trends, prevent diseases, combat crime and so on"". Scientists, business executives, medical practitioners, advertising and governments alike regularly meet difficulties with large data-sets in areas including Internet searches, fintech, healthcare analytics, geographic information systems, urban informatics, and business informatics.  Scientists encounter limitations in e-Science work, including meteorology, genomics, connectomics, complex physics simulations, biology, and environmental research.","Current usage of the term big data tends to refer to the use of predictive analytics, user behavior analytics, or certain other advanced data analytics methods that extract value from big data, and seldom to a particular size of data set. ""There is little doubt that the quantities of data now available are indeed large, but that's not the most relevant characteristic of this new data ecosystem.""","[' What term is used to refer to the use of predictive analytics, user behavior analytics, or other advanced data analytics methods that extract value from big data?', ' What does the current usage of the term big data refer to?', ' Big data is rarely related to a particular size of what?', ' What is not the most relevant characteristic of this new data ecosystem?']","['big data', 'the use of predictive analytics', 'data set', 'the quantities of data now available are indeed large']"
402,big data,Summary,"The size and number of available data sets have grown rapidly as data is collected by devices such as mobile devices, cheap and numerous information-sensing Internet of things devices, aerial (remote sensing), software logs, cameras, microphones, radio-frequency identification (RFID) readers and wireless sensor networks. The world's technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s; as of 2012, every day 2.5 exabytes (2.5×260 bytes) of data are generated. Based on an IDC report prediction, the global data volume was predicted to grow exponentially from 4.4 zettabytes to 44 zettabytes between 2013 and 2020. By 2025, IDC predicts there will be 163 zettabytes of data. One question for large enterprises is determining who should own big-data initiatives that affect the entire organization.","The size and number of available data sets have grown rapidly as data is collected by devices such as mobile devices, cheap and numerous information-sensing Internet of things devices, aerial (remote sensing), software logs, cameras, microphones, radio-frequency identification (RFID) readers and wireless sensor networks. The world's technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s; as of 2012, every day 2.5 exabytes (2.5×260 bytes) of data are generated.","[' What is a term for aerial sensing?', ' What is another term for radio-frequency identification?', "" How often has the world's per-capita capacity to store information doubled?"", ' As of 2012, how many exabytes of data are generated every day?']","['remote sensing', 'RFID', 'every 40 months', '2.5']"
403,big data,Summary,"Relational database management systems and desktop statistical software packages used to visualize data often have difficulty processing and analyzing big data. The processing and analysis of big data may require ""massively parallel software running on tens, hundreds, or even thousands of servers"". What qualifies as ""big data"" varies depending on the capabilities of those analyzing it and their tools.  Furthermore, expanding capabilities make big data a moving target. ""For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration.""","Relational database management systems and desktop statistical software packages used to visualize data often have difficulty processing and analyzing big data. The processing and analysis of big data may require ""massively parallel software running on tens, hundreds, or even thousands of servers"".","[' Relational database management systems and desktop statistical software packages have difficulty processing and analyzing what?', ' The processing and analysis of big data may require what kind of software?']","['big data', 'massively parallel']"
404,big data,Definition,"The term big data has been in use since the 1990s, with some giving credit to John Mashey for popularizing the term.
Big data usually includes data sets with sizes beyond the ability of commonly used software tools to capture, curate, manage, and process data within a tolerable elapsed time. Big data philosophy encompasses unstructured, semi-structured and structured data, however the main focus is on unstructured data. Big data ""size"" is a constantly moving target; as of 2012 ranging from a few dozen terabytes to many zettabytes of data.
Big data requires a set of techniques and technologies with new forms of integration to reveal insights from data-sets that are diverse, complex, and of a massive scale.","The term big data has been in use since the 1990s, with some giving credit to John Mashey for popularizing the term. Big data usually includes data sets with sizes beyond the ability of commonly used software tools to capture, curate, manage, and process data within a tolerable elapsed time.","[' What term has been in use since the 1990s?', ' Who popularized the term big data?', ' What usually includes data sets with sizes beyond the ability of commonly used software tools?']","['big data', 'John Mashey', 'Big data']"
405,big data,Definition,"""Variety"", ""veracity"", and various other ""Vs"" are added by some organizations to describe it, a revision challenged by some industry authorities.  The Vs of big data were often referred to as the ""three Vs"", ""four Vs"", and ""five Vs"".  They represented the qualities of big data in volume, variety, velocity, veracity, and value.  Variability is often included as an additional quality of big data.
","""Variety"", ""veracity"", and various other ""Vs"" are added by some organizations to describe it, a revision challenged by some industry authorities. The Vs of big data were often referred to as the ""three Vs"", ""four Vs"", and ""five Vs"".","[' What are some organizations adding to describe big data?', ' What were the Vs of big data often called?']","['""Variety"", ""veracity"", and various other ""Vs""', 'three Vs"", ""four Vs"", and ""five Vs"".']"
406,big data,Definition,"In a comparative study of big datasets, Kitchin and McArdle found that none of the commonly considered characteristics of big data appear consistently across all of the analyzed cases. For this reason, other studies identified the redefinition of power dynamics in knowledge discovery as the defining trait. Instead of focusing on intrinsic characteristics of big data, this alternative perspective pushes forward a relational understanding of the object claiming that what matters is the way in which data is collected, stored, made available and analyzed.
","In a comparative study of big datasets, Kitchin and McArdle found that none of the commonly considered characteristics of big data appear consistently across all of the analyzed cases. For this reason, other studies identified the redefinition of power dynamics in knowledge discovery as the defining trait.","[' Kitchin and McArdle found that none of the commonly considered characteristics of big data appear consistently across all of the analyzed cases?', ' Other studies identified the redefinition of power dynamics in what?']","['comparative study of big datasets', 'knowledge discovery']"
407,big data,Architecture,"Big data repositories have existed in many forms, often built by corporations with a special need.  Commercial vendors historically offered parallel database management systems for big data beginning in the 1990s.  For many years, WinterCorp published the largest database report.","Big data repositories have existed in many forms, often built by corporations with a special need. Commercial vendors historically offered parallel database management systems for big data beginning in the 1990s.",[' When did commercial vendors begin offering parallel database management systems for big data?'],['1990s']
408,big data,Architecture,"Teradata Corporation in 1984 marketed the parallel processing DBC 1012 system. Teradata systems were the first to store and analyze 1 terabyte of data in 1992. Hard disk drives were 2.5 GB in 1991 so the definition of big data continuously evolves. Teradata installed the first petabyte class RDBMS based system in 2007. As of 2017, there are a few dozen petabyte class Teradata relational databases installed, the largest of which exceeds 50 PB. Systems up until 2008 were 100% structured relational data.  Since then, Teradata has added unstructured data types including XML, JSON, and Avro.
",Teradata Corporation in 1984 marketed the parallel processing DBC 1012 system. Teradata systems were the first to store and analyze 1 terabyte of data in 1992.,"[' In what year did Teradata Corporation launch the DBC 1012 system?', ' What was the first system to store and analyze 1 terabyte of data?']","['1984', 'Teradata']"
409,big data,Architecture,"In 2000, Seisint Inc. (now LexisNexis Risk Solutions) developed a C++-based distributed platform for data processing and querying known as the HPCC Systems platform. This system automatically partitions, distributes, stores and delivers structured, semi-structured, and unstructured data across multiple commodity servers.  Users can write data processing pipelines and queries in a declarative dataflow programming language called ECL. Data analysts working in ECL are not required to define data schemas upfront and can rather focus on the particular problem at hand, reshaping data in the best possible manner as they develop the solution. In 2004, LexisNexis acquired Seisint Inc. and their high-speed parallel processing platform and successfully used this platform to integrate the data systems of Choicepoint Inc. when they acquired that company in 2008. In 2011, the HPCC systems platform was open-sourced under the Apache v2.0 License.
","In 2000, Seisint Inc. (now LexisNexis Risk Solutions) developed a C++-based distributed platform for data processing and querying known as the HPCC Systems platform. This system automatically partitions, distributes, stores and delivers structured, semi-structured, and unstructured data across multiple commodity servers.","[' In what year did Seisint Inc. develop a distributed platform for data processing and querying?', ' What is the HPCC Systems platform?', ' How does HPCC System system partition, distribute, and store data?']","['2000', 'C++-based distributed platform for data processing and querying', 'automatically']"
410,big data,Architecture,"In 2004, Google published a paper on a process called MapReduce that uses a similar architecture. The MapReduce concept provides a parallel processing model, and an associated implementation was released to process huge amounts of data.  With MapReduce, queries are split and distributed across parallel nodes and processed in parallel (the ""map"" step). The results are then gathered and delivered (the ""reduce"" step). The framework was very successful, so others wanted to replicate the algorithm. Therefore, an implementation of the MapReduce framework was adopted by an Apache open-source project named ""Hadoop"". Apache Spark was developed in 2012 in response to limitations in the MapReduce paradigm, as it adds the ability to set up many operations (not just map followed by reducing).
","In 2004, Google published a paper on a process called MapReduce that uses a similar architecture. The MapReduce concept provides a parallel processing model, and an associated implementation was released to process huge amounts of data.","[' When did Google publish a paper on a process called MapReduce that uses a similar architecture?', ' What concept provides a parallel processing model and an associated implementation was released to process huge amounts of data?']","['2004', 'MapReduce']"
411,big data,Architecture,"MIKE2.0 is an open approach to information management that acknowledges the need for revisions due to big data implications identified in an article titled ""Big Data Solution Offering"". The methodology addresses handling big data in terms of useful permutations of data sources, complexity in interrelationships, and difficulty in deleting (or modifying) individual records.","MIKE2.0 is an open approach to information management that acknowledges the need for revisions due to big data implications identified in an article titled ""Big Data Solution Offering"". The methodology addresses handling big data in terms of useful permutations of data sources, complexity in interrelationships, and difficulty in deleting (or modifying) individual records.","[' What is MIKE2.0?', ' What is the name of the article titled ""Big Data Solution Offering""?', ' How does MIKE 2.0 handle big data?', ' Data sources, complexity in interrelationships, and difficulty in deleting (or modifying) individual records?']","['an open approach to information management', 'MIKE2.0', 'in terms of useful permutations of data sources, complexity in interrelationships, and difficulty in deleting (or modifying) individual records', 'MIKE2.0']"
412,big data,Architecture,"Studies in 2012 showed that a multiple-layer architecture was one option to address the issues that big data presents. A distributed parallel architecture distributes data across multiple servers; these parallel execution environments can dramatically improve data processing speeds. This type of architecture inserts data into a parallel DBMS, which implements the use of MapReduce and Hadoop frameworks. This type of framework looks to make the processing power transparent to the end-user by using a front-end application server.",Studies in 2012 showed that a multiple-layer architecture was one option to address the issues that big data presents. A distributed parallel architecture distributes data across multiple servers; these parallel execution environments can dramatically improve data processing speeds.,"[' In what year did studies show that a multiple-layer architecture was one option to address the issues that big data presents?', ' A distributed parallel architecture distributes data across multiple servers?', ' Parallel execution environments can dramatically improve what?']","['2012', 'parallel execution environments can dramatically improve data processing speeds', 'data processing speeds']"
413,big data,Architecture,"The data lake allows an organization to shift its focus from centralized control to a shared model to respond to the changing dynamics of information management. This enables quick segregation of data into the data lake, thereby reducing the overhead time.","The data lake allows an organization to shift its focus from centralized control to a shared model to respond to the changing dynamics of information management. This enables quick segregation of data into the data lake, thereby reducing the overhead time.","[' What allows an organization to shift its focus from centralized control to a shared model?', ' What allows quick segregation of data into the data lake?']","['The data lake', 'The data lake']"
414,big data,Technologies,"Multidimensional big data can also be represented as OLAP data cubes or, mathematically, tensors. Array database systems have set out to provide storage and high-level query support on this data type.
Additional technologies being applied to big data include efficient tensor-based computation, such as multilinear subspace learning, massively parallel-processing (MPP) databases, search-based applications, data mining, distributed file systems, distributed cache (e.g., burst buffer and Memcached), distributed databases, cloud and HPC-based infrastructure (applications, storage and computing resources), and the Internet. Although, many approaches and technologies have been developed, it still remains difficult to carry out machine learning with big data.","Multidimensional big data can also be represented as OLAP data cubes or, mathematically, tensors. Array database systems have set out to provide storage and high-level query support on this data type.","[' What can be represented as OLAP data cubes or, mathematically, tensors?', ' Array database systems have set out to provide what?']","['Multidimensional big data', 'storage and high-level query support']"
415,big data,Technologies,"Some MPP relational databases have the ability to store and manage petabytes of data. Implicit is the ability to load, monitor, back up, and optimize the use of the large data tables in the RDBMS.","Some MPP relational databases have the ability to store and manage petabytes of data. Implicit is the ability to load, monitor, back up, and optimize the use of the large data tables in the RDBMS.","[' Some MPP relational databases have the ability to store and manage how many petabytes of data?', ' Implicit is what ability to load, monitor, back up, and optimize the use of large data tables in the RDBMS?']","['petabytes', 'MPP relational databases']"
416,big data,Technologies,"The practitioners of big data analytics processes are generally hostile to slower shared storage, preferring direct-attached storage (DAS) in its various forms from solid state drive (SSD) to high capacity SATA disk buried inside parallel processing nodes. The perception of shared storage architectures—storage area network (SAN) and network-attached storage (NAS)— is that they are relatively slow, complex, and expensive. These qualities are not consistent with big data analytics systems that thrive on system performance, commodity infrastructure, and low cost.
","The practitioners of big data analytics processes are generally hostile to slower shared storage, preferring direct-attached storage (DAS) in its various forms from solid state drive (SSD) to high capacity SATA disk buried inside parallel processing nodes. The perception of shared storage architectures—storage area network (SAN) and network-attached storage (NAS)— is that they are relatively slow, complex, and expensive.","[' What do practitioners of big data analytics prefer?', ' What is direct-attached storage?', ' Where is high capacity SATA disk buried?', ' What are two common shared storage architectures?', ' What are SAN and NAS?']","['direct-attached storage', 'DAS', 'inside parallel processing nodes', 'storage area network (SAN) and network-attached storage (NAS', 'storage area network (SAN) and network-attached storage']"
417,big data,Technologies,"Real or near-real-time information delivery is one of the defining characteristics of big data analytics. Latency is therefore avoided whenever and wherever possible. Data in direct-attached memory or disk is good—data on memory or disk at the other end of an FC SAN connection is not. The cost of an SAN at the scale needed for analytics applications is much higher than other storage techniques.
",Real or near-real-time information delivery is one of the defining characteristics of big data analytics. Latency is therefore avoided whenever and wherever possible.,"[' What is one of the defining characteristics of big data analytics?', ' What is avoided whenever and wherever possible?']","['Real or near-real-time information delivery', 'Latency']"
418,big data,Applications,"Big data has increased the demand of information management specialists so much so that Software AG, Oracle Corporation, IBM, Microsoft, SAP, EMC, HP, and Dell have spent more than $15 billion on software firms specializing in data management and analytics. In 2010, this industry was worth more than $100 billion and was growing at almost 10 percent a year: about twice as fast as the software business as a whole.","Big data has increased the demand of information management specialists so much so that Software AG, Oracle Corporation, IBM, Microsoft, SAP, EMC, HP, and Dell have spent more than $15 billion on software firms specializing in data management and analytics. In 2010, this industry was worth more than $100 billion and was growing at almost 10 percent a year: about twice as fast as the software business as a whole.","[' How much money did Software AG, Oracle Corporation, IBM, Microsoft, SAP, EMC, HP, and Dell spend on software firms specializing in data management and analytics?', ' What industry was worth more than $100 billion in 2010?', ' What was the value of the software industry in 2010?', ' How fast was the software business growing?']","['$15\xa0billion', 'information management specialists so much so that Software AG, Oracle Corporation, IBM, Microsoft, SAP, EMC, HP, and Dell have spent more than $15\xa0billion on software firms specializing in data management and analytics', 'more than $100\xa0billion', 'twice as fast']"
419,big data,Applications,"Developed economies increasingly use data-intensive technologies. There are 4.6 billion mobile-phone subscriptions worldwide, and between 1 billion and 2 billion people accessing the internet. Between 1990 and 2005, more than 1 billion people worldwide entered the middle class, which means more people became more literate, which in turn led to information growth. The world's effective capacity to exchange information through telecommunication networks was 281 petabytes in 1986, 471 petabytes in 1993, 2.2 exabytes in 2000, 65 exabytes in 2007 and predictions put the amount of internet traffic at 667 exabytes annually by 2014. According to one estimate, one-third of the globally stored information is in the form of alphanumeric text and still image data, which is the format most useful for most big data applications. This also shows the potential of yet unused data (i.e. in the form of video and audio content).
","Developed economies increasingly use data-intensive technologies. There are 4.6 billion mobile-phone subscriptions worldwide, and between 1 billion and 2 billion people accessing the internet.","[' How many mobile phone subscriptions worldwide are there?', ' How many people access the internet?', ' What type of technologies are developing economies increasingly using?']","['4.6\xa0billion', 'between 1\xa0billion and 2\xa0billion', 'data-intensive']"
420,big data,Research activities,"Encrypted search and cluster formation in big data were demonstrated in March 2014 at the American Society of Engineering Education. Gautam Siwach engaged at Tackling the challenges of Big Data by MIT Computer Science and Artificial Intelligence Laboratory and Amir Esmailpour at the UNH Research Group investigated the key features of big data as the formation of clusters and their interconnections. They focused on the security of big data and the orientation of the term towards the presence of different types of data in an encrypted form at cloud interface by providing the raw definitions and real-time examples within the technology. Moreover, they proposed an approach for identifying the encoding technique to advance towards an expedited search over encrypted text leading to the security enhancements in big data.",Encrypted search and cluster formation in big data were demonstrated in March 2014 at the American Society of Engineering Education. Gautam Siwach engaged at Tackling the challenges of Big Data by MIT Computer Science and Artificial Intelligence Laboratory and Amir Esmailpour at the UNH Research Group investigated the key features of big data as the formation of clusters and their interconnections.,"[' What was demonstrated at the American Society of Engineering Education in March 2014?', ' Who was engaged at Tackling the challenges of Big Data by MIT Computer Science and Artificial Intelligence Laboratory?', ' Where was Amir Esmailpour from?', ' Who investigated the formation of clusters and their interconnections?']","['Encrypted search and cluster formation in big data', 'Gautam Siwach', 'UNH', 'Amir Esmailpour']"
421,big data,Research activities,"The initiative included a National Science Foundation ""Expeditions in Computing"" grant of $10 million over five years to the AMPLab at the University of California, Berkeley. The AMPLab also received funds from DARPA, and over a dozen industrial sponsors and uses big data to attack a wide range of problems from predicting traffic congestion to fighting cancer.","The initiative included a National Science Foundation ""Expeditions in Computing"" grant of $10 million over five years to the AMPLab at the University of California, Berkeley. The AMPLab also received funds from DARPA, and over a dozen industrial sponsors and uses big data to attack a wide range of problems from predicting traffic congestion to fighting cancer.","[' How much money did the National Science Foundation give to the AMPLab?', ' How long did the grant last?', ' What is the name of the lab that uses big data to attack a wide range of problems?', ' What does big data do to a wide range of problems?']","['$10 million', 'five years', 'AMPLab', 'predicting traffic congestion to fighting cancer']"
422,big data,Research activities,"The White House Big Data Initiative also included a commitment by the Department of Energy to provide $25 million in funding over five years to establish the Scalable Data Management, Analysis and Visualization (SDAV) Institute, led by the Energy Department's Lawrence Berkeley National Laboratory. The SDAV Institute aims to bring together the expertise of six national laboratories and seven universities to develop new tools to help scientists manage and visualize data on the department's supercomputers.
","The White House Big Data Initiative also included a commitment by the Department of Energy to provide $25 million in funding over five years to establish the Scalable Data Management, Analysis and Visualization (SDAV) Institute, led by the Energy Department's Lawrence Berkeley National Laboratory. The SDAV Institute aims to bring together the expertise of six national laboratories and seven universities to develop new tools to help scientists manage and visualize data on the department's supercomputers.","[' How much money did the Department of Energy commit to provide for the SDAV Institute?', ' Who is the leader of the Scalable Data Management, Analysis and Visualization institute?', ' What is the name of the Institute that the Energy Department is led by?', ' What is the SDAV Institute aiming to do?', ' How many national laboratories and seven universities will be able to work together to develop new tools?']","['$25 million', 'Lawrence Berkeley National Laboratory', 'Lawrence Berkeley National Laboratory', ""bring together the expertise of six national laboratories and seven universities to develop new tools to help scientists manage and visualize data on the department's supercomputers"", 'six']"
423,big data,Research activities,"The U.S. state of Massachusetts announced the Massachusetts Big Data Initiative in May 2012, which provides funding from the state government and private companies to a variety of research institutions. The Massachusetts Institute of Technology hosts the Intel Science and Technology Center for Big Data in the MIT Computer Science and Artificial Intelligence Laboratory, combining government, corporate, and institutional funding and research efforts.","The U.S. state of Massachusetts announced the Massachusetts Big Data Initiative in May 2012, which provides funding from the state government and private companies to a variety of research institutions. The Massachusetts Institute of Technology hosts the Intel Science and Technology Center for Big Data in the MIT Computer Science and Artificial Intelligence Laboratory, combining government, corporate, and institutional funding and research efforts.","[' When was the Massachusetts Big Data Initiative announced?', ' What is the name of the program that provides funding to research institutions?', ' Where is the Intel Science and Technology Center for Big Data located?', ' Where is the Technology Center for Big Data located?', ' What is the MIT Computer Science and Artificial Intelligence Laboratory?']","['May 2012', 'Massachusetts Big Data Initiative', 'Massachusetts Institute of Technology', 'Massachusetts Institute of Technology', 'Intel Science and Technology Center for Big Data']"
424,big data,Research activities,"The European Commission is funding the two-year-long Big Data Public Private Forum through their Seventh Framework Program to engage companies, academics and other stakeholders in discussing big data issues. The project aims to define a strategy in terms of research and innovation to guide supporting actions from the European Commission in the successful implementation of the big data economy. Outcomes of this project will be used as input for Horizon 2020, their next framework program.","The European Commission is funding the two-year-long Big Data Public Private Forum through their Seventh Framework Program to engage companies, academics and other stakeholders in discussing big data issues. The project aims to define a strategy in terms of research and innovation to guide supporting actions from the European Commission in the successful implementation of the big data economy.","[' How long is the Big Data Public Private Forum?', ' Who is funding the big data public private forum?', "" What is the European Commission's role in the implementation of big data economy?""]","['two-year', 'The European Commission', 'supporting actions']"
425,big data,Research activities,"Computational social sciences – Anyone can use application programming interfaces (APIs) provided by big data holders, such as Google and Twitter, to do research in the social and behavioral sciences. Often these APIs are provided for free. Tobias Preis et al. used Google Trends data to demonstrate that Internet users from countries with a higher per capita gross domestic products (GDPs) are more likely to search for information about the future than information about the past. The findings suggest there may be a link between online behaviors and real-world economic indicators. The authors of the study examined Google queries logs made by ratio of the volume of searches for the coming year (2011) to the volume of searches for the previous year (2009), which they call the ""future orientation index"". They compared the future orientation index to the per capita GDP of each country, and found a strong tendency for countries where Google users inquire more about the future to have a higher GDP. 
","Computational social sciences – Anyone can use application programming interfaces (APIs) provided by big data holders, such as Google and Twitter, to do research in the social and behavioral sciences. Often these APIs are provided for free.","[' What type of social sciences can anyone use?', ' What are APIs provided by big data holders?']","['Computational', 'application programming interfaces']"
426,big data,Research activities,"Tobias Preis and his colleagues Helen Susannah Moat and H. Eugene Stanley introduced a method to identify online precursors for stock market moves, using trading strategies based on search volume data provided by Google Trends. Their analysis of Google search volume for 98 terms of varying financial relevance, published in Scientific Reports, suggests that increases in search volume for financially relevant search terms tend to precede large losses in financial markets.","Tobias Preis and his colleagues Helen Susannah Moat and H. Eugene Stanley introduced a method to identify online precursors for stock market moves, using trading strategies based on search volume data provided by Google Trends. Their analysis of Google search volume for 98 terms of varying financial relevance, published in Scientific Reports, suggests that increases in search volume for financially relevant search terms tend to precede large losses in financial markets.","[' Tobias Preis and Helen Susannah Moat developed a method to identify online precursors for what?', ' What company provided trading strategies based on search volume data?', ' How many terms of varying financial relevance were analysed?', ' How many terms of varying financial relevance were published in Scientific Reports?', ' Increases in search volume for financially relevant search terms tend to precede large losses in what?']","['stock market moves', 'Google Trends', '98', '98', 'financial markets']"
427,big data,Research activities,"Big data sets come with algorithmic challenges that previously did not exist. Hence, there is seen by some to be a need to fundamentally change the processing ways.","Big data sets come with algorithmic challenges that previously did not exist. Hence, there is seen by some to be a need to fundamentally change the processing ways.","[' Big data sets come with challenges that previously did not exist what?', ' What is seen by some as a need to fundamentally change the processing ways?']","['algorithmic', 'Big data sets']"
428,big data,Research activities,"The Workshops on Algorithms for Modern Massive Data Sets (MMDS) bring together computer scientists, statisticians, mathematicians, and data analysis practitioners to discuss algorithmic challenges of big data. Regarding big data, such concepts of magnitude are relative.  As it is stated ""If the past is of any guidance, then today's big data most likely will not be considered as such in the near future.""","The Workshops on Algorithms for Modern Massive Data Sets (MMDS) bring together computer scientists, statisticians, mathematicians, and data analysis practitioners to discuss algorithmic challenges of big data. Regarding big data, such concepts of magnitude are relative.","[' What are the Workshops on Algorithms for Modern Massive Data Sets (MMDS) called?', ' Computer scientists, statisticians, mathematicians, and data analysis practitioners gather to discuss the algorithmic challenges of big data?']","['algorithmic challenges of big data', 'The Workshops on Algorithms for Modern Massive Data Sets']"
429,big data,Critique,"Critiques of the big data paradigm come in two flavors: those that question the implications of the approach itself, and those that question the way it is currently done. One approach to this criticism is the field of critical data studies.
","Critiques of the big data paradigm come in two flavors: those that question the implications of the approach itself, and those that question the way it is currently done. One approach to this criticism is the field of critical data studies.","[' Critics of the big data paradigm come in what two flavors?', ' Critics that question the implications of the approach itself and the way it is currently done are called what?']","['those that question the implications of the approach itself, and those that question the way it is currently done', 'Critiques']"
430,information retrieval,Summary,"Information retrieval (IR) in computing and information science is the process of obtaining information system resources that are relevant to an information need from a collection of those resources.  Searches can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata that describes data, and for databases of texts, images or sounds.
",Information retrieval (IR) in computing and information science is the process of obtaining information system resources that are relevant to an information need from a collection of those resources. Searches can be based on full-text or other content-based indexing.,"[' What is the process of obtaining information system resources that are relevant to an information need from a collection of resources?', ' Searches can be based on what?']","['Information retrieval', 'full-text or other content-based indexing']"
431,information retrieval,Summary,"Automated information retrieval systems are used to reduce what has been called information overload. An IR system is a software system that provides access to books, journals and other documents; stores and manages those documents. Web search engines are the most visible IR applications.
","Automated information retrieval systems are used to reduce what has been called information overload. An IR system is a software system that provides access to books, journals and other documents; stores and manages those documents.","[' Automated information retrieval systems are used to reduce what?', ' What is an IR system?']","['information overload', 'a software system that provides access to books, journals and other documents']"
432,information retrieval,Overview,"An information retrieval process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines. In information retrieval a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of relevancy.
","An information retrieval process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines.","[' When does an information retrieval process begin?', ' What is a formal statement of information needs?']","['when a user enters a query into the system', 'Queries']"
433,information retrieval,Overview,"An object is an entity that is represented by information in a content collection or database. User queries are matched against the database information. However, as opposed to classical SQL queries of a database, in information retrieval the results returned may or may not match the query, so results are typically ranked. This ranking of results is a key difference of information retrieval searching compared to database searching.",An object is an entity that is represented by information in a content collection or database. User queries are matched against the database information.,"[' What is an entity that is represented by information in a content collection or database?', ' What are user queries matched against?']","['An object', 'database information']"
434,information retrieval,Overview,"Depending on the application the data objects may be, for example, text documents, images, audio, mind maps or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or metadata.
","Depending on the application the data objects may be, for example, text documents, images, audio, mind maps or videos. Often the documents themselves are not kept or stored directly in the IR system, but are instead represented in the system by document surrogates or metadata.","[' Text documents, images, audio, mind maps and videos are examples of what?', ' Documents themselves are not kept or stored directly in what system?']","['data objects', 'IR']"
435,information retrieval,Overview,"Most IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user. The process may then be iterated if the user wishes to refine the query.","Most IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user.","[' What do most IR systems compute on how well each object in a database matches a query?', ' What are the top ranking objects shown to the user?']","['a numeric score', 'numeric score on how well each object in the database']"
436,information retrieval,History,"there is ... a machine called the Univac ... whereby letters and figures are coded as a pattern of magnetic spots on a long steel tape. By this means the text of a document, preceded by its subject code symbol, can be recorded ... the machine ... automatically selects and types out those references which have been coded in any desired way at a rate of 120 words a minute","there is ... a machine called the Univac ... whereby letters and figures are coded as a pattern of magnetic spots on a long steel tape. By this means the text of a document, preceded by its subject code symbol, can be recorded ... the machine ... automatically selects and types out those references which have been coded in any desired way at a rate of 120 words a minute","[' What is the Univac?', ' What is coded as a pattern of magnetic spots on a long steel tape?', ' How many words a minute is the machine able to type out?']","['a machine', 'letters and figures', '120']"
437,information retrieval,History,"The idea of using computers to search for relevant pieces of information was popularized in the article As We May Think by Vannevar Bush in 1945. It would appear that Bush was inspired by patents for a 'statistical machine' - filed by Emanuel Goldberg in the 1920s and '30s - that searched for documents stored on film. The first description of a computer searching for information was described by Holmstrom in 1948, detailing an early mention of the Univac computer. Automated information retrieval systems were introduced in the 1950s: one even featured in the 1957 romantic comedy, Desk Set. In the 1960s, the first large information retrieval research group was formed by Gerard Salton at Cornell. By the 1970s several different retrieval techniques had been shown to perform well on small text corpora such as the Cranfield collection (several thousand documents). Large-scale retrieval systems, such as the Lockheed Dialog system, came into use early in the 1970s.
",The idea of using computers to search for relevant pieces of information was popularized in the article As We May Think by Vannevar Bush in 1945. It would appear that Bush was inspired by patents for a 'statistical machine' - filed by Emanuel Goldberg in the 1920s and '30s - that searched for documents stored on film.,"[' Who popularized the idea of using computers to search for relevant pieces of information?', ' In what year was the article As We May Think published?', ' Who filed patents for a statistical machine in the 1920s and 30s?', ' Who filed documents in the 1920s and 30s?', ' Who searched for documents stored on film?']","['Vannevar Bush', '1945', 'Emanuel Goldberg', 'Emanuel Goldberg', 'Emanuel Goldberg']"
438,information retrieval,History,"In 1992, the US Department of Defense along with the National Institute of Standards and Technology (NIST), cosponsored the Text Retrieval Conference (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection. This catalyzed research on methods that scale to huge corpora. The introduction of web search engines has boosted the need for very large scale retrieval systems even further.
","In 1992, the US Department of Defense along with the National Institute of Standards and Technology (NIST), cosponsored the Text Retrieval Conference (TREC) as part of the TIPSTER text program. The aim of this was to look into the information retrieval community by supplying the infrastructure that was needed for evaluation of text retrieval methodologies on a very large text collection.","[' In what year did the US Department of Defense and NIST co-sponsored the Text Retrieval Conference?', ' What was the purpose of the TREC?', ' The TREC was part of what program?', ' What community provided the infrastructure needed for evaluation of text retrieval methodologies on a very large text collection?']","['1992', 'to look into the information retrieval community', 'TIPSTER', 'information retrieval community']"
439,information retrieval,Model types,"For effectively retrieving relevant documents by IR strategies, the documents are typically transformed into a suitable representation. Each retrieval strategy incorporates a specific model for its document representation purposes. The picture on the right illustrates the relationship of some common models. In the picture, the models are categorized according to two dimensions: the mathematical basis and the properties of the model.
","For effectively retrieving relevant documents by IR strategies, the documents are typically transformed into a suitable representation. Each retrieval strategy incorporates a specific model for its document representation purposes.","[' How are documents typically transformed into a suitable representation?', ' Each retrieval strategy incorporates a specific model for what purpose?']","['IR strategies', 'document representation purposes']"
440,information retrieval,Performance and correctness measures,"The evaluation of an information retrieval system' is the process of assessing how well a system meets the information needs of its users. In general, measurement considers a collection of documents to be searched and a search query. Traditional evaluation metrics, designed for Boolean retrieval or top-k retrieval, include precision and recall. All measures assume a ground truth notion of relevancy: every document is known to be either relevant or non-relevant to a particular query. In practice, queries may be ill-posed and there may be different shades of relevancy.
","The evaluation of an information retrieval system' is the process of assessing how well a system meets the information needs of its users. In general, measurement considers a collection of documents to be searched and a search query.","[' What is the process of assessing how well a system meets the information needs of its users?', ' What does measurement consider a collection of documents to be searched and?']","['The evaluation of an information retrieval system', 'a search query']"
441,natural language processing,Summary,"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.  The goal is a computer capable of ""understanding"" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.
","Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of ""understanding"" the contents of documents, including the contextual nuances of the language within them.","[' What subfield of linguistics, computer science, and artificial intelligence is concerned with the interactions between computers and human language?', ' What is the goal of NLP?', ' How can a computer understand the contents of natural language data?', ' What is the goal of a computer capable of ""understanding"" the contents of documents?', ' What are the nuances of the language within a document?']","['Natural language processing', 'a computer capable of ""understanding"" the contents of documents', 'understanding"" the contents of documents, including the contextual nuances of the language within them', 'Natural language processing', 'contextual']"
442,natural language processing,History,"Natural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled ""Computing Machinery and Intelligence"" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence. The proposed test includes a task that involves the automated interpretation and generation of natural language.
","Natural language processing has its roots in the 1950s. Already in 1950, Alan Turing published an article titled ""Computing Machinery and Intelligence"" which proposed what is now called the Turing test as a criterion of intelligence, though at the time that was not articulated as a problem separate from artificial intelligence.","[' In what decade did natural language processing begin?', ' Who published the article ""Computing Machinery and Intelligence"" in 1950?', ' What did Alan Turing propose as a criterion of intelligence?', ' What was not articulated as a problem separate from artificial intelligence?']","['1950s', 'Alan Turing', 'the Turing test', 'the Turing test']"
443,natural language processing,Common NLP tasks,"The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.
","The following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.","[' What are some of the most commonly researched tasks in natural language processing?', ' Some of the tasks have what kind of applications?']","['direct real-world applications', 'direct real-world']"
444,natural language processing,Common NLP tasks,"Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.
","Though natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.","[' What are natural language processing tasks closely intertwined with?', ' What can be subdivided into categories?']","['categories', 'natural language processing tasks']"
445,natural language processing,General tendencies and (possible) future directions,"Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:","Based on long-standing trends in the field, it is possible to extrapolate future directions of NLP. As of 2020, three trends among the topics of the long-standing series of CoNLL Shared Tasks can be observed:","[' What is it possible to extrapolate future directions of?', ' As of 2020, how many trends can be observed among the topics of the long-standing series of CoNLL Shared Tasks?']","['NLP', 'three']"
446,feature extraction,Summary,"In machine learning, pattern recognition, and image processing, feature extraction starts from an initial set of measured data and builds derived values (features) intended to be informative and non-redundant, facilitating the subsequent learning and generalization steps, and in some cases leading to better human interpretations. Feature extraction is related to dimensionality reduction.","In machine learning, pattern recognition, and image processing, feature extraction starts from an initial set of measured data and builds derived values (features) intended to be informative and non-redundant, facilitating the subsequent learning and generalization steps, and in some cases leading to better human interpretations. Feature extraction is related to dimensionality reduction.","[' In machine learning, pattern recognition, and image processing, what does feature extraction start from?', ' What are derived values intended to be?', ' Feature extraction is related to what?']","['an initial set of measured data', 'informative and non-redundant', 'dimensionality reduction']"
447,feature extraction,Summary,"When the input data to an algorithm is too large to be processed and it is suspected to be redundant (e.g. the same measurement in both feet and meters, or the repetitiveness of images presented as pixels), then it can be transformed into a reduced set of features (also named a feature vector). Determining a subset of the initial features is called feature selection. The selected features are expected to contain the relevant information from the input data, so that the desired task can be performed by using this reduced representation instead of the complete initial data.
","When the input data to an algorithm is too large to be processed and it is suspected to be redundant (e.g. the same measurement in both feet and meters, or the repetitiveness of images presented as pixels), then it can be transformed into a reduced set of features (also named a feature vector).","[' When the input data to an algorithm is too large to be processed and is suspected to be redundant, what can it be transformed into?', ' What is another name for a reduced set of features?']","['a reduced set of features', 'feature vector']"
448,feature extraction,General,"Feature extraction involves reducing the number of resources required to describe a large set of data. When performing analysis of complex data one of the major problems stems from the number of variables involved. Analysis with a large number of variables generally requires a large amount of memory and computation power, also it may cause a classification algorithm to overfit to training samples and generalize poorly to new samples. Feature extraction is a general term for methods of constructing combinations of the variables to get around these problems while still describing the data with sufficient accuracy.  Many machine learning practitioners believe that properly optimized feature extraction is the key to effective model construction.",Feature extraction involves reducing the number of resources required to describe a large set of data. When performing analysis of complex data one of the major problems stems from the number of variables involved.,"[' What involves reducing the number of resources required to describe a large set of data?', ' When performing analysis of complex data one of the major problems stems from how many variables involved?']","['Feature extraction', 'number']"
449,feature extraction,General,"Results can be improved using constructed sets of application-dependent features, typically built by an expert. One such process is called feature engineering. Alternatively, general dimensionality reduction techniques are used such as:
","Results can be improved using constructed sets of application-dependent features, typically built by an expert. One such process is called feature engineering.","[' What can be improved by using constructed sets of application-dependent features?', ' What is a process called that is typically built by an expert?']","['Results', 'feature engineering']"
450,feature extraction,Image processing,"One very important area of application is image processing, in which algorithms are used to detect and isolate various desired portions or shapes (features) of a digitized image or video stream. It is particularly important in the area of optical character recognition.
","One very important area of application is image processing, in which algorithms are used to detect and isolate various desired portions or shapes (features) of a digitized image or video stream. It is particularly important in the area of optical character recognition.","[' What is a very important area of application?', ' What are algorithms used to detect and isolate various desired portions or shapes of a digitized image or video stream?']","['image processing', 'image processing']"
451,feature extraction,Implementations,"Many data analysis software packages provide for feature extraction and dimension reduction. Common numerical programming environments such as MATLAB, SciLab, NumPy, Sklearn and the R language provide some of the simpler feature extraction techniques (e.g. principal component analysis) via built-in commands. More specific algorithms are often available as publicly available scripts or third-party add-ons.  There are also software packages targeting specific software machine learning applications that specialize in feature extraction.","Many data analysis software packages provide for feature extraction and dimension reduction. Common numerical programming environments such as MATLAB, SciLab, NumPy, Sklearn and the R language provide some of the simpler feature extraction techniques (e.g.","[' What does MATLAB, SciLab, NumPy, Sklearn and the R language provide?', ' What are some of the simpler feature extraction techniques?']","['some of the simpler feature extraction techniques', 'Common numerical programming environments such as MATLAB, SciLab, NumPy, Sklearn and the R language']"
452,wireless sensor networks,Summary,"Wireless sensor networks (WSNs) refer to networks of spatially dispersed and dedicated sensors that monitor and record the physical conditions of the environment and forward the collected data to a central location. WSNs can measure environmental conditions such as temperature, sound, pollution levels, humidity and wind.","Wireless sensor networks (WSNs) refer to networks of spatially dispersed and dedicated sensors that monitor and record the physical conditions of the environment and forward the collected data to a central location. WSNs can measure environmental conditions such as temperature, sound, pollution levels, humidity and wind.","[' What does WSN stand for?', ' What do WSNs monitor and record?', ' Where can WSN data be sent?']","['Wireless sensor networks', 'the physical conditions of the environment', 'central location']"
453,wireless sensor networks,Summary,"These are similar to wireless ad hoc networks in the sense that they rely on wireless connectivity and spontaneous formation of networks so that sensor data can be transported wirelessly. WSNs monitor physical or environmental conditions, such as temperature, sound, and pressure. Modern networks are bi-directional, both collecting data and enabling control of sensor activity.  The development of these networks was motivated by military applications such as battlefield surveillance. Such networks are used in industrial and consumer applications, such as industrial process monitoring and control and machine health monitoring.
","These are similar to wireless ad hoc networks in the sense that they rely on wireless connectivity and spontaneous formation of networks so that sensor data can be transported wirelessly. WSNs monitor physical or environmental conditions, such as temperature, sound, and pressure.","[' What type of networks are similar to wireless ad hoc networks in the sense that they rely on wireless connectivity and spontaneous formation of networks?', ' WSNs monitor physical or environmental conditions, such as temperature, sound, and pressure.']","['WSNs', 'wireless ad hoc networks']"
454,wireless sensor networks,Summary,"A WSN is built of ""nodes"" – from a few to hundreds or thousands, where each node is connected to other sensors. Each such node typically has several parts: a radio transceiver with an internal antenna or connection to an external antenna, a microcontroller, an electronic circuit for interfacing with the sensors and an energy source, usually a battery or an embedded form of energy harvesting. A sensor node might vary in size from a shoebox to (theoretically) a grain of dust, although microscopic dimensions have yet to be realized. Sensor node cost is similarly variable, ranging from a few to hundreds of dollars, depending on node sophistication. Size and cost constraints constrain resources such as energy, memory, computational speed and communications bandwidth. The topology of a WSN can vary from a simple star network to an advanced multi-hop wireless mesh network. Propagation can employ routing or flooding.","A WSN is built of ""nodes"" – from a few to hundreds or thousands, where each node is connected to other sensors. Each such node typically has several parts: a radio transceiver with an internal antenna or connection to an external antenna, a microcontroller, an electronic circuit for interfacing with the sensors and an energy source, usually a battery or an embedded form of energy harvesting.","[' What is a WSN built of?', ' How many sensors are connected to each node?', ' What is the name of the electronic circuit for interfacing with other sensors?', ' What is a microcontroller?', ' What is an energy source?']","['nodes', 'hundreds or thousands', 'microcontroller', 'an electronic circuit for interfacing with the sensors', 'a battery']"
455,wireless sensor networks,Summary,"In computer science and telecommunications, wireless sensor networks are an active research area supporting many workshops and conferences, including International Workshop on Embedded Networked Sensors (EmNetS), IPSN, SenSys, MobiCom and EWSN. As of 2010, wireless sensor networks had deployed approximately 120 million remote units worldwide.","In computer science and telecommunications, wireless sensor networks are an active research area supporting many workshops and conferences, including International Workshop on Embedded Networked Sensors (EmNetS), IPSN, SenSys, MobiCom and EWSN. As of 2010, wireless sensor networks had deployed approximately 120 million remote units worldwide.","[' In what fields are wireless sensor networks an active research area?', ' What is the International Workshop on Embedded Networked Sensors?', ' As of 2010, how many remote units were deployed worldwide?']","['computer science and telecommunications', 'EmNetS', '120\xa0million']"
456,wireless sensor networks,Characteristics,"Cross-layer is becoming an important studying area for wireless communications. In addition, the traditional layered approach presents three main problems:
","Cross-layer is becoming an important studying area for wireless communications. In addition, the traditional layered approach presents three main problems:","[' What is becoming an important studying area for wireless communications?', ' How many main problems does the traditional layered approach present?']","['Cross-layer', 'three']"
457,wireless sensor networks,Characteristics,"So the cross-layer can be used to make the optimal modulation to improve the transmission performance, such as data rate, energy efficiency, quality of service (QoS), etc. Sensor nodes can be imagined as small computers which are extremely basic in terms of their interfaces and their components. They usually consist of a processing unit with limited computational power and limited memory, sensors or MEMS (including specific conditioning circuitry), a communication device (usually radio transceivers or alternatively optical), and a power source usually in the form of a battery. Other possible inclusions are energy harvesting modules, secondary ASICs, and possibly secondary communication interface (e.g. RS-232 or USB).
","So the cross-layer can be used to make the optimal modulation to improve the transmission performance, such as data rate, energy efficiency, quality of service (QoS), etc. Sensor nodes can be imagined as small computers which are extremely basic in terms of their interfaces and their components.","[' What can be used to make the optimal modulation to improve the transmission performance?', ' Sensor nodes can be imagined as small computers which are what?']","['the cross-layer', 'extremely basic']"
458,wireless sensor networks,Characteristics,"The base stations are one or more components of the WSN with much more computational, energy and communication resources. They act as a gateway between sensor nodes and the end user as they typically forward data from the WSN on to a server. Other special components in routing based networks are routers, designed to compute, calculate and distribute the routing tables.","The base stations are one or more components of the WSN with much more computational, energy and communication resources. They act as a gateway between sensor nodes and the end user as they typically forward data from the WSN on to a server.","[' What are base stations?', ' What do base stations act as between sensor nodes and end user?']","['one or more components of the WSN with much more computational, energy and communication resources', 'gateway']"
459,wireless sensor networks,Simulation,"At present, agent-based modeling and simulation is the only paradigm which allows the simulation of complex behavior in the environments of wireless sensors (such as flocking). Agent-based simulation of wireless sensor and ad hoc networks is a relatively new paradigm. Agent-based modelling was originally based on social simulation.
","At present, agent-based modeling and simulation is the only paradigm which allows the simulation of complex behavior in the environments of wireless sensors (such as flocking). Agent-based simulation of wireless sensor and ad hoc networks is a relatively new paradigm.","[' What is the only paradigm that allows the simulation of complex behavior in the environments of wireless sensors?', ' What is a relatively new paradigm?']","['agent-based modeling and simulation', 'Agent-based simulation of wireless sensor and ad hoc networks']"
460,independent component analysis,Summary,"In signal processing, independent component analysis (ICA) is a computational method for separating a multivariate signal into additive subcomponents. This is done by assuming that the subcomponents are, potentially, non-Gaussian signals and that they are statistically independent from each other. ICA is a special case of blind source separation. A common example application is the ""cocktail party problem"" of listening in on one person's speech in a noisy room.","In signal processing, independent component analysis (ICA) is a computational method for separating a multivariate signal into additive subcomponents. This is done by assuming that the subcomponents are, potentially, non-Gaussian signals and that they are statistically independent from each other.","[' What is ICA?', ' What is it called when a multivariate signal is separated into additive subcomponents?']","['independent component analysis', 'independent component analysis']"
461,independent component analysis,Introduction,"Independent component analysis attempts to decompose a multivariate signal into independent non-Gaussian signals. As an example, sound is usually a signal that is composed of the numerical addition, at each time t, of signals from several sources. The question then is whether it is possible to separate these contributing sources from the observed total signal. When the statistical independence assumption is correct, blind ICA separation of a mixed signal gives very good results. It is also used for signals that are not supposed to be generated by mixing for analysis purposes.
","Independent component analysis attempts to decompose a multivariate signal into independent non-Gaussian signals. As an example, sound is usually a signal that is composed of the numerical addition, at each time t, of signals from several sources.","[' What does independent component analysis try to decompose a multivariate signal into?', ' Sound is usually a signal that is composed of the numerical addition, at each time t, of signals from several sources.']","['independent non-Gaussian signals', 'sound']"
462,independent component analysis,Introduction,"A simple application of ICA is the ""cocktail party problem"", where the underlying speech signals are separated from a sample data consisting of people talking simultaneously in a room. Usually the problem is simplified by assuming no time delays or echoes. Note that a filtered and delayed signal is a copy of a dependent component, and thus the statistical independence assumption is not violated.
","A simple application of ICA is the ""cocktail party problem"", where the underlying speech signals are separated from a sample data consisting of people talking simultaneously in a room. Usually the problem is simplified by assuming no time delays or echoes.","[' What is a simple application of ICA?', ' What is the ""cocktail party problem""?', ' How is the problem simplified?']","['cocktail party problem', 'the underlying speech signals are separated from a sample data consisting of people talking simultaneously in a room', 'by assuming no time delays or echoes']"
463,independent component analysis,Introduction,"Mixing weights for constructing the 



M


{\textstyle M}
 observed signals from the 



N


{\textstyle N}
 components can be placed in an 



M
×
N


{\textstyle M\times N}
 matrix. An important thing to consider is that if 



N


{\textstyle N}
 sources are present, at least 



N


{\textstyle N}
 observations (e.g. microphones if the observed signal is audio) are needed to recover the original signals. When there are an equal number of observations and source signals, the mixing matrix is square (



M
=
N


{\textstyle M=N}
). Other cases of underdetermined (



M
<
N


{\textstyle M<N}
) and overdetermined (



M
>
N


{\textstyle M>N}
) have been investigated.
","Mixing weights for constructing the 



M


{\textstyle M}
 observed signals from the 



N


{\textstyle N}
 components can be placed in an 



M
×
N


{\textstyle M\times N}
 matrix. An important thing to consider is that if 



N


{\textstyle N}
 sources are present, at least 



N


{\textstyle N}
 observations (e.g.",[' What can be placed in an M <unk> N <unk>textstyle M<unk>times N matrix?'],['N\n\n\n{\\textstyle N}\n components']
464,independent component analysis,Introduction,"That the ICA separation of mixed signals gives very good results is based on two assumptions and three effects of mixing source signals. Two assumptions:
",That the ICA separation of mixed signals gives very good results is based on two assumptions and three effects of mixing source signals. Two assumptions:,"[' How many assumptions does the ICA have?', ' How many effects of mixing source signals are there?']","['two', 'three']"
465,independent component analysis,Introduction,"Those principles contribute to the basic establishment of ICA. If the signals extracted from a set of mixtures are independent, and have non-Gaussian histograms or have low complexity, then they must be source signals.","Those principles contribute to the basic establishment of ICA. If the signals extracted from a set of mixtures are independent, and have non-Gaussian histograms or have low complexity, then they must be source signals.","[' What contributes to the basic establishment of ICA?', ' If the signals extracted from a set of mixtures are independent, and have non-Gaussian histograms, or have low complexity, then they must be source signals?']","['principles', 'ICA']"
466,independent component analysis,Defining component independence,"ICA finds the independent components (also called factors, latent variables or sources) by maximizing the statistical independence of the estimated components. We may choose one of many ways to define a proxy for independence, and this choice governs the form of the ICA algorithm. The two broadest definitions of independence for ICA are
","ICA finds the independent components (also called factors, latent variables or sources) by maximizing the statistical independence of the estimated components. We may choose one of many ways to define a proxy for independence, and this choice governs the form of the ICA algorithm.","[' ICA finds the independent components by maximizing what?', ' What is a proxy for independence called?']","['statistical independence of the estimated components', 'sources']"
467,independent component analysis,Defining component independence,"The Minimization-of-Mutual information (MMI) family of ICA algorithms uses measures like Kullback-Leibler Divergence and maximum entropy.  The non-Gaussianity family of ICA algorithms, motivated by the central limit theorem, uses kurtosis and negentropy.
","The Minimization-of-Mutual information (MMI) family of ICA algorithms uses measures like Kullback-Leibler Divergence and maximum entropy. The non-Gaussianity family of ICA algorithms, motivated by the central limit theorem, uses kurtosis and negentropy.","[' What family of ICA algorithms uses measures like Kullback-Leibler Divergence and maximum entropy?', ' What is motivated by the central limit theorem?']","['Minimization-of-Mutual information (MMI)', 'The non-Gaussianity family of ICA algorithms']"
468,independent component analysis,Defining component independence,"Typical algorithms for ICA use centering (subtract the mean to create a zero mean signal), whitening (usually with the eigenvalue decomposition), and dimensionality reduction as preprocessing steps in order to simplify and reduce the complexity of the problem for the actual iterative algorithm. Whitening and dimension reduction can be achieved with principal component analysis or singular value decomposition. Whitening ensures that all dimensions are treated equally a priori before the algorithm is run. Well-known algorithms for ICA include infomax, FastICA, JADE, and kernel-independent component analysis, among others. In general, ICA cannot identify the actual number of source signals, a uniquely correct ordering of the source signals, nor the proper scaling (including sign) of the source signals.
","Typical algorithms for ICA use centering (subtract the mean to create a zero mean signal), whitening (usually with the eigenvalue decomposition), and dimensionality reduction as preprocessing steps in order to simplify and reduce the complexity of the problem for the actual iterative algorithm. Whitening and dimension reduction can be achieved with principal component analysis or singular value decomposition.","[' What do algorithms for ICA use to simplify and reduce the complexity of the problem for the actual iterative algorithm?', ' What does centering do?', ' What can be achieved with principal component analysis or singular value decomposition?']","['centering (subtract the mean to create a zero mean signal), whitening (usually with the eigenvalue decomposition), and dimensionality reduction', 'subtract the mean to create a zero mean signal', 'Whitening and dimension reduction']"
469,independent component analysis,Defining component independence,"ICA is important to blind signal separation and has many practical applications. It is closely related to (or even a special case of) the search for a factorial code of the data, i.e., a new vector-valued representation of each data vector such that it gets uniquely encoded by the resulting code vector (loss-free coding), but the code components are statistically independent.
","ICA is important to blind signal separation and has many practical applications. It is closely related to (or even a special case of) the search for a factorial code of the data, i.e., a new vector-valued representation of each data vector such that it gets uniquely encoded by the resulting code vector (loss-free coding), but the code components are statistically independent.","[' What is important to blind signal separation?', ' What is closely related to the search for a factorial code of the data?', ' How does each data vector get uniquely encoded?', ' What is loss-free coding?']","['ICA', 'ICA', 'code vector', 'a new vector-valued representation of each data vector such that it gets uniquely encoded by the resulting code vector']"
470,independent component analysis,Mathematical definitions,"Linear independent component analysis can be divided into noiseless and noisy cases, where noiseless ICA is a special case of noisy ICA. Nonlinear ICA should be considered as a separate case.
","Linear independent component analysis can be divided into noiseless and noisy cases, where noiseless ICA is a special case of noisy ICA. Nonlinear ICA should be considered as a separate case.","[' What can be divided into noiseless and noisy cases?', ' Noiseless ICA is a special case of what?', ' What should be considered as a separate case?']","['Linear independent component analysis', 'noisy ICA', 'Nonlinear ICA']"
471,independent component analysis,Binary ICA,"A special variant of ICA is binary ICA in which both signal sources and monitors are in binary form and observations from monitors are disjunctive mixtures of binary independent sources. The problem was shown to have applications in many domains including medical diagnosis, multi-cluster assignment, network tomography and internet resource management.
","A special variant of ICA is binary ICA in which both signal sources and monitors are in binary form and observations from monitors are disjunctive mixtures of binary independent sources. The problem was shown to have applications in many domains including medical diagnosis, multi-cluster assignment, network tomography and internet resource management.","[' What is a special variant of ICA?', ' Signal sources and monitors are in what form?', ' What are observations from monitors disjunctive mixtures of?', ' In addition to medical diagnosis, multi-cluster assignment, network tomography, and internet resource management, what else is covered by the Internet?']","['binary ICA', 'binary', 'binary independent sources', 'binary ICA']"
472,independent component analysis,Binary ICA,"Let 





x

1


,

x

2


,
…
,

x

m





{\displaystyle {x_{1},x_{2},\ldots ,x_{m}}}
 be the set of binary variables from 



m


{\displaystyle m}
 monitors and 





y

1


,

y

2


,
…
,

y

n





{\displaystyle {y_{1},y_{2},\ldots ,y_{n}}}
 be the set of binary variables from 



n


{\displaystyle n}
 sources. Source-monitor connections are represented by the (unknown) mixing matrix 




G



{\textstyle {\boldsymbol {G}}}
, where 




g

i
j


=
1


{\displaystyle g_{ij}=1}
 indicates that signal from the i-th source can be observed by the j-th monitor. The system works as follows: at any time, if a source 



i


{\displaystyle i}
 is active (




y

i


=
1


{\displaystyle y_{i}=1}
) and it is connected to the monitor 



j


{\displaystyle j}
 (




g

i
j


=
1


{\displaystyle g_{ij}=1}
) then the monitor 



j


{\displaystyle j}
 will observe some activity (




x

j


=
1


{\displaystyle x_{j}=1}
). Formally we have:
","Let 





x

1


,

x

2


,
…
,

x

m





{\displaystyle {x_{1},x_{2},\ldots ,x_{m}}}
 be the set of binary variables from 



m


{\displaystyle m}
 monitors and 





y

1


,

y

2


,
…
,

y

n





{\displaystyle {y_{1},y_{2},\ldots ,y_{n}}}
 be the set of binary variables from 



n


{\displaystyle n}
 sources. Source-monitor connections are represented by the (unknown) mixing matrix 




G



{\textstyle {\boldsymbol {G}}}
, where 




g

i
j


=
1


{\displaystyle g_{ij}=1}
 indicates that signal from the i-th source can be observed by the j-th monitor.","[' What is represented by the mixing matrix G <unk>textstyle <unk>boldsymbol <unk>G?', ' What indicates that signal from the i-th source?', ' Where g i j = 1 indicates that signal from the i-th source can be observed by the j-th monitor?']","['Source-monitor connections', 'g\n\ni\nj\n\n\n=\n1\n\n\n{\\displaystyle g_{ij}=1}', 'g\n\ni\nj\n\n\n=\n1']"
473,independent component analysis,Binary ICA,"where 



∧


{\displaystyle \wedge }
 is Boolean AND and 



∨


{\displaystyle \vee }
 is Boolean OR. Note that noise is not explicitly modelled, rather, can be treated as independent sources.
","where 



∧


{\displaystyle \wedge }
 is Boolean AND and 



∨


{\displaystyle \vee }
 is Boolean OR. Note that noise is not explicitly modelled, rather, can be treated as independent sources.","[' Where <unk> <unk>displaystyle <unk>wedge <unk> is Boolean AND?', ' What is not explicitly modelled, rather can be treated as what?']","['∧', 'independent sources']"
474,mobile robot,Summary,"A mobile robot, is a robot that is capable of moving in the surrounding (locomotion). Mobile robotics is usually considered to be a subfield of robotics and information engineering.","A mobile robot, is a robot that is capable of moving in the surrounding (locomotion). Mobile robotics is usually considered to be a subfield of robotics and information engineering.","[' What is a robot that is capable of moving in the surrounding?', ' What is mobile robotics usually considered to be a subfield of?']","['mobile robot', 'robotics and information engineering']"
475,mobile robot,Summary,"Mobile robots have the capability to move around in their environment and are not fixed to one physical location.  Mobile robots can be ""autonomous"" (AMR - autonomous mobile robot) which means they are capable of navigating an uncontrolled environment without the need for physical or electro-mechanical guidance devices.  Alternatively, mobile robots can rely on guidance devices that allow them to travel a pre-defined navigation route in relatively controlled space.  By contrast, industrial robots are usually more-or-less stationary, consisting of a jointed arm (multi-linked manipulator) and gripper assembly (or end effector), attached to a fixed surface. The joint-arm are controlled by linear actuator or servo motor or stepper motor.
","Mobile robots have the capability to move around in their environment and are not fixed to one physical location. Mobile robots can be ""autonomous"" (AMR - autonomous mobile robot) which means they are capable of navigating an uncontrolled environment without the need for physical or electro-mechanical guidance devices.","[' What are mobile robots able to do?', ' What does AMR stand for?']","['move around in their environment', 'autonomous mobile robot']"
476,mobile robot,Summary,"Mobile robots have become more commonplace in commercial and industrial settings.  Hospitals have been using autonomous mobile robots to move materials for many years.  Warehouses have installed mobile robotic systems to efficiently move materials from stocking shelves to order fulfillment zones. Mobile robots are also a major focus of current research and almost every major university has one or more labs that focus on mobile robot research. Mobile robots are also found in industrial, military and security settings.
",Mobile robots have become more commonplace in commercial and industrial settings. Hospitals have been using autonomous mobile robots to move materials for many years.,"[' What has become more commonplace in commercial and industrial settings?', ' Hospitals have been using mobile robots to move materials for many years?']","['Mobile robots', 'Mobile robots have become more commonplace in commercial and industrial settings.']"
477,mobile robot,Summary,"The components of a mobile robot are a controller, sensors, actuators and power system. The controller is generally a microprocessor, embedded microcontroller or a personal computer (PC). The sensors used are dependent upon the requirements of the robot. The requirements could be dead reckoning, tactile and proximity sensing, triangulation ranging, collision avoidance, position location and other specific applications. Actuators usually refer to the motors that move the robot can be wheeled or legged. To power a mobile robot usually we use DC power supply (which is battery) instead of AC.
","The components of a mobile robot are a controller, sensors, actuators and power system. The controller is generally a microprocessor, embedded microcontroller or a personal computer (PC).","[' What are the components of a mobile robot?', ' What is a microprocessor, embedded microcontroller or PC?']","['a controller, sensors, actuators and power system', 'The controller']"
478,business process,Summary,"A business process, business method or business function is a collection of related, structured activities or tasks by people or equipment in which a specific sequence produces a service or product (serves a particular business goal) for a particular customer or customers. Business processes occur at all organizational levels and may or may not be visible to the customers. A business process may often be visualized (modeled) as a flowchart of a sequence of activities with interleaving decision points or as a process matrix of a sequence of activities with relevance rules based on data in the process. The benefits of using business processes include improved customer satisfaction and improved agility for reacting to rapid market change. Process-oriented organizations break down the barriers of structural departments and try to avoid functional silos.","A business process, business method or business function is a collection of related, structured activities or tasks by people or equipment in which a specific sequence produces a service or product (serves a particular business goal) for a particular customer or customers. Business processes occur at all organizational levels and may or may not be visible to the customers.","[' What is a business process?', ' What are business processes?', ' Where do business processes occur?', ' What are business processes visible to?']","['a collection of related, structured activities or tasks by people or equipment', 'a collection of related, structured activities or tasks by people or equipment', 'at all organizational levels', 'customers']"
479,business process,Overview,"A business process begins with a mission objective (an external event) and ends with achievement of the business objective of providing a result that provides customer value. Additionally, a process may be divided into subprocesses (process decomposition), the particular inner functions of the process. Business processes may also have a process owner, a responsible party for ensuring the process runs smoothly from start to finish.","A business process begins with a mission objective (an external event) and ends with achievement of the business objective of providing a result that provides customer value. Additionally, a process may be divided into subprocesses (process decomposition), the particular inner functions of the process.","[' What does a business process begin with?', ' What is a mission objective?', ' A business process ends with what?']","['a mission objective', 'an external event', 'achievement of the business objective of providing a result that provides customer value']"
480,business process,Overview,"Broadly speaking, business processes can be organized into three types, according to von Rosing et al.:","Broadly speaking, business processes can be organized into three types, according to von Rosing et al. :","[' According to von Rosing, how many types of business processes can be organized?']",['three']
481,business process,Overview,"A complex business process may be decomposed into several subprocesses, which have their own attributes but also contribute to achieving the overall goal of the business. The analysis of business processes typically includes the mapping or modeling of processes and sub-processes down to activity/task level. Processes can be modeled through a large number of methods and techniques. For instance, the Business Process Modeling Notation is a business process modeling technique that can be used for drawing business processes in a visualized workflow. While decomposing processes into process types and categories can be useful, care must be taken in doing so as there may be crossover. In the end, all processes are part of a largely unified outcome, one of ""customer value creation."" This goal is expedited with business process management, which aims to analyze, improve, and enact business processes.","A complex business process may be decomposed into several subprocesses, which have their own attributes but also contribute to achieving the overall goal of the business. The analysis of business processes typically includes the mapping or modeling of processes and sub-processes down to activity/task level.","[' What may be decomposed into several subprocesses?', ' The analysis of business processes typically includes mapping or modeling of processes down to what level?']","['A complex business process', 'activity/task']"
482,business process,Importance of the process chain,"Business processes comprise a set of sequential sub-processes or tasks with alternative paths, depending on certain conditions as applicable, performed to achieve a given objective or produce given outputs. Each process has one or more needed inputs. The inputs and outputs may be received from, or sent to other business processes, other organizational units, or internal or external stakeholders.","Business processes comprise a set of sequential sub-processes or tasks with alternative paths, depending on certain conditions as applicable, performed to achieve a given objective or produce given outputs. Each process has one or more needed inputs.","[' Business processes are a set of what?', ' Each process has how many needed inputs?']","['sequential sub-processes', 'one or more']"
483,business process,Importance of the process chain,"Typically, some process tasks will be manual, while some will be computer-based, and these tasks may be sequenced in many ways. In other words, the data and information that are being handled through the process may pass through manual or computer tasks in any given order.
","Typically, some process tasks will be manual, while some will be computer-based, and these tasks may be sequenced in many ways. In other words, the data and information that are being handled through the process may pass through manual or computer tasks in any given order.","[' Some process tasks will typically be what?', ' Other than manual, what other type of task will be computer-based?', ' What may be sequenced in many ways?']","['manual', 'process', 'process tasks']"
484,business process,"Policies, processes and procedures","The above improvement areas are equally applicable to policies, processes, detailed procedures (sub-processes/tasks) and work instructions. There is a cascading effect of improvements made at a higher level on those made at a lower level.","The above improvement areas are equally applicable to policies, processes, detailed procedures (sub-processes/tasks) and work instructions. There is a cascading effect of improvements made at a higher level on those made at a lower level.",[' What is a cascading effect of improvements made at a higher level on those made at the lower level?'],"['policies, processes, detailed procedures (sub-processes/tasks) and work instructions']"
485,business process,Reporting as an essential base for execution,"Business processes must include up-to-date and accurate reports to ensure effective action. An example of this is the availability of purchase order status reports for supplier delivery follow-up as described in the section on effectiveness above. There are numerous examples of this in every possible business process.
",Business processes must include up-to-date and accurate reports to ensure effective action. An example of this is the availability of purchase order status reports for supplier delivery follow-up as described in the section on effectiveness above.,"[' Business processes must include up-to-date and accurate reports to ensure what?', ' The availability of purchase order status reports for supplier delivery follow-up is described in what section?']","['effective action', 'section on effectiveness above']"
486,business process,Reporting as an essential base for execution,"Another example from production is the process of analysis of line rejections occurring on the shop floor. This process should include systematic periodical analysis of rejections by reason, and present the results in a suitable information report that pinpoints the major reasons, and trends in these reasons, for management to take corrective actions to control rejections and keep them within acceptable limits. Such a process of analysis and summarisation of line rejection events is clearly superior to a process which merely inquires into each individual rejection as it occurs.
","Another example from production is the process of analysis of line rejections occurring on the shop floor. This process should include systematic periodical analysis of rejections by reason, and present the results in a suitable information report that pinpoints the major reasons, and trends in these reasons, for management to take corrective actions to control rejections and keep them within acceptable limits.","[' What should be included in the analysis of line rejections on the shop floor?', ' What should management present the results of the analysis in?', ' What are the major reasons for management to take corrective actions to control rejections and keep them within acceptable limits?']","['systematic periodical analysis of rejections by reason', 'a suitable information report', 'trends in these reasons']"
487,business process,Reporting as an essential base for execution,"Business process owners and operatives should realise that process improvement often occurs with introduction of appropriate transaction, operational, highlight, exception or M.I.S. reports, provided these are consciously used for day-to-day or periodical decision-making. With this understanding would hopefully come the willingness to invest time and other resources in business process improvement by introduction of useful and relevant reporting systems.
","Business process owners and operatives should realise that process improvement often occurs with introduction of appropriate transaction, operational, highlight, exception or M.I.S. reports, provided these are consciously used for day-to-day or periodical decision-making.","[' What should business process owners and operatives realize that process improvement often occurs with?', ' What should be consciously used for day-to-day or periodic decision-making?']","['introduction of appropriate transaction, operational, highlight, exception or M.I.S. reports', 'M.I.S. reports']"
488,fuzzy logic,Summary,"Fuzzy logic is a form of many-valued logic in which the truth value of variables may be any real number between 0 and 1. It is employed to handle the concept of partial truth, where the truth value may range between completely true and completely false. By contrast, in Boolean logic, the truth values of variables may only be the integer values 0 or 1.
","Fuzzy logic is a form of many-valued logic in which the truth value of variables may be any real number between 0 and 1. It is employed to handle the concept of partial truth, where the truth value may range between completely true and completely false.","[' What is a form of many-valued logic?', ' The truth value of variables may be any real number between 0 and what?', ' Fuzzy logic is employed to handle what concept?']","['Fuzzy logic', '1', 'partial truth']"
489,fuzzy logic,Summary,"The term fuzzy logic was introduced with the 1965 proposal of fuzzy set theory by scientist Lotfi Zadeh.  Fuzzy logic had, however,  been studied since the 1920s, as infinite-valued logic—notably by Łukasiewicz and Tarski.","The term fuzzy logic was introduced with the 1965 proposal of fuzzy set theory by scientist Lotfi Zadeh. Fuzzy logic had, however,  been studied since the 1920s, as infinite-valued logic—notably by Łukasiewicz and Tarski.","[' When was the term fuzzy logic introduced?', ' Who proposed fuzzy set theory?', ' When was fuzzy logic studied as infinite-valued logic?', ' What was the name given to fuzzy logic by Lotfi Zadeh?']","['1965', 'Lotfi Zadeh', 'since the 1920s', 'fuzzy set theory']"
490,fuzzy logic,Summary,"Fuzzy logic is based on the observation that people make decisions based on imprecise and non-numerical information. Fuzzy models or sets are mathematical means of representing vagueness and imprecise information (hence the term fuzzy). These models have the capability of recognising, representing, manipulating, interpreting, and using data and information that are vague and lack certainty.",Fuzzy logic is based on the observation that people make decisions based on imprecise and non-numerical information. Fuzzy models or sets are mathematical means of representing vagueness and imprecise information (hence the term fuzzy).,"[' What is fuzzy logic based on?', ' What are fuzzy models or sets mathematical means of representing?']","['the observation that people make decisions based on imprecise and non-numerical information', 'vagueness and imprecise information']"
491,fuzzy logic,Overview,"Classical logic only permits conclusions that are either true or false. However, there are also propositions with variable answers, such as one might find when asking a group of people to identify a color.  In such instances, the truth appears as the result of reasoning from inexact or partial knowledge in which the sampled answers are mapped on a spectrum.","Classical logic only permits conclusions that are either true or false. However, there are also propositions with variable answers, such as one might find when asking a group of people to identify a color.","[' Classical logic permits conclusions that are either true or what?', ' There are also propositions with what kind of answers?']","['false', 'variable']"
492,fuzzy logic,Forming a consensus of inputs and fuzzy rules,"Since the fuzzy system output is a consensus of all of the inputs and all of the rules, fuzzy logic systems can be well behaved when input values are not available or are not trustworthy. Weightings can be optionally added to each rule in the rulebase and weightings can be used to regulate the degree to which a rule affects the output values. These rule weightings can be based upon the priority, reliability or consistency of each rule. These rule weightings may be static or can be changed dynamically, even based upon the output from other rules.
","Since the fuzzy system output is a consensus of all of the inputs and all of the rules, fuzzy logic systems can be well behaved when input values are not available or are not trustworthy. Weightings can be optionally added to each rule in the rulebase and weightings can be used to regulate the degree to which a rule affects the output values.","[' What can be optionally added to each rule in the rulebase?', ' How can fuzzy logic systems be behaved when input values are not available?', ' What can be used to regulate the degree to which a rule affects the output values?']","['Weightings', 'well', 'weightings']"
493,fuzzy logic,Applications,"Charles Elkan writes ""It turns out that the useful applications of fuzzy logic are not in high-level artificial intelligence but rather in lower-level machine control, especially in consumer products."" It is used in control systems to allow experts to contribute vague rules such as ""if you are close to the destination station and moving fast, increase the train's brake pressure""; these vague rules can then be numerically refined within the system. 
","Charles Elkan writes ""It turns out that the useful applications of fuzzy logic are not in high-level artificial intelligence but rather in lower-level machine control, especially in consumer products."" It is used in control systems to allow experts to contribute vague rules such as ""if you are close to the destination station and moving fast, increase the train's brake pressure""; these vague rules can then be numerically refined within the system.","[' What is used in control systems to allow experts to contribute vague rules?', ' How can vague rules be numerically refined within the system?']","['fuzzy logic', 'fuzzy logic']"
494,fuzzy logic,Applications,"Many of the early successful applications of fuzzy logic were implemented in Japan. The first notable application was on the subway train in Sendai, in which fuzzy logic was able to improve the economy, comfort, and precision of the ride. It has also been used for handwriting recognition in Sony pocket computers, helicopter flight aids, subway system controls, improving automobile fuel efficiency, single-button washing machine controls, automatic power controls in vacuum cleaners, and early recognition of earthquakes through the Institute of Seismology Bureau of Meteorology, Japan.","Many of the early successful applications of fuzzy logic were implemented in Japan. The first notable application was on the subway train in Sendai, in which fuzzy logic was able to improve the economy, comfort, and precision of the ride.","[' Where were many of the early successful applications of fuzzy logic implemented?', ' What was the first notable application for fuzzy logic?', ' On what train was fuzzy logic able to improve economy, comfort, and precision?']","['Japan', 'the subway train', 'subway']"
495,fuzzy logic,Compensatory fuzzy logic,"Compensatory fuzzy logic (CFL) is a branch of fuzzy logic with modified rules for conjunction and disjunction. When the truth value of one component of a conjunction or disjunction is increased or decreased, the other component is decreased or increased to compensate. This increase or decrease in truth value may be offset by the increase or decrease in another component. An offset may be blocked when certain thresholds are met. Proponents claim that CFL allows for better computational semantic behaviors and mimic natural language.","Compensatory fuzzy logic (CFL) is a branch of fuzzy logic with modified rules for conjunction and disjunction. When the truth value of one component of a conjunction or disjunction is increased or decreased, the other component is decreased or increased to compensate.","[' What branch of fuzzy logic has modified rules for conjunction and disjunction?', ' When the truth value of one component of a conjunction is increased or decreased, the other component is decreased or increased to compensate?']","['Compensatory fuzzy logic', 'increased']"
496,fuzzy logic,Compensatory fuzzy logic,Compensatory fuzzy logic consists of four continuous operators: conjunction (c); disjunction (d); fuzzy strict order (or); and negation (n). The conjunction is the geometric mean and its dual as conjunctive and disjunctive operators.,Compensatory fuzzy logic consists of four continuous operators: conjunction (c); disjunction (d); fuzzy strict order (or); and negation (n). The conjunction is the geometric mean and its dual as conjunctive and disjunctive operators.,"[' How many continuous operators does compensatory fuzzy logic consist of?', ' What is the conjunction the geometric mean?']","['four', 'conjunction is the geometric mean and its dual as conjunctive and disjunctive operators']"
497,fuzzy logic,Markup Language Standardization,"The IEEE 1855, the IEEE STANDARD 1855–2016, is about a specification language named Fuzzy Markup Language (FML) developed by the IEEE Standards Association. FML allows modelling a fuzzy logic system in a human-readable and hardware independent way. FML is based on eXtensible Markup Language (XML). The designers of fuzzy systems with FML have a unified and high-level methodology for describing interoperable fuzzy systems. IEEE STANDARD 1855–2016 uses the W3C XML Schema definition language to define the syntax and semantics of the FML programs.
","The IEEE 1855, the IEEE STANDARD 1855–2016, is about a specification language named Fuzzy Markup Language (FML) developed by the IEEE Standards Association. FML allows modelling a fuzzy logic system in a human-readable and hardware independent way.","[' What is the name of the specification language developed by the IEEE Standards Association?', ' What does FML stand for?', ' Who developed FML?']","['Fuzzy Markup Language', 'Fuzzy Markup Language', 'IEEE Standards Association']"
498,image segmentation,Summary,"In digital image processing and computer vision, image segmentation is the process of partitioning a digital image into multiple image segments, also known as image regions or image objects (sets of pixels). The goal of segmentation is to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze. Image segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images. More precisely, image segmentation is the process of assigning a label to every pixel in an image such that pixels with the same label share certain characteristics.
","In digital image processing and computer vision, image segmentation is the process of partitioning a digital image into multiple image segments, also known as image regions or image objects (sets of pixels). The goal of segmentation is to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze.","[' What is the process of partitioning a digital image into multiple image segments?', ' What are image regions and image objects also known as?', ' What do you change the representation of an image into?']","['image segmentation', 'image segmentation is the process of partitioning a digital image into multiple image segments, also known as image regions or image objects (sets of pixels', 'something that is more meaningful and easier to analyze']"
499,image segmentation,Summary,"The result of image segmentation is a set of segments that collectively cover the entire image, or a set of contours extracted from the image (see edge detection). Each of the pixels in a region are similar with respect to some characteristic or computed property
, such as color, intensity, or texture. Adjacent regions are significantly different color respect to the same characteristic(s). When applied to a stack of images, typical in medical imaging, the resulting contours after image segmentation can be used to create 3D reconstructions with the help of interpolation algorithms like marching cubes.","The result of image segmentation is a set of segments that collectively cover the entire image, or a set of contours extracted from the image (see edge detection). Each of the pixels in a region are similar with respect to some characteristic or computed property
, such as color, intensity, or texture.","[' What is the result of image segmentation?', ' What is a set of segments that collectively cover the entire image?', ' Some characteristic or computed property such as color, intensity, or texture?']","['a set of segments that collectively cover the entire image', 'image segmentation', 'Each of the pixels in a region']"
500,image segmentation,Applications,"Several general-purpose algorithms and techniques have been developed for image segmentation. To be useful, these techniques must typically be combined with a domain's specific knowledge in order to effectively solve the domain's segmentation problems.
","Several general-purpose algorithms and techniques have been developed for image segmentation. To be useful, these techniques must typically be combined with a domain's specific knowledge in order to effectively solve the domain's segmentation problems.","[' Several general-purpose algorithms and techniques have been developed for what?', "" To be useful, techniques must typically be combined with a domain's specific knowledge in order to effectively solve what problem?""]","['image segmentation', 'image segmentation']"
501,image segmentation,Thresholding,"The simplest method of image segmentation is called the thresholding method. This method is based on a clip-level (or a threshold value) to turn a gray-scale image into a binary image.
",The simplest method of image segmentation is called the thresholding method. This method is based on a clip-level (or a threshold value) to turn a gray-scale image into a binary image.,"[' What is the simplest method of image segmentation?', ' What does the thresholding method use to turn a gray-scale image into a binary image?']","['thresholding method', 'a clip-level']"
502,image segmentation,Thresholding,"The key of this method is to select the threshold value (or values when multiple-levels are selected). Several popular methods are used in industry including the maximum entropy method, balanced histogram thresholding, Otsu's method (maximum variance), and k-means clustering.
","The key of this method is to select the threshold value (or values when multiple-levels are selected). Several popular methods are used in industry including the maximum entropy method, balanced histogram thresholding, Otsu's method (maximum variance), and k-means clustering.","[' What is the key of the Otsu method?', ' What are some popular methods used in industry?']","['maximum variance', ""the maximum entropy method, balanced histogram thresholding, Otsu's method (maximum variance), and k-means clustering""]"
503,image segmentation,Thresholding,"Recently, methods have been developed for thresholding computed tomography (CT) images. The key idea is that, unlike Otsu's method, the thresholds are derived from the radiographs instead of the (reconstructed) image.","Recently, methods have been developed for thresholding computed tomography (CT) images. The key idea is that, unlike Otsu's method, the thresholds are derived from the radiographs instead of the (reconstructed) image.","[' What is a term for computed tomography images?', ' What are the thresholds derived from instead of the reconstructed image?']","['CT', 'the radiographs']"
504,image segmentation,Thresholding,New methods suggested the usage of multi-dimensional fuzzy rule-based non-linear thresholds. In these works decision over each pixel's membership to a segment is based on multi-dimensional rules derived from fuzzy logic and evolutionary algorithms based on image lighting environment and application.,New methods suggested the usage of multi-dimensional fuzzy rule-based non-linear thresholds. In these works decision over each pixel's membership to a segment is based on multi-dimensional rules derived from fuzzy logic and evolutionary algorithms based on image lighting environment and application.,"[' What new methods suggested the use of multi-dimensional fuzzy rule-based non-linear thresholds?', "" Decision over each pixel's membership to a segment is based on what?""]","['fuzzy logic and evolutionary algorithms', 'multi-dimensional rules']"
505,image segmentation,Clustering methods,"The K-means algorithm is an iterative technique that is used to partition an image into K clusters. The basic algorithm is
",The K-means algorithm is an iterative technique that is used to partition an image into K clusters. The basic algorithm is,"[' What is the iterative technique used to partition an image into K clusters?', ' What does the K-means algorithm do?']","['K-means algorithm', 'partition an image into K clusters']"
506,image segmentation,Clustering methods,"In this case, distance is the squared or absolute difference between a pixel and a cluster center. The difference is typically based on pixel color, intensity, texture, and location, or a weighted combination of these factors. K can be selected manually, randomly, or by a heuristic. This algorithm is guaranteed to converge, but it may not return the optimal solution. The quality of the solution depends on the initial set of clusters and the value of K.
","In this case, distance is the squared or absolute difference between a pixel and a cluster center. The difference is typically based on pixel color, intensity, texture, and location, or a weighted combination of these factors.","[' What is the squared or absolute difference between a pixel and a cluster center?', ' The difference is typically based on what?']","['distance', 'pixel color, intensity, texture, and location']"
507,image segmentation,Motion and interactive segmentation,"The idea is simple: look at the differences between a pair of images. Assuming the object of interest is moving, the difference will be exactly that object.
","The idea is simple: look at the differences between a pair of images. Assuming the object of interest is moving, the difference will be exactly that object.",[' What is a simple way to look at the differences between two images?'],['The idea']
508,image segmentation,Motion and interactive segmentation,"Improving on this idea, Kenney et al. proposed interactive segmentation [2]. They use a robot to poke objects in order to generate the motion signal necessary for motion-based segmentation.
","Improving on this idea, Kenney et al. proposed interactive segmentation [2].",[' Kenney et al. proposed what type of segmentation?'],['interactive']
509,image segmentation,Compression-based methods,"Compression based methods postulate that the optimal segmentation is the one that minimizes, over all possible segmentations, the coding length of the data. The connection between these two concepts is that segmentation tries to find patterns in an image and any regularity in the image can be used to compress it. The method describes each segment by its texture and boundary shape. Each of these components is modeled by a probability distribution function and its coding length is computed as follows:
","Compression based methods postulate that the optimal segmentation is the one that minimizes, over all possible segmentations, the coding length of the data. The connection between these two concepts is that segmentation tries to find patterns in an image and any regularity in the image can be used to compress it.","[' What do compression based methods postulate?', ' What does segmentation try to find in an image?', ' Any regularity in the image can be used to compress it?']","['the optimal segmentation is the one that minimizes, over all possible segmentations, the coding length of the data', 'patterns', 'segmentation']"
510,image segmentation,Compression-based methods,"For any given segmentation of an image, this scheme yields the number of bits required to encode that image based on the given segmentation. Thus, among all possible segmentations of an image, the goal is to find the segmentation which produces the shortest coding length. This can be achieved by a simple agglomerative clustering method. The distortion in the lossy compression determines the coarseness of the segmentation and its optimal value may differ for each image. This parameter can be estimated heuristically from the contrast of textures in an image. For example, when the textures in an image are similar, such as in camouflage images, stronger sensitivity and thus lower quantization is required.
","For any given segmentation of an image, this scheme yields the number of bits required to encode that image based on the given segmentation. Thus, among all possible segmentations of an image, the goal is to find the segmentation which produces the shortest coding length.","[' What is the goal of a segmentation of an image?', ' How many bits are required to encode an image based on the given segmentation?']","['to find the segmentation which produces the shortest coding length', 'number']"
511,image segmentation,Histogram-based methods,"Histogram-based methods are very efficient compared to other image segmentation methods because they typically require only one pass through the pixels. In this technique, a histogram is computed from all of the pixels in the image, and the peaks and valleys in the histogram are used to locate the clusters in the image. Color or intensity can be used as the measure.
","Histogram-based methods are very efficient compared to other image segmentation methods because they typically require only one pass through the pixels. In this technique, a histogram is computed from all of the pixels in the image, and the peaks and valleys in the histogram are used to locate the clusters in the image.","[' Why are histogram-based methods so efficient compared to other image segmentation methods?', ' What is computed from all of the pixels in the image?', ' How are the peaks and valleys in the Histogram used?', ' What is used to locate clusters in the image?']","['they typically require only one pass through the pixels', 'a histogram', 'to locate the clusters in the image', 'the peaks and valleys in the histogram']"
512,image segmentation,Histogram-based methods,A refinement of this technique is to recursively apply the histogram-seeking method to clusters in the image in order to divide them into smaller clusters. This operation is repeated with smaller and smaller clusters until no more clusters are formed.,A refinement of this technique is to recursively apply the histogram-seeking method to clusters in the image in order to divide them into smaller clusters. This operation is repeated with smaller and smaller clusters until no more clusters are formed.,"[' What is a refinement of the histogram-seeking method?', ' What is repeated with smaller and smaller clusters until no more clusters are formed?']","['to recursively apply the histogram-seeking method to clusters in the image in order to divide them into smaller clusters', 'histogram-seeking method']"
513,image segmentation,Histogram-based methods,"Histogram-based approaches can also be quickly adapted to apply to multiple frames, while maintaining their single pass efficiency. The histogram can be done in multiple fashions when multiple frames are considered. The same approach that is taken with one frame can be applied to multiple, and after the results are merged, peaks and valleys that were previously difficult to identify are more likely to be distinguishable. The histogram can also be applied on a per-pixel basis where the resulting information is used to determine the most frequent color for the pixel location. This approach segments based on active objects and a static environment, resulting in a different type of segmentation useful in video tracking.
","Histogram-based approaches can also be quickly adapted to apply to multiple frames, while maintaining their single pass efficiency. The histogram can be done in multiple fashions when multiple frames are considered.","[' What can be quickly adapted to apply to multiple frames while maintaining their single pass efficiency?', ' How can the histogram be done in multiple fashions when multiple frames are considered?']","['Histogram-based approaches', 'Histogram-based approaches']"
514,image segmentation,Edge detection,"Edge detection is a well-developed field on its own within image processing. Region boundaries and edges are closely related,
since there is often a sharp adjustment in intensity at the region boundaries. Edge detection techniques have therefore been used as the base of another segmentation technique.
","Edge detection is a well-developed field on its own within image processing. Region boundaries and edges are closely related,
since there is often a sharp adjustment in intensity at the region boundaries.","[' Edge detection is a well-developed field on its own within what?', ' Region boundaries and edges are closely related because there is often a sharp adjustment in intensity at the region boundaries?']","['image processing', 'Edge detection']"
515,image segmentation,Edge detection,"The edges identified by edge detection are often disconnected. To segment an object from an image however, one needs closed region boundaries. The desired edges are the boundaries between such objects or spatial-taxons.","The edges identified by edge detection are often disconnected. To segment an object from an image however, one needs closed region boundaries.","[' The edges identified by edge detection are often what?', ' To segment an object from an image, one needs closed region boundaries?']","['disconnected', 'edge detection']"
516,image segmentation,Edge detection,"Spatial-taxons are information granules, consisting of a crisp pixel region, stationed at abstraction levels within a hierarchical nested scene architecture. They are similar to the Gestalt psychological designation of figure-ground, but are extended to include foreground, object groups, objects and salient object parts. Edge detection methods can be applied to the spatial-taxon region, in the same manner they would be applied to a silhouette. This method is particularly useful when the disconnected edge is part of an illusory contour","Spatial-taxons are information granules, consisting of a crisp pixel region, stationed at abstraction levels within a hierarchical nested scene architecture. They are similar to the Gestalt psychological designation of figure-ground, but are extended to include foreground, object groups, objects and salient object parts.","[' What are information granules consisting of?', ' Where are spatial-taxons stationed?', ' What is the Gestalt psychological designation of figure-ground?']","['a crisp pixel region', 'abstraction levels within a hierarchical nested scene architecture', 'Spatial-taxons']"
517,image segmentation,Edge detection,"Segmentation methods can also be applied to edges obtained from edge detectors. Lindeberg and Li developed an integrated method that segments edges into straight and curved edge segments for parts-based object recognition, based on a minimum description length (MDL) criterion that was optimized by a split-and-merge-like method with candidate breakpoints obtained from complementary junction cues to obtain more likely points at which to consider partitions into different segments.
","Segmentation methods can also be applied to edges obtained from edge detectors. Lindeberg and Li developed an integrated method that segments edges into straight and curved edge segments for parts-based object recognition, based on a minimum description length (MDL) criterion that was optimized by a split-and-merge-like method with candidate breakpoints obtained from complementary junction cues to obtain more likely points at which to consider partitions into different segments.","[' How can Segmentation methods be applied to edges obtained from edge detectors?', ' What did Lindeberg and Li develop that segments edges into straight and curved edge segments for parts-based object recognition?', ' What was optimized by a split-and-merge-like method?', ' What were candidate breakpoints obtained from?']","['segments edges into straight and curved edge segments', 'integrated method', 'minimum description length (MDL) criterion', 'complementary junction cues']"
518,image segmentation,Dual clustering method,"This method is a combination of three characteristics of the image: partition of the image based on histogram analysis is checked by high compactness of the clusters (objects), and high gradients of their borders. For that purpose two spaces have to be introduced: one space is the one-dimensional histogram of brightness H = H(B); the second space is the dual 3-dimensional space of the original image itself B = B(x, y). The first space allows to measure how compactly the brightness of the image is distributed by calculating a minimal clustering kmin. Threshold brightness T corresponding to kmin defines the binary (black-and-white) image – bitmap b = φ(x, y), where φ(x, y) = 0, if B(x, y) < T, and φ(x, y) = 1, if B(x, y) ≥ T. The bitmap b is an object in dual space. On that bitmap a measure has to be defined reflecting how compact distributed black (or white) pixels are. So, the goal is to find objects with good borders. For all T the measure MDC = G/(k × L) has to be calculated (where k is difference in brightness between the object and the background, L is length of all borders, and G is mean gradient on the borders). Maximum of MDC defines the segmentation.","This method is a combination of three characteristics of the image: partition of the image based on histogram analysis is checked by high compactness of the clusters (objects), and high gradients of their borders. For that purpose two spaces have to be introduced: one space is the one-dimensional histogram of brightness H = H(B); the second space is the dual 3-dimensional space of the original image itself B = B(x, y).","[' What is checked by high compactness of the clusters and high gradients of their borders?', ' How many spaces have to be introduced for partition of the image based on histogram analysis?', ' What is one space is the one-dimensional Histogram of an image?', ' What is the one-dimensional histogram of brightness H = H(B); the second space is the dual 3-dimensional space of the original image itself B = B(x, y)?']","['partition of the image', 'two', 'brightness H =\xa0H(B);', 'one space']"
519,image segmentation,Region-growing methods,"Region-growing methods rely mainly on the assumption that the neighboring pixels within one region have similar values. The common procedure is to compare one pixel with its neighbors. If a similarity criterion is satisfied, the pixel can be set to belong to the same cluster as one or more of its neighbors. The selection of the similarity criterion is significant and the results are influenced by noise in all instances.
",Region-growing methods rely mainly on the assumption that the neighboring pixels within one region have similar values. The common procedure is to compare one pixel with its neighbors.,"[' What do region-growing methods rely on?', ' What is the common procedure to compare one pixel with its neighbors?']","['the assumption that the neighboring pixels within one region have similar values', 'Region-growing methods']"
520,image segmentation,Region-growing methods,"The method of Statistical Region Merging (SRM) starts by building the graph of pixels using 4-connectedness with edges weighted by the absolute value of the intensity difference. Initially each pixel forms a single pixel region. SRM then sorts those edges in a priority queue and decides whether or not to merge the current regions belonging to the edge pixels using a statistical predicate.
",The method of Statistical Region Merging (SRM) starts by building the graph of pixels using 4-connectedness with edges weighted by the absolute value of the intensity difference. Initially each pixel forms a single pixel region.,"[' What is the method of Statistical Region Merging?', ' How is the graph of pixels built?', ' The edges of the graph are weighted by what?']","['building the graph of pixels using 4-connectedness with edges weighted by the absolute value of the intensity difference', '4-connectedness with edges weighted by the absolute value of the intensity difference', 'absolute value of the intensity difference']"
521,image segmentation,Region-growing methods,"One region-growing method is the seeded region growing method. This method takes a set of seeds as input along with the image. The seeds mark each of the objects to be segmented. The regions are iteratively grown by comparison of all unallocated neighboring pixels to the regions. The difference between a pixel's intensity value and the region's mean, 



δ


{\displaystyle \delta }
, is used as a measure of similarity. The pixel with the smallest difference measured in this way is assigned to the respective region. This process continues until all pixels are assigned to a region. Because seeded region growing requires seeds as additional input, the segmentation results are dependent on the choice of seeds, and noise in the image can cause the seeds to be poorly placed.
",One region-growing method is the seeded region growing method. This method takes a set of seeds as input along with the image.,"[' What is the name of the region growing method?', ' What does the method take as input along with the image?']","['seeded region growing method', 'a set of seeds']"
522,image segmentation,Region-growing methods,"Another region-growing method is the unseeded region growing method. It is a modified algorithm that does not require explicit seeds. It starts with a single region 




A

1




{\displaystyle A_{1}}
—the pixel chosen here does not markedly influence the final segmentation. At each iteration it considers the neighboring pixels in the same way as seeded region growing. It differs from seeded region growing in that if the minimum 



δ


{\displaystyle \delta }
 is less than a predefined threshold 



T


{\displaystyle T}
 then it is added to the respective region 




A

j




{\displaystyle A_{j}}
. If not, then the pixel is considered different from all current regions 




A

i




{\displaystyle A_{i}}
 and a new region 




A

n
+
1




{\displaystyle A_{n+1}}
 is created with this pixel.
",Another region-growing method is the unseeded region growing method. It is a modified algorithm that does not require explicit seeds.,"[' What is another region growing method?', ' What is the modified algorithm that does not require explicit seeds?']","['unseeded region growing method', 'unseeded region growing method']"
523,image segmentation,Region-growing methods,"One variant of this technique, proposed by Haralick and Shapiro (1985), is based on pixel intensities. The mean and scatter of the region and the intensity of the candidate pixel are used to compute a test statistic. If the test statistic is sufficiently small, the pixel is added to the region, and the region’s mean and scatter are recomputed. Otherwise, the pixel is rejected, and is used to form a new region.
","One variant of this technique, proposed by Haralick and Shapiro (1985), is based on pixel intensities. The mean and scatter of the region and the intensity of the candidate pixel are used to compute a test statistic.","[' Haralick and Shapiro proposed a variant of what technique?', ' What are the mean and scatter of the region and the intensity of the candidate pixel used to compute?']","['pixel intensities', 'a test statistic']"
524,image segmentation,Region-growing methods,"A special region-growing method is called 



λ


{\displaystyle \lambda }
-connected segmentation (see also lambda-connectedness). It is based on pixel intensities and neighborhood-linking paths. A degree of connectivity (connectedness) is calculated based on a path that is formed by pixels. For a certain value of 



λ


{\displaystyle \lambda }
, two pixels are called 



λ


{\displaystyle \lambda }
-connected if there is a path linking those two pixels and the connectedness of this path is at least 



λ


{\displaystyle \lambda }
. 



λ


{\displaystyle \lambda }
-connectedness is an equivalence relation.","A special region-growing method is called 



λ


{\displaystyle \lambda }
-connected segmentation (see also lambda-connectedness). It is based on pixel intensities and neighborhood-linking paths.","[' What is a special region growing method called?', ' What is lambda connected segmentation based on?']","['λ\n\n\n{\\displaystyle \\lambda }\n-connected segmentation', 'pixel intensities and neighborhood-linking paths']"
525,image segmentation,Region-growing methods,"Split-and-merge segmentation is based on a quadtree partition of an image. It is sometimes called quadtree segmentation.
",Split-and-merge segmentation is based on a quadtree partition of an image. It is sometimes called quadtree segmentation.,"[' What is split-and-merge segmentation based on?', ' What is it sometimes called?']","['a quadtree partition of an image', 'quadtree segmentation']"
526,image segmentation,Region-growing methods,"This method starts at the root of the tree that represents the whole image. If it is found non-uniform (not homogeneous), then it is split into four child squares (the splitting process), and so on. If, in contrast, four child squares are homogeneous, they are merged as several connected components (the merging process). The node in the tree is a segmented node. This process continues recursively until no further splits or merges are possible. When a special data structure is involved in the implementation of the algorithm of the method, its time complexity can reach 



O
(
n
log
⁡
n
)


{\displaystyle O(n\log n)}
, an optimal algorithm of the method.","This method starts at the root of the tree that represents the whole image. If it is found non-uniform (not homogeneous), then it is split into four child squares (the splitting process), and so on.","[' Where does the splitting process start?', ' What is the name of the method that starts at the root of the tree that represents the whole image?', ' If the tree is found non-uniform, what is the process called?']","['the root of the tree', 'splitting process', 'the splitting process']"
527,image segmentation,Partial differential equation-based methods,"Using a partial differential equation (PDE)-based method and solving the PDE equation by a numerical scheme, one can segment the image. Curve propagation is a popular technique in this category, with numerous applications to object extraction, object tracking, stereo reconstruction, etc. The central idea is to evolve an initial curve towards the lowest potential of a cost function, where its definition reflects the task to be addressed. As for most inverse problems, the minimization of the cost functional is non-trivial and imposes certain smoothness constraints on the solution, which in the present case can be expressed as geometrical constraints on the evolving curve.
","Using a partial differential equation (PDE)-based method and solving the PDE equation by a numerical scheme, one can segment the image. Curve propagation is a popular technique in this category, with numerous applications to object extraction, object tracking, stereo reconstruction, etc.","[' What is PDE?', ' What is a popular technique in this category?', ' Curve propagation has numerous applications to what?']","['partial differential equation', 'Curve propagation', 'object extraction']"
528,image segmentation,Variational methods,"The goal of variational methods is to find a segmentation
which is optimal with respect to a specific energy functional. The functionals consist of a data fitting term and a regularizing terms. A classical representative is the Potts model defined for an image 



f


{\displaystyle f}
 by
","The goal of variational methods is to find a segmentation
which is optimal with respect to a specific energy functional. The functionals consist of a data fitting term and a regularizing terms.","[' The goal of variational methods is to find a segmentation which is optimal with respect to what?', ' What are the functionals consist of?']","['a specific energy functional', 'a data fitting term and a regularizing terms']"
529,image processing,Summary,"
Digital image processing is the use of a digital computer to process digital images through an algorithm. As a subcategory or field of digital signal processing, digital image processing has many advantages over analog image processing. It allows a much wider range of algorithms to be applied to the input data and can avoid problems such as the build-up of noise and distortion during processing. Since images are defined over two dimensions (perhaps more) digital image processing may be modeled in the form of multidimensional systems. The generation and development of digital image processing are mainly affected by three factors: first, the development of computers; second, the development of mathematics (especially the creation and improvement of discrete mathematics theory); third, the demand for a wide range of applications in environment, agriculture, military, industry and medical science has increased.
","
Digital image processing is the use of a digital computer to process digital images through an algorithm. As a subcategory or field of digital signal processing, digital image processing has many advantages over analog image processing.","[' What is the use of a digital computer to process digital images through an algorithm?', ' Digital image processing has many advantages over what?']","['Digital image processing', 'analog image processing']"
530,image processing,History,"Many of the techniques of digital image processing, or digital picture processing as it often was called, were developed in the 1960s, at Bell Laboratories, the Jet Propulsion Laboratory, Massachusetts Institute of Technology, University of Maryland, and a few other research facilities, with application to satellite imagery, wire-photo standards conversion, medical imaging, videophone, character recognition, and photograph enhancement. The purpose of early image processing was to improve the quality of the image. It was aimed for human beings to improve the visual effect of people. In image processing, the input is a low-quality image, and the output is an image with improved quality. Common image processing include image enhancement, restoration, encoding, and compression. The first successful application was the American Jet Propulsion Laboratory (JPL). They used image processing techniques such as geometric correction, gradation transformation, noise removal, etc. on the thousands of lunar photos sent back by the Space Detector Ranger 7 in 1964, taking into account the position of the sun and the environment of the moon. The impact of the successful mapping of the moon's surface map by the computer has been a huge success. Later, more complex image processing was performed on the nearly 100,000 photos sent back by the spacecraft, so that the topographic map, color map and panoramic mosaic of the moon were obtained, which achieved extraordinary results and laid a solid foundation for human landing on the moon.","Many of the techniques of digital image processing, or digital picture processing as it often was called, were developed in the 1960s, at Bell Laboratories, the Jet Propulsion Laboratory, Massachusetts Institute of Technology, University of Maryland, and a few other research facilities, with application to satellite imagery, wire-photo standards conversion, medical imaging, videophone, character recognition, and photograph enhancement. The purpose of early image processing was to improve the quality of the image.","[' When were many of the techniques of digital image processing developed?', ' Bell Laboratories, the Jet Propulsion Laboratory, the Massachusetts Institute of Technology, and the University of Maryland developed techniques of what?', ' What was the purpose of early image processing?']","['1960s', 'digital image processing', 'improve the quality of the image']"
531,image processing,History,"The cost of processing was fairly high, however, with the computing equipment of that era. That changed in the 1970s, when digital image processing proliferated as cheaper computers and dedicated hardware became available. This led to images being processed in real-time, for some dedicated problems such as television standards conversion. As general-purpose computers became faster, they started to take over the role of dedicated hardware for all but the most specialized and computer-intensive operations. With the fast computers and signal processors available in the 2000s, digital image processing has become the most common form of image processing, and is generally used because it is not only the most versatile method, but also the cheapest.
","The cost of processing was fairly high, however, with the computing equipment of that era. That changed in the 1970s, when digital image processing proliferated as cheaper computers and dedicated hardware became available.","[' When did the cost of processing change?', ' When did digital image processing begin to proliferate?']","['1970s', '1970s']"
532,image processing,Fatigue detection and monitoring technologies,"There were significant advancements in fatigue monitoring technology the past decade. These innovative technology solutions are now commercially available and offer real safety benefits to drivers, operators and other shift workers across all industries.","There were significant advancements in fatigue monitoring technology the past decade. These innovative technology solutions are now commercially available and offer real safety benefits to drivers, operators and other shift workers across all industries.","[' What technology has seen significant advancements in the past decade?', ' What are fatigue monitoring solutions now commercially available for?']","['fatigue monitoring', 'real safety benefits to drivers, operators and other shift workers across all industries']"
533,image processing,Fatigue detection and monitoring technologies,"Software developers, engineers and scientists develop fatigue detection software using various physiological cues to determine the state of fatigue or drowsiness. The measurement of brain activity (electroencephalogram) is widely accepted as the standard in fatigue monitoring. Other technology used to determine fatigue related impairment include behavioural symptom measurements such as; eye behaviour, gaze direction, micro-corrections in steering and throttle use as well as heart rate variability.","Software developers, engineers and scientists develop fatigue detection software using various physiological cues to determine the state of fatigue or drowsiness. The measurement of brain activity (electroencephalogram) is widely accepted as the standard in fatigue monitoring.","[' What do software developers, engineers and scientists use to determine the state of fatigue or drowsiness?', ' What is widely accepted as the standard in fatigue monitoring?']","['physiological cues', 'The measurement of brain activity']"
534,internet of things,Summary,"The field has evolved due to the convergence of multiple technologies, including ubiquitous computing, commodity sensors, increasingly powerful embedded systems, and machine learning. Traditional fields of embedded systems, wireless sensor networks, control systems, automation (including home and building automation), independently and collectively enable the Internet of things. In the consumer market, IoT technology is most synonymous with products pertaining to the concept of the ""smart home"", including devices and appliances (such as lighting fixtures, thermostats, home security systems and cameras, and other home appliances) that support one or more common ecosystems, and can be controlled via devices associated with that ecosystem, such as smartphones and smart speakers. The IoT can also be used in healthcare systems.","The field has evolved due to the convergence of multiple technologies, including ubiquitous computing, commodity sensors, increasingly powerful embedded systems, and machine learning. Traditional fields of embedded systems, wireless sensor networks, control systems, automation (including home and building automation), independently and collectively enable the Internet of things.","[' The field of automation has evolved due to the convergence of multiple technologies, including what?', ' Traditional fields of embedded systems, wireless sensor networks, control systems, and automation (including home and building automation) enable the Internet of things, independently and collectively?']","['ubiquitous computing', 'Traditional']"
535,internet of things,History,"The main concept of a network of smart devices was discussed as early as 1982, with a modified Coca-Cola vending machine at Carnegie Mellon University becoming the first ARPANET-connected appliance, able to report its inventory and whether newly loaded drinks were cold or not. Mark Weiser's 1991 paper on ubiquitous computing, ""The Computer of the 21st Century"", as well as academic venues such as UbiComp and PerCom produced the contemporary vision of the IOT. In 1994, Reza Raji described the concept in IEEE Spectrum as ""[moving] small packets of data to a large set of nodes, so as to integrate and automate everything from home appliances to entire factories"". Between 1993 and 1997, several companies proposed solutions like Microsoft's at Work or Novell's NEST. The field gained momentum when Bill Joy envisioned device-to-device communication as a part of his ""Six Webs"" framework, presented at the World Economic Forum at Davos in 1999.","The main concept of a network of smart devices was discussed as early as 1982, with a modified Coca-Cola vending machine at Carnegie Mellon University becoming the first ARPANET-connected appliance, able to report its inventory and whether newly loaded drinks were cold or not. Mark Weiser's 1991 paper on ubiquitous computing, ""The Computer of the 21st Century"", as well as academic venues such as UbiComp and PerCom produced the contemporary vision of the IOT.","[' When was the main concept of a network of smart devices discussed?', ' What was the first ARPANET-connected appliance?', ' Where was a modified Coca-Cola vending machine located?', "" What was the name of Mark Weiser's 1991 paper on ubiquitous computing?"", "" What was Mark Weizer's paper called?"", ' Along with UbiComp and PerCom, what other academic venues produced the contemporary vision of the IOT?']","['1982', 'Coca-Cola vending machine', 'Carnegie Mellon University', 'The Computer of the 21st Century', 'The Computer of the 21st Century', 'Carnegie Mellon University']"
536,internet of things,History,"The concept of the ""Internet of things"" and the term itself, first appeared in a speech by Peter T. Lewis, to the Congressional Black Caucus Foundation 15th Annual Legislative Weekend in Washington, D.C, published in September 1985. According to Lewis, ""The Internet of Things, or IoT, is the integration of people, processes and technology with connectable devices and sensors to enable remote monitoring, status, manipulation and evaluation of trends of such devices.""
","The concept of the ""Internet of things"" and the term itself, first appeared in a speech by Peter T. Lewis, to the Congressional Black Caucus Foundation 15th Annual Legislative Weekend in Washington, D.C, published in September 1985. According to Lewis, ""The Internet of Things, or IoT, is the integration of people, processes and technology with connectable devices and sensors to enable remote monitoring, status, manipulation and evaluation of trends of such devices.""","[' Who first introduced the concept of the Internet of Things?', ' When did Peter T. Lewis give a speech to the Congressional Black Caucus Foundation?', ' Where was the 15th Annual Legislative Weekend held?', ' What does IoT stand for?', ' What is the purpose of the Internet of Things?']","['Peter T. Lewis', 'September 1985', 'Washington, D.C', 'The Internet of Things', 'the integration of people, processes and technology with connectable devices and sensors to enable remote monitoring, status, manipulation and evaluation of trends of such devices']"
537,internet of things,History,"The term ""Internet of things"" was coined independently by Kevin Ashton of Procter & Gamble, later MIT's Auto-ID Center, in 1999, though he prefers the phrase ""Internet for things"". At that point, he viewed radio-frequency identification (RFID) as essential to the Internet of things, which would allow computers to manage all individual things. The main theme of the Internet of things is to embed short-range mobile transceivers in various gadgets and daily necessities to enable new forms of communication between people and things, and between things themselves.","The term ""Internet of things"" was coined independently by Kevin Ashton of Procter & Gamble, later MIT's Auto-ID Center, in 1999, though he prefers the phrase ""Internet for things"". At that point, he viewed radio-frequency identification (RFID) as essential to the Internet of things, which would allow computers to manage all individual things.","[' Who coined the term ""Internet of things""?', "" What was Kevin Ashton's job title?"", ' When was the Internet of things created?', "" Who was the founder of MIT's Auto-ID Center?"", ' What would allow computers to manage all individual things?']","['Kevin Ashton', 'Procter & Gamble', '1999', 'Kevin Ashton', 'radio-frequency identification']"
538,internet of things,Trends and characteristics,"The IoT's major significant trend in recent years is the explosive growth of devices connected and controlled by the Internet. The wide range of applications for IoT technology mean that the specifics can be very different from one device to the next but there are basic characteristics shared by most.
",The IoT's major significant trend in recent years is the explosive growth of devices connected and controlled by the Internet. The wide range of applications for IoT technology mean that the specifics can be very different from one device to the next but there are basic characteristics shared by most.,"[' What is the major trend in the IoT in recent years?', ' What are devices connected and controlled by the Internet connected to?', ' The wide range of applications for what technology mean that the specifics can be very different?']","['explosive growth of devices connected and controlled by the Internet', 'The IoT', 'IoT']"
539,internet of things,Trends and characteristics,The number of IoT devices increased 31% year-over-year to 8.4 billion in the year 2017 and it is estimated that there will be 30 billion devices by 2020. The global market value of the IoT is projected to reach $7.1 trillion by 2020.,The number of IoT devices increased 31% year-over-year to 8.4 billion in the year 2017 and it is estimated that there will be 30 billion devices by 2020. The global market value of the IoT is projected to reach $7.1 trillion by 2020.,"[' How much did the number of IoT devices increase in 2017?', ' How many devices are estimated to be there by 2020?', ' What is the global market value of the Internet of Things?']","['31%', '30 billion', '$7.1 trillion']"
540,internet of things,Enabling technologies for IoT,"There are many technologies that enable the IoT. Crucial to the field is the network used to communicate between devices of an IoT installation, a role that several wireless or wired technologies may fulfill:","There are many technologies that enable the IoT. Crucial to the field is the network used to communicate between devices of an IoT installation, a role that several wireless or wired technologies may fulfill:","[' What is crucial to the field of IoT?', ' What is the role of the network used to communicate between devices?']","['the network used to communicate between devices of an IoT installation', 'Crucial']"
541,internet of things,Politics and civic engagement,"Some scholars and activists argue that the IoT can be used to create new models of civic engagement if device networks can be open to user control and inter-operable platforms. Philip N. Howard, a professor and author, writes that political life in both democracies and authoritarian regimes will be shaped by the way the IoT will be used for civic engagement. For that to happen, he argues that any connected device should be able to divulge a list of the ""ultimate beneficiaries"" of its sensor data and that individual citizens should be able to add new organisations to the beneficiary list. In addition, he argues that civil society groups need to start developing their IoT strategy for making use of data and engaging with the public.","Some scholars and activists argue that the IoT can be used to create new models of civic engagement if device networks can be open to user control and inter-operable platforms. Philip N. Howard, a professor and author, writes that political life in both democracies and authoritarian regimes will be shaped by the way the IoT will be used for civic engagement.","[' What can be used to create new models of civic engagement?', ' Who is a professor and author?', ' What will political life in both democracies and authoritarian regimes be shaped by?', ' What will the IoT be used for in democracies and authoritarian regimes?']","['the IoT', 'Philip N. Howard', 'the way the IoT will be used for civic engagement', 'civic engagement']"
542,internet of things,Government regulation on IoT,"One of the key drivers of the IoT is data. The success of the idea of connecting devices to make them more efficient is dependent upon access to and storage & processing of data. For this purpose, companies working on the IoT collect data from multiple sources and store it in their cloud network for further processing. This leaves the door wide open for privacy and security dangers and single point vulnerability of multiple systems. The other issues pertain to consumer choice and ownership of data and how it is used. Though still in their infancy, regulations and governance regarding these issues of privacy, security, and data ownership continue to develop. IoT regulation depends on the country. Some examples of legislation that is relevant to privacy and data collection are: the US Privacy Act of 1974, OECD Guidelines on the Protection of Privacy and Transborder Flows of Personal Data of 1980, and the EU Directive 95/46/EC of 1995.",One of the key drivers of the IoT is data. The success of the idea of connecting devices to make them more efficient is dependent upon access to and storage & processing of data.,"[' What is one of the key drivers of the IoT?', ' What is the success of the idea of connecting devices to make them more efficient?']","['data', 'dependent upon access to and storage & processing of data']"
543,internet of things,Government regulation on IoT,"However, the FTC stopped at just making recommendations for now. According to an FTC analysis, the existing framework, consisting of the FTC Act, the Fair Credit Reporting Act, and the Children's Online Privacy Protection Act, along with developing consumer education and business guidance, participation in multi-stakeholder efforts and advocacy to other agencies at the federal, state and local level, is sufficient to protect consumer rights.","However, the FTC stopped at just making recommendations for now. According to an FTC analysis, the existing framework, consisting of the FTC Act, the Fair Credit Reporting Act, and the Children's Online Privacy Protection Act, along with developing consumer education and business guidance, participation in multi-stakeholder efforts and advocacy to other agencies at the federal, state and local level, is sufficient to protect consumer rights.","[' What did the FTC stop making for now?', ' What is the framework consisting of?', "" The FTC Act, the Fair Credit Reporting Act, and the Children's Online Privacy Protection Act are examples of what?"", ' What does participation in multi-stakeholder efforts and advocacy to other agencies at the federal, state and local level do?']","['recommendations', ""the FTC Act, the Fair Credit Reporting Act, and the Children's Online Privacy Protection Act"", 'existing framework', 'sufficient to protect consumer rights']"
544,internet of things,Government regulation on IoT,"A resolution passed by the Senate in March 2015, is already being considered by the Congress. This resolution recognized the need for formulating a National Policy on IoT and the matter of privacy, security and spectrum. Furthermore, to provide an impetus to the IoT ecosystem, in March 2016, a bipartisan group of four Senators proposed a bill, The Developing Innovation and Growing the Internet of Things (DIGIT) Act, to direct the Federal Communications Commission to assess the need for more spectrum to connect IoT devices.
","A resolution passed by the Senate in March 2015, is already being considered by the Congress. This resolution recognized the need for formulating a National Policy on IoT and the matter of privacy, security and spectrum.","[' In what year was a resolution passed by the Senate?', ' What was the resolution that recognized the need for formulating a National Policy on IoT?']","['2015', 'A resolution passed by the Senate']"
545,internet of things,Government regulation on IoT,"Approved on 28 September 2018, Senate Bill No. 327 goes into effect on 1 January 2020. The bill requires ""a manufacturer of a connected device, as those terms are defined, to equip the device with a reasonable security feature or features that are appropriate to the nature and function of the device, appropriate to the information it may collect, contain, or transmit, and designed to protect the device and any information contained therein from unauthorized access, destruction, use, modification, or disclosure,""
","Approved on 28 September 2018, Senate Bill No. 327 goes into effect on 1 January 2020.","[' On what date was Senate Bill No. 327 approved?', ' What date does Senate Bill 327 go into effect?']","['28 September 2018', '1 January 2020']"
546,internet of things,Government regulation on IoT,"Several standards for the IoT industry are actually being established relating to automobiles because most concerns arising from use of connected cars apply to healthcare devices as well. In fact, the National Highway Traffic Safety Administration (NHTSA) is preparing cybersecurity guidelines and a database of best practices to make automotive computer systems more secure.","Several standards for the IoT industry are actually being established relating to automobiles because most concerns arising from use of connected cars apply to healthcare devices as well. In fact, the National Highway Traffic Safety Administration (NHTSA) is preparing cybersecurity guidelines and a database of best practices to make automotive computer systems more secure.","[' What are some standards for the IoT industry being established relating to?', ' What do most concerns arising from use of connected cars apply to as well?', ' Who is preparing cybersecurity guidelines and a database of best practices?', ' What is the goal of a database of best practices to make automotive computer systems more secure?']","['automobiles', 'healthcare devices', 'National Highway Traffic Safety Administration (NHTSA)', 'cybersecurity guidelines']"
547,internet of things,Government regulation on IoT,"A recent report from the World Bank examines the challenges and opportunities in government adoption of IoT. These include –
",A recent report from the World Bank examines the challenges and opportunities in government adoption of IoT. These include –,"[' What does a recent report from the World Bank examine?', ' What are the challenges and opportunities in government adoption of IoT?']","['the challenges and opportunities in government adoption of IoT', '–']"
548,internet of things,Government regulation on IoT,"In early December 2021, the U.K. government introduced the Product Security and Telecommunications Infrastructure bill (PST), an effort to legislate IoT distributors, manufacturers, and importers to meet certain cybersecurity standards. The bill also seeks to improve the security credentials of consumer IoT devices.","In early December 2021, the U.K. government introduced the Product Security and Telecommunications Infrastructure bill (PST), an effort to legislate IoT distributors, manufacturers, and importers to meet certain cybersecurity standards. The bill also seeks to improve the security credentials of consumer IoT devices.","[' When did the U.K. introduce the Product Security and Telecommunications Infrastructure bill?', ' What is the product security and telecommunications infrastructure bill aimed at?']","['early December 2021', 'legislate IoT distributors, manufacturers, and importers to meet certain cybersecurity standards']"
549,convolutional neural network,Summary,"In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of artificial neural network, most commonly applied to analyze visual imagery. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation equivariant responses known as feature maps. Counter-intuitively, most convolutional neural networks are only equivariant, as opposed to invariant, to translation. They have applications in image and video recognition, recommender systems, image classification, image segmentation, medical image analysis, natural language processing, brain-computer interfaces, and financial time series.","In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of artificial neural network, most commonly applied to analyze visual imagery. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation equivariant responses known as feature maps.","[' What is another name for a convolutional neural network?', ' What is the most common use of a ConvNet?', ' SIANN is based on what?', ' What is the shared-weight architecture of the convolution kernels or filters?', ' What are feature maps?']","['ConvNet', 'analyze visual imagery', 'shared-weight architecture', 'slide along input features and provide translation equivariant responses known as feature maps', 'translation equivariant responses']"
550,convolutional neural network,Summary,"CNNs are regularized versions of multilayer perceptrons. Multilayer perceptrons usually mean fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The ""full connectivity"" of these networks make them prone to overfitting data. Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout, etc.) CNNs take a different approach towards regularization: they take advantage of the hierarchical pattern in data and assemble patterns of increasing complexity using smaller and simpler patterns embossed in their filters. Therefore, on a scale of connectivity and complexity, CNNs are on the lower extreme.
","CNNs are regularized versions of multilayer perceptrons. Multilayer perceptrons usually mean fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer.","[' CNNs are regularized versions of what?', ' Multilayer perceptrons usually mean what kind of networks?', ' Each neuron in one layer is connected to all neurons in the next?']","['multilayer perceptrons', 'fully connected networks', 'Multilayer perceptrons usually mean fully connected networks']"
551,convolutional neural network,Summary,"Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.
",Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field.,"[' What was inspired by biological processes?', ' The connectivity pattern between neurons resembles the organization of what?', ' Individual cortical neurons respond to stimuli only in what region of the visual field?']","['Convolutional networks', 'the animal visual cortex', 'receptive field']"
552,convolutional neural network,Summary,"CNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns to optimize the filters (or kernels) through automated learning, whereas in traditional algorithms these filters are hand-engineered. This independence from prior knowledge and human intervention in feature extraction is a major advantage.
","CNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns to optimize the filters (or kernels) through automated learning, whereas in traditional algorithms these filters are hand-engineered.","[' How much pre-processing do CNNs use compared to other image classification algorithms?', ' How does the network learn to optimize the filters?', ' What are traditional algorithms hand-engineered?']","['relatively little', 'automated learning', 'filters']"
553,convolutional neural network,Definition,"The name ""convolutional neural network"" indicates that the network employs a mathematical operation called convolution.
Convolutional networks are a specialized type of neural networks that use convolution in place of general matrix multiplication in at least one of their layers.","The name ""convolutional neural network"" indicates that the network employs a mathematical operation called convolution. Convolutional networks are a specialized type of neural networks that use convolution in place of general matrix multiplication in at least one of their layers.","[' What does the name ""convolutional neural network"" indicate?', ' What is a specialized type of neural networks that use convolution instead of general matrix multiplication?']","['the network employs a mathematical operation called convolution', 'Convolutional networks']"
554,convolutional neural network,Architecture,"A convolutional neural network consists of an input layer, hidden layers and an output layer. In any feed-forward neural network, any middle layers are called hidden because their inputs and outputs are masked by the activation function and final convolution. In a convolutional neural network, the hidden layers include layers that perform convolutions. Typically this includes a layer that performs a dot product of the convolution kernel with the layer's input matrix. This product is usually the Frobenius inner product, and its activation function is commonly ReLU. As the convolution kernel slides along the input matrix for the layer, the convolution operation generates a feature map, which in turn contributes to the input of the next layer. This is followed by other layers such as pooling layers, fully connected layers, and normalization layers.
","A convolutional neural network consists of an input layer, hidden layers and an output layer. In any feed-forward neural network, any middle layers are called hidden because their inputs and outputs are masked by the activation function and final convolution.","[' What is a convolutional neural network composed of?', ' What are the middle layers of a feed-forward neural network called?']","['an input layer, hidden layers and an output layer', 'hidden']"
555,convolutional neural network,Distinguishing features,"In the past, traditional multilayer perceptron (MLP) models were used for image recognition. However, the full connectivity between nodes caused the curse of dimensionality, and was computationally intractable with higher resolution images. A 1000×1000-pixel image with RGB color channels has 3 million weights, which is too high to feasibly process efficiently at scale with full connectivity.
","In the past, traditional multilayer perceptron (MLP) models were used for image recognition. However, the full connectivity between nodes caused the curse of dimensionality, and was computationally intractable with higher resolution images.","[' What was used for image recognition in the past?', ' What caused the curse of dimensionality?', ' How was the full connectivity between nodes computationally intractable?']","['traditional multilayer perceptron (MLP) models', 'full connectivity between nodes', 'with higher resolution images']"
556,convolutional neural network,Distinguishing features,"For example, in CIFAR-10, images are only of size 32×32×3 (32 wide, 32 high, 3 color channels), so a single fully connected neuron in the first hidden layer of a regular neural network would have 32*32*3 = 3,072 weights. A 200×200 image, however, would lead to neurons that have 200*200*3 = 120,000 weights.
","For example, in CIFAR-10, images are only of size 32×32×3 (32 wide, 32 high, 3 color channels), so a single fully connected neuron in the first hidden layer of a regular neural network would have 32*32*3 = 3,072 weights. A 200×200 image, however, would lead to neurons that have 200*200*3 = 120,000 weights.","[' How many weights would a single fully connected neuron in the first hidden layer of a regular neural network have?', ' A 200<unk>200 image would lead to neurons that have what?', ' What image would lead to neurons that have 200*200*3 = 120,000 weights?']","['3,072', '200*200*3 = 120,000 weights', '200×200']"
557,convolutional neural network,Distinguishing features,"Also, such network architecture does not take into account the spatial structure of data, treating input pixels which are far apart in the same way as pixels that are close together. This ignores locality of reference in data with a grid-topology (such as images), both computationally and semantically. Thus, full connectivity of neurons is wasteful for purposes such as image recognition that are dominated by spatially local input patterns.
","Also, such network architecture does not take into account the spatial structure of data, treating input pixels which are far apart in the same way as pixels that are close together. This ignores locality of reference in data with a grid-topology (such as images), both computationally and semantically.","[' What does network architecture not take into account?', ' What is ignored in data with a grid-topology?']","['the spatial structure of data', 'locality of reference']"
558,convolutional neural network,Distinguishing features,"Convolutional neural networks are variants of multilayer perceptrons, designed to emulate the behavior of a visual cortex. These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images. As opposed to MLPs, CNNs have the following distinguishing features:
","Convolutional neural networks are variants of multilayer perceptrons, designed to emulate the behavior of a visual cortex. These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images.","[' What are convolutional neural networks designed to emulate?', ' What are these models designed to mitigate the challenges posed by the MLP architecture?']","['the behavior of a visual cortex', 'Convolutional neural networks']"
559,convolutional neural network,Distinguishing features,"Together, these properties allow CNNs to achieve better generalization on vision problems. Weight sharing dramatically reduces the number of free parameters learned, thus lowering the memory requirements for running the network and allowing the training of larger, more powerful networks.
","Together, these properties allow CNNs to achieve better generalization on vision problems. Weight sharing dramatically reduces the number of free parameters learned, thus lowering the memory requirements for running the network and allowing the training of larger, more powerful networks.","[' What do weight sharing properties allow CNNs to achieve?', ' What reduces the number of free parameters learned?', ' How does weight sharing help CNNs?']","['better generalization on vision problems', 'Weight sharing', 'dramatically reduces the number of free parameters learned']"
560,convolutional neural network,Building blocks,"
A CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function. A few distinct types of layers are commonly used. These are further discussed below.","
A CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function.","[' What is formed by a stack of distinct layers that transform the input volume into an output volume?', ' What does a CNN architecture hold?']","['CNN architecture', 'class scores']"
561,convolutional neural network,Hyperparameters,"Hyperparameters are various settings that are used to control the learning process. CNNs use more hyperparameters than a standard multilayer perceptron (MLP).
",Hyperparameters are various settings that are used to control the learning process. CNNs use more hyperparameters than a standard multilayer perceptron (MLP).,"[' What are various settings that are used to control the learning process?', ' What do CNNs use more than a standard multilayer perceptron?']","['Hyperparameters', 'Hyperparameters are various settings that are used to control the learning process. CNNs use more hyperparameters']"
562,convolutional neural network,Translation Equivariance and Aliasing,"It is commonly assumed that CNNs are invariant to shifts of the input. Convolution or pooling layers within a CNN that do not have a stride greater than one are indeed equivariant to translations of the input. However, layers with a stride greater than one ignore the Nyquist-Shannon sampling theorem and might lead to aliasing of the input signal While, in principle, CNNs are capable of implementing anti-aliasing filters, it has been observed that this does not happens in practice  and yield models that are not equivariant to translations.
Furthermore, if a CNN makes use of fully connected layers, translation equivariance does not imply translation invariance, as the fully connected layers are not invariant to shifts of the input. One solution for complete translation invariance is avoiding any down-sampling throughout the network and applying global average pooling at the last layer. Additionally, several other partial solutions have been proposed, such as anti-aliasing before downsampling operations, spatial transformer networks, data augmentation, subsampling combined with pooling, and capsule neural networks.",It is commonly assumed that CNNs are invariant to shifts of the input. Convolution or pooling layers within a CNN that do not have a stride greater than one are indeed equivariant to translations of the input.,"[' What is commonly assumed that CNNs are invariant to shifts of the input?', ' Convolution or pooling layers within a CNN that do not have a stride greater than one are indeed what?']","['Convolution or pooling layers within a CNN that do not have a stride greater than one are indeed equivariant to translations', 'equivariant to translations of the input']"
563,convolutional neural network,Evaluation,"The accuracy of the final model based on a sub-part of the dataset set apart at the start, often called a test-set. Other times methods such as k-fold cross-validation are applied. Other strategies include using conformal prediction.","The accuracy of the final model based on a sub-part of the dataset set apart at the start, often called a test-set. Other times methods such as k-fold cross-validation are applied.","[' What is the term for the accuracy of the final model based on a sub-part of a dataset set apart at the start?', ' What is a test-set often called?', ' Other times methods such as k-fold cross-validation are applied?']","['a test-set', 'The accuracy of the final model based on a sub-part of the dataset set apart at the start', 'The accuracy of the final model based on a sub-part of the dataset set apart at the start']"
564,convolutional neural network,Regularization methods,"Regularization is a process of introducing additional information to solve an ill-posed problem or to prevent overfitting. CNNs use various types of regularization.
",Regularization is a process of introducing additional information to solve an ill-posed problem or to prevent overfitting. CNNs use various types of regularization.,"[' What is regularization a process of introducing?', ' What does regularization do?']","['additional information', 'introducing additional information to solve an ill-posed problem or to prevent overfitting']"
565,convolutional neural network,Hierarchical coordinate frames,"Pooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition. Overlapping the pools so that each feature occurs in multiple pools, helps retain the information. Translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint, such as a different orientation or scale. On the other hand, people are very good at extrapolating; after seeing a new shape once they can recognize it from a different viewpoint.",Pooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition.,"[' Pooling loses the precise spatial relationships between high-level parts, such as nose and mouth in a face image?']",['Pooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition']
566,convolutional neural network,Hierarchical coordinate frames,"An earlier common way to deal with this problem is to train the network on transformed data in different orientations, scales, lighting, etc. so that the network can cope with these variations. This is computationally intensive for large data-sets. The alternative is to use a hierarchy of coordinate frames and use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the retina. The pose relative to the retina is the relationship between the coordinate frame of the retina and the intrinsic features' coordinate frame.","An earlier common way to deal with this problem is to train the network on transformed data in different orientations, scales, lighting, etc. so that the network can cope with these variations.","[' What was an earlier common way to deal with this problem?', ' What is a way to train the network on transformed data?']","['to train the network on transformed data', 'different orientations, scales, lighting, etc.']"
567,convolutional neural network,Hierarchical coordinate frames,"Thus, one way to represent something is to embed the coordinate frame within it. This allows large features to be recognized by using the consistency of the poses of their parts (e.g. nose and mouth poses make a consistent prediction of the pose of the whole face). This approach ensures that the higher-level entity (e.g. face) is present when the lower-level (e.g. nose and mouth) agree on its prediction of the pose. The vectors of neuronal activity that represent pose (""pose vectors"") allow spatial transformations modeled as linear operations that make it easier for the network to learn the hierarchy of visual entities and generalize across viewpoints. This is similar to the way the human visual system imposes coordinate frames in order to represent shapes.","Thus, one way to represent something is to embed the coordinate frame within it. This allows large features to be recognized by using the consistency of the poses of their parts (e.g.","[' What is one way to represent something?', ' What allows large features to be recognized?']","['to embed the coordinate frame within it', 'consistency of the poses of their parts']"
568,convolutional neural network,Fine-tuning,"For many applications, the training data is less available. Convolutional neural networks usually require a large amount of training data in order to avoid overfitting. A common technique is to train the network on a larger data set from a related domain. Once the network parameters have converged an additional training step is performed using the in-domain data to fine-tune the network weights, this is known as transfer learning. Furthermore, this technique allows convolutional network architectures to successfully be applied to problems with tiny training sets.","For many applications, the training data is less available. Convolutional neural networks usually require a large amount of training data in order to avoid overfitting.","[' For many applications, what is less available?', ' Convolutional neural networks usually require a large amount of what?']","['training data', 'training data']"
569,convolutional neural network,Human interpretable explanations,"End-to-end training and prediction are common practice in computer vision. However, human interpretable explanations are required for critical systems such as a self-driving cars. With recent advances in visual salience, spatial and temporal attention, the most critical spatial regions/temporal instants could be visualized to justify the CNN predictions.","End-to-end training and prediction are common practice in computer vision. However, human interpretable explanations are required for critical systems such as a self-driving cars.","[' What is common practice in computer vision?', ' What is required for critical systems such as self-driving cars?']","['End-to-end training and prediction', 'human interpretable explanations']"
570,temporal logic,Summary,"In logic, temporal logic is any system of rules and symbolism for representing, and reasoning about, propositions qualified in terms of time (for example, ""I am always hungry"", ""I will eventually be hungry"", or ""I will be hungry until I eat something""). It is sometimes also used to refer to tense logic, a modal logic-based system of temporal logic introduced by Arthur Prior in the late 1950s, with important contributions by Hans Kamp. It has been further developed by computer scientists, notably Amir Pnueli, and logicians.
","In logic, temporal logic is any system of rules and symbolism for representing, and reasoning about, propositions qualified in terms of time (for example, ""I am always hungry"", ""I will eventually be hungry"", or ""I will be hungry until I eat something""). It is sometimes also used to refer to tense logic, a modal logic-based system of temporal logic introduced by Arthur Prior in the late 1950s, with important contributions by Hans Kamp.","[' What is a system of rules and symbolism for representing and reasoning about propositions qualified in terms of time?', ' What is temporal logic sometimes used to refer to?', ' What is a modal logic-based system of temporal logic introduced by Arthur Prior in the late 1950s?', ' Who contributed important contributions to tense logic?']","['temporal logic', 'tense logic', 'tense logic', 'Hans Kamp']"
571,temporal logic,Summary,"Temporal logic has found an important application in formal verification, where it is used to state requirements of hardware or software systems. For instance, one may wish to say that whenever a request is made, access to a resource is eventually granted, but it is never granted to two requestors simultaneously. Such a statement can conveniently be expressed in a temporal logic.
","Temporal logic has found an important application in formal verification, where it is used to state requirements of hardware or software systems. For instance, one may wish to say that whenever a request is made, access to a resource is eventually granted, but it is never granted to two requestors simultaneously.","[' Temporal logic has found an important application in what?', ' What is used to state requirements of hardware or software systems?', ' How many requestors are never granted access to a resource?', ' How many requestors can a grant be granted to?']","['formal verification', 'Temporal logic', 'two', 'two']"
572,temporal logic,Motivation,"Consider the statement ""I am hungry"". Though its meaning is constant in time, the statement's truth value can vary in time. Sometimes it is true, and sometimes false, but never simultaneously true and false. In a temporal logic, a statement can have a truth value that varies in time—in contrast with an atemporal logic, which applies only to statements whose truth values are constant in time. This treatment of truth value over time differentiates temporal logic from computational verb logic.
","Consider the statement ""I am hungry"". Though its meaning is constant in time, the statement's truth value can vary in time.","[' What is the meaning of the statement ""I am hungry""?', "" What can the statement's truth value vary in time?""]","['constant in time', 'I am hungry']"
573,temporal logic,Motivation,"Temporal logic always has the ability to reason about a timeline. So-called linear ""time logics"" are restricted to this type of reasoning. Branching logics, however, can reason about multiple timelines. This presupposes an environment that may act unpredictably.
To continue the example, in a branching logic we may state that ""there is a possibility that I will stay hungry forever"", and that ""there is a possibility that eventually I am no longer hungry"". If we do not know whether or not I will ever be fed, these statements can both be true.
","Temporal logic always has the ability to reason about a timeline. So-called linear ""time logics"" are restricted to this type of reasoning.","[' What type of logic always has the ability to reason about a timeline?', ' What is restricted to this type of reasoning?']","['Temporal', 'linear ""time logics']"
574,temporal logic,History,"Although Aristotle's logic is almost entirely concerned with the theory of the categorical syllogism, there are passages in his work that are now seen as anticipations of temporal logic, and may imply an early, partially developed form of first-order temporal modal binary logic. Aristotle was particularly concerned with the problem of future contingents, where he could not accept that the principle of bivalence applies to statements about future events, i.e. that we can presently decide if a statement about a future event is true or false, such as ""there will be a sea battle tomorrow"".","Although Aristotle's logic is almost entirely concerned with the theory of the categorical syllogism, there are passages in his work that are now seen as anticipations of temporal logic, and may imply an early, partially developed form of first-order temporal modal binary logic. Aristotle was particularly concerned with the problem of future contingents, where he could not accept that the principle of bivalence applies to statements about future events, i.e.","["" Aristotle's logic is almost entirely concerned with the theory of what?"", ' What are passages in his work that are now seen as anticipations of temporal logic?', ' Aristotle was particularly concerned with the problem of future contingents.', ' What principle did he not accept to apply to statements about future events?']","['categorical syllogism', 'Aristotle', ""Aristotle's logic is almost entirely concerned with the theory of the categorical syllogism, there are passages in his work that are now seen as anticipations of temporal logic, and may imply an early, partially developed form of first-order temporal modal binary logic. Aristotle was particularly concerned with the problem of future contingents, where he could not accept that the principle of bivalence applies to statements about future events"", 'bivalence']"
575,temporal logic,History,Time has usually been considered by logicians to be what is called 'extralogical' matter. I have never shared this opinion. But I have thought that logic had not yet reached the state of development at which the introduction of temporal modifications of its forms would not result in great confusion; and I am much of that way of thinking yet.,Time has usually been considered by logicians to be what is called 'extralogical' matter. I have never shared this opinion.,"[' What is the term used by logicians to describe time?', ' What is time considered to be?']","[""extralogical' matter"", ""extralogical' matter""]"
576,temporal logic,History,"Surprisingly for Peirce, the first system of temporal logic was constructed, as far as we know, in the first half of 20th century. Although Arthur Prior is widely known as a founder of temporal logic, the first formalization of such logic was provided in 1947 by Polish logician, Jerzy Łoś. In his work Podstawy Analizy Metodologicznej Kanonów Milla (The Foundations of a Methodological Analysis of Mill’s Methods) he presented a formalization of Mill's canons. In Łoś.' approach emphasis was placed on the time factor. Thus, to reach his goal, he had to create a logic which could provide means for formalization of temporal functions. The logic could be seen as a byproduct of Łoś' main aim, albeit it was the first positional logic which, as a framework was used later for Łoś' inventions in epistemic logic. The logic itself has syntax very different than Prior's tense logic, which uses modal operators. Language of Łoś' logic rather uses a specific for positional logic, realization operator that binds the expression with the specific context in which its truth-value is considered. In Łoś' work this considered context was only temporal, thus expressions were binded with specific moments or intervals of time.
","Surprisingly for Peirce, the first system of temporal logic was constructed, as far as we know, in the first half of 20th century. Although Arthur Prior is widely known as a founder of temporal logic, the first formalization of such logic was provided in 1947 by Polish logician, Jerzy Łoś.","[' In what century was the first system of temporal logic constructed?', ' Who is widely known as a founder of temporal logic?', ' In what year did Jerzy <unk>o<unk> provide the first formalization of the temporality logic system?', ' What is the name of the Polish logician who first formalized temporal logical systems?']","['20th', 'Arthur Prior', '1947', 'Jerzy Łoś']"
577,temporal logic,History,"In the following years, research of temporal logic by Arthur Prior began. He was concerned with the philosophical implications of free will and predestination. According to his wife, he first considered formalizing temporal logic in 1953. Results of his research were firstly presented at the conference in Wellington in 1954. The system Prior presented, was similar syntactically to Łoś' logic, although not until 1955 he explicitly referred to Łoś' work, in last section of Appendix 1 in Prior’s Formal Logic.","In the following years, research of temporal logic by Arthur Prior began. He was concerned with the philosophical implications of free will and predestination.","[' Who began researching temporal logic?', ' Arthur Prior was concerned with the philosophical implications of what?']","['Arthur Prior', 'free will and predestination']"
578,temporal logic,History,"Prior gave lectures on the topic at the University of Oxford in 1955–6, and in 1957 published a book, Time and Modality, in which he introduced a propositional modal logic with two temporal connectives (modal operators), F and P, corresponding to ""sometime in the future"" and ""sometime in the past"". In this early work, Prior considered time to be linear. In 1958 however, he received a letter from Saul Kripke, who pointed out that this assumption is perhaps unwarranted. In a development that foreshadowed a similar one in computer science, Prior took this under advisement, and developed two theories of branching time, which he called ""Ockhamist"" and ""Peircean"". Between 1958 and 1965 Prior also corresponded with Charles Leonard Hamblin, and a number of early developments in the field can be traced to this correspondence, for example Hamblin implications. Prior published his most mature work on the topic, the book Past, Present, and Future in 1967. He died two years later.","Prior gave lectures on the topic at the University of Oxford in 1955–6, and in 1957 published a book, Time and Modality, in which he introduced a propositional modal logic with two temporal connectives (modal operators), F and P, corresponding to ""sometime in the future"" and ""sometime in the past"". In this early work, Prior considered time to be linear.","[' When did Prior give lectures on Time and Modality?', "" What was Prior's book called?"", ' How many temporal connectives did Prior introduce?', ' Prior gave lectures at what university?', ' What did Prior consider time to be in his early work?']","['1955–', 'Time and Modality', 'two', 'University of Oxford', 'linear']"
579,temporal logic,History,"Along with tense logic, Prior constructed few systems of positional logic, which inherited main ideas from Łoś. Work in positional temporal logics was continued by Nicholas Rescher in 60's and 70's. In such works as Note  on  Chronological  Logic (1966), On  the  Logic  of  Chronological  Propositions (1968), Topological Logic (1968), Temporal Logic (1971) he researched connections between Łoś' and Prior's systems. Moreover he proved that Prior's tense operators could be defined using realization operator in specific positional logics. Rescher in his work also created more general systems of positional logics. Althought the first ones were constructed for purely temporal uses, he proposed a term of topological logics that were meant to contain a realization operator but had no specific temporal axioms - like the clock axiom.
","Along with tense logic, Prior constructed few systems of positional logic, which inherited main ideas from Łoś. Work in positional temporal logics was continued by Nicholas Rescher in 60's and 70's.","["" Who continued work in positional temporal logics in the 60's and 70's?""]",['Nicholas Rescher']
580,temporal logic,History,"Two early contenders in formal verifications were linear temporal logic, a linear time logic by Amir Pnueli, and computation tree logic, a branching time logic by Mordechai Ben-Ari, Zohar Manna and Amir Pnueli. An almost equivalent formalism to CTL was suggested around the same time by E. M. Clarke and E. A. Emerson. The fact that the second logic can be decided more efficiently than the first does not reflect on branching and linear logics in general, as has sometimes been argued. Rather, Emerson and Lei show that any linear logic can be extended to a branching logic that can be decided with the same complexity.
","Two early contenders in formal verifications were linear temporal logic, a linear time logic by Amir Pnueli, and computation tree logic, a branching time logic by Mordechai Ben-Ari, Zohar Manna and Amir Pnueli. An almost equivalent formalism to CTL was suggested around the same time by E. M. Clarke and E. A. Emerson.","["" Amir Pnueli's linear time logic was based on what?"", "" Amir Ben-Ari and Zohar Manna's computation tree logic is based off of what logic?"", ' Who suggested a formalism to CTL?', ' E. M. Clarke and E. A. Emerson suggested around what time?']","['computation tree logic', 'branching time logic', 'E. M. Clarke and E. A. Emerson', 'the same time']"
581,temporal logic,Łoś' positional logic,"Łoś’s logic was published as his master’s thesis was entitled in 1947 in Polish. His philosophical and formal concepts could be seen as continuation of Lviv-Warsaw School of Logic as his supervisor was Jerzy Słupecki, disciple of Jan Łukasiewicz. The paper was not translated into English until 1977, although Henryk Hiż presented in 1951 a brief, but informative review in the Journal of Symbolic Logic. His review contained core concepts of his work and was enough to popularize Łoś’s results among logical community. The main aim of this work was to present Mill's canons in the framework of formal logic. To achieve this goal the author researched importance of temporal functions in the structure of Miil's concept. Having that he provided his axiomatic system of logic that would fit as a framework for Mill's canons along with their temporal aspects.
","Łoś’s logic was published as his master’s thesis was entitled in 1947 in Polish. His philosophical and formal concepts could be seen as continuation of Lviv-Warsaw School of Logic as his supervisor was Jerzy Słupecki, disciple of Jan Łukasiewicz.","["" In what language was <unk>o<unk>'s master's thesis published?"", ' What school of Logic was he a follower of?', ' Who was a disciple of <unk>ukasiewicz?']","['Polish', 'Lviv-Warsaw School of Logic', 'Jerzy Słupecki']"
582,temporal logic,Temporal operators,"Temporal logic has two kinds of operators: logical operators and modal operators [1]. Logical operators are usual truth-functional operators (



¬
,
∨
,
∧
,
→


{\displaystyle \neg ,\lor ,\land ,\rightarrow }
). The modal operators used in linear temporal logic and computation tree logic are defined as follows.
","Temporal logic has two kinds of operators: logical operators and modal operators [1]. Logical operators are usual truth-functional operators (



¬
,
∨
,
∧
,
→


{\displaystyle \neg ,\lor ,\land ,\rightarrow }
).","[' How many kinds of operators does temporal logic have?', ' What are logical operators?', ' Logical operators are what?']","['two', 'usual truth-functional operators', 'usual truth-functional operators']"
583,temporal logic,Temporal operators,"Unary operators are well-formed formulas whenever B(φ) is well-formed. Binary operators are well-formed formulas whenever B(φ) and C(φ) are well-formed.
",Unary operators are well-formed formulas whenever B(φ) is well-formed. Binary operators are well-formed formulas whenever B(φ) and C(φ) are well-formed.,"[' When are unary operators well-formed formulas?', ' When are binary operators well formed formulas when B(<unk>) is well formed?']","['whenever B(φ) is well-formed', 'whenever B(φ) and C(φ) are well-formed']"
584,temporal logic,Temporal operators,"In some logics, some operators cannot be expressed. For example, N operator cannot be expressed in temporal logic of actions.
","In some logics, some operators cannot be expressed. For example, N operator cannot be expressed in temporal logic of actions.","[' In some logics, some operators cannot be expressed what?', ' In what logic can an N operator not be expressed?']","['temporal logic of actions', 'temporal']"
585,hash function,Summary,"A hash function is any function that can be used to map data of arbitrary size to fixed-size values. The values returned by a hash function are called hash values, hash codes, digests, or simply hashes.  The values are usually used to index a fixed-size table called a hash table. Use of a hash function to index a hash table is called hashing or scatter storage addressing.
","A hash function is any function that can be used to map data of arbitrary size to fixed-size values. The values returned by a hash function are called hash values, hash codes, digests, or simply hashes.","[' What is a function that can map data of arbitrary size to fixed-size values?', ' What are the values returned by a hash function called?']","['hash function', 'hash values']"
586,hash function,Summary,"Hash functions and their associated hash tables are used in data storage and retrieval applications to access data in a small and nearly constant time per retrieval. They require an amount of storage space only fractionally greater than the total space required for the data or records themselves. Hashing is a computationally and storage space-efficient form of data access that avoids the non-linear access time of ordered and unordered lists and structured trees, and the often exponential storage requirements of direct access of state spaces of large or variable-length keys.
",Hash functions and their associated hash tables are used in data storage and retrieval applications to access data in a small and nearly constant time per retrieval. They require an amount of storage space only fractionally greater than the total space required for the data or records themselves.,"[' What are used in data storage and retrieval applications to access data in a small and nearly constant time per retrieval?', ' What do hash functions and their associated hash tables require a fraction of greater than the total space required for?']","['Hash functions and their associated hash tables', 'the data or records themselves']"
587,hash function,Summary,"Hash functions are related to (and often confused with) checksums, check digits, fingerprints, lossy compression, randomization functions, error-correcting codes, and ciphers. Although the concepts overlap to some extent, each one has its own uses and requirements and is designed and optimized differently. The hash function differs from these concepts mainly in terms of data integrity.
","Hash functions are related to (and often confused with) checksums, check digits, fingerprints, lossy compression, randomization functions, error-correcting codes, and ciphers. Although the concepts overlap to some extent, each one has its own uses and requirements and is designed and optimized differently.","[' Hash functions are often confused with checksums, check digits, fingerprints, lossy compression, randomization functions, error-correcting codes, and what else?']",['ciphers']
588,hash function,Overview,"A hash function takes an input as a key, which is associated with a datum or record and used to identify it to the data storage and retrieval application. The keys may be fixed length, like an integer, or variable length, like a name.  In some cases, the key is the datum itself.  The output is a hash code used to index a hash table holding the data or records, or pointers to them.
","A hash function takes an input as a key, which is associated with a datum or record and used to identify it to the data storage and retrieval application. The keys may be fixed length, like an integer, or variable length, like a name.","[' What does a hash function take input as?', ' What is associated with a datum or record and used to identify it to the data storage and retrieval application?']","['a key', 'A hash function takes an input as a key']"
589,hash function,Overview,"A good hash function satisfies two basic properties: 1) it should be very fast to compute; 2) it should minimize duplication of output values (collisions).  Hash functions rely on generating favourable probability distributions for their effectiveness, reducing access time to nearly constant.  High table loading factors, pathological key sets and poorly designed hash functions can result in access times approaching linear in the number of items in the table. 
Hash functions can be designed to give the best worst-case performance, good performance under high table loading factors, and in special cases, perfect (collisionless) mapping of keys into hash codes. Implementation is based on parity-preserving bit operations (XOR and ADD), multiply, or divide. A necessary adjunct to the hash function is a collision-resolution method that employs an auxiliary data structure like linked lists, or systematic probing of the table to find an empty slot.
","A good hash function satisfies two basic properties: 1) it should be very fast to compute; 2) it should minimize duplication of output values (collisions). Hash functions rely on generating favourable probability distributions for their effectiveness, reducing access time to nearly constant.","[' What are two basic properties of a good hash function?', ' What should a hash functions be very fast to compute?', ' How do hashes function rely on generating favourable probability distributions?']","['it should be very fast to compute; 2) it should minimize duplication of output values (collisions).', '1) it should be very fast to compute; 2) it should minimize duplication of output values', 'for their effectiveness']"
590,hash function,Hash tables,"Hash functions are used in conjunction with hash tables to store and retrieve data items or data records. The hash function translates the key associated with each datum or record into a hash code, which is used to index the hash table. When an item is to be added to the table, the hash code may index an empty slot (also called a bucket), in which case the item is added to the table there.  If the hash code indexes a full slot, some kind of collision resolution is required: the new item may be omitted (not added to the table), or replace the old item, or it can be added to the table in some other location by a specified procedure.  That procedure depends on the structure of the hash table: In chained hashing, each slot is the head of a linked list or chain, and items that collide at the slot are added to the chain.  Chains may be kept in random order and searched linearly, or in serial order, or as a self-ordering list by frequency to speed up access.  In open address hashing, the table is probed starting from the occupied slot in a specified manner, usually by linear probing, quadratic probing, or double hashing until an open slot is located or the entire table is probed (overflow).  Searching for the item follows the same procedure until the item is located, an open slot is found or the entire table has been searched (item not in table).
","Hash functions are used in conjunction with hash tables to store and retrieve data items or data records. The hash function translates the key associated with each datum or record into a hash code, which is used to index the hash table.","[' Hash functions are used in conjunction with what to store and retrieve data items or data records?', ' The hash function translates the key associated with each datum or record into what?', ' What is used to index the hash table?']","['hash tables', 'a hash code', 'The hash function translates the key associated with each datum or record into a hash code']"
591,hash function,Hashing integer data types,"There are several common algorithms for hashing integers.  The method giving the best distribution is data-dependent.  One of the simplest and most common methods in practice is the modulo division method.
",There are several common algorithms for hashing integers. The method giving the best distribution is data-dependent.,"[' How many common algorithms are there for hashing integers?', ' The method giving the best distribution is what?']","['several', 'data-dependent']"
592,hash function,Hashing variable-length data,"When the data values are long (or variable-length) character strings—such as personal names, web page addresses, or mail messages—their distribution is usually very uneven, with complicated dependencies. For example, text in any natural language has highly non-uniform distributions of characters, and character pairs, characteristic of the language. For such data, it is prudent to use a hash function that depends on all characters of the string—and depends on each character in a different way.","When the data values are long (or variable-length) character strings—such as personal names, web page addresses, or mail messages—their distribution is usually very uneven, with complicated dependencies. For example, text in any natural language has highly non-uniform distributions of characters, and character pairs, characteristic of the language.","[' When data values are long, what is their distribution?', ' What is a characteristic of a language?', ' Text in any natural language has what?']","['uneven', 'character pairs', 'highly non-uniform distributions of characters']"
593,hash function,Analysis,"Worst case result for a hash function can be assessed two ways: theoretical and practical. Theoretical worst case is the probability that all keys map to a single slot.  Practical worst case is expected longest probe sequence (hash function + collision resolution method).  This analysis considers uniform hashing, that is, any key will map to any particular slot with probability 1/m, characteristic of universal hash functions.
",Worst case result for a hash function can be assessed two ways: theoretical and practical. Theoretical worst case is the probability that all keys map to a single slot.,"["" What are the two ways that a hash function's worst case result can be assessed?"", ' What is the probability that all keys map to a single slot?']","['theoretical and practical', 'Theoretical worst case']"
594,hash function,Analysis,"While Knuth worries about adversarial attack on real time systems, Gonnet has shown that the probability of such a case is ""ridiculously small"". His representation was that the probability of k of n keys mapping to a single slot is 







e

−
α



α

k




k
!





{\displaystyle {\frac {e^{-\alpha }\alpha ^{k}}{k!}}}
 where α is the load factor, n/m.","While Knuth worries about adversarial attack on real time systems, Gonnet has shown that the probability of such a case is ""ridiculously small"". His representation was that the probability of k of n keys mapping to a single slot is 







e

−
α



α

k




k
!","[' What does Knuth worry about?', ' What has Gonnet shown about the probability of an adversarial attack on real time systems?']","['adversarial attack on real time systems', 'ridiculously small']"
595,hash function,History,"The term ""hash"" offers a natural analogy with its non-technical meaning (to ""chop"" or ""make a mess"" out of something), given how hash functions scramble their input data to derive their output. In his research for the precise origin of the term, Donald Knuth notes that, while Hans Peter Luhn of IBM appears to have been the first to use the concept of a hash function in a memo dated January 1953, the term itself would only appear in published literature in the late 1960s, on Herbert Hellerman's Digital Computer System Principles, even though it was already widespread jargon by then.","The term ""hash"" offers a natural analogy with its non-technical meaning (to ""chop"" or ""make a mess"" out of something), given how hash functions scramble their input data to derive their output. In his research for the precise origin of the term, Donald Knuth notes that, while Hans Peter Luhn of IBM appears to have been the first to use the concept of a hash function in a memo dated January 1953, the term itself would only appear in published literature in the late 1960s, on Herbert Hellerman's Digital Computer System Principles, even though it was already widespread jargon by then.","[' What does the term ""hash"" offer a natural analogy with its non-technical meaning?', ' How do hash functions scramble their input data to derive their output?', "" What is Donald Knuth's research for the precise origin of the term?"", ' Who was the first to use the concept of a hash function?', ' When did the term ""hash function"" first appear in published literature?', ' Who wrote ""Digital Computer""?', ' In what decade did Herbert Hellerman publish Digital Computer System Principles?']","['to ""chop"" or ""make a mess"" out of something', 'hash', 'hash', 'Hans Peter Luhn', 'late 1960s', 'Herbert Hellerman', '1960s']"
596,usability,Summary,"Usability can be described as the capacity of a system to provide a condition for its users to perform the tasks safely, effectively, and efficiently while enjoying the experience. In software engineering, usability is the degree to which a software can be used by specified consumers to achieve quantified objectives with effectiveness, efficiency, and satisfaction in a quantified context of use.","Usability can be described as the capacity of a system to provide a condition for its users to perform the tasks safely, effectively, and efficiently while enjoying the experience. In software engineering, usability is the degree to which a software can be used by specified consumers to achieve quantified objectives with effectiveness, efficiency, and satisfaction in a quantified context of use.","[' What can be described as the capacity of a system to provide a condition for its users to perform tasks safely, effectively, and efficiently while enjoying the experience?', ' What is the degree to which a software can be used by specified consumers to achieve quantified objectives?', ' What can be used by specified consumers to achieve quantified objectives with effectiveness, efficiency, and satisfaction?']","['Usability', 'usability', 'a software']"
597,usability,Summary,"The object of use can be a software application, website, book, tool, machine, process, vehicle, or anything a human interacts with. A usability study may be conducted as a primary job function by a usability analyst or as a secondary job function by designers, technical writers, marketing personnel, and others. It is widely used in consumer electronics, communication, and knowledge transfer objects (such as a cookbook, a document or online help) and mechanical objects such as a door handle or a hammer.
","The object of use can be a software application, website, book, tool, machine, process, vehicle, or anything a human interacts with. A usability study may be conducted as a primary job function by a usability analyst or as a secondary job function by designers, technical writers, marketing personnel, and others.","[' What can be an object of use?', ' Who conducts a usability study?', ' What can designers, technical writers, marketing personnel do?']","['a software application', 'usability analyst', 'A usability study']"
598,usability,Summary,"Usability includes methods of measuring usability, such as needs analysis and the study of the principles behind an object's perceived efficiency or elegance. In human-computer interaction and computer science, usability studies the elegance and clarity with which the interaction with a computer program or a web site (web usability) is designed. Usability considers user satisfaction and utility as quality components, and aims to improve user experience through iterative design.","Usability includes methods of measuring usability, such as needs analysis and the study of the principles behind an object's perceived efficiency or elegance. In human-computer interaction and computer science, usability studies the elegance and clarity with which the interaction with a computer program or a web site (web usability) is designed.","[' What is a method of measuring usability?', "" What is the study of the principles behind an object's perceived efficiency or elegance?"", ' In human-computer interaction and computer science, what studies the elegance and clarity with which the interaction with a computer program or a web site is done?', ' What is the term for the way a computer program or a web site is designed?']","['needs analysis', 'Usability', 'usability', 'web usability']"
599,usability,Introduction,"Complex computer systems find their way into everyday life, and at the same time the market is saturated with competing brands. This has made usability more popular and widely recognized in recent years, as companies see the benefits of researching and developing their products with user-oriented methods instead of technology-oriented methods. By understanding and researching the interaction between product and user, the usability expert can also provide insight that is unattainable by traditional company-oriented market research. For example, after observing and interviewing users, the usability expert may identify needed functionality or design flaws that were not anticipated. A method called contextual inquiry does this in the naturally occurring context of the users own environment. In the user-centered design paradigm, the product is designed with its intended users in mind at all times. In the user-driven or participatory design paradigm, some of the users become actual or de facto members of the design team.","Complex computer systems find their way into everyday life, and at the same time the market is saturated with competing brands. This has made usability more popular and widely recognized in recent years, as companies see the benefits of researching and developing their products with user-oriented methods instead of technology-oriented methods.","[' What type of systems find their way into everyday life?', ' What has made usability more popular and widely recognized in recent years?', ' Companies see the benefits of researching and developing their products with what methods instead of technology-oriented methods?', ' What type of methods do companies use instead of technology-oriented methods?']","['Complex computer systems', 'Complex computer systems find their way into everyday life, and at the same time the market is saturated with competing brands. This has made usability more popular and widely recognized in recent years, as companies see the benefits of researching and developing their products with user-oriented methods instead of technology-oriented methods', 'user-oriented methods', 'user-oriented methods']"
600,usability,Introduction,"The term user friendly is often used as a synonym for usable, though it may also refer to accessibility. Usability describes the quality of user experience across websites, software, products, and environments. There is no consensus about the relation of the terms ergonomics (or human factors) and usability. Some think of usability as the software specialization of the larger topic of ergonomics. Others view these topics as tangential, with ergonomics focusing on physiological matters (e.g., turning a door handle) and usability focusing on psychological matters (e.g., recognizing that a door can be opened by turning its handle). Usability is also important in website development (web usability). According to Jakob Nielsen, ""Studies of user behavior on the Web find a low tolerance for difficult designs or slow sites. People don't want to wait. And they don't want to learn how to use a home page. There's no such thing as a training class or a manual for a Web site. People have to be able to grasp the functioning of the site immediately after scanning the home page—for a few seconds at most."" Otherwise, most casual users simply leave the site and browse or shop elsewhere.
","The term user friendly is often used as a synonym for usable, though it may also refer to accessibility. Usability describes the quality of user experience across websites, software, products, and environments.","[' What term is often used as a synonym for usable?', ' What describes the quality of user experience across websites, software, products, and environments?']","['user friendly', 'Usability']"
601,usability,Definition,"ISO defines usability as ""The extent to which a product can be used by specified users to achieve specified goals with effectiveness, efficiency, and satisfaction in a specified context of use."" The word ""usability"" also refers to methods for improving ease-of-use during the design process. Usability consultant Jakob Nielsen and computer science professor Ben Shneiderman have written (separately) about a framework of system acceptability, where usability is a part of ""usefulness"" and is composed of:","ISO defines usability as ""The extent to which a product can be used by specified users to achieve specified goals with effectiveness, efficiency, and satisfaction in a specified context of use."" The word ""usability"" also refers to methods for improving ease-of-use during the design process.","[' ISO defines usability as ""The extent to which a product can be used by specified users to achieve specified goals with effectiveness, efficiency, and satisfaction in a specified context of use""?', ' The word ""usability"" refers to methods for improving what during the design process?']","['usability as ""The extent to which a product can be used by specified users to achieve specified goals with effectiveness, efficiency, and satisfaction in a specified context of use."" The word ""usability"" also refers to methods for improving ease-of-use during the design process.', 'ease-of-use']"
602,usability,Definition,"Usability is often associated with the functionalities of the product (cf. ISO definition, below), in addition to being solely a characteristic of the user interface (cf. framework of system acceptability, also below, which separates usefulness into usability and utility). For example, in the context of mainstream consumer products, an automobile lacking a reverse gear could be considered unusable according to the former view, and lacking in utility according to the latter view. When evaluating user interfaces for usability, the definition can be as simple as ""the perception of a target user of the effectiveness (fit for purpose) and efficiency (work or time required to use) of the Interface"". Each component may be measured subjectively against criteria, e.g., Principles of User Interface Design, to provide a metric, often expressed as a percentage. It is important to distinguish between usability testing and usability engineering. Usability testing is the measurement of ease of use of a product or piece of software. In contrast, usability engineering (UE) is the research and design process that ensures a product with good usability. Usability is a non-functional requirement. As with other non-functional requirements, usability cannot be directly measured but must be quantified by means of indirect measures or attributes such as, for example, the number of reported problems with ease-of-use of a system.
","Usability is often associated with the functionalities of the product (cf. ISO definition, below), in addition to being solely a characteristic of the user interface (cf.","[' What is often associated with the functionalities of a product?', ' What is a characteristic of the user interface?']","['Usability', 'Usability']"
603,usability,Designing for usability,"Any system or device designed for use by people should be easy to use, easy to learn, easy to remember (the instructions), and helpful to users. John Gould and Clayton Lewis recommend that designers striving for usability follow these three design principles","Any system or device designed for use by people should be easy to use, easy to learn, easy to remember (the instructions), and helpful to users. John Gould and Clayton Lewis recommend that designers striving for usability follow these three design principles","[' What should a system or device designed for use by people be?', ' Who recommend that designers striving for usability follow these three design principles?']","['easy to use, easy to learn, easy to remember (the instructions), and helpful to users', 'John Gould and Clayton Lewis']"
604,usability,Evaluation methods,"There are a variety of usability evaluation methods. Certain methods use data from users, while others rely on usability experts. There are usability evaluation methods for all stages of design and development, from product definition to final design modifications. When choosing a method, consider cost, time constraints, and appropriateness. For a brief overview of methods, see Comparison of usability evaluation methods or continue reading below. Usability methods can be further classified into the subcategories below.
","There are a variety of usability evaluation methods. Certain methods use data from users, while others rely on usability experts.","[' How many usability evaluation methods are there?', ' What do some methods use to evaluate usability?', ' Who does some methods rely on?']","['a variety', 'data from users', 'usability experts']"
605,usability,Professional development,"Usability practitioners are sometimes trained as industrial engineers, psychologists, kinesiologists, systems design engineers, or with a degree in information architecture, information or library science, or Human-Computer Interaction (HCI). More often though they are people who are trained in specific applied fields who have taken on a usability focus within their organization. Anyone who aims to make tools easier to use and more effective for their desired function within the context of work or everyday living can benefit from studying usability principles and guidelines. For those seeking to extend their training, the User Experience Professionals' Association offers online resources, reference lists, courses, conferences, and local chapter meetings. The UXPA also sponsors World Usability Day each November. Related professional organizations include the Human Factors and Ergonomics Society (HFES) and the Association for Computing Machinery's special interest groups in Computer Human Interaction (SIGCHI), Design of Communication (SIGDOC) and Computer Graphics and Interactive Techniques (SIGGRAPH). The Society for Technical Communication also has a special interest group on Usability and User Experience (UUX). They publish a quarterly newsletter called Usability Interface.","Usability practitioners are sometimes trained as industrial engineers, psychologists, kinesiologists, systems design engineers, or with a degree in information architecture, information or library science, or Human-Computer Interaction (HCI). More often though they are people who are trained in specific applied fields who have taken on a usability focus within their organization.","[' What is the term for Human-Computer Interaction?', ' What are usability practitioners trained as?', ' Who has taken on a usability focus within their organization?']","['HCI', 'industrial engineers, psychologists, kinesiologists, systems design engineers', 'people who are trained in specific applied fields']"
606,computer vision,Summary,"Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to understand and automate tasks that the human visual system can do.","Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to understand and automate tasks that the human visual system can do.","[' Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from what?', ' From the perspective of engineering, what seeks to understand and automate tasks that the human visual system can do?']","['digital images or videos', 'Computer vision']"
607,computer vision,Summary,"Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions. Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.","Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions.","[' Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images and extracting high-dimensional data from the real world?']",['numerical or symbolic information']
608,computer vision,Summary,"The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner, or medical scanning device. The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems.
","The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner, or medical scanning device.","[' What is the scientific discipline of computer vision concerned with?', ' What can image data take many forms?']","['the theory behind artificial systems that extract information from images', 'video sequences']"
609,computer vision,Definition,"Computer vision is an interdisciplinary field that deals with how computers and can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do. ""Computer vision is concerned with the automatic extraction, analysis and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding."" As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems.
","Computer vision is an interdisciplinary field that deals with how computers and can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do.","[' Computer vision is an interdisciplinary field that deals with how computers can gain high-level understanding from what?', ' From the perspective of engineering, what does computer vision seek to automate?']","['digital images or videos', 'tasks that the human visual system can do']"
610,computer vision,History,"In the late 1960s, computer vision began at universities which were pioneering artificial intelligence. It was meant to mimic the human visual system, as a stepping stone to endowing robots with intelligent behavior. In 1966, it was believed that this could be achieved through a summer project, by attaching a camera to a computer and having it ""describe what it saw"".","In the late 1960s, computer vision began at universities which were pioneering artificial intelligence. It was meant to mimic the human visual system, as a stepping stone to endowing robots with intelligent behavior.","[' When did computer vision begin at universities?', ' What was computer vision meant to mimic?']","['late 1960s', 'the human visual system']"
611,computer vision,History,"What distinguished computer vision from the prevalent field of digital image processing at that time was a desire to extract three-dimensional structure from images with the goal of achieving full scene understanding. Studies in the 1970s formed the early foundations for many of the computer vision algorithms that exist today, including extraction of edges from images, labeling of lines, non-polyhedral and polyhedral modeling, representation of objects as interconnections of smaller structures, optical flow, and motion estimation.","What distinguished computer vision from the prevalent field of digital image processing at that time was a desire to extract three-dimensional structure from images with the goal of achieving full scene understanding. Studies in the 1970s formed the early foundations for many of the computer vision algorithms that exist today, including extraction of edges from images, labeling of lines, non-polyhedral and polyhedral modeling, representation of objects as interconnections of smaller structures, optical flow, and motion estimation.","[' What was the main difference between computer vision and digital image processing?', ' What did computer vision want to extract from images with the goal of achieving full scene understanding?', ' What are some of the computer vision algorithms that exist today?', ' How are edges extracted from images?', ' What are non-polyhedral modeling algorithms?']","['a desire to extract three-dimensional structure from images', 'three-dimensional structure', 'extraction of edges from images, labeling of lines, non-polyhedral and polyhedral modeling, representation of objects as interconnections of smaller structures, optical flow, and motion estimation', 'algorithms', 'polyhedral modeling']"
612,computer vision,History,"The next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes. Researchers also realized that many of these mathematical concepts could be treated within the same optimization framework as regularization and Markov random fields.
By the 1990s, some of the previous research topics became more active than the others. Research in projective 3-D reconstructions led to better understanding of camera calibration. With the advent of optimization methods for camera calibration, it was realized that a lot of the ideas were already explored in bundle adjustment theory from the field of photogrammetry. This led to methods for sparse 3-D reconstructions of scenes from multiple images. Progress was made on the dense stereo correspondence problem and further multi-view stereo techniques. At the same time, variations of graph cut were used to solve image segmentation. This decade also marked the first time statistical learning techniques were used in practice to recognize faces in images (see Eigenface). Toward the end of the 1990s, a significant change came about with the increased interaction between the fields of computer graphics and computer vision. This included image-based rendering, image morphing, view interpolation, panoramic image stitching and early light-field rendering.","The next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes.","[' What did the next decade see studies based on more rigorous mathematical analysis and quantitative aspects of computer vision?', ' What is the inference of shape from various cues such as shading, texture and focus called?']","['the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes', 'snakes']"
613,computer vision,History,"Recent work has seen the resurgence of feature-based methods, used in conjunction with machine learning techniques and complex optimization frameworks. 
The advancement of Deep Learning techniques has brought further life to the field of computer vision. The accuracy of deep learning algorithms on several benchmark computer vision data sets for tasks ranging from classification, segmentation and optical flow has surpassed prior methods.","Recent work has seen the resurgence of feature-based methods, used in conjunction with machine learning techniques and complex optimization frameworks. The advancement of Deep Learning techniques has brought further life to the field of computer vision.","[' What have feature-based methods seen a resurgence of in recent work?', ' What has Deep Learning techniques brought further life to the field of computer vision?']","['machine learning techniques and complex optimization frameworks', 'The advancement']"
614,computer vision,Applications,"Applications range from tasks such as industrial machine vision systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them. The computer vision and machine vision fields have significant overlap. Computer vision covers the core technology of automated image analysis which is used in many fields. Machine vision usually refers to a process of combining automated image analysis with other methods and technologies to provide automated inspection and robot guidance in industrial applications. In many computer-vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common. Examples of applications of computer vision include systems for:
","Applications range from tasks such as industrial machine vision systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them. The computer vision and machine vision fields have significant overlap.","[' What do industrial machine vision systems inspect on a production line?', ' What do computer vision and machine vision fields overlap with?']","['bottles', 'research into artificial intelligence']"
615,computer vision,Typical tasks,"Each of the application areas described above employ a range of computer vision tasks; more or less well-defined measurement problems or processing problems, which can be solved using a variety of methods. Some examples of typical computer vision tasks are presented below.
","Each of the application areas described above employ a range of computer vision tasks; more or less well-defined measurement problems or processing problems, which can be solved using a variety of methods. Some examples of typical computer vision tasks are presented below.","[' What do each of the application areas described above employ?', ' What are some examples of typical computer vision tasks?']","['a range of computer vision tasks', 'presented below']"
616,computer vision,Typical tasks,"Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g., in the forms of decisions. Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that can interface with other thought processes and elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.","Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g., in the forms of decisions. Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that can interface with other thought processes and elicit appropriate action.","[' What are computer vision tasks used for?', ' What is the purpose of computer vision?', ' In computer vision, what is the input of visual images?', ' What is the input of the retina?', ' What does the transformation of visual images into descriptions of the world do?']","['acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world', 'to produce numerical or symbolic information', 'the retina', 'visual images', 'can interface with other thought processes and elicit appropriate action']"
617,computer vision,System methods,"The organization of a computer vision system is highly application-dependent. Some systems are stand-alone applications that solve a specific measurement or detection problem, while others constitute a sub-system of a larger design which, for example, also contains sub-systems for control of mechanical actuators, planning, information databases, man-machine interfaces, etc. The specific implementation of a computer vision system also depends on whether its functionality is pre-specified or if some part of it can be learned or modified during operation. Many functions are unique to the application. There are, however, typical functions that are found in many computer vision systems.
","The organization of a computer vision system is highly application-dependent. Some systems are stand-alone applications that solve a specific measurement or detection problem, while others constitute a sub-system of a larger design which, for example, also contains sub-systems for control of mechanical actuators, planning, information databases, man-machine interfaces, etc.","[' What is the organization of a computer vision system highly application-dependent?', ' Some systems are stand-alone applications that solve a specific measurement or detection problem.']","['Some systems are stand-alone applications that solve a specific measurement or detection problem', 'computer vision system']"
618,computer vision,Hardware,"There are many kinds of computer vision systems; however, all of them contain these basic elements: a power source, at least one image acquisition device (camera, ccd, etc.), a processor, and control and communication cables or some kind of wireless interconnection mechanism. In addition, a practical vision system contains software, as well as a display in order to monitor the system. Vision systems for inner spaces, as most industrial ones, contain an illumination system and may be placed in a controlled environment. Furthermore, a completed system includes many accessories such as camera supports, cables and connectors.
","There are many kinds of computer vision systems; however, all of them contain these basic elements: a power source, at least one image acquisition device (camera, ccd, etc. ), a processor, and control and communication cables or some kind of wireless interconnection mechanism.","[' How many types of computer vision systems are there?', ' What are the basic elements of a computer vision system?', ' A power source, an image acquisition device, a processor, and control and communication cables are what?']","['many', 'a power source, at least one image acquisition device (camera, ccd, etc. ), a processor, and control and communication cables or some kind of wireless interconnection mechanism', 'computer vision systems']"
619,computer vision,Hardware,"A few computer vision systems use image-acquisition hardware with active illumination or something other than visible light or both, such as structured-light 3D scanners, thermographic cameras, hyperspectral imagers, radar imaging, lidar scanners, magnetic resonance images, side-scan sonar, synthetic aperture sonar, etc. Such hardware captures ""images"" that are then processed often using the same computer vision algorithms used to process visible-light images.
","A few computer vision systems use image-acquisition hardware with active illumination or something other than visible light or both, such as structured-light 3D scanners, thermographic cameras, hyperspectral imagers, radar imaging, lidar scanners, magnetic resonance images, side-scan sonar, synthetic aperture sonar, etc. Such hardware captures ""images"" that are then processed often using the same computer vision algorithms used to process visible-light images.","[' What type of hardware does a few computer vision systems use?', ' What kind of hardware captures images that are then processed?', ' What does hardware capture?', ' What are the same computer vision algorithms used to process?']","['image-acquisition', 'image-acquisition', '""images""', 'visible-light images']"
620,computer vision,Hardware,"While traditional broadcast and consumer video systems operate at a rate of 30 frames per second, advances in digital signal processing and consumer graphics hardware has made high-speed image acquisition, processing, and display possible for real-time systems on the order of hundreds to thousands of frames per second. For applications in robotics, fast, real-time video systems are critically important and often can simplify the processing needed for certain algorithms. When combined with a high-speed projector, fast image acquisition allows 3D measurement and feature tracking to be realised.","While traditional broadcast and consumer video systems operate at a rate of 30 frames per second, advances in digital signal processing and consumer graphics hardware has made high-speed image acquisition, processing, and display possible for real-time systems on the order of hundreds to thousands of frames per second. For applications in robotics, fast, real-time video systems are critically important and often can simplify the processing needed for certain algorithms.","[' How many frames per second do traditional broadcast and consumer video systems operate?', ' Advances in digital signal processing and consumer graphics hardware have made what possible for real-time systems?', ' How many frames per second are needed for robotics applications?', ' What kind of video systems are critically important?', ' How can real-time video systems simplify processing?']","['30', 'high-speed image acquisition, processing, and display', 'hundreds to thousands', 'fast, real-time', 'certain algorithms']"
621,reinforcement learning,Summary,"Reinforcement learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.
","Reinforcement learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.","[' What does RL stand for?', ' What is one of the three basic machine learning paradigms?']","['Reinforcement learning', 'Reinforcement learning']"
622,reinforcement learning,Summary,"Reinforcement learning differs from supervised learning in not needing labelled input/output pairs be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge). Partially supervised RL algorithms can combine the advantages of supervised and RL algorithms.","Reinforcement learning differs from supervised learning in not needing labelled input/output pairs be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).","[' Reinforcement learning differs from supervised learning in what two ways?', ' What is the focus of reinforcement learning?', ' Exploration (of uncharted territory) and exploitation (of current knowledge) are the two main areas of focus for reinforcement learning.']","['not needing labelled input/output pairs be presented, and in not needing sub-optimal actions to be explicitly corrected', 'finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).', 'exploration']"
623,reinforcement learning,Summary,"
The environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context use dynamic programming techniques. The main difference between the classical dynamic programming methods  and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible.","
The environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context use dynamic programming techniques. The main difference between the classical dynamic programming methods  and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible.","[' What is a Markov decision process?', ' What does MDP stand for?', ' Why do reinforcement learning algorithms use dynamic programming?', ' What do the latter not assume knowledge of?', ' What do they target large MDPs where exact methods become infeasible?']","['MDP', 'Markov decision process', 'the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible', 'an exact mathematical model of the MDP', 'reinforcement learning algorithms']"
624,reinforcement learning,Introduction,"Due to its generality, reinforcement learning is studied in many disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, and statistics. In the operations research and control literature, reinforcement learning is called approximate dynamic programming, or neuro-dynamic programming. The problems of interest in reinforcement learning have also been studied in the theory of optimal control, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation, particularly in the absence of a mathematical model of the environment. In economics and game theory, reinforcement learning may be used to explain how equilibrium may arise under bounded rationality.
","Due to its generality, reinforcement learning is studied in many disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, and statistics. In the operations research and control literature, reinforcement learning is called approximate dynamic programming, or neuro-dynamic programming.","[' What is reinforcement learning known as in the operations research and control literature?', ' What is neuro-dynamic programming?']","['approximate dynamic programming', 'approximate dynamic programming']"
625,reinforcement learning,Introduction,"The purpose of reinforcement learning is for the agent to learn an optimal, or nearly-optimal, policy that maximizes the ""reward function"" or other user-provided reinforcement signal that accumulates from the immediate rewards. This is similar to processes that appear to occur in animal psychology. For example, biological brains are hardwired to interpret signals such as pain and hunger as negative reinforcements, and interpret pleasure and food intake as positive reinforcements. In some circumstances, animals can learn to engage in behaviors that optimize these rewards. This suggests that animals are capable of reinforcement learning.","The purpose of reinforcement learning is for the agent to learn an optimal, or nearly-optimal, policy that maximizes the ""reward function"" or other user-provided reinforcement signal that accumulates from the immediate rewards. This is similar to processes that appear to occur in animal psychology.","[' What is the purpose of reinforcement learning?', ' What does the agent learn to maximize?']","['for the agent to learn an optimal, or nearly-optimal, policy', 'reward function']"
626,reinforcement learning,Introduction,"A basic reinforcement learning agent AI interacts with its environment in discrete time steps. At each time t, the agent receives the current state 




s

t




{\displaystyle s_{t}}
 and reward 




r

t




{\displaystyle r_{t}}
. It then chooses an action 




a

t




{\displaystyle a_{t}}
 from the set of available actions, which is subsequently sent to the environment. The environment moves to a new state 




s

t
+
1




{\displaystyle s_{t+1}}
 and the reward 




r

t
+
1




{\displaystyle r_{t+1}}
 associated with the transition 



(

s

t


,

a

t


,

s

t
+
1


)


{\displaystyle (s_{t},a_{t},s_{t+1})}
 is determined. The goal of a reinforcement learning agent is to learn a policy: 



π
:
A
×
S
→
[
0
,
1
]


{\displaystyle \pi :A\times S\rightarrow [0,1]}
, 



π
(
a
,
s
)
=
Pr
(

a

t


=
a
∣

s

t


=
s
)


{\displaystyle \pi (a,s)=\Pr(a_{t}=a\mid s_{t}=s)}
 which maximizes the expected cumulative reward.
","A basic reinforcement learning agent AI interacts with its environment in discrete time steps. At each time t, the agent receives the current state 




s

t




{\displaystyle s_{t}}
 and reward 




r

t




{\displaystyle r_{t}}
.","[' How does a basic reinforcement learning agent interact with its environment?', ' At each time t, the agent receives what?']","['in discrete time steps', 'the current state']"
627,reinforcement learning,Introduction,"Formulating the problem as an MDP assumes the agent directly observes the current environmental state; in this case the problem is said to have full observability. If the agent only has access to a subset of states, or if the observed states are corrupted by noise, the agent is said to have partial observability, and formally the problem must be formulated as a Partially observable Markov decision process. In both cases, the set of actions available to the agent can be restricted. For example, the state of an account balance could be restricted to be positive; if the current value of the state is 3 and the state transition attempts to reduce the value by 4, the transition will not be allowed.
","Formulating the problem as an MDP assumes the agent directly observes the current environmental state; in this case the problem is said to have full observability. If the agent only has access to a subset of states, or if the observed states are corrupted by noise, the agent is said to have partial observability, and formally the problem must be formulated as a Partially observable Markov decision process.","[' What assumes the agent observes the current environmental state?', ' If the agent only has access to a subset of states, or if the observed states are corrupted by noise, the agent is said to have what?', ' What causes observed states to be corrupted?', ' What is the agent said to have?', ' The problem must be formulated as what?']","['Formulating the problem as an MDP', 'partial observability', 'noise', 'partial observability', 'Partially observable Markov decision process']"
628,reinforcement learning,Introduction,"When the agent's performance is compared to that of an agent that acts optimally, the difference in performance gives rise to the notion of regret. In order to act near optimally, the agent must reason about the long-term consequences of its actions (i.e., maximize future income), although the immediate reward associated with this might be negative.
","When the agent's performance is compared to that of an agent that acts optimally, the difference in performance gives rise to the notion of regret. In order to act near optimally, the agent must reason about the long-term consequences of its actions (i.e., maximize future income), although the immediate reward associated with this might be negative.","["" When an agent's performance is compared to that of an agent that acts optimally, the difference in performance gives rise to the notion of what?"", ' What must an agent reason about in order to act near optimally?', ' What does it do to maximize future income?', ' What is the immediate reward associated with this?']","['regret', 'the long-term consequences of its actions', 'reason about the long-term consequences of its actions', 'negative']"
629,reinforcement learning,Introduction,"Thus, reinforcement learning is particularly well-suited to problems that include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, including robot control, elevator scheduling, telecommunications, backgammon, checkers and Go (AlphaGo).
","Thus, reinforcement learning is particularly well-suited to problems that include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, including robot control, elevator scheduling, telecommunications, backgammon, checkers and Go (AlphaGo).","[' What is reinforcement learning particularly well suited to?', ' What has been applied successfully to various problems?']","['problems that include a long-term versus short-term reward trade-off', 'reinforcement learning']"
630,reinforcement learning,Introduction,"Two elements make reinforcement learning powerful: the use of samples to optimize performance and the use of function approximation to deal with large environments. Thanks to these two key components, reinforcement learning can be used in large environments in the following situations:
","Two elements make reinforcement learning powerful: the use of samples to optimize performance and the use of function approximation to deal with large environments. Thanks to these two key components, reinforcement learning can be used in large environments in the following situations:","[' What two elements make reinforcement learning powerful?', ' The use of samples to optimize performance and the use of function approximation to deal with large environments can be used in what situations?']","['the use of samples to optimize performance and the use of function approximation to deal with large environments', 'the following situations']"
631,reinforcement learning,Introduction,"The first two of these problems could be considered planning problems (since some form of model is available), while the last one could be considered to be a genuine learning problem. However, reinforcement learning converts both planning problems to machine learning problems.
","The first two of these problems could be considered planning problems (since some form of model is available), while the last one could be considered to be a genuine learning problem. However, reinforcement learning converts both planning problems to machine learning problems.","[' What could the first two of these problems be considered?', ' What would the last one be considered as?', ' Reinforcement learning converts planning problems to what?']","['planning problems', 'a genuine learning problem', 'machine learning problems']"
632,reinforcement learning,Exploration,"Reinforcement learning requires clever exploration mechanisms; randomly selecting actions, without reference to an estimated probability distribution, shows poor performance. The case of (small) finite Markov decision processes is relatively well understood. However, due to the lack of algorithms that scale well with the number of states (or scale to problems with infinite state spaces), simple exploration methods are the most practical.
","Reinforcement learning requires clever exploration mechanisms; randomly selecting actions, without reference to an estimated probability distribution, shows poor performance. The case of (small) finite Markov decision processes is relatively well understood.","[' What requires clever exploration mechanisms?', ' Randomly selecting actions without reference to an estimated probability distribution shows poor performance?', ' The case of (small) finite Markov decision processes is relatively well understood.']","['Reinforcement learning', 'randomly', 'Reinforcement learning']"
633,reinforcement learning,Exploration,"One such method is 



ε


{\displaystyle \varepsilon }
-greedy, where 



0
<
ε
<
1


{\displaystyle 0<\varepsilon <1}
 is a parameter controlling the amount of exploration vs. exploitation.  With probability 



1
−
ε


{\displaystyle 1-\varepsilon }
, exploitation is chosen, and the agent chooses the action that it believes has the best long-term effect (ties between actions are broken uniformly at random). Alternatively, with probability 



ε


{\displaystyle \varepsilon }
, exploration is chosen, and the action is chosen uniformly at random. 



ε


{\displaystyle \varepsilon }
 is usually a fixed parameter but can be adjusted either according to a schedule (making the agent explore progressively less), or adaptively based on heuristics.","One such method is 



ε


{\displaystyle \varepsilon }
-greedy, where 



0
<
ε
<
1


{\displaystyle 0<\varepsilon <1}
 is a parameter controlling the amount of exploration vs. exploitation. With probability 



1
−
ε


{\displaystyle 1-\varepsilon }
, exploitation is chosen, and the agent chooses the action that it believes has the best long-term effect (ties between actions are broken uniformly at random).","[' What is one such method?', ' What is a parameter controlling the amount of exploration vs. exploitation?', ' With probability 1 <unk> <unk>displaystyle 1-<unk>varepsilon <unk>, exploitation is chosen?', ' What is the best long-term effect?', ' What is broken uniformly at random?']","['-greedy', 'ε\n\n\n{\\displaystyle \\varepsilon }\n-greedy', '−\nε', 'ties between actions are broken uniformly at random', 'ties between actions']"
634,reinforcement learning,Theory,"Both the asymptotic and finite-sample behaviors of most algorithms are well understood. Algorithms with provably good online performance (addressing the exploration issue) are known.
",Both the asymptotic and finite-sample behaviors of most algorithms are well understood. Algorithms with provably good online performance (addressing the exploration issue) are known.,"[' What are both the asymptotic and finite-sample behaviors of most algorithms?', ' Algorithms with provably good online performance are known?']","['well understood', 'algorithms']"
635,reinforcement learning,Theory,"Efficient exploration of MDPs is given in  Burnetas and Katehakis (1997). Finite-time performance bounds have also appeared for many algorithms, but these bounds are expected to be rather loose and thus more work is needed to better understand the relative advantages and limitations.
","Efficient exploration of MDPs is given in  Burnetas and Katehakis (1997). Finite-time performance bounds have also appeared for many algorithms, but these bounds are expected to be rather loose and thus more work is needed to better understand the relative advantages and limitations.","[' In Burnetas and Katehakis, what is given as an example of efficient exploration of MDPs?', ' Finite-time performance bounds have also appeared for many algorithms, but they are expected to be what?']","['Finite-time performance bounds', 'rather loose']"
636,reinforcement learning,Theory,"For incremental algorithms, asymptotic convergence issues have been settled. Temporal-difference-based algorithms converge under a wider set of conditions than was previously possible (for example, when used with arbitrary, smooth function approximation).
","For incremental algorithms, asymptotic convergence issues have been settled. Temporal-difference-based algorithms converge under a wider set of conditions than was previously possible (for example, when used with arbitrary, smooth function approximation).","[' Asymptotic convergence issues have been settled for what?', ' Temporal-difference-based algorithms converge under a wider set of conditions than was previously possible.']","['incremental algorithms', 'incremental algorithms']"
637,evaluation,Summary,"Evaluation is a
systematic determination of a subject's merit, worth and significance, using criteria governed by a set of standards. It can assist an organization, program, design, project or any other intervention or initiative to assess any aim, realisable concept/proposal, or any alternative, to help in decision-making; or to ascertain the degree of achievement or value in regard to the aim and objectives and results of any such action that has been completed. The primary purpose of evaluation, in addition to gaining insight into prior or existing initiatives, is to enable reflection and assist in the identification of future change. Evaluation is often used to characterize and appraise subjects of interest in a wide range of human enterprises, including the arts, criminal justice, foundations, non-profit organizations, government, health care, and other human services. It is long term and done at the end of a period of time.
","Evaluation is a
systematic determination of a subject's merit, worth and significance, using criteria governed by a set of standards. It can assist an organization, program, design, project or any other intervention or initiative to assess any aim, realisable concept/proposal, or any alternative, to help in decision-making; or to ascertain the degree of achievement or value in regard to the aim and objectives and results of any such action that has been completed.","["" What is a systematic determination of a subject's merit, worth and significance?"", ' What is an organization, program, design, project or any other intervention or initiative able to assess?', ' Any alternative, to help in decision-making, or to ascertain the degree of achievement or value in regard to the aim and objectives and results of any such action?']","['Evaluation', 'Evaluation', 'Evaluation']"
638,evaluation,Definition,"Evaluation is the structured interpretation and giving of meaning to predicted or actual impacts of proposals or results. It looks at original objectives, and at what is either predicted or what was accomplished and how it was accomplished. So evaluation can be formative, that is taking place during the development of a concept or proposal, project or organization, with the intention of improving the value or effectiveness of the proposal, project, or organisation. It can also be summative, drawing lessons from a completed action or project or an organisation at a later point in time or circumstance.","Evaluation is the structured interpretation and giving of meaning to predicted or actual impacts of proposals or results. It looks at original objectives, and at what is either predicted or what was accomplished and how it was accomplished.","[' What is the term for the structured interpretation and giving of meaning to predicted or actual impacts of proposals or results?', ' What does evaluation look at?']","['Evaluation', 'original objectives']"
639,evaluation,Definition,"Evaluation is inherently a theoretically informed approach (whether explicitly or not), and consequently any particular definition of evaluation would have been tailored to its context – the theory, needs, purpose, and methodology of the evaluation process itself. Having said this, evaluation has been defined as:
","Evaluation is inherently a theoretically informed approach (whether explicitly or not), and consequently any particular definition of evaluation would have been tailored to its context – the theory, needs, purpose, and methodology of the evaluation process itself. Having said this, evaluation has been defined as:","[' What is inherently a theoretically informed approach?', ' Any particular definition of evaluation would have been tailored to what?', ' What has been defined as:']","['Evaluation', 'its context', 'evaluation']"
640,evaluation,Standards,"Evaluating programs and projects, regarding their value and impact within the context they are implemented, can be ethically challenging. Evaluators may encounter complex, culturally specific systems resistant to external evaluation. Furthermore, the project organization or other stakeholders may be invested in a particular evaluation outcome. Finally, evaluators themselves may encounter ""conflict of interest (COI)"" issues, or experience interference or pressure to present findings that support a particular assessment.
","Evaluating programs and projects, regarding their value and impact within the context they are implemented, can be ethically challenging. Evaluators may encounter complex, culturally specific systems resistant to external evaluation.","[' Evaluating programs and projects regarding their value and impact within the context they are implemented can be what?', ' Evaluators may encounter complex, culturally specific systems resistant to external evaluation?']","['ethically challenging', 'Evaluating programs']"
641,evaluation,Standards,"General professional codes of conduct, as determined by the employing organization, usually cover three broad aspects of behavioral standards, and include inter-collegial relations (such as respect for diversity and privacy), operational issues (due competence, documentation accuracy and appropriate use of resources), and conflicts of interest (nepotism, accepting gifts and other kinds of favoritism). However, specific guidelines particular to the evaluator's role that can be utilized in the management of unique ethical challenges are required. The Joint Committee on Standards for Educational Evaluation has developed standards for program, personnel, and student evaluation. The Joint Committee standards are broken into four sections: Utility, Feasibility, Propriety, and Accuracy. Various European institutions have also prepared their own standards, more or less related to those produced by the Joint Committee. They provide guidelines about basing value judgments on systematic inquiry, evaluator competence and integrity, respect for people, and regard for the general and public welfare.","General professional codes of conduct, as determined by the employing organization, usually cover three broad aspects of behavioral standards, and include inter-collegial relations (such as respect for diversity and privacy), operational issues (due competence, documentation accuracy and appropriate use of resources), and conflicts of interest (nepotism, accepting gifts and other kinds of favoritism). However, specific guidelines particular to the evaluator's role that can be utilized in the management of unique ethical challenges are required.","[' How many broad aspects of behavioral standards do general professional codes of conduct cover?', ' Inter-collegial relations, such as respect for diversity and privacy, are examples of what?', ' Operational issues include what types of issues?', ' Nepotism, accepting gifts and other conflicts of interest are examples?', ' What are some examples of conflicts of interest?', ' What can be used in the management of unique ethical challenges?']","['three', 'General professional codes of conduct', 'due competence, documentation accuracy and appropriate use of resources', 'nepotism', 'nepotism, accepting gifts and other kinds of favoritism', ""specific guidelines particular to the evaluator's role""]"
642,evaluation,Standards,"The American Evaluation Association has created a set of Guiding Principles for evaluators. The order of these principles does not imply priority among them; priority will vary by situation and evaluator role. The principles run as follows:
",The American Evaluation Association has created a set of Guiding Principles for evaluators. The order of these principles does not imply priority among them; priority will vary by situation and evaluator role.,"[' Who has created a set of Guiding Principles for evaluators?', ' The order of the principles does not imply what?', ' Priority will vary by situation and what role?']","['The American Evaluation Association', 'priority', 'evaluator']"
643,evaluation,Standards,"Independence is attained through ensuring independence of judgment is upheld such that evaluation
conclusions are not influenced or pressured by another party, and avoidance of conflict of
interest, such that the evaluator does not have a stake in a particular conclusion. Conflict of
interest is at issue particularly where funding of evaluations is provided by particular bodies
with a stake in conclusions of the evaluation, and this is seen as potentially compromising the
independence of the evaluator. Whilst it is acknowledged that evaluators may be familiar with
agencies or projects that they are required to evaluate, independence requires that they not have
been involved in the planning or implementation of the project. A declaration of interest should
be made where any benefits or association with project are stated. Independence of judgment is
required to be maintained against any pressures brought to bear on evaluators, for example, by
project funders wishing to modify evaluations such that the project appears more effective than
","Independence is attained through ensuring independence of judgment is upheld such that evaluation
conclusions are not influenced or pressured by another party, and avoidance of conflict of
interest, such that the evaluator does not have a stake in a particular conclusion. Conflict of
interest is at issue particularly where funding of evaluations is provided by particular bodies
with a stake in conclusions of the evaluation, and this is seen as potentially compromising the
independence of the evaluator.","[' What is attained through ensuring independence of judgment is upheld?', ' What is avoided by ensuring that the evaluator does not have a stake in a particular conclusion?', ' Conflict of interest is at issue particularly where funding of evaluations is what?', ' What is at issue where funding of evaluations is provided by particular bodies with a stake in conclusions of the evaluation?', ' What is seen as potentially compromising the independence of the evaluator?']","['Independence', 'conflict of\ninterest', 'particular bodies\nwith a stake in conclusions of the evaluation', 'Conflict of\ninterest', 'Conflict of\ninterest']"
644,evaluation,Standards,"Impartiality pertains to findings being a fair and thorough assessment of strengths and
weaknesses of a project or program. This requires taking due input from all stakeholders involved
and findings presented without bias and with a transparent, proportionate, and persuasive link
between findings and recommendations. Thus evaluators are required to delimit their findings to
evidence. A mechanism to ensure impartiality is external and internal review. Such review is
required of significant (determined in terms of cost or sensitivity) evaluations. The review is
based on quality of work and the degree to which a demonstrable link is provided between findings
","Impartiality pertains to findings being a fair and thorough assessment of strengths and
weaknesses of a project or program. This requires taking due input from all stakeholders involved
and findings presented without bias and with a transparent, proportionate, and persuasive link
between findings and recommendations.","[' What refers to findings being a fair and thorough assessment of strengths and weaknesses of a project or program?', ' What requires taking due input from all stakeholders involved and findings presented without bias?']","['Impartiality', 'Impartiality']"
645,evaluation,Standards,"Transparency requires that stakeholders are aware of the reason for the evaluation, the criteria
by which evaluation occurs and the purposes to which the findings will be applied. Access to the
evaluation document should be facilitated through findings being easily readable, with clear
explanations of evaluation methodologies, approaches, sources of information, and costs
","Transparency requires that stakeholders are aware of the reason for the evaluation, the criteria
by which evaluation occurs and the purposes to which the findings will be applied. Access to the
evaluation document should be facilitated through findings being easily readable, with clear
explanations of evaluation methodologies, approaches, sources of information, and costs","[' What requires that stakeholders are aware of the reason for the evaluation?', ' What should be facilitated by findings being easily readable?']","['Transparency', 'Access to the\nevaluation document']"
646,evaluation,Standards,"Furthermore, the international organizations such as the I.M.F. and the World Bank have independent evaluation functions. The various funds, programmes, and agencies of the United Nations has a mix of independent, semi-independent and self-evaluation functions, which have organized themselves as a system-wide UN Evaluation Group (UNEG), that works together to strengthen the function, and to establish UN norms and standards for evaluation. There is also an evaluation group within the OECD-DAC, which endeavors to improve development evaluation standards. The independent evaluation units of the major multinational development banks (MDBs) have also created the Evaluation Cooperation Group  to strengthen the use of evaluation for greater MDB effectiveness and accountability, share lessons from MDB evaluations, and promote evaluation harmonization and collaboration.
","Furthermore, the international organizations such as the I.M.F. and the World Bank have independent evaluation functions.",[' The I.M.F. and the World Bank have what kind of functions?'],['independent evaluation']
647,evaluation,Perspectives,"The word ""evaluation"" has various connotations for different people, raising issues related to this process that include; what type of evaluation should be conducted; why there should be an evaluation process and how the evaluation is integrated into a program, for the purpose of gaining greater knowledge and awareness? There are also various factors inherent in the evaluation process, for example; to critically examine influences within a program that involve the gathering and analyzing of relative information about a program. 
","The word ""evaluation"" has various connotations for different people, raising issues related to this process that include; what type of evaluation should be conducted; why there should be an evaluation process and how the evaluation is integrated into a program, for the purpose of gaining greater knowledge and awareness? There are also various factors inherent in the evaluation process, for example; to critically examine influences within a program that involve the gathering and analyzing of relative information about a program.","[' What does the word ""evaluation"" have different connotations for different people?', ' What should be conducted?', ' How is evaluation integrated into a program?', ' What are some factors inherent in the evaluation process?', ' What involves gathering and analyzing relative information about a program?']","['raising issues', 'evaluation"" has various connotations for different people, raising issues related to this process that include; what type of evaluation', 'for the purpose of gaining greater knowledge and awareness', 'to critically examine influences within a program that involve the gathering and analyzing of relative information about a program', 'evaluation process']"
648,evaluation,Perspectives,"Founded on another perspective of evaluation by Thomson and Hoffman in 2003, it is possible for a situation to be encountered, in which the process could not be considered advisable; for instance, in the event of a program being unpredictable, or unsound. This would include it lacking a consistent routine; or the concerned parties unable to reach an agreement regarding the purpose of the program. In addition, an influencer, or manager, refusing to incorporate relevant, important central issues within the evaluation
","Founded on another perspective of evaluation by Thomson and Hoffman in 2003, it is possible for a situation to be encountered, in which the process could not be considered advisable; for instance, in the event of a program being unpredictable, or unsound. This would include it lacking a consistent routine; or the concerned parties unable to reach an agreement regarding the purpose of the program.","["" Thomson and Hoffman's perspective of evaluation was based on what?"", ' What would be an example of a situation in which a program could not be considered advisable?', ' What would a program lack if it was unsound?', ' What would the parties be unable to reach regarding the purpose of the program?']","['it is possible for a situation to be encountered, in which the process could not be considered advisable; for instance, in the event of a program being unpredictable, or unsound. This would include it lacking a consistent routine', 'a program being unpredictable, or unsound', 'a consistent routine', 'agreement']"
649,evaluation,Approaches,"There exist several conceptually distinct ways of thinking about, designing, and conducting evaluation efforts. Many of the evaluation approaches in use today make truly unique contributions to solving important problems, while others refine existing approaches in some way.
","There exist several conceptually distinct ways of thinking about, designing, and conducting evaluation efforts. Many of the evaluation approaches in use today make truly unique contributions to solving important problems, while others refine existing approaches in some way.","[' How many conceptually distinct ways of thinking about, designing, and conducting evaluation efforts exist?', ' Many of the evaluation approaches in use today make truly unique contributions to solving important problems?']","['several', 'There exist several conceptually distinct ways of thinking about, designing, and conducting evaluation efforts. Many of the evaluation approaches in use today make truly unique contributions to solving important problems, while others refine existing approaches']"
650,evaluation,Methods and techniques,"Evaluation is methodologically diverse. Methods may be qualitative or quantitative, and include case studies, survey research, statistical analysis, model building, and many more such as:
","Evaluation is methodologically diverse. Methods may be qualitative or quantitative, and include case studies, survey research, statistical analysis, model building, and many more such as:","[' What types of methods are used in evaluation?', ' What are some examples of methods used in evaluating?']","['qualitative or quantitative', 'case studies, survey research, statistical analysis, model building']"
651,cellular automaton,Summary,"A cellular automaton (pl. cellular automata, abbrev. CA) is a discrete model of computation studied in automata theory. Cellular automata are also called cellular spaces, tessellation automata, homogeneous structures, cellular structures, tessellation structures, and iterative arrays. Cellular automata have found application in various areas, including physics, theoretical biology and microstructure modeling.
","A cellular automaton (pl. cellular automata, abbrev.",[' What is the abbreviation for a cellular automaton?'],['cellular automata']
652,cellular automaton,Summary,"A cellular automaton consists of a regular grid of cells, each in one of a finite number of states, such as on and off (in contrast to a coupled map lattice). The grid can be in any finite number of dimensions. For each cell, a set of cells called its neighborhood is defined relative to the specified cell. An initial state (time t = 0) is selected by assigning a state for each cell. A new generation is created (advancing t by 1), according to some fixed rule (generally, a mathematical function) that determines the new state of each cell in terms of the current state of the cell and the states of the cells in its neighborhood. Typically, the rule for updating the state of cells is the same for each cell and does not change over time, and is applied to the whole grid simultaneously, though exceptions are known, such as the stochastic cellular automaton and asynchronous cellular automaton.
","A cellular automaton consists of a regular grid of cells, each in one of a finite number of states, such as on and off (in contrast to a coupled map lattice). The grid can be in any finite number of dimensions.","[' What type of automaton consists of a regular grid of cells?', ' What can a cellular automaton be in any finite number of dimensions?']","['cellular', 'The grid']"
653,cellular automaton,Summary,"The concept was originally discovered in the 1940s by Stanislaw Ulam and John von Neumann while they were contemporaries at Los Alamos National Laboratory. While studied by some throughout the 1950s and 1960s, it was not until the 1970s and Conway's Game of Life, a two-dimensional cellular automaton, that interest in the subject expanded beyond academia. In the 1980s, Stephen Wolfram engaged in a systematic study of one-dimensional cellular automata, or what he calls elementary cellular automata; his research assistant Matthew Cook showed that one of these rules is Turing-complete.
","The concept was originally discovered in the 1940s by Stanislaw Ulam and John von Neumann while they were contemporaries at Los Alamos National Laboratory. While studied by some throughout the 1950s and 1960s, it was not until the 1970s and Conway's Game of Life, a two-dimensional cellular automaton, that interest in the subject expanded beyond academia.","[' Who discovered the concept of cellular automatons in the 1940s?', ' Where were Stanislaw Ulam and John von Neumann?', "" What was the name of Conway's game of life?"", ' When was Game of Life released?', "" What was Conway's Game of Life?"", ' What was the name of the two-dimensional cellular automaton that interest in the subject expanded beyond?']","['Stanislaw Ulam and John von Neumann', 'Los Alamos National Laboratory', 'a two-dimensional cellular automaton', '1970s', 'a two-dimensional cellular automaton', ""Conway's Game of Life""]"
654,cellular automaton,Summary,"The primary classifications of cellular automata, as outlined by Wolfram, are numbered one to four. They are, in order, automata in which patterns generally stabilize into homogeneity, automata in which patterns evolve into mostly stable or oscillating structures, automata in which patterns evolve in a seemingly chaotic fashion, and automata in which patterns become extremely complex and may last for a long time, with stable local structures.  This last class is thought to be computationally universal, or capable of simulating a Turing machine. Special types of cellular automata are reversible, where only a single configuration leads directly to a subsequent one, and totalistic, in which the future value of individual cells only depends on the total value of a group of neighboring cells. Cellular automata can simulate a variety of real-world systems, including biological and chemical ones.
","The primary classifications of cellular automata, as outlined by Wolfram, are numbered one to four. They are, in order, automata in which patterns generally stabilize into homogeneity, automata in which patterns evolve into mostly stable or oscillating structures, automata in which patterns evolve in a seemingly chaotic fashion, and automata in which patterns become extremely complex and may last for a long time, with stable local structures.","[' What are the primary classifications of cellular automata?', ' Who outlined the classifications?', ' What is the order in which patterns generally stabilize into homogeneity?', ' How do patterns evolve?', ' Which patterns evolve in a seemingly chaotic fashion?', ' Which patterns become extremely complex and may last for a long time?']","['one to four', 'Wolfram', 'automata', 'in a seemingly chaotic fashion', 'automata', 'automata']"
655,cellular automaton,Overview,"One way to simulate a two-dimensional cellular automaton is with an infinite sheet of graph paper along with a set of rules for the cells to follow. Each square is called a ""cell"" and each cell has two possible states, black and white. The neighborhood of a cell is the nearby, usually adjacent, cells. The two most common types of neighborhoods are the von Neumann neighborhood and the Moore neighborhood. The former, named after the founding cellular automaton theorist, consists of the four orthogonally adjacent cells. The latter includes the von Neumann neighborhood as well as the four diagonally adjacent cells. For such a cell and its Moore neighborhood, there are 512 (= 29) possible patterns. For each of the 512 possible patterns, the rule table would state whether the center cell will be black or white on the next time interval. Conway's Game of Life is a popular version of this model. Another common neighborhood type is the extended von Neumann neighborhood, which includes the two closest cells in each orthogonal direction, for a total of eight. The general equation for such a system of rules is kks, where k is the number of possible states for a cell, and s is the number of neighboring cells (including the cell to be calculated itself) used to determine the cell's next state. Thus, in the two-dimensional system with a Moore neighborhood, the total number of automata possible would be 229, or 1.34×10154.
","One way to simulate a two-dimensional cellular automaton is with an infinite sheet of graph paper along with a set of rules for the cells to follow. Each square is called a ""cell"" and each cell has two possible states, black and white.","[' What is one way to simulate a two-dimensional cellular automaton?', ' What is each square called?', ' How many possible states does each cell have?']","['with an infinite sheet of graph paper', 'a ""cell', 'two']"
656,cellular automaton,Overview,"It is usually assumed that every cell in the universe starts in the same state, except for a finite number of cells in other states; the assignment of state values is called a configuration. More generally, it is sometimes assumed that the universe starts out covered with a periodic pattern, and only a finite number of cells violate that pattern. The latter assumption is common in one-dimensional cellular automata.
","It is usually assumed that every cell in the universe starts in the same state, except for a finite number of cells in other states; the assignment of state values is called a configuration. More generally, it is sometimes assumed that the universe starts out covered with a periodic pattern, and only a finite number of cells violate that pattern.","[' What is the assignment of state values called?', ' What is a configuration?', ' What does the universe start out covered with?', ' How many cells violate that pattern?']","['a configuration', 'the assignment of state values', 'a periodic pattern', 'a finite number']"
657,cellular automaton,Overview,"Cellular automata are often simulated on a finite grid rather than an infinite one. In two dimensions, the universe would be a rectangle instead of an infinite plane. The obvious problem with finite grids is how to handle the cells on the edges. How they are handled will affect the values of all the cells in the grid. One possible method is to allow the values in those cells to remain constant. Another method is to define neighborhoods differently for these cells. One could say that they have fewer neighbors, but then one would also have to define new rules for the cells located on the edges. These cells are usually handled with a toroidal arrangement: when one goes off the top, one comes in at the corresponding position on the bottom, and when one goes off the left, one comes in on the right. (This essentially simulates an infinite periodic tiling, and in the field of partial differential equations is sometimes referred to as periodic boundary conditions.) This can be visualized as taping the left and right edges of the rectangle to form a tube, then taping the top and bottom edges of the tube to form a torus (doughnut shape). Universes of other dimensions are handled similarly. This solves boundary problems with neighborhoods, but another advantage is that it is easily programmable using modular arithmetic functions. For example, in a 1-dimensional cellular automaton like the examples below, the neighborhood of a cell xit is {xi−1t−1, xit−1, xi+1t−1}, where t is the time step (vertical), and i is the index (horizontal) in one generation.
","Cellular automata are often simulated on a finite grid rather than an infinite one. In two dimensions, the universe would be a rectangle instead of an infinite plane.","[' Cellular automata are often simulated on what kind of grid?', ' In two dimensions, the universe would be a rectangle instead of what?']","['finite', 'an infinite plane']"
658,cellular automaton,History,"Stanislaw Ulam, while working at the Los Alamos National Laboratory in the 1940s, studied the growth of crystals, using a simple lattice network as his model. At the same time, John von Neumann, Ulam's colleague at Los Alamos, was working on the problem of self-replicating systems. Von Neumann's initial design was founded upon the notion of one robot building another robot. This design is known as the kinematic model. As he developed this design, von Neumann came to realize the great difficulty of building a self-replicating robot, and of the great cost in providing the robot with a ""sea of parts"" from which to build its replicant. Neumann wrote a paper entitled ""The general and logical theory of automata"" for the Hixon Symposium in 1948. Ulam was the one who suggested using a discrete system for creating a reductionist model of self-replication. Nils Aall Barricelli performed many of the earliest explorations of these models of artificial life.
","Stanislaw Ulam, while working at the Los Alamos National Laboratory in the 1940s, studied the growth of crystals, using a simple lattice network as his model. At the same time, John von Neumann, Ulam's colleague at Los Alamos, was working on the problem of self-replicating systems.","[' Stanislaw Ulam worked at what lab in the 1940s?', "" What was Ulam's model of crystal growth?"", ' John von Neumann was working on what problem?']","['Los Alamos National Laboratory', 'a simple lattice network', 'self-replicating systems']"
659,cellular automaton,History,"Ulam and von Neumann created a method for calculating liquid motion in the late 1950s. The driving concept of the method was to consider a liquid as a group of discrete units and calculate the motion of each based on its neighbors' behaviors. Thus was born the first system of cellular automata. Like Ulam's lattice network, von Neumann's cellular automata are two-dimensional, with his self-replicator implemented algorithmically. The result was a universal copier and constructor working within a cellular automaton with a small neighborhood (only those cells that touch are neighbors; for von Neumann's cellular automata, only orthogonal cells), and with 29 states per cell. Von Neumann gave an existence proof that a particular pattern would make endless copies of itself within the given cellular universe by designing a 200,000 cell configuration that could do so. This design is known as the tessellation model, and is called a von Neumann universal constructor.",Ulam and von Neumann created a method for calculating liquid motion in the late 1950s. The driving concept of the method was to consider a liquid as a group of discrete units and calculate the motion of each based on its neighbors' behaviors.,"[' When did Ulam and von Neumann create a method for calculating liquid motion?', ' What was the driving concept of the method?']","['late 1950s', ""to consider a liquid as a group of discrete units and calculate the motion of each based on its neighbors' behaviors""]"
660,cellular automaton,History,"Also in the 1940s, Norbert Wiener and Arturo Rosenblueth developed a model of excitable media with some of the characteristics of a cellular automaton. Their specific motivation was the mathematical description of impulse conduction in cardiac systems. However their model is not a cellular automaton because the medium in which signals propagate is continuous, and wave fronts are curves. A true cellular automaton model of excitable media was developed and studied by J. M. Greenberg and S. P. Hastings in 1978; see Greenberg-Hastings cellular automaton. The original work of Wiener and Rosenblueth contains many insights and continues to be cited in modern research publications on cardiac arrhythmia and excitable systems.","Also in the 1940s, Norbert Wiener and Arturo Rosenblueth developed a model of excitable media with some of the characteristics of a cellular automaton. Their specific motivation was the mathematical description of impulse conduction in cardiac systems.","[' In what decade did Norbert Wiener and Arturo Rosenblueth develop a model of excitable media with some of the characteristics of a cellular automaton?', "" What was the specific motivation of Norbert Viennaer's model?"", ' In what system was impulse conduction described?']","['1940s', 'mathematical description of impulse conduction in cardiac systems', 'cardiac']"
661,cellular automaton,History,"In the 1960s, cellular automata were studied as a particular type of dynamical system and the connection with the mathematical field of symbolic dynamics was established for the first time. In 1969, Gustav A. Hedlund compiled many results following this point of view in what is still considered as a seminal paper for the mathematical study of cellular automata. The most fundamental result is the characterization in the Curtis–Hedlund–Lyndon theorem of the set of global rules of cellular automata as the set of continuous endomorphisms of shift spaces.
","In the 1960s, cellular automata were studied as a particular type of dynamical system and the connection with the mathematical field of symbolic dynamics was established for the first time. In 1969, Gustav A. Hedlund compiled many results following this point of view in what is still considered as a seminal paper for the mathematical study of cellular automata.","[' When were cellular automata studied as a particular type of dynamical system?', ' What was established for the first time?', ' Who compiled many results following this point of view in 1969?', ' What is still considered a seminal paper for the mathematical study of cellular automata?']","['1960s', 'the connection with the mathematical field of symbolic dynamics', 'Gustav A. Hedlund', 'Gustav A. Hedlund']"
662,cellular automaton,History,"Also in 1969 computer scientist Alvy Ray Smith completed a Stanford PhD dissertation on Cellular Automata Theory, the first mathematical treatment of CA as a general class of computers. Many papers came from this dissertation: He showed the equivalence of neighborhoods of various shapes, how to reduce a Moore to a von Neumann neighborhood or how to reduce any neighborhood to a von Neumann neighborhood. He proved that two-dimensional CA are computation universal, introduced 1-dimensional CA, and showed that they too are computation universal, even with simple neighborhoods. He showed how to subsume the complex von Neumann proof of construction universality (and hence self-reproducing machines) into a consequence of computation universality in a 1-dimensional CA. Intended as the introduction to the German edition of von Neumann's book on CA, he wrote a survey of the field with dozens of references to papers, by many authors in many countries over a decade or so of work, often overlooked by modern CA researchers.","Also in 1969 computer scientist Alvy Ray Smith completed a Stanford PhD dissertation on Cellular Automata Theory, the first mathematical treatment of CA as a general class of computers. Many papers came from this dissertation: He showed the equivalence of neighborhoods of various shapes, how to reduce a Moore to a von Neumann neighborhood or how to reduce any neighborhood to a von Neumann neighborhood.","[' When did Alvy Ray Smith complete his PhD?', ' What was the first mathematical treatment of CA as a general class of computers?', ' Who completed a PhD on Cellular Automata Theory?', ' How to reduce a Moore to a von Neumann neighborhood?']","['1969', 'Cellular Automata Theory', 'Alvy Ray Smith', 'Alvy Ray Smith']"
663,cellular automaton,History,"In the 1970s a two-state, two-dimensional cellular automaton named Game of Life became widely known, particularly among the early computing community. Invented by John Conway and popularized by Martin Gardner in a Scientific American article, its rules are as follows:
","In the 1970s a two-state, two-dimensional cellular automaton named Game of Life became widely known, particularly among the early computing community. Invented by John Conway and popularized by Martin Gardner in a Scientific American article, its rules are as follows:","[' What was the name of the two-state, two-dimensional cellular automaton that became widely known in the 1970s?', ' Who invented the Game of Life?', ' In what article did Martin Gardner popularize the game of life?']","['Game of Life', 'John Conway', 'Scientific American']"
664,cellular automaton,History,"Despite its simplicity, the system achieves an impressive diversity of behavior, fluctuating between apparent randomness and order. One of the most apparent features of the Game of Life is the frequent occurrence of gliders, arrangements of cells that essentially move themselves across the grid. It is possible to arrange the automaton so that the gliders interact to perform computations, and after much effort it has been shown that the Game of Life can emulate a universal Turing machine. It was viewed as a largely recreational topic, and little follow-up work was done outside of investigating the particularities of the Game of Life and a few related rules in the early 1970s.","Despite its simplicity, the system achieves an impressive diversity of behavior, fluctuating between apparent randomness and order. One of the most apparent features of the Game of Life is the frequent occurrence of gliders, arrangements of cells that essentially move themselves across the grid.","[' What is one of the most apparent features of the Game of Life?', ' What are arrangements of cells that move themselves across the grid?']","['gliders', 'gliders']"
665,cellular automaton,History,"Stephen Wolfram independently began working on cellular automata in mid-1981 after considering how complex patterns seemed formed in nature in violation of the Second Law of Thermodynamics. His investigations were initially spurred by an interest in modelling systems such as neural networks. He published his first paper in Reviews of Modern Physics investigating elementary cellular automata (Rule 30 in particular) in June 1983. The unexpected complexity of the behavior of these simple rules led Wolfram to suspect that complexity in nature may be due to similar mechanisms. His investigations, however, led him to realize that cellular automata were poor at modelling neural networks. Additionally, during this period Wolfram formulated the concepts of intrinsic randomness and computational irreducibility, and suggested that rule 110 may be universal—a fact proved later by Wolfram's research assistant Matthew Cook in the 1990s.",Stephen Wolfram independently began working on cellular automata in mid-1981 after considering how complex patterns seemed formed in nature in violation of the Second Law of Thermodynamics. His investigations were initially spurred by an interest in modelling systems such as neural networks.,"[' When did Stephen Wolfram begin working on cellular automata?', ' What law did Wolfram think complex patterns formed in nature?', "" Wolfram's investigations were initially spurred by an interest in what?""]","['mid-1981', 'Second Law of Thermodynamics', 'modelling systems']"
666,cellular automaton,Classification,"Wolfram, in A New Kind of Science and several papers dating from the mid-1980s, defined four classes into which cellular automata and several other simple computational models can be divided depending on their behavior. While earlier studies in cellular automata tended to try to identify type of patterns for specific rules, Wolfram's classification was the first attempt to classify the rules themselves. In order of complexity the classes are:
","Wolfram, in A New Kind of Science and several papers dating from the mid-1980s, defined four classes into which cellular automata and several other simple computational models can be divided depending on their behavior. While earlier studies in cellular automata tended to try to identify type of patterns for specific rules, Wolfram's classification was the first attempt to classify the rules themselves.","[' How many classes did Wolfram define in A New Kind of Science?', ' What did earlier studies in cellular automata tend to try to identify for specific patterns?', ' What was the first attempt to classify the rules themselves?']","['four', 'rules', ""Wolfram's classification""]"
667,cellular automaton,Classification,"These definitions are qualitative in nature and there is some room for interpretation. According to Wolfram, ""...with almost any general classification scheme there are inevitably cases which get assigned to one class by one definition and another class by another definition. And so it is with cellular automata: there are occasionally rules...that show some features of one class and some of another."" Wolfram's classification has been empirically matched to a clustering of the compressed lengths of the outputs of cellular automata.","These definitions are qualitative in nature and there is some room for interpretation. According to Wolfram, ""...with almost any general classification scheme there are inevitably cases which get assigned to one class by one definition and another class by another definition.","[' What are the definitions qualitative in nature?', ' Wolfram says there are cases which get assigned to one class by which definition?']","['definitions', 'one definition']"
668,cellular automaton,Classification,"There have been several attempts to classify cellular automata in formally rigorous classes, inspired by the Wolfram's classification. For instance, Culik and Yu proposed three well-defined classes (and a fourth one for the automata not matching any of these), which are sometimes called Culik-Yu classes; membership in these proved undecidable.
Wolfram's class 2 can be partitioned into two subgroups of stable (fixed-point) and oscillating (periodic) rules.","There have been several attempts to classify cellular automata in formally rigorous classes, inspired by the Wolfram's classification. For instance, Culik and Yu proposed three well-defined classes (and a fourth one for the automata not matching any of these), which are sometimes called Culik-Yu classes; membership in these proved undecidable.","[' How many well-defined classes did Culik and Yu propose?', ' What are Culik-Yu classes sometimes called?']","['three', 'well-defined classes']"
669,cellular automaton,Elementary cellular automata,"The simplest nontrivial cellular automaton would be one-dimensional, with two possible states per cell, and a cell's neighbors defined as the adjacent cells on either side of it. A cell and its two neighbors form a neighborhood of 3 cells, so there are 23 = 8 possible patterns for a neighborhood. A rule consists of deciding, for each pattern, whether the cell will be a 1 or a 0 in the next generation. There are then 28 = 256 possible rules.","The simplest nontrivial cellular automaton would be one-dimensional, with two possible states per cell, and a cell's neighbors defined as the adjacent cells on either side of it. A cell and its two neighbors form a neighborhood of 3 cells, so there are 23 = 8 possible patterns for a neighborhood.","[' What would be the simplest nontrivial cellular automaton?', ' How many possible states would a cell have per cell?', ' A cell and its two neighbors form a neighborhood of how many cells?']","['one-dimensional', 'two', '3']"
670,cellular automaton,Elementary cellular automata,"These 256 cellular automata are generally referred to by their Wolfram code, a standard naming convention invented by Wolfram that gives each rule a number from 0 to 255. A number of papers have analyzed and compared these 256 cellular automata. The rule 30 and rule 110 cellular automata are particularly interesting. The images below show the history of each when the starting configuration consists of a 1 (at the top of each image) surrounded by 0s. Each row of pixels represents a generation in the history of the automaton, with t=0 being the top row. Each pixel is colored white for 0 and black for 1.
","These 256 cellular automata are generally referred to by their Wolfram code, a standard naming convention invented by Wolfram that gives each rule a number from 0 to 255. A number of papers have analyzed and compared these 256 cellular automata.","[' What is a standard naming convention invented by Wolfram?', ' How many cellular automata are generally referred to by their Wolfram code?', ' What gives each rule a number from 0 to 255?']","['Wolfram code', '256', 'Wolfram code']"
671,cellular automaton,Elementary cellular automata,"Rule 110, like the Game of Life, exhibits what Wolfram calls class 4 behavior, which is neither completely random nor completely repetitive. Localized structures appear and interact in various complicated-looking ways. In the course of the development of A New Kind of Science, as a research assistant to Wolfram in 1994, Matthew Cook proved that some of these structures were rich enough to support universality. This result is interesting because rule 110 is an extremely simple one-dimensional system, and difficult to engineer to perform specific behavior. This result therefore provides significant support for Wolfram's view that class 4 systems are inherently likely to be universal. Cook presented his proof at a Santa Fe Institute conference on Cellular Automata in 1998, but Wolfram blocked the proof from being included in the conference proceedings, as Wolfram did not want the proof announced before the publication of A New Kind of Science. In 2004, Cook's proof was finally published in Wolfram's journal Complex Systems (Vol. 15, No. 1), over ten years after Cook came up with it. Rule 110 has been the basis for some of the smallest universal Turing machines.","Rule 110, like the Game of Life, exhibits what Wolfram calls class 4 behavior, which is neither completely random nor completely repetitive. Localized structures appear and interact in various complicated-looking ways.","[' What does Wolfram call class 4 behavior?', ' What type of structures appear and interact in various complicated ways?']","['which is neither completely random nor completely repetitive', 'Localized']"
672,cellular automaton,Rule space,"An elementary cellular automaton rule is specified by 8 bits, and all elementary cellular automaton rules can be considered to sit on the vertices of the 8-dimensional unit hypercube. This unit hypercube is the cellular automaton rule space. For next-nearest-neighbor cellular automata, a rule is specified by 25 = 32 bits, and the cellular automaton rule space is a 32-dimensional unit hypercube. A distance between two rules can be defined by the number of steps required to move from one vertex, which represents the first rule, and another vertex, representing another rule, along the edge of the hypercube. This rule-to-rule distance is also called the Hamming distance.
","An elementary cellular automaton rule is specified by 8 bits, and all elementary cellular automaton rules can be considered to sit on the vertices of the 8-dimensional unit hypercube. This unit hypercube is the cellular automaton rule space.","[' How many bits are specified in an elementary cellular automaton rule?', ' What is the unit hypercube called?']","['8', 'the cellular automaton rule space']"
673,cellular automaton,Rule space,"Cellular automaton rule space allows us to ask the question concerning whether rules with similar dynamical behavior are ""close"" to each other. Graphically drawing a high dimensional hypercube on the 2-dimensional plane remains a difficult task, and one crude locator of a rule in the hypercube is the number of bit-1 in the 8-bit string for elementary rules (or 32-bit string for the next-nearest-neighbor rules). Drawing the rules in different Wolfram classes in these slices of the rule space show that class 1 rules tend to have lower number of bit-1s, thus located in one region of the space, whereas class 3 rules tend to have higher proportion (50%) of bit-1s.","Cellular automaton rule space allows us to ask the question concerning whether rules with similar dynamical behavior are ""close"" to each other. Graphically drawing a high dimensional hypercube on the 2-dimensional plane remains a difficult task, and one crude locator of a rule in the hypercube is the number of bit-1 in the 8-bit string for elementary rules (or 32-bit string for the next-nearest-neighbor rules).","[' What allows us to ask whether rules with similar dynamical behavior are ""close"" to each other?', ' Graphically drawing a high dimensional hypercube on the 2-dimensional plane remains a difficult task?', ' What is the number of bit-1 in the 8-bit string for elementary rules?', ' How many bit strings are there for the next-nearest-neighbor rules in the hypercube?']","['Cellular automaton rule space', 'Cellular automaton rule space', '32-bit string', '32']"
674,cellular automaton,Rule space,"For larger cellular automaton rule space, it is shown that class 4 rules are located between the class 1 and class 3 rules. This observation is the foundation for the phrase edge of chaos, and is reminiscent of the phase transition in thermodynamics.
","For larger cellular automaton rule space, it is shown that class 4 rules are located between the class 1 and class 3 rules. This observation is the foundation for the phrase edge of chaos, and is reminiscent of the phase transition in thermodynamics.","[' Where are class 4 rules located for larger cellular automaton rule space?', ' What is the foundation for the phrase edge of chaos?']","['between the class 1 and class 3 rules', 'class 4 rules are located between the class 1 and class 3 rules']"
675,genetic algorithms,Summary,"In computer science and operations research, a genetic algorithm (GA) is a metaheuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms (EA). Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on biologically inspired operators such as mutation, crossover and selection. Some examples of GA applications include optimizing decision trees for better performance, automatically solve sudoku puzzles, hyperparameter optimization, etc.
","In computer science and operations research, a genetic algorithm (GA) is a metaheuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms (EA). Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on biologically inspired operators such as mutation, crossover and selection.","[' What is a genetic algorithm inspired by?', ' What is the larger class of evolutionary algorithms?', ' Genetic algorithms are commonly used to generate what?', ' What type of operators are used to solve optimization and search problems?']","['the process of natural selection', 'EA', 'high-quality solutions to optimization and search problems', 'biologically inspired']"
676,genetic algorithms,The building block hypothesis,"Genetic algorithms are simple to implement, but their behavior is difficult to understand. In particular, it is difficult to understand why these algorithms frequently succeed at generating solutions of high fitness when applied to practical problems. The building block hypothesis (BBH) consists of:
","Genetic algorithms are simple to implement, but their behavior is difficult to understand. In particular, it is difficult to understand why these algorithms frequently succeed at generating solutions of high fitness when applied to practical problems.","[' Genetic algorithms are simple to implement, but their behavior is difficult to understand what?', ' Genetic algorithms often succeed at generating solutions of high fitness when applied to what problem?']","['why these algorithms frequently succeed at generating solutions of high fitness when applied to practical problems', 'practical']"
677,genetic algorithms,The building block hypothesis,"Despite the lack of consensus regarding the validity of the building-block hypothesis, it has been consistently evaluated and used as reference throughout the years. Many estimation of distribution algorithms, for example, have been proposed in an attempt to provide an environment in which the hypothesis would hold. Although good results have been reported for some classes of problems, skepticism concerning the generality and/or practicality of the building-block hypothesis as an explanation for GAs efficiency still remains. Indeed, there is a reasonable amount of work that attempts to understand its limitations from the perspective of estimation of distribution algorithms.","Despite the lack of consensus regarding the validity of the building-block hypothesis, it has been consistently evaluated and used as reference throughout the years. Many estimation of distribution algorithms, for example, have been proposed in an attempt to provide an environment in which the hypothesis would hold.","[' What has been consistently evaluated and used as reference throughout the years?', ' Many estimation of distribution algorithms have been proposed in an attempt to provide an environment in which the hypothesis would hold?']","['building-block hypothesis', 'building-block hypothesis']"
678,genetic algorithms,Problem domains,"Problems which appear to be particularly appropriate for solution by genetic algorithms include timetabling and scheduling problems, and many scheduling software packages are based on GAs. GAs have also been applied to engineering. Genetic algorithms are often applied as an approach to solve global optimization problems.
","Problems which appear to be particularly appropriate for solution by genetic algorithms include timetabling and scheduling problems, and many scheduling software packages are based on GAs. GAs have also been applied to engineering.","[' What are two problems that are particularly suitable for genetic algorithms?', ' What are many scheduling software packages based on?', ' GAs have been applied to what?']","['timetabling and scheduling problems', 'GAs', 'engineering']"
679,genetic algorithms,Problem domains,"As a general rule of thumb genetic algorithms might be useful in problem domains that have a complex fitness landscape as mixing, i.e., mutation in combination with crossover, is designed to move the population away from local optima that a traditional hill climbing algorithm might get stuck in. Observe that commonly used crossover operators cannot change any uniform population. Mutation alone can provide ergodicity of the overall genetic algorithm process (seen as a Markov chain).
","As a general rule of thumb genetic algorithms might be useful in problem domains that have a complex fitness landscape as mixing, i.e., mutation in combination with crossover, is designed to move the population away from local optima that a traditional hill climbing algorithm might get stuck in. Observe that commonly used crossover operators cannot change any uniform population.","[' Genetic algorithms might be useful in problem domains that have a complex what?', ' Mutation in combination with crossover is designed to move the population away from local optima that a traditional hill climbing algorithm might get stuck in?', ' What kind of algorithm might get stuck in?', ' Commonly used crossover operators cannot change what?']","['fitness landscape', 'mixing', 'hill climbing', 'any uniform population']"
680,genetic algorithms,Problem domains,"[I]t is quite unnatural to model applications in terms of genetic operators like mutation and crossover on bit strings. The pseudobiology adds another level of complexity between you and your problem. Second, genetic algorithms take a very long time on nontrivial problems. [...] [T]he analogy with evolution—where significant progress require [sic] millions of years—can be quite appropriate.
",[I]t is quite unnatural to model applications in terms of genetic operators like mutation and crossover on bit strings. The pseudobiology adds another level of complexity between you and your problem.,"[' What is it unnatural to model applications in terms of genetic operators like mutation and crossover on bit strings?', ' What adds another level of complexity between you and your problem?']","['I]t is quite unnatural to model applications in terms of genetic operators like mutation and crossover on bit strings. The pseudobiology', 'The pseudobiology']"
681,genetic algorithms,Problem domains,"
I have never encountered any problem where genetic algorithms seemed to me the right way to attack it. Further, I have never seen any computational results reported using genetic algorithms that have favorably impressed me. Stick to simulated annealing for your heuristic search voodoo needs.","
I have never encountered any problem where genetic algorithms seemed to me the right way to attack it. Further, I have never seen any computational results reported using genetic algorithms that have favorably impressed me.","[' I have never encountered a problem where genetic algorithms seemed to me the right way to attack it?', ' What have I never seen reported using genetic algorithms that have favorably impressed me?']","['I have never encountered any problem where genetic algorithms seemed to me the right way to attack it.', 'computational results']"
682,genetic algorithms,History,"In 1950, Alan Turing proposed a ""learning machine"" which would parallel the principles of evolution. Computer simulation of evolution started as early as in 1954 with the work of Nils Aall Barricelli, who was using the computer at the Institute for Advanced Study in Princeton, New Jersey. His 1954 publication was not widely noticed. Starting in 1957, the Australian quantitative geneticist Alex Fraser published a series of papers on simulation of artificial selection of organisms with multiple loci controlling a measurable trait. From these beginnings, computer simulation of evolution by biologists became more common in the early 1960s, and the methods were described in books by Fraser and Burnell (1970) and Crosby (1973). Fraser's simulations included all of the essential elements of modern genetic algorithms. In addition, Hans-Joachim Bremermann published a series of papers in the 1960s that also adopted a population of solution to optimization problems, undergoing recombination, mutation, and selection. Bremermann's research also included the elements of modern genetic algorithms. Other noteworthy early pioneers include Richard Friedberg, George Friedman, and Michael Conrad. Many early papers are reprinted by Fogel (1998).","In 1950, Alan Turing proposed a ""learning machine"" which would parallel the principles of evolution. Computer simulation of evolution started as early as in 1954 with the work of Nils Aall Barricelli, who was using the computer at the Institute for Advanced Study in Princeton, New Jersey.","[' When did Alan Turing propose a ""learning machine""?', ' When did computer simulation of evolution start?', ' Who was using the computer at the Institute for Advanced Study?']","['1950', '1954', 'Nils Aall Barricelli']"
683,genetic algorithms,History,"Although Barricelli, in work he reported in 1963, had simulated the evolution of ability to play a simple game, artificial evolution only became a widely recognized optimization method as a result of the work of Ingo Rechenberg and Hans-Paul Schwefel in the 1960s and early 1970s – Rechenberg's group was able to solve complex engineering problems through evolution strategies. Another approach was the evolutionary programming technique of Lawrence J. Fogel, which was proposed for generating artificial intelligence. Evolutionary programming originally used finite state machines for predicting environments, and used variation and selection to optimize the predictive logics. Genetic algorithms in particular became popular through the work of John Holland in the early 1970s, and particularly his book Adaptation in Natural and Artificial Systems (1975). His work originated with studies of cellular automata, conducted by Holland and his students at the University of Michigan. Holland introduced a formalized framework for predicting the quality of the next generation, known as Holland's Schema Theorem. Research in GAs remained largely theoretical until the mid-1980s, when The First International Conference on Genetic Algorithms was held in Pittsburgh, Pennsylvania.
","Although Barricelli, in work he reported in 1963, had simulated the evolution of ability to play a simple game, artificial evolution only became a widely recognized optimization method as a result of the work of Ingo Rechenberg and Hans-Paul Schwefel in the 1960s and early 1970s – Rechenberg's group was able to solve complex engineering problems through evolution strategies. Another approach was the evolutionary programming technique of Lawrence J. Fogel, which was proposed for generating artificial intelligence.","[' In what year did Barricelli report on the evolution of ability to play a simple game?', ' What was the result of the work of Ingo Rechenberg and Hans-Paul Schwefel in the 1960s and early 1970s?', "" Rechenberg's group was able to solve complex engineering problems through what?"", ' What technique was proposed for generating artificial intelligence?']","['1963', 'artificial evolution only became a widely recognized optimization method', 'evolution strategies', 'evolutionary programming technique']"
684,turing machine,Summary,"A Turing machine is a mathematical model of computation that defines an abstract machine that manipulates symbols on a strip of tape according to a table of rules. Despite the model's simplicity, given any computer algorithm, a Turing machine capable of implementing that algorithm's logic can be constructed.","A Turing machine is a mathematical model of computation that defines an abstract machine that manipulates symbols on a strip of tape according to a table of rules. Despite the model's simplicity, given any computer algorithm, a Turing machine capable of implementing that algorithm's logic can be constructed.","[' What is a Turing machine?', ' What is the mathematical model of computation?']","['a mathematical model of computation', 'A Turing machine']"
685,turing machine,Summary,"The machine operates on an infinite memory tape divided into discrete ""cells"". The machine positions its ""head"" over a cell and ""reads"" or ""scans"" the symbol there. Then, based on the symbol and the machine's own present state in a ""finite table"" of user-specified instructions, the machine first writes a symbol (e.g., a digit or a letter from a finite alphabet) in the cell (some models allow symbol erasure or no writing), then either moves the tape one cell left or right (some models allow no motion, some models move the head), then, based on the observed symbol and the machine's own state in the table, either proceeds to another instruction or halts computation.","The machine operates on an infinite memory tape divided into discrete ""cells"". The machine positions its ""head"" over a cell and ""reads"" or ""scans"" the symbol there.","[' What does the machine operate on?', ' What is the machine divided into?']","['infinite memory tape', 'discrete ""cells"".']"
686,turing machine,Summary,"The Turing machine was invented in 1936 by Alan Turing, who called it an ""a-machine"" (automatic machine). With this model, Turing was able to answer two questions in the negative:
","The Turing machine was invented in 1936 by Alan Turing, who called it an ""a-machine"" (automatic machine). With this model, Turing was able to answer two questions in the negative:","[' In what year was the Turing machine invented?', ' What did Alan Turing call the machine?', ' How many questions did Turing answer with his model?']","['1936', 'a-machine', 'two']"
687,turing machine,Summary,"Turing machines proved the existence of fundamental limitations on the power of mechanical computation. While they can express arbitrary computations, their minimalist design makes them unsuitable for computation in practice: real-world computers are based on different designs that, unlike Turing machines, use random-access memory.
","Turing machines proved the existence of fundamental limitations on the power of mechanical computation. While they can express arbitrary computations, their minimalist design makes them unsuitable for computation in practice: real-world computers are based on different designs that, unlike Turing machines, use random-access memory.","[' What proved the existence of fundamental limitations on the power of mechanical computation?', ' What makes Turing machines unsuitable for computation in practice?', ' Real-world computers are based on different designs that use what?']","['Turing machines', 'their minimalist design', 'random-access memory']"
688,turing machine,Summary,"Turing completeness is the ability for a system of instructions to simulate a Turing machine. A programming language that is Turing complete is theoretically capable of expressing all tasks accomplishable by computers; nearly all programming languages are Turing complete if the limitations of finite memory are ignored.
",Turing completeness is the ability for a system of instructions to simulate a Turing machine. A programming language that is Turing complete is theoretically capable of expressing all tasks accomplishable by computers; nearly all programming languages are Turing complete if the limitations of finite memory are ignored.,"[' What is the ability for a system of instructions to simulate a Turing machine?', ' A programming language that is Turing complete is theoretically capable of expressing all tasks accomplishable by computers?', ' Nearly all programming languages are what?']","['Turing completeness', 'Turing completeness', 'Turing complete']"
689,turing machine,Overview,"A Turing machine is a general example of a central processing unit (CPU) that controls all data manipulation done by a computer, with the canonical machine using sequential memory to store data. More specifically, it is a machine (automaton) capable of enumerating some arbitrary subset of valid strings of an alphabet; these strings are part of a recursively enumerable set. A Turing machine has a tape of infinite length on which it can perform read and write operations.
","A Turing machine is a general example of a central processing unit (CPU) that controls all data manipulation done by a computer, with the canonical machine using sequential memory to store data. More specifically, it is a machine (automaton) capable of enumerating some arbitrary subset of valid strings of an alphabet; these strings are part of a recursively enumerable set.","[' What is a Turing machine a general example of?', ' What controls all data manipulation done by a computer?', ' The canonical machine uses what to store data?', ' What is a recursively enumerable set of strings?']","['a central processing unit (CPU)', 'central processing unit (CPU)', 'sequential memory', 'an alphabet']"
690,turing machine,Overview,"Assuming a black box, the Turing machine cannot know whether it will eventually enumerate any one specific string of the subset with a given program. This is due to the fact that the halting problem is unsolvable, which has major implications for the theoretical limits of computing.
","Assuming a black box, the Turing machine cannot know whether it will eventually enumerate any one specific string of the subset with a given program. This is due to the fact that the halting problem is unsolvable, which has major implications for the theoretical limits of computing.","[' The Turing machine cannot know whether it will eventually enumerate any one specific string of the subset with a given program?', ' The halting problem is unsolvable, which has major implications for what?']","['Assuming a black box', 'the theoretical limits of computing']"
691,turing machine,Overview,"The Turing machine is capable of processing an unrestricted grammar, which further implies that it is capable of robustly evaluating first-order logic in an infinite number of ways. This is famously demonstrated through lambda calculus.
","The Turing machine is capable of processing an unrestricted grammar, which further implies that it is capable of robustly evaluating first-order logic in an infinite number of ways. This is famously demonstrated through lambda calculus.","[' What machine is capable of processing an unrestricted grammar?', ' The Turing machine is also capable of robustly evaluating what kind of logic?', ' What is famously demonstrated through lambda calculus?']","['Turing machine', 'first-order', 'The Turing machine is capable of processing an unrestricted grammar, which further implies that it is capable of robustly evaluating first-order logic']"
692,turing machine,Overview,"A Turing machine that is able to simulate any other Turing machine is called a universal Turing machine (UTM, or simply a universal machine). A more mathematically oriented definition with a similar ""universal"" nature was introduced by Alonzo Church, whose work on lambda calculus intertwined with Turing's in a formal theory of computation known as the Church–Turing thesis. The thesis states that Turing machines indeed capture the informal notion of effective methods in logic and mathematics, and provide a precise definition of an algorithm or ""mechanical procedure"". Studying their  abstract properties yields many insights into computer science and complexity theory.
","A Turing machine that is able to simulate any other Turing machine is called a universal Turing machine (UTM, or simply a universal machine). A more mathematically oriented definition with a similar ""universal"" nature was introduced by Alonzo Church, whose work on lambda calculus intertwined with Turing's in a formal theory of computation known as the Church–Turing thesis.","[' What is a Turing machine able to simulate?', ' What is UTM?', ' Who introduced a definition with a similar ""universal"" nature?', "" Alonzo Church's work intertwined with what?"", ' What is a formal theory of computation?', ' What is the Church-Turing thesis?']","['any other Turing machine', 'universal Turing machine', 'Alonzo Church', ""Turing's"", 'Church–Turing thesis', 'formal theory of computation']"
693,turing machine,Description,"The Turing machine mathematically models a machine that mechanically operates on a tape.  On this tape are symbols, which the machine can read and write, one at a time, using a tape head. Operation is fully determined by a finite set of elementary instructions such as ""in state 42, if the symbol seen is 0, write a 1; if the symbol seen is 1, change into state 17; in state 17, if the symbol seen is 0, write a 1 and change to state 6;"" etc. In the original article (""On Computable Numbers, with an Application to the Entscheidungsproblem"", see also references below), Turing imagines not a mechanism, but a person whom he calls the ""computer"", who executes these deterministic mechanical rules slavishly (or as Turing puts it, ""in a desultory manner"").
","The Turing machine mathematically models a machine that mechanically operates on a tape. On this tape are symbols, which the machine can read and write, one at a time, using a tape head.","[' What machine model a machine that mechanically operates on a tape?', ' What can the machine read and write, one at a time?']","['Turing machine', 'symbols']"
694,turing machine,Description,"In the 4-tuple models, erasing or writing a symbol (aj1) and moving the head left or right (dk) are specified as separate instructions. The table tells the machine to (ia) erase or write a symbol or (ib) move the head left or right, and then (ii) assume the same or a new state as prescribed, but not both actions (ia) and (ib) in the same instruction. In some models, if there is no entry in the table for the current combination of symbol and state, then the machine will halt; other models require all entries to be filled.
","In the 4-tuple models, erasing or writing a symbol (aj1) and moving the head left or right (dk) are specified as separate instructions. The table tells the machine to (ia) erase or write a symbol or (ib) move the head left or right, and then (ii) assume the same or a new state as prescribed, but not both actions (ia) and (ib) in the same instruction.","[' In the 4-tuple models, erasing or writing a symbol (aj1) and moving the head left or right (dk) are specified as what?', ' The table tells the machine to (ia) erase or write a sign or (ib) move the head right or what else?', ' What is the difference between assuming the same or a new state as prescribed?']","['separate instructions', 'left or right', 'not both actions (ia) and (ib) in the same instruction']"
695,turing machine,Description,"Every part of the machine (i.e. its state, symbol-collections, and used tape at any given time) and its actions (such as printing, erasing and tape motion) is finite, discrete and distinguishable; it is the unlimited amount of tape and runtime that gives it an unbounded amount of storage space.
","Every part of the machine (i.e. its state, symbol-collections, and used tape at any given time) and its actions (such as printing, erasing and tape motion) is finite, discrete and distinguishable; it is the unlimited amount of tape and runtime that gives it an unbounded amount of storage space.","[' What are some of the actions of a tape machine?', ' What gives a machine an unbounded amount of storage space?']","['printing, erasing and tape motion', 'unlimited amount of tape and runtime']"
696,turing machine,Formal definition,"In addition, the Turing machine can also have a reject state to make rejection more explicit. In that case there are three possibilities: accepting, rejecting, and running forever. Another possibility is to regard the final values on the tape as the output. However, if the only output is the final state the machine ends up in (or never halting), the machine can still effectively output a longer string by taking in an integer that tells it which bit of the string to output.
","In addition, the Turing machine can also have a reject state to make rejection more explicit. In that case there are three possibilities: accepting, rejecting, and running forever.","[' What can the Turing machine have to make rejection more explicit?', ' How many possibilities are there in a reject state?']","['reject state', 'three']"
697,turing machine,Equivalent models,"Many machines that might be thought to have more computational capability than a simple universal Turing machine can be shown to have no more power (Hopcroft and Ullman p. 159, cf. Minsky (1967)). They might compute faster, perhaps, or use less memory, or their instruction set might be smaller, but they cannot compute more powerfully (i.e. more mathematical functions). (The Church–Turing thesis hypothesizes this to be true for any kind of machine: that anything that can be ""computed"" can be computed by some Turing machine.)
","Many machines that might be thought to have more computational capability than a simple universal Turing machine can be shown to have no more power (Hopcroft and Ullman p. 159, cf. Minsky (1967)).",[' What can be shown to have no more power than a simple universal Turing machine?'],['Many machines']
698,turing machine,Equivalent models,"A Turing machine is equivalent to a single-stack pushdown automaton (PDA) that has been made more flexible and concise by relaxing the last-in-first-out (LIFO) requirement of its stack. In addition, a Turing machine is also equivalent to a two-stack PDA with standard LIFO  semantics, by using one stack to model the tape left of the head and the other stack for the tape to the right.
","A Turing machine is equivalent to a single-stack pushdown automaton (PDA) that has been made more flexible and concise by relaxing the last-in-first-out (LIFO) requirement of its stack. In addition, a Turing machine is also equivalent to a two-stack PDA with standard LIFO  semantics, by using one stack to model the tape left of the head and the other stack for the tape to the right.","[' What is a Turing machine equivalent to?', ' What has been made more flexible and concise by relaxing the last-in-first-out requirement of its stack?', ' What does standard LIFO semantics use to model the tape left of the head?', ' What does the other stack do to the tape to the right?']","['a single-stack pushdown automaton (PDA)', 'PDA)', 'one stack', 'model']"
699,turing machine,Equivalent models,"At the other extreme, some very simple models turn out to be Turing-equivalent, i.e. to have the same computational power as the Turing machine model.
","At the other extreme, some very simple models turn out to be Turing-equivalent, i.e. to have the same computational power as the Turing machine model.",[' What is the term for a model that has the same computational power as a Turing machine model?'],['Turing-equivalent']
700,turing machine,Equivalent models,"A relevant question is whether or not the computation model represented by concrete programming languages is Turing equivalent. While the computation of a real computer is based on finite states and thus not capable to simulate a Turing machine, programming languages themselves do not necessarily have this limitation. Kirner et al., 2009 have shown that among the general-purpose programming languages some are Turing complete while others are not. For example, ANSI C is not Turing-equivalent, as all instantiations of ANSI C (different instantiations are possible as the standard deliberately leaves certain behaviour undefined for legacy reasons) imply a finite-space memory. This is because the size of memory reference data types, called pointers, is accessible inside the language. However, other programming languages like Pascal do not have this feature, which allows them to be Turing complete in principle.
It is just Turing complete in principle, as memory allocation in a programming language is allowed to fail, which means the programming language can be Turing complete when ignoring failed memory allocations, but the compiled programs executable on a real computer cannot.
","A relevant question is whether or not the computation model represented by concrete programming languages is Turing equivalent. While the computation of a real computer is based on finite states and thus not capable to simulate a Turing machine, programming languages themselves do not necessarily have this limitation.","[' What is the computation model represented by concrete programming languages?', ' The computation of a real computer is based on what?']","['Turing equivalent', 'finite states']"
701,turing machine,"Choice c-machines, oracle o-machines","...whose motion is only partially determined by the configuration ... When such a machine reaches one of these ambiguous configurations, it cannot go on until some arbitrary choice has been made by an external operator. This would be the case if we were using machines to deal with axiomatic systems.","...whose motion is only partially determined by the configuration ... When such a machine reaches one of these ambiguous configurations, it cannot go on until some arbitrary choice has been made by an external operator.","[' What is only partially determined by the configuration?', ' When a machine reaches a ambiguous configuration, it cannot go on until some arbitrary choice has been made by an external operator?']","['motion', '...whose motion is only partially determined by the configuration']"
702,turing machine,"Choice c-machines, oracle o-machines","Turing (1936) does not elaborate further except in a footnote in which he describes how to use an a-machine to ""find all the provable formulae of the [Hilbert] calculus"" rather than use a choice machine. He ""suppose[s] that the choices are always between two possibilities 0 and 1. Each proof will then be determined by a sequence of choices i1, i2, ..., in (i1 = 0 or 1, i2 = 0 or 1, ..., in = 0 or 1), and hence the number 2n + i12n-1 + i22n-2 + ... +in completely determines the proof. The automatic machine carries out successively proof 1, proof 2, proof 3, ..."" (Footnote ‡, The Undecidable, p. 138)
","Turing (1936) does not elaborate further except in a footnote in which he describes how to use an a-machine to ""find all the provable formulae of the [Hilbert] calculus"" rather than use a choice machine. He ""suppose[s] that the choices are always between two possibilities 0 and 1.","[' What does Turing describe in a footnote in 1936?', ' What did Turing use instead of a choice machine to find all the provable formulae of the Hilbert calculus?']","['how to use an a-machine to ""find all the provable formulae of the [Hilbert] calculus""', 'a-machine']"
703,turing machine,Universal Turing machines,"It is possible to invent a single machine which can be used to compute any computable sequence. If this machine U is supplied with the tape on the beginning of which is written the string of quintuples separated by semicolons of some computing machine M, then U will compute the same sequence as M.","It is possible to invent a single machine which can be used to compute any computable sequence. If this machine U is supplied with the tape on the beginning of which is written the string of quintuples separated by semicolons of some computing machine M, then U will compute the same sequence as M.","[' What is it possible to invent a single machine which can be used to compute any computable sequence?', ' What is the tape on the beginning of which is written the string of quintuples separated by?', ' What is the name of the computing machine that computes the same sequence as M?']","['If this machine U is supplied with the tape on the beginning of which is written the string of quintuples separated by semicolons of some computing machine M', 'semicolons', 'U']"
704,turing machine,Universal Turing machines,"This finding is now taken for granted, but at the time (1936) it was considered astonishing. The model of computation that Turing called his ""universal machine""—""U"" for short—is considered by some (cf. Davis (2000)) to have been the fundamental theoretical breakthrough that led to the notion of the stored-program computer.
","This finding is now taken for granted, but at the time (1936) it was considered astonishing. The model of computation that Turing called his ""universal machine""—""U"" for short—is considered by some (cf.","["" What was the name of Turing's model of computation?"", ' What did Turing call his ""universal machine""?']","['universal machine', 'model of computation']"
705,turing machine,Universal Turing machines,"In terms of computational complexity, a multi-tape universal Turing machine need only be slower by logarithmic factor compared to the machines it simulates. This result was obtained in 1966 by F. C. Hennie and R. E. Stearns. (Arora and Barak, 2009, theorem 1.9)
","In terms of computational complexity, a multi-tape universal Turing machine need only be slower by logarithmic factor compared to the machines it simulates. This result was obtained in 1966 by F. C. Hennie and R. E. Stearns.","[' In terms of computational complexity, a multi-tape universal Turing machine needs only be slower by what factor?', ' In 1966, F. C. Hennie and R. E. Stearns obtained what result?']","['logarithmic', 'a multi-tape universal Turing machine']"
706,turing machine,Comparison with real machines,"It is often believed that Turing machines, unlike simpler automata, are as powerful as real machines, and are able to execute any operation that a real program can. What is neglected in this statement is that, because a real machine can only have a finite number of configurations, it is nothing but a finite-state machine, whereas a Turing machine has an unlimited amount of storage space available for its computations.
","It is often believed that Turing machines, unlike simpler automata, are as powerful as real machines, and are able to execute any operation that a real program can. What is neglected in this statement is that, because a real machine can only have a finite number of configurations, it is nothing but a finite-state machine, whereas a Turing machine has an unlimited amount of storage space available for its computations.","[' Turing machines are not as powerful as what?', ' What is a Turing machine not able to do?', ' A real machine can only have how many configurations?', ' What is a finite-state machine?', ' What has an unlimited amount of storage space available for its computations?']","['simpler automata', 'computations', 'finite number', 'a real machine', 'Turing machine']"
707,access control,Summary,"In the fields of physical security and information security, access control (AC) is the selective restriction of access to a place or other resource, while access management describes the process. The act of accessing may mean consuming, entering, or using. Permission to access a resource is called authorization.
","In the fields of physical security and information security, access control (AC) is the selective restriction of access to a place or other resource, while access management describes the process. The act of accessing may mean consuming, entering, or using.","[' In the fields of physical security and information security, what is the selective restriction of access to a place or other resource?', ' What describes the process of accessing?']","['access control', 'access management']"
708,access control,Physical security,"Geographical access control may be enforced by personnel (e.g. border guard, bouncer, ticket checker), or with a device such as a turnstile. There may be fences to avoid circumventing this access control. An alternative of access control in the strict sense (physically controlling access itself) is a system of checking authorized presence, see e.g. Ticket controller (transportation). A variant is exit control, e.g. of a shop (checkout) or a country.","Geographical access control may be enforced by personnel (e.g. border guard, bouncer, ticket checker), or with a device such as a turnstile.","[' Who enforces geographic access control?', ' What type of device is used to enforce geographic access?']","['personnel', 'a turnstile']"
709,access control,Physical security,"The term access control refers to the practice of restricting entrance to a property, a building, or a room to authorized persons. Physical access control can be achieved by a human (a guard, bouncer, or receptionist), through mechanical means such as locks and keys, or through technological means such as access control systems like the mantrap. Within these environments, physical key management may also be employed as a means of further managing and monitoring access to mechanically keyed areas or access to certain small assets.","The term access control refers to the practice of restricting entrance to a property, a building, or a room to authorized persons. Physical access control can be achieved by a human (a guard, bouncer, or receptionist), through mechanical means such as locks and keys, or through technological means such as access control systems like the mantrap.","[' What term refers to the practice of restricting entrance to a property, a building, or a room to authorized persons?', ' What can be achieved by a human?', ' How can physical access control be achieved?', ' What is an example of a technology that can be used to control a vehicle?']","['access control', 'Physical access control', 'by a human', 'the mantrap']"
710,access control,Physical security,"Physical access control is a matter of who, where, and when. An access control system determines who is allowed to enter or exit, where they are allowed to exit or enter, and when they are allowed to enter or exit. Historically, this was partially accomplished through keys and locks. When a door is locked, only someone with a key can enter through the door, depending on how the lock is configured. Mechanical locks and keys do not allow restriction of the key holder to specific times or dates. Mechanical locks and keys do not provide records of the key used on any specific door, and the keys can be easily copied or transferred to an unauthorized person. When a mechanical key is lost or the key holder is no longer authorized to use the protected area, the locks must be re-keyed.","Physical access control is a matter of who, where, and when. An access control system determines who is allowed to enter or exit, where they are allowed to exit or enter, and when they are allowed to enter or exit.","[' Physical access control is a matter of what?', ' An access control system determines who is allowed to enter or exit, where they are allowed to exit or enter, and when?']","['who, where, and when', 'when they are allowed to enter or exit']"
711,access control,Computer security,"In computer security, general access control includes authentication, authorization, and audit. A more narrow definition of access control would cover only access approval, whereby the system makes a decision to grant or reject an access request from an already authenticated subject, based on what the subject is authorized to access. Authentication and access control are often combined into a single operation, so that access is approved based on successful authentication, or based on an anonymous access token. Authentication methods and tokens include passwords, biometric analysis, physical keys, electronic keys and devices, hidden paths, social barriers, and monitoring by humans and automated systems.","In computer security, general access control includes authentication, authorization, and audit. A more narrow definition of access control would cover only access approval, whereby the system makes a decision to grant or reject an access request from an already authenticated subject, based on what the subject is authorized to access.","[' In computer security, what includes authentication, authorization, and audit?', ' A more narrow definition of access control would cover only what?', ' What does access approval do?']","['general access control', 'access approval', 'the system makes a decision to grant or reject an access request from an already authenticated subject']"
712,access control,Computer security,"In any access-control model, the entities that can perform actions on the system are called subjects, and the entities representing resources to which access may need to be controlled are called objects (see also Access Control Matrix). Subjects and objects should both be considered as software entities, rather than as human users: any human users can only have an effect on the system via the software entities that they control.","In any access-control model, the entities that can perform actions on the system are called subjects, and the entities representing resources to which access may need to be controlled are called objects (see also Access Control Matrix). Subjects and objects should both be considered as software entities, rather than as human users: any human users can only have an effect on the system via the software entities that they control.","[' What are the entities that can perform actions on a system called?', ' The entities that represent resources to which access may need to be controlled are called what?', ' What should both subjects and objects be considered as?', ' What should both humans and software entities be considered as?', ' Who can only have an effect on the system via the software entities that they control?']","['subjects', 'objects', 'software entities', 'Subjects and objects should both be considered as software entities, rather than as human users', 'any human users']"
713,mobile agent,Summary,"In computer science, a mobile agent is a composition of computer software and data that is able to migrate (move) from one computer to another autonomously and continue its execution on the destination computer. In reality, the mobile agent is the code/object on the move which travels in its itinerary within the network of connected nodes.
","In computer science, a mobile agent is a composition of computer software and data that is able to migrate (move) from one computer to another autonomously and continue its execution on the destination computer. In reality, the mobile agent is the code/object on the move which travels in its itinerary within the network of connected nodes.","[' In computer science, what is a mobile agent?', ' What is the code/object on the move?', ' The code/object on the move travels in its itinerary within what network of connected nodes?']","['a composition of computer software and data', 'the mobile agent', 'mobile agent']"
714,mobile agent,Definition and overview,"More specifically, a mobile agent is a process that can transport its state from one environment to another, with its data intact, and be capable of performing appropriately in the new environment. Mobile agents decide when and where to move.  Movement is often evolved from RPC methods.  Just as a user directs an Internet browser to ""visit"" a website (the browser merely downloads a copy of the site, or one version of it in the case of dynamic web sites), a mobile agent accomplishes a move through data duplication. When a mobile agent decides to move, it saves its own state (process image), transports this saved state to the new host and resumes execution from the saved state.
","More specifically, a mobile agent is a process that can transport its state from one environment to another, with its data intact, and be capable of performing appropriately in the new environment. Mobile agents decide when and where to move.","[' What is a process that can transport its state from one environment to another?', ' What does a mobile agent decide when and where to move?']","['a mobile agent', 'transport its state from one environment to another, with its data intact, and be capable of performing appropriately in the new environment']"
715,mobile agent,Definition and overview,"A mobile agent is a specific form of mobile code, within the field of code mobility. However, in contrast to the remote evaluation and code on demand programming paradigms, mobile agents are active in that they can choose to migrate between computers at any time during their execution. This makes them a powerful tool for implementing distributed applications in a computer network.
","A mobile agent is a specific form of mobile code, within the field of code mobility. However, in contrast to the remote evaluation and code on demand programming paradigms, mobile agents are active in that they can choose to migrate between computers at any time during their execution.","[' What is a specific form of mobile code?', ' What is the field of code mobility?', ' Where can mobile agents migrate?']","['A mobile agent', 'mobile agent', 'between computers']"
716,mobile agent,Definition and overview,"There are two types of mobile agents. The classification is based on their migration path.
",There are two types of mobile agents. The classification is based on their migration path.,"[' How many types of mobile agents are there?', ' What is the classification based on?']","['two', 'their migration path']"
717,social media,Summary,"Social media are interactive technologies and digital channels that facilitate the creation and sharing of information, ideas, interests, and other forms of expression through virtual communities and networks. While challenges to the definition of social media arise due to the variety of stand-alone and built-in social media services currently available, there are some common features:","Social media are interactive technologies and digital channels that facilitate the creation and sharing of information, ideas, interests, and other forms of expression through virtual communities and networks. While challenges to the definition of social media arise due to the variety of stand-alone and built-in social media services currently available, there are some common features:","[' What are interactive technologies and digital channels that facilitate the creation and sharing of information, ideas, interests, and other forms of expression through virtual communities and networks?', ' What are some common features of stand-alone social media services?']","['Social media', 'built-in']"
718,social media,Summary,"Users usually access social media services through web-based apps on desktops or download services that offer social media functionality to their mobile devices (e.g., smartphones and tablets). As users engage with these electronic services, they create highly interactive platforms which individuals, communities, and organizations can share, co-create, discuss, participate, and modify user-generated or self-curated content posted online. Additionally, social media are used to document memories; learn about and explore things; advertise oneself; and form friendships along with the growth of ideas from the creation of blogs, podcasts, videos, and gaming sites. This changing relationship between humans and technology is the focus of the emerging field of technological self-studies. Some of the most popular social media websites, with more than 100 million registered users, include Facebook (and its associated Facebook Messenger), TikTok, WeChat, Instagram, QZone, Weibo, Twitter, Tumblr, Baidu Tieba, and LinkedIn.  Depending on interpretation, other popular platforms that are sometimes 
referred to as social media services include YouTube, QQ, Quora, Telegram, WhatsApp, Signal, LINE, Snapchat, Pinterest, Viber, Reddit, Discord, VK, Microsoft Teams, and more. Wikis are examples of collaborative content creation.
","Users usually access social media services through web-based apps on desktops or download services that offer social media functionality to their mobile devices (e.g., smartphones and tablets). As users engage with these electronic services, they create highly interactive platforms which individuals, communities, and organizations can share, co-create, discuss, participate, and modify user-generated or self-curated content posted online.","[' What do users typically access social media services through?', ' What are two examples of mobile devices that offer social media functionality to their mobile devices?', ' Who can share, co-create, discuss, participate, and modify user-generated or self-curated content posted online?']","['web-based apps', 'smartphones and tablets', 'individuals, communities, and organizations']"
719,social media,Summary,"Many social media outlets differ from traditional media (e.g., print magazines and newspapers, TV, and radio broadcasting) in many ways, including quality, reach, frequency, usability, relevancy, and permanence. Additionally, social media outlets operate in a dialogic transmission system, i.e., many sources to many receivers, while traditional media outlets operate under a monologic transmission model (i.e., one source to many receivers). For instance, a newspaper is delivered to many subscribers and a radio station broadcasts the same programs to an entire city.","Many social media outlets differ from traditional media (e.g., print magazines and newspapers, TV, and radio broadcasting) in many ways, including quality, reach, frequency, usability, relevancy, and permanence. Additionally, social media outlets operate in a dialogic transmission system, i.e., many sources to many receivers, while traditional media outlets operate under a monologic transmission model (i.e., one source to many receivers).","[' What type of transmission system do social media outlets operate in?', ' What is one of the main differences between social media and traditional media?', ' What type of transmission model do traditional media outlets operate under?', ' What is one source to many receivers?']","['dialogic', 'quality, reach, frequency, usability, relevancy, and permanence', 'monologic', 'monologic transmission model']"
720,social media,Summary,"Since the dramatic expansion of the Internet, digital media or digital rhetoric can be used to represent or identify a culture. Studying how the rhetoric that exists in the digital environment has become a crucial new process for many scholars.
","Since the dramatic expansion of the Internet, digital media or digital rhetoric can be used to represent or identify a culture. Studying how the rhetoric that exists in the digital environment has become a crucial new process for many scholars.","[' What can be used to represent or identify a culture?', ' What has become a crucial new process for many scholars?']","['digital media or digital rhetoric', 'Studying how the rhetoric that exists in the digital environment']"
721,social media,Summary,"Observers have noted a wide range of positive and negative impacts when it comes to the use of social media. Social media can help to improve an individual's sense of connectedness with real or online communities and can be an effective communication (or marketing) tool for corporations, entrepreneurs, non-profit organizations, advocacy groups, political parties, and governments. Observers have also seen that there has been a rise in social movements using social media as a tool for communicating and organizing in times of political unrest. Despite that much of the research focuses on the negative impact of social media use, social media, by connecting individuals to new ties and social network, is found to increase entrepreneurship and innovation, especially for those individuals who lack conventional information channels due to their lower socioeconomic background .
","Observers have noted a wide range of positive and negative impacts when it comes to the use of social media. Social media can help to improve an individual's sense of connectedness with real or online communities and can be an effective communication (or marketing) tool for corporations, entrepreneurs, non-profit organizations, advocacy groups, political parties, and governments.","["" What can social media help to improve an individual's sense of connectedness with real or online communities?"", ' Social media can be an effective communication tool for corporations, entrepreneurs, and what else?', ' What is an effective communication tool for corporations, entrepreneurs, non-profit organizations, advocacy groups, political parties, and governments?']","['can be an effective communication (or marketing) tool', 'non-profit organizations, advocacy groups, political parties, and governments', 'Social media']"
722,social media,Definition and features,"The idea that social media are defined simply by their ability to bring people together has been seen as too broad, as this would suggest that fundamentally different technologies like the telegraph and telephone are also social media. The terminology is unclear, with some early researchers referring to social media as social networks or social networking services in the mid 2000s. A more recent paper from 2015 reviewed the prominent literature in the area and identified four common features unique to then-current social media services:","The idea that social media are defined simply by their ability to bring people together has been seen as too broad, as this would suggest that fundamentally different technologies like the telegraph and telephone are also social media. The terminology is unclear, with some early researchers referring to social media as social networks or social networking services in the mid 2000s.","[' What has the idea that social media are defined simply by their ability to bring people together been seen as?', ' What would suggest that fundamentally different technologies like the telegraph and telephone are also social media?', ' What did some early researchers refer to social media as in the mid 2000s?']","['too broad', 'The idea that social media are defined simply by their ability to bring people together has been seen as too broad', 'social networks or social networking services']"
723,social media,Statistics on usage and membership,"According to Statista, it is estimated that, in 2020, there are around 3.6 billion people using social media around the globe; up from 3.4 billion in 2019. This number is expected to increase to 4.41 billion in 2025.","According to Statista, it is estimated that, in 2020, there are around 3.6 billion people using social media around the globe; up from 3.4 billion in 2019. This number is expected to increase to 4.41 billion in 2025.","[' What is the estimated number of people using social media in 2020?', ' How many people are expected to use social media by 2025 according to Statista?']","['3.6 billion', '4.41 billion']"
724,social media,"Criticism, debate and controversy<span id=""Criticisms""></span>","Criticisms of social media range from criticisms of the ease of use of specific platforms and their capabilities, disparity of information available, issues with trustworthiness and reliability of information presented, the impact of social media use on an individual's concentration, ownership of media content, and the meaning of interactions created by social media. Although some social media platforms, such as servers in the decentralized Fediverse, offer users the opportunity to cross-post between independently run servers using a standard protocol such as ActivityPub, the dominant social network platforms have been criticized for poor interoperability between platforms, which leads to the creation of information silos, viz. isolated pockets of data contained in one social media platform. However, it is also argued that social media has positive effects, such as allowing the democratization of the Internet while also allowing individuals to advertise themselves and form friendships. Others have noted that the term ""social"" cannot account for technological features of a platform alone, hence the level of sociability should be determined by the actual performances of its users. There has been a dramatic decrease in face-to-face interactions as more and more social media platforms have been introduced with the threat of cyber-bullying and online sexual predators including groomers being more prevalent. Social media may expose children to images of alcohol, tobacco, and sexual behaviors. In regards to cyber-bullying, it has been proven that individuals who have no experience with cyber-bullying often have a better well-being than individuals who have been bullied online.","Criticisms of social media range from criticisms of the ease of use of specific platforms and their capabilities, disparity of information available, issues with trustworthiness and reliability of information presented, the impact of social media use on an individual's concentration, ownership of media content, and the meaning of interactions created by social media. Although some social media platforms, such as servers in the decentralized Fediverse, offer users the opportunity to cross-post between independently run servers using a standard protocol such as ActivityPub, the dominant social network platforms have been criticized for poor interoperability between platforms, which leads to the creation of information silos, viz.","[' What are some of the criticisms of social media?', ' What is an issue with trustworthiness and reliability of information presented?', "" How does social media affect an individual's concentration?"", ' What is the name of the decentralized social media platform?', ' What does ActivityPub stand for?', ' Which social network platforms have been criticized?', ' What social network platform has been criticized for poor interoperability between platforms?', ' What kind of silos have been created?']","['criticisms of the ease of use of specific platforms and their capabilities, disparity of information available, issues with trustworthiness and reliability of information presented', 'Criticisms of social media', 'the impact', 'Fediverse', 'standard protocol', 'dominant', 'dominant', 'information']"
725,social media,"Criticism, debate and controversy<span id=""Criticisms""></span>","Twitter is increasingly a target of heavy activity of marketers. Their actions focused on gaining massive numbers of followers, include use of advanced scripts and manipulation techniques that distort the prime idea of social media by abusing human trustfulness.  British-American entrepreneur and author Andrew Keen criticized social media in his 2007 book The Cult of the Amateur, writing, ""Out of this anarchy, it suddenly became clear that what was governing the infinite monkeys now inputting away on the Internet was the law of digital Darwinism, the survival of the loudest and most opinionated. Under these rules, the only way to intellectually prevail is by infinite filibustering.""
This is also relative to the issue ""justice"" in the social network. For example, the phenomenon ""Human flesh search engine"" in Asia raised the discussion of ""private-law"" brought by social network platform. Comparative media professor José van Dijck contends in her book The Culture of Connectivity (2013) that to understand the full weight of social media, their technological dimensions should be connected to the social and the cultural. She critically describes six social media platforms. One of her findings is the way Facebook had been successful in framing the term 'sharing' in such a way that third party use of user data is neglected in favor of intra-user connectedness.
","Twitter is increasingly a target of heavy activity of marketers. Their actions focused on gaining massive numbers of followers, include use of advanced scripts and manipulation techniques that distort the prime idea of social media by abusing human trustfulness.","[' What is increasingly a target of heavy activity of marketers?', ' What are advanced scripts and manipulation techniques that distort the prime idea of social media?']","['Twitter', 'abusing human trustfulness']"
726,social media,Deceased users,"Social media content, like most content on the web, will continue to persist unless the user deletes it. This brings up the inevitable question of what to do once a social media user dies, and no longer has access to their content. As it is a topic that is often left undiscussed, it is important to note that each social media platform, e.g., Twitter, Facebook, Instagram, LinkedIn, and Pinterest, has created its own guidelines for users who have died. In most cases on social media, the platforms require a next-of-kin to prove that the user is deceased, and then give them the option of closing the account or maintaining it in a 'legacy' status. Ultimately, social media users should make decisions about what happens to their social media accounts before they pass, and make sure their instructions are passed on to their next-of-kin.
","Social media content, like most content on the web, will continue to persist unless the user deletes it. This brings up the inevitable question of what to do once a social media user dies, and no longer has access to their content.","[' What happens to most content on the web unless a user deletes it?', ' What is the inevitable question of what to do once a social media user dies?']","['continue to persist', 'Social media content']"
727,regular language,Summary,"Alternatively, a regular language can be defined as a language recognized by a finite automaton. The equivalence of regular expressions and finite automata is known as Kleene's theorem (after American mathematician Stephen Cole Kleene). In the Chomsky hierarchy, regular languages are the languages generated by Type-3 grammars.
","Alternatively, a regular language can be defined as a language recognized by a finite automaton. The equivalence of regular expressions and finite automata is known as Kleene's theorem (after American mathematician Stephen Cole Kleene).","[' What is the equivalence of regular expressions and finite automata known as?', "" What is Kleene's theorem named after?""]","[""Kleene's theorem"", 'Stephen Cole Kleene']"
728,regular language,Examples,"All finite languages are regular; in particular the empty string language {ε} = Ø* is regular. Other typical examples include the language consisting of all strings over the alphabet {a, b} which contain an even number of as, or the language consisting of all strings of the form: several as followed by several bs.
","All finite languages are regular; in particular the empty string language {ε} = Ø* is regular. Other typical examples include the language consisting of all strings over the alphabet {a, b} which contain an even number of as, or the language consisting of all strings of the form: several as followed by several bs.","[' All finite languages are what?', ' The language consisting of all strings over the alphabet <unk>a, b<unk> contain an even number of as?', ' What is a language consisting of all strings of the form?']","['regular', '{', 'the language consisting of all strings over the alphabet']"
729,regular language,Examples,"A simple example of a language that is not regular is the set of strings { anbn | n ≥ 0 }. Intuitively, it cannot be recognized with a finite automaton, since a finite automaton has finite memory and it cannot remember the exact number of a's. Techniques to prove this fact rigorously are given below.
","A simple example of a language that is not regular is the set of strings { anbn | n ≥ 0 }. Intuitively, it cannot be recognized with a finite automaton, since a finite automaton has finite memory and it cannot remember the exact number of a's.","[' What is a simple example of a language that is not regular?', ' A finite automaton has how much memory?']","['the set of strings', 'finite']"
730,regular language,Equivalent formalisms,"Properties 10. and 11. are purely algebraic approaches to define regular languages; a similar set of statements can be formulated for a monoid M ⊆ Σ*. In this case, equivalence over M leads to the concept of a recognizable language.
","Properties 10. and 11. are purely algebraic approaches to define regular languages; a similar set of statements can be formulated for a monoid M ⊆ Σ*. In this case, equivalence over M leads to the concept of a recognizable language.","[' What are properties 10 and 11 purely algebraic approaches to define?', ' A similar set of statements can be formulated for what?', ' Equivalence over what leads to the concept of a recognizable language?']","['regular languages', 'a monoid M', 'M']"
731,regular language,Equivalent formalisms,"Some authors use one of the above properties different from ""1."" as an alternative definition of regular languages.
","Some authors use one of the above properties different from ""1."" as an alternative definition of regular languages.","[' Some authors use one of the properties different from ""1."" as an alternative definition of what?']",['regular languages']
732,regular language,Equivalent formalisms,"Some of the equivalences above, particularly those among the first four formalisms, are called Kleene's theorem in textbooks. Precisely which one (or which subset) is called such varies between authors. One textbook calls the equivalence of regular expressions and NFAs (""1."" and ""2."" above) ""Kleene's theorem"". Another textbook calls the equivalence of regular expressions and DFAs (""1."" and ""3."" above) ""Kleene's theorem"". Two other textbooks first prove the expressive equivalence of NFAs and DFAs (""2."" and ""3."") and then state ""Kleene's theorem"" as the equivalence between regular expressions and finite automata (the latter said to describe ""recognizable languages""). A linguistically oriented text first equates regular grammars (""4."" above) with DFAs and NFAs, calls the languages generated by (any of) these ""regular"", after which it introduces regular expressions which it terms to describe ""rational languages"", and finally states ""Kleene's theorem"" as the coincidence of regular and rational languages. Other authors simply define ""rational expression"" and ""regular expressions"" as synonymous and do the same with ""rational languages"" and ""regular languages"".","Some of the equivalences above, particularly those among the first four formalisms, are called Kleene's theorem in textbooks. Precisely which one (or which subset) is called such varies between authors.","[' What are some of the equivalences among the first four formalisms called?', "" In textbooks, which one is called Kleene's theorem?""]","[""Kleene's theorem"", 'equivalences above, particularly those among the first four formalisms']"
733,regular language,Equivalent formalisms,"Apparently, the term ""regular"" originates from a 1951 technical report where Kleene introduced ""regular events"" and explicitly welcomed ""any suggestions as to a more descriptive term"". Noam Chomsky, in his 1959 seminal article, used the term ""regular"" in a different meaning at first (referring to what is called ""Chomsky normal form"" today), but noticed that his ""finite state languages"" were equivalent to Kleene's ""regular events"".","Apparently, the term ""regular"" originates from a 1951 technical report where Kleene introduced ""regular events"" and explicitly welcomed ""any suggestions as to a more descriptive term"". Noam Chomsky, in his 1959 seminal article, used the term ""regular"" in a different meaning at first (referring to what is called ""Chomsky normal form"" today), but noticed that his ""finite state languages"" were equivalent to Kleene's ""regular events"".","[' What year did Kleene introduce ""regular events""?', ' Who used the term regular in a different meaning at first?', ' What is the term ""Chomsky normal"" referring to?', ' What was ""Chomsky normal form"" today called?', ' What was Kleene\'s ""finite state languages"" equivalent to?']","['1951', 'Noam Chomsky', 'form', 'finite state languages', 'regular events']"
734,regular language,Decidability properties,"Given two deterministic finite automata A and B, it is decidable whether they accept the same language.
As a consequence, using the above closure properties, the following problems are also decidable for arbitrarily given deterministic finite automata A and B, with accepted languages LA and LB, respectively:
","Given two deterministic finite automata A and B, it is decidable whether they accept the same language. As a consequence, using the above closure properties, the following problems are also decidable for arbitrarily given deterministic finite automata A and B, with accepted languages LA and LB, respectively:",[' What is it decidable if two deterministic finite automata A and B accept the same language?'],['the following problems']
735,regular language,Decidability properties,"For regular expressions, the universality problem is NP-complete already for a singleton alphabet.
For larger alphabets, that problem is PSPACE-complete. If regular expressions are extended to allow also a squaring operator, with ""A2"" denoting the same as ""AA"", still just regular languages can be described, but the universality problem has an exponential space lower bound, and is in fact complete for exponential space with respect to polynomial-time reduction.","For regular expressions, the universality problem is NP-complete already for a singleton alphabet. For larger alphabets, that problem is PSPACE-complete.","[' What is the universality problem for a singleton alphabet?', ' For larger alphabets, what is the problem?']","['NP-complete', 'PSPACE-complete']"
736,regular language,Complexity results,"In computational complexity theory, the complexity class of all regular languages is sometimes referred to as REGULAR or REG and equals DSPACE(O(1)), the decision problems that can be solved in constant space (the space used is independent of the input size). REGULAR ≠ AC0, since it (trivially) contains the parity problem of determining whether the number of 1 bits in the input is even or odd and this problem is not in AC0. On the other hand, REGULAR does not contain AC0, because the nonregular language of palindromes, or the nonregular language 



{

0

n



1

n


:
n
∈

N

}


{\displaystyle \{0^{n}1^{n}:n\in \mathbb {N} \}}
 can both be recognized in AC0.","In computational complexity theory, the complexity class of all regular languages is sometimes referred to as REGULAR or REG and equals DSPACE(O(1)), the decision problems that can be solved in constant space (the space used is independent of the input size). REGULAR ≠ AC0, since it (trivially) contains the parity problem of determining whether the number of 1 bits in the input is even or odd and this problem is not in AC0.","[' In computational complexity theory, what is the complexity class of all regular languages sometimes referred to as?', ' What is the decision problems that can be solved in constant space?', ' REGULAR <unk> AC0 contains what?', ' What is the parity problem of determining whether the number of 1 bits in the input is even or odd?']","['REGULAR or REG', 'the space used is independent of the input size', 'the parity problem of determining whether the number of 1 bits in the input is even or odd', 'AC0']"
737,regular language,Complexity results,"If a language is not regular, it requires a machine with at least Ω(log log n) space to recognize (where n is the input size). In other words, DSPACE(o(log log n)) equals the class of regular languages. In practice, most nonregular problems are solved by machines taking at least logarithmic space.
","If a language is not regular, it requires a machine with at least Ω(log log n) space to recognize (where n is the input size). In other words, DSPACE(o(log log n)) equals the class of regular languages.","[' If a language is not regular, it requires a machine with at least what to recognize?', ' What is the input size?', ' DSPACE equals what class of regular languages?']","['Ω(log log n) space', 'n', 'language is not regular']"
738,regular language,"Location in the Chomsky hierarchy<span id=""Subclasses""></span>","To locate the regular languages in the Chomsky hierarchy, one notices that every regular language is context-free. The converse is not true: for example the language consisting of all strings having the same number of a's as b's is context-free but not regular. To prove that a language is not regular, one often uses the Myhill–Nerode theorem and the pumping lemma. Other approaches include using the closure properties of regular languages or quantifying Kolmogorov complexity.","To locate the regular languages in the Chomsky hierarchy, one notices that every regular language is context-free. The converse is not true: for example the language consisting of all strings having the same number of a's as b's is context-free but not regular.","[' What does one notice to locate the regular languages in the Chomsky hierarchy?', "" The language consisting of all strings having the same number of a's as b's is what?""]","['every regular language is context-free', 'context-free']"
739,regular language,The number of words in a regular language,"Let 




s

L


(
n
)


{\displaystyle s_{L}(n)}
 denote the number of words of length 



n


{\displaystyle n}
 in 



L


{\displaystyle L}
.  The ordinary generating function for L is the formal power series
","Let 




s

L


(
n
)


{\displaystyle s_{L}(n)}
 denote the number of words of length 



n


{\displaystyle n}
 in 



L


{\displaystyle L}
. The ordinary generating function for L is the formal power series","[' What denotes the number of words of length n in L <unk>displaystyle L?', ' The ordinary generating function for L is what?']","['L', 'the formal power series']"
740,regular language,The number of words in a regular language,"The generating function of a language L is a rational function if L is regular.  Hence for every regular language 



L


{\displaystyle L}
 the sequence 




s

L


(
n

)

n
≥
0




{\displaystyle s_{L}(n)_{n\geq 0}}
 is constant-recursive; that is, there exist an integer constant 




n

0




{\displaystyle n_{0}}
, complex constants 




λ

1


,

…
,


λ

k




{\displaystyle \lambda _{1},\,\ldots ,\,\lambda _{k}}
 and complex polynomials 




p

1


(
x
)
,

…
,


p

k


(
x
)


{\displaystyle p_{1}(x),\,\ldots ,\,p_{k}(x)}

such that for every 



n
≥

n

0




{\displaystyle n\geq n_{0}}
 the number 




s

L


(
n
)


{\displaystyle s_{L}(n)}
 of words of length 



n


{\displaystyle n}
 in 



L


{\displaystyle L}
 is





s

L


(
n
)
=

p

1


(
n
)

λ

1


n


+
⋯
+

p

k


(
n
)

λ

k


n




{\displaystyle s_{L}(n)=p_{1}(n)\lambda _{1}^{n}+\dotsb +p_{k}(n)\lambda _{k}^{n}}
.","The generating function of a language L is a rational function if L is regular. Hence for every regular language 



L


{\displaystyle L}
 the sequence 




s

L


(
n

)

n
≥
0




{\displaystyle s_{L}(n)_{n\geq 0}}
 is constant-recursive; that is, there exist an integer constant 




n

0




{\displaystyle n_{0}}
, complex constants 




λ

1


,

…
,


λ

k




{\displaystyle \lambda _{1},\,\ldots ,\,\lambda _{k}}
 and complex polynomials 




p

1


(
x
)
,

…
,


p

k


(
x
)


{\displaystyle p_{1}(x),\,\ldots ,\,p_{k}(x)}

such that for every 



n
≥

n

0




{\displaystyle n\geq n_{0}}
 the number 




s

L


(
n
)


{\displaystyle s_{L}(n)}
 of words of length 



n


{\displaystyle n}
 in 



L


{\displaystyle L}
 is





s

L


(
n
)
=

p

1


(
n
)

λ

1


n


+
⋯
+

p

k


(
n
)

λ

k


n




{\displaystyle s_{L}(n)=p_{1}(n)\lambda _{1}^{n}+\dotsb +p_{k}(n)\lambda _{k}^{n}}
.","[' The generating function of a language L is a rational function if what is regular?', ' For every regular language L <unk>displaystyle L<unk> the sequence s L ( n ) n is what?', ' How many polynomials does p 1 (x) have?', ' What is the number of words of length n?']","['L', 'constant-recursive', 's_{L}(n)=p_{1}(n)\\lambda _{1}^{n}+\\dotsb', 's_{L}(n)_{n\\geq 0']"
741,regular language,The number of words in a regular language,"Thus, non-regularity of certain languages 




L
′



{\displaystyle L'}
 can be proved by counting the words of a given length in





L
′



{\displaystyle L'}
. Consider, for example, the Dyck language of strings of balanced parentheses. The number of words of length 



2
n


{\displaystyle 2n}

in the Dyck language is equal to the Catalan number 




C

n


∼



4

n




n

3

/

2




π







{\displaystyle C_{n}\sim {\frac {4^{n}}{n^{3/2}{\sqrt {\pi }}}}}
, which is not of the form 



p
(
n
)

λ

n




{\displaystyle p(n)\lambda ^{n}}
,
witnessing the non-regularity of the Dyck language. Care must be taken since some of the eigenvalues 




λ

i




{\displaystyle \lambda _{i}}
 could have the same magnitude. For example, the number of words of length 



n


{\displaystyle n}
 in the language of all even binary words is not of the form 



p
(
n
)

λ

n




{\displaystyle p(n)\lambda ^{n}}
, but the number of words of even or odd length are of this form; the corresponding eigenvalues are 



2
,
−
2


{\displaystyle 2,-2}
. In general, for every regular language there exists a constant 



d


{\displaystyle d}
 such that for all 



a


{\displaystyle a}
, the number of words of length 



d
m
+
a


{\displaystyle dm+a}
 is asymptotically 




C

a



m


p

a





λ

a


m




{\displaystyle C_{a}m^{p_{a}}\lambda _{a}^{m}}
.","Thus, non-regularity of certain languages 




L
′



{\displaystyle L'}
 can be proved by counting the words of a given length in





L
′



{\displaystyle L'}
. Consider, for example, the Dyck language of strings of balanced parentheses.","["" What can be proved by counting the words of a given length in L ′ <unk>displaystyle L'<unk>?"", ' What language of strings of balanced parentheses is an example?']","['non-regularity', 'Dyck']"
742,regular language,Generalizations,"Rational set generalizes the notion (of regular/rational language) to monoids that are not necessarily free. Likewise, the notion of a recognizable language (by a finite automaton) has namesake as recognizable set over a monoid that is not necessarily free. Howard Straubing notes in relation to these facts that “The term ""regular language"" is a bit unfortunate. Papers influenced by Eilenberg's monograph often use either the term ""recognizable language"", which refers to the behavior of automata, or ""rational language"", which refers to important analogies between regular expressions and rational power series. (In fact, Eilenberg defines rational and recognizable subsets of arbitrary monoids; the two notions do not, in general, coincide.) This terminology, while better motivated, never really caught on, and ""regular language"" is used almost universally.”","Rational set generalizes the notion (of regular/rational language) to monoids that are not necessarily free. Likewise, the notion of a recognizable language (by a finite automaton) has namesake as recognizable set over a monoid that is not necessarily free.","[' Rational set generalizes the notion of regular/rational language to monoids that are not necessarily free.', ' What is the concept of a recognizable language by a finite automaton called?']","['Rational set generalizes the notion (of regular/rational language) to monoids that are not necessarily free. Likewise, the notion of a recognizable language', 'recognizable set']"
743,regular language,Generalizations,"Rational series is another generalization, this time in the context of a formal power series over a semiring. This approach gives rise to weighted rational expressions and weighted automata. In this algebraic context, the regular languages (corresponding to Boolean-weighted rational expressions) are usually called rational languages. Also in this context, Kleene's theorem finds a generalization called the Kleene-Schützenberger theorem.
","Rational series is another generalization, this time in the context of a formal power series over a semiring. This approach gives rise to weighted rational expressions and weighted automata.","[' Rational series is another generalization in the context of a formal power series over what?', ' What gives rise to weighted rational expressions?']","['a semiring', 'Rational series']"
744,semantic web,Summary,"The Semantic Web, sometimes known as Web 3.0, is an extension of the World Wide Web through standards set by the World Wide Web Consortium (W3C). The goal of the Semantic Web is to make Internet data machine-readable. 
","The Semantic Web, sometimes known as Web 3.0, is an extension of the World Wide Web through standards set by the World Wide Web Consortium (W3C). The goal of the Semantic Web is to make Internet data machine-readable.","[' What is the Semantic Web sometimes known as?', ' What is an extension of the World Wide Web through standards set by the W3C?']","['Web 3.0', 'The Semantic Web']"
745,semantic web,Summary,"To enable the encoding of semantics with the data, technologies such as Resource Description Framework (RDF) and Web Ontology Language (OWL) are used. These technologies are used to formally represent metadata. For example, ontology can describe concepts, relationships between entities, and categories of things. These embedded semantics offer significant advantages such as reasoning over data and operating with heterogeneous data sources.","To enable the encoding of semantics with the data, technologies such as Resource Description Framework (RDF) and Web Ontology Language (OWL) are used. These technologies are used to formally represent metadata.","[' What are two technologies used to enable the encoding of semantics with data?', ' What are Resource Description Framework (RDF) and Web Ontology Language (OWL) used for?']","['Resource Description Framework (RDF) and Web Ontology Language (OWL)', 'to formally represent metadata']"
746,semantic web,Summary,"These standards promote common data formats and exchange protocols on the Web, fundamentally the RDF. According to the W3C, ""The Semantic Web provides a common framework that allows data to be shared and reused across application, enterprise, and community boundaries."" The Semantic Web is therefore regarded as an integrator across different content and information applications and systems.
","These standards promote common data formats and exchange protocols on the Web, fundamentally the RDF. According to the W3C, ""The Semantic Web provides a common framework that allows data to be shared and reused across application, enterprise, and community boundaries.""","[' What does the W3C say the Semantic Web provides a common framework that allows data to be shared and reused across application, enterprise and community boundaries?']",['standards']
747,semantic web,Summary,"The term was coined by Tim Berners-Lee for a web of data (or data web) that can be processed by machines—that is, one in which much of the meaning is machine-readable. While its critics have questioned its feasibility, proponents argue that applications in library and information science, industry, biology and human sciences research have already proven the validity of the original concept.","The term was coined by Tim Berners-Lee for a web of data (or data web) that can be processed by machines—that is, one in which much of the meaning is machine-readable. While its critics have questioned its feasibility, proponents argue that applications in library and information science, industry, biology and human sciences research have already proven the validity of the original concept.","[' Who coined the term ""data web""?', ' What is a web of data that can be processed by machines?', ' Who was Tim Berners-Lee?', ' Applications in library and information science, industry, biology, and human sciences have already proven the validity of the original concept?']","['Tim Berners-Lee', 'data web', 'a web of data (or data web) that can be processed by machines', 'proponents']"
748,semantic web,Summary,"I have a dream for the Web [in which computers] become capable of analyzing all the data on the Web – the content, links, and transactions between people and computers. A ""Semantic Web"", which makes this possible, has yet to emerge, but when it does, the day-to-day mechanisms of trade, bureaucracy and our daily lives will be handled by machines talking to machines. The ""intelligent agents"" people have touted for ages will finally materialize.","I have a dream for the Web [in which computers] become capable of analyzing all the data on the Web – the content, links, and transactions between people and computers. A ""Semantic Web"", which makes this possible, has yet to emerge, but when it does, the day-to-day mechanisms of trade, bureaucracy and our daily lives will be handled by machines talking to machines.","[' What is my dream for the Web?', ' What makes the Web possible?', ' When does the Semantic Web emerge?', ' What are the day-to-day mechanisms of trade, bureaucracy and our daily lives handled by?']","['computers] become capable of analyzing all the data on the Web', 'Semantic Web', 'yet to emerge', 'machines talking to machines']"
749,semantic web,Summary,"The 2001 Scientific American article by Berners-Lee, Hendler, and Lassila described an expected evolution of the existing Web to a Semantic Web. In 2006, Berners-Lee and colleagues stated that: ""This simple idea…remains largely unrealized"".
In 2013, more than four million Web domains (out of roughly 250 million total) contained Semantic Web markup.","The 2001 Scientific American article by Berners-Lee, Hendler, and Lassila described an expected evolution of the existing Web to a Semantic Web. In 2006, Berners-Lee and colleagues stated that: ""This simple idea…remains largely unrealized"".","[' In what year did Berners-Lee, Hendler, and Lassila write an article about the evolution of the Web to a Semantic Web?', ' What did the article describe in 2001?', ' In 2006, who stated that this simple idea remains unrealized?']","['2001', 'an expected evolution of the existing Web to a Semantic Web', 'Berners-Lee and colleagues']"
750,semantic web,Example,"In the following example, the text ""Paul Schuster was born in Dresden"" on a website will be annotated, connecting a person with their place of birth. The following HTML fragment shows how a small graph is being described, in RDFa-syntax using a schema.org vocabulary and a Wikidata ID:
","In the following example, the text ""Paul Schuster was born in Dresden"" on a website will be annotated, connecting a person with their place of birth. The following HTML fragment shows how a small graph is being described, in RDFa-syntax using a schema.org vocabulary and a Wikidata ID:","[' Where was Paul Schuster born?', ' What is annotated on a website?', ' How is a small graph described?']","['Dresden', 'the text ""Paul Schuster was born in Dresden', 'in RDFa-syntax using a schema.org vocabulary and a Wikidata ID']"
751,semantic web,Example,"The example defines the following five triples (shown in Turtle syntax). Each triple represents one edge in the resulting graph: the first element of the triple (the subject) is the name of the node where the edge starts, the second element (the predicate) the type of the edge, and the last and third element (the object) either the name of the node where the edge ends or a literal value (e.g. a text, a number, etc.).
","The example defines the following five triples (shown in Turtle syntax). Each triple represents one edge in the resulting graph: the first element of the triple (the subject) is the name of the node where the edge starts, the second element (the predicate) the type of the edge, and the last and third element (the object) either the name of the node where the edge ends or a literal value (e.g.","[' How many triples does the example define?', ' How many edges does each triple represent in the resulting graph?', ' What is the name of the node where the edge starts?', ' What element is the type of the edge?', ' What is the last and third element?', ' The object is either the name of the node where the edge ends or what?']","['five', 'one', 'the first element', 'second', 'the object', 'a literal value']"
752,semantic web,Example,"One of the advantages of using Uniform Resource Identifiers (URIs) is that they can be dereferenced using the HTTP protocol. According to the so-called Linked Open Data principles, such a dereferenced URI should result in a document that offers further data about the given URI. In this example, all URIs, both for edges and nodes (e.g. http://schema.org/Person, http://schema.org/birthPlace, http://www.wikidata.org/entity/Q1731) can be dereferenced and will result in further RDF graphs, describing the URI, e.g. that Dresden is a city in Germany, or that a person, in the sense of that URI, can be fictional.
","One of the advantages of using Uniform Resource Identifiers (URIs) is that they can be dereferenced using the HTTP protocol. According to the so-called Linked Open Data principles, such a dereferenced URI should result in a document that offers further data about the given URI.","[' What is one of the advantages of using Uniform Resource Identifiers?', ' What can be dereferenced using the HTTP protocol?', ' According to what principles should a document that offers further data about the given URI result in?']","['they can be dereferenced using the HTTP protocol', 'Uniform Resource Identifiers', 'Linked Open Data']"
753,semantic web,Background,"The concept of the semantic network model was formed in the early 1960s by researchers such as the cognitive scientist Allan M. Collins, linguist M. Ross Quillian and psychologist Elizabeth F. Loftus as a form to represent semantically structured knowledge. When applied in the context of the modern internet, it extends the network of hyperlinked human-readable web pages by inserting machine-readable metadata about pages and how they are related to each other.  This enables automated agents to access the Web more intelligently and perform more tasks on behalf of users. The term ""Semantic Web"" was coined by Tim Berners-Lee, the inventor of the World Wide Web and director of the World Wide Web Consortium (""W3C""), which oversees the development of proposed Semantic Web standards. He defines the Semantic Web as ""a web of data that can be processed directly and indirectly by machines"".
","The concept of the semantic network model was formed in the early 1960s by researchers such as the cognitive scientist Allan M. Collins, linguist M. Ross Quillian and psychologist Elizabeth F. Loftus as a form to represent semantically structured knowledge. When applied in the context of the modern internet, it extends the network of hyperlinked human-readable web pages by inserting machine-readable metadata about pages and how they are related to each other.","[' In what decade was the concept of the semantic network model formed?', ' What was the term used by researchers to represent semantically structured knowledge?', ' Who was a linguist in the early 1960s?', ' When applied in the context of the modern internet, what extends the network of hyperlinked human-readable web pages?', ' What does machine-readable metadata about pages and how they are related to each other do?']","['1960s', 'semantic network model', 'M. Ross Quillian', 'semantic network model', 'extends the network of hyperlinked human-readable web pages']"
754,semantic web,Background,"Many of the technologies proposed by the W3C already existed before they were positioned under the W3C umbrella. These are used in various contexts, particularly those dealing with information that encompasses a limited and defined domain, and where sharing data is a common necessity, such as scientific research or data exchange among businesses. In addition, other technologies with similar goals have emerged, such as microformats.
","Many of the technologies proposed by the W3C already existed before they were positioned under the W3C umbrella. These are used in various contexts, particularly those dealing with information that encompasses a limited and defined domain, and where sharing data is a common necessity, such as scientific research or data exchange among businesses.","[' Many of the technologies proposed by the W3C already existed before they were positioned under what umbrella?', ' What is a common necessity when sharing data?', ' What is a common necessity, such as scientific research or data exchange among businesses?']","['W3C', 'scientific research or data exchange among businesses', 'sharing data']"
755,semantic web,Challenges,"Some of the challenges for the Semantic Web include vastness, vagueness, uncertainty, inconsistency, and deceit. Automated reasoning systems will have to deal with all of these issues in order to deliver on the promise of the Semantic Web.
","Some of the challenges for the Semantic Web include vastness, vagueness, uncertainty, inconsistency, and deceit. Automated reasoning systems will have to deal with all of these issues in order to deliver on the promise of the Semantic Web.","[' What are some of the challenges for the Semantic Web?', ' Automated reasoning systems will have to deal with all of these issues in order to deliver on what promise?']","['vastness, vagueness, uncertainty, inconsistency, and deceit', 'Semantic Web']"
756,semantic web,Challenges,"This list of challenges is illustrative rather than exhaustive, and it focuses on the challenges to the ""unifying logic"" and ""proof"" layers of the Semantic Web. The World Wide Web Consortium (W3C) Incubator Group for Uncertainty Reasoning for the World Wide Web (URW3-XG) final report lumps these problems together under the single heading of ""uncertainty"". Many of the techniques mentioned here will require extensions to the Web Ontology Language (OWL) for example to annotate conditional probabilities. This is an area of active research.","This list of challenges is illustrative rather than exhaustive, and it focuses on the challenges to the ""unifying logic"" and ""proof"" layers of the Semantic Web. The World Wide Web Consortium (W3C) Incubator Group for Uncertainty Reasoning for the World Wide Web (URW3-XG) final report lumps these problems together under the single heading of ""uncertainty"".","["" The W3C's Incubator Group for Uncertainty Reasoning for the World Wide Web lumps these problems together under what name?"", ' What is the name of the final report that lumps these problems together?']","['uncertainty', 'URW3-XG)']"
757,semantic web,Applications,"Such services could be useful to public search engines, or could be used for knowledge management within an organization.  Business applications include:
","Such services could be useful to public search engines, or could be used for knowledge management within an organization. Business applications include:","[' What could such services be useful to?', ' What could be used for knowledge management within an organization?']","['public search engines', 'public search engines']"
758,semantic web,Applications,"In a corporation, there is a closed group of users and the management is able to enforce company guidelines like the adoption of specific ontologies and use of semantic annotation. Compared to the public Semantic Web there are lesser requirements on scalability and the information circulating within a company can be more trusted in general; privacy is less of an issue outside of handling of customer data.
","In a corporation, there is a closed group of users and the management is able to enforce company guidelines like the adoption of specific ontologies and use of semantic annotation. Compared to the public Semantic Web there are lesser requirements on scalability and the information circulating within a company can be more trusted in general; privacy is less of an issue outside of handling of customer data.","[' What is a closed group of users in a corporation?', ' What are two examples of company guidelines?', ' What is less of an issue outside of handling customer data?']","['there is a closed group of users and the management is able to enforce company guidelines', 'adoption of specific ontologies and use of semantic annotation', 'privacy']"
759,semantic web,Research activities on corporate applications,"The first research group explicitly focusing on the Corporate Semantic Web was the ACACIA team at INRIA-Sophia-Antipolis, founded in 2002. Results of their work include the RDF(S) based Corese search engine, and the application of semantic web technology in the realm of distributed artificial intelligence for knowledge management (e.g. ontologies and multi-agent systems for corporate semantic Web)  and E-learning.","The first research group explicitly focusing on the Corporate Semantic Web was the ACACIA team at INRIA-Sophia-Antipolis, founded in 2002. Results of their work include the RDF(S) based Corese search engine, and the application of semantic web technology in the realm of distributed artificial intelligence for knowledge management (e.g.","[' When was the ACACIA team founded?', ' What was the name of the first research group specifically focusing on the Corporate Semantic Web?', ' Where was the INRIA-Sophia-Antipolis located?']","['2002', 'ACACIA team at INRIA-Sophia-Antipolis', '2002']"
760,nash equilibrium,Summary,"In game theory, the Nash equilibrium, named after the mathematician John Forbes Nash Jr., is the most common way to define the solution of a non-cooperative game involving two or more players. In a Nash equilibrium, each player is assumed to know the equilibrium strategies of the other players, and no player has anything to gain by changing only their own strategy. The principle of Nash equilibrium dates back to the time of Cournot, who in 1838 applied it to competing firms choosing outputs.","In game theory, the Nash equilibrium, named after the mathematician John Forbes Nash Jr., is the most common way to define the solution of a non-cooperative game involving two or more players. In a Nash equilibrium, each player is assumed to know the equilibrium strategies of the other players, and no player has anything to gain by changing only their own strategy.","[' Who is the Nash equilibrium named after?', ' What is the most common way to define the solution of a non-cooperative game involving two or more players?', ' To know the equilibrium strategies of other players, what does no player have anything to gain by changing only their own strategy?']","['John Forbes Nash Jr', 'the Nash equilibrium', 'Nash equilibrium, named after the mathematician John Forbes Nash Jr., is the most common way to define the solution of a non-cooperative game involving two or more players. In a Nash equilibrium']"
761,nash equilibrium,Summary,"If two players Alice and Bob choose strategies A and B, (A, B) is a Nash equilibrium if Alice has no other strategy available that does better than A at maximizing her payoff in response to Bob choosing B, and Bob has no other strategy available that does better than B at maximizing his payoff in response to Alice choosing A. In a game in which Carol and Dan are also players, (A, B, C, D) is a Nash equilibrium if A is Alice's best response to (B, C, D), B is Bob's best response to (A, C, D), and so forth.
","If two players Alice and Bob choose strategies A and B, (A, B) is a Nash equilibrium if Alice has no other strategy available that does better than A at maximizing her payoff in response to Bob choosing B, and Bob has no other strategy available that does better than B at maximizing his payoff in response to Alice choosing A. In a game in which Carol and Dan are also players, (A, B, C, D) is a Nash equilibrium if A is Alice's best response to (B, C, D), B is Bob's best response to (A, C, D), and so forth.","[' What is a Nash equilibrium if Alice has no other strategy available that does better than A at maximizing her payoff in response to Bob choosing B?', ' Bob has no other strategy available that does better than what?', "" What is a Nash equilibrium if A is Alice's best response to (B, C, D)?"", "" What is Alice's best response to (A, C, D), and B is Bob's?""]","['A, B)', 'B', '(A, B, C, D)', 'A']"
762,nash equilibrium,Applications,"Game theorists use Nash equilibrium to analyze the outcome of the strategic interaction of several decision makers. In a strategic interaction, the outcome for each decision-maker depends on the decisions of the others as well as their own. The simple insight underlying Nash's idea is that one cannot predict the choices of multiple decision makers if one analyzes those decisions in isolation. Instead, one must ask what each player would do taking into account what she/he expects the others to do. Nash equilibrium requires that their choices be consistent: no player wishes to undo their decision given what the others are deciding.
","Game theorists use Nash equilibrium to analyze the outcome of the strategic interaction of several decision makers. In a strategic interaction, the outcome for each decision-maker depends on the decisions of the others as well as their own.","[' What do game theorists use to analyze the outcome of the strategic interaction of several decision makers?', ' What depends on the decisions of the others as well as their own decisions in a strategic interaction?']","['Nash equilibrium', 'the outcome for each decision-maker']"
763,nash equilibrium,Applications,"The concept has been used to analyze hostile situations such as wars and arms races (see prisoner's dilemma), and also how conflict may be mitigated by repeated interaction (see tit-for-tat). It has also been used to study to what extent people with different preferences can cooperate (see battle of the sexes), and whether they will take risks to achieve a cooperative outcome (see stag hunt). It has been used to study the adoption of technical standards, and also the occurrence of bank runs and currency crises (see coordination game). Other applications include traffic flow (see Wardrop's principle), how to organize auctions (see auction theory), the outcome of efforts exerted by multiple parties in the education process, regulatory legislation such as environmental regulations (see tragedy of the commons), natural resource management, analysing strategies in marketing,  even penalty kicks in football (see matching pennies), energy systems, transportation systems, evacuation problems and wireless communications.","The concept has been used to analyze hostile situations such as wars and arms races (see prisoner's dilemma), and also how conflict may be mitigated by repeated interaction (see tit-for-tat). It has also been used to study to what extent people with different preferences can cooperate (see battle of the sexes), and whether they will take risks to achieve a cooperative outcome (see stag hunt).","[' What has been used to analyze hostile situations such as wars and arms races?', ' What may be mitigated by repeated interaction?', ' How can people with different preferences cooperate?', ' People with different preferences can cooperate (see battle of the sexes) and whether they will take risks to achieve a cooperative outcome (see stag hunt)']","['The concept', 'conflict', 'to what extent people with different preferences can cooperate (see battle of the sexes), and whether they will take risks to achieve a cooperative outcome', ""The concept has been used to analyze hostile situations such as wars and arms races (see prisoner's dilemma), and also how conflict may be mitigated by repeated interaction (see tit-for-tat). It has also been used to study to what extent""]"
764,nash equilibrium,History,"Nash equilibrium is named after American mathematician John Forbes Nash Jr. The same idea was used in a particular application in 1838 by Antoine Augustin Cournot in his theory of oligopoly. In Cournot's theory, each of several firms choose how much output to produce to maximize its profit. The best output for one firm depends on the outputs of the others. A Cournot equilibrium occurs when each firm's output maximizes its profits given the output of the other firms,  which is a pure-strategy Nash equilibrium. Cournot also introduced the concept of best response dynamics in his analysis of the stability of equilibrium. Cournot did not use the idea in any other applications, however, or define it generally.
",Nash equilibrium is named after American mathematician John Forbes Nash Jr. The same idea was used in a particular application in 1838 by Antoine Augustin Cournot in his theory of oligopoly.,"[' Who is the Nash equilibrium named after?', ' What American mathematician was John Forbes Nash Jr.?', "" In what year was Antoine Augustin Cournot's theory of oligopoly applied?""]","['John Forbes Nash Jr', 'Nash equilibrium', '1838']"
765,nash equilibrium,History,"The modern concept of Nash equilibrium is instead defined in terms of mixed strategies, where players choose a probability distribution over possible  pure strategies (which might put 100% of the probability on one pure strategy; such pure strategies are a subset of mixed strategies). The concept of a mixed-strategy equilibrium was introduced by John von Neumann and Oskar Morgenstern in their 1944 book The Theory of Games and Economic Behavior, but their analysis was restricted to the special case of zero-sum games. They showed that a mixed-strategy Nash equilibrium will exist for any zero-sum game with a finite set of actions. The contribution of Nash in his 1951 article ""Non-Cooperative Games"" was to define a mixed-strategy Nash equilibrium for any game with a finite set of actions and prove that at least one (mixed-strategy) Nash equilibrium must exist in such a game. The key to Nash's ability to prove existence far more generally than von Neumann lay in his definition of equilibrium. According to Nash, ""an equilibrium point is an n-tuple such that each player's mixed strategy maximizes his payoff if the strategies of the others are held fixed. Thus each player's strategy is optimal against those of the others."" Putting the problem in this framework allowed Nash to employ the Kakutani fixed-point theorem in his 1950 paper to prove existence of equilibria. His 1951 paper used the simpler Brouwer fixed-point theorem for the same purpose.","The modern concept of Nash equilibrium is instead defined in terms of mixed strategies, where players choose a probability distribution over possible  pure strategies (which might put 100% of the probability on one pure strategy; such pure strategies are a subset of mixed strategies). The concept of a mixed-strategy equilibrium was introduced by John von Neumann and Oskar Morgenstern in their 1944 book The Theory of Games and Economic Behavior, but their analysis was restricted to the special case of zero-sum games.","[' What is the modern concept of Nash equilibrium defined in terms of?', ' Players choose a probability distribution over what?', ' What might put 100% of the probability on one pure strategy?', ' What is a subset of mixed strategies?', ' Who introduced the concept of a mixed-strategy equilibrium?', ' When was the book The Theory of Games and Economic Behavior published?']","['mixed strategies', 'possible  pure strategies', 'mixed strategies', 'pure strategies', 'John von Neumann and Oskar Morgenstern', '1944']"
766,nash equilibrium,History,"Game theorists have discovered that in some circumstances Nash equilibrium makes invalid predictions or fails to make a unique prediction. They have proposed many solution concepts ('refinements' of Nash equilibria) designed to rule out implausible Nash equilibria. One particularly important issue is that some Nash equilibria may be based on threats that are not 'credible'. In 1965 Reinhard Selten proposed subgame perfect equilibrium as a refinement that eliminates equilibria which depend on non-credible threats. Other extensions of the Nash equilibrium concept have addressed what happens if a game is repeated, or what happens if a game is played in the absence of complete information. However, subsequent refinements and extensions of Nash equilibrium share the main insight on which Nash's concept rests: the equilibrium is a set of strategies such that each player's strategy is optimal given the choices of the others.
",Game theorists have discovered that in some circumstances Nash equilibrium makes invalid predictions or fails to make a unique prediction. They have proposed many solution concepts ('refinements' of Nash equilibria) designed to rule out implausible Nash equilibria.,"[' Game theorists have discovered that in some circumstances Nash equilibrium makes invalid predictions or fails to make a unique prediction?', ' They have proposed many solution concepts designed to rule out what?']","['Nash equilibria) designed to rule out implausible Nash equilibria', 'implausible Nash equilibria']"
767,nash equilibrium,Stability,"If these cases are both met, then a player with the small change in their mixed strategy will return immediately to the Nash equilibrium. The equilibrium is said to be stable. If condition one does not hold then the equilibrium is unstable. If only condition one holds then there are likely to be an infinite number of optimal strategies for the player who changed.
","If these cases are both met, then a player with the small change in their mixed strategy will return immediately to the Nash equilibrium. The equilibrium is said to be stable.","[' What equilibrium is said to be stable?', ' What happens if both cases are met?']","['Nash equilibrium', 'a player with the small change in their mixed strategy will return immediately to the Nash equilibrium']"
768,nash equilibrium,Stability,"In the ""driving game"" example above there are both stable and unstable equilibria. The equilibria involving mixed strategies with 100% probabilities are stable. If either player changes their probabilities slightly, they will be both at a disadvantage, and their opponent will have no reason to change their strategy in turn. The (50%,50%) equilibrium is unstable. If either player changes their probabilities (which would neither benefit or damage the expectation of the player who did the change, if the other player's mixed strategy is still (50%,50%)), then the other player immediately has a better strategy at either (0%, 100%) or (100%, 0%).
","In the ""driving game"" example above there are both stable and unstable equilibria. The equilibria involving mixed strategies with 100% probabilities are stable.",[' What are the equilibria involving mixed strategies with 100% probabilities?'],['stable']
769,nash equilibrium,Stability,"Stability is crucial in practical applications of Nash equilibria, since the mixed strategy of each player is not perfectly known, but has to be inferred from statistical distribution of their actions in the game. In this case unstable equilibria are very unlikely to arise in practice, since any minute change in the proportions of each strategy seen will lead to a change in strategy and the breakdown of the equilibrium.
","Stability is crucial in practical applications of Nash equilibria, since the mixed strategy of each player is not perfectly known, but has to be inferred from statistical distribution of their actions in the game. In this case unstable equilibria are very unlikely to arise in practice, since any minute change in the proportions of each strategy seen will lead to a change in strategy and the breakdown of the equilibrium.","[' What is crucial in practical applications of Nash equilibria?', ' The mixed strategy of each player is not what?', ' What has to be inferred from statistical distribution of their actions in the game?', ' What is unlikely to occur in practice?', ' Any minute change in the proportions of each strategy seen will lead to a change in what?']","['Stability', 'perfectly known', 'the mixed strategy', 'unstable equilibria', 'strategy and the breakdown of the equilibrium']"
770,nash equilibrium,Stability,"The Nash equilibrium defines stability only in terms of unilateral deviations. In cooperative games such a concept is not convincing enough. Strong Nash equilibrium allows for deviations by every conceivable coalition. Formally, a strong Nash equilibrium is a Nash equilibrium in which no coalition, taking the actions of its complements as given, can cooperatively deviate in a way that benefits all of its members. However, the strong Nash concept is sometimes perceived as too ""strong"" in that the environment allows for unlimited private communication. In fact, strong Nash equilibrium has to be Pareto efficient. As a result of these requirements, strong Nash is too rare to be useful in many branches of game theory. However, in games such as elections with many more players than possible outcomes, it can be more common than a stable equilibrium.
",The Nash equilibrium defines stability only in terms of unilateral deviations. In cooperative games such a concept is not convincing enough.,"[' What does the Nash equilibrium define only in terms of unilateral deviations?', ' What is not convincing enough in cooperative games?']","['stability', 'The Nash equilibrium']"
771,nash equilibrium,Stability,"A refined Nash equilibrium known as coalition-proof Nash equilibrium (CPNE) occurs when players cannot do better even if they are allowed to communicate and make ""self-enforcing"" agreement to deviate. Every correlated strategy supported by iterated strict dominance and on the Pareto frontier is a CPNE.  Further, it is possible for a game to have a Nash equilibrium that is resilient against coalitions less than a specified size, k. CPNE is related to the theory of the core.
","A refined Nash equilibrium known as coalition-proof Nash equilibrium (CPNE) occurs when players cannot do better even if they are allowed to communicate and make ""self-enforcing"" agreement to deviate. Every correlated strategy supported by iterated strict dominance and on the Pareto frontier is a CPNE.","[' What is the term for a refined Nash equilibrium?', ' What happens when players cannot do better even if they are allowed to communicate?', ' Every correlated strategy supported by iterated strict dominance and on the Pareto frontier is what?']","['coalition-proof Nash equilibrium', 'coalition-proof Nash equilibrium', 'a CPNE']"
772,nash equilibrium,Stability,"Finally in the eighties, building with great depth on such ideas Mertens-stable equilibria were introduced as a solution concept. Mertens stable equilibria satisfy both forward induction and backward induction. In a game theory context stable equilibria now usually refer to Mertens stable equilibria.
","Finally in the eighties, building with great depth on such ideas Mertens-stable equilibria were introduced as a solution concept. Mertens stable equilibria satisfy both forward induction and backward induction.","[' When were Mertens-stable equilibria introduced as a solution concept?', ' What satisfy both forward induction and backward induction?']","['eighties', 'Mertens stable equilibria']"
773,nash equilibrium,Occurrence,"If a game has a unique Nash equilibrium and is played among players under certain conditions, then the NE strategy set will be adopted. Sufficient conditions to guarantee that the Nash equilibrium is played are:
","If a game has a unique Nash equilibrium and is played among players under certain conditions, then the NE strategy set will be adopted. Sufficient conditions to guarantee that the Nash equilibrium is played are:","[' What will be adopted if a game has a unique Nash equilibrium?', ' Under what conditions will the NE strategy set be adopted?']","['the NE strategy set', 'If a game has a unique Nash equilibrium and is played among players under certain conditions']"
774,nash equilibrium,NE and non-credible threats,"The Nash equilibrium is a superset of the subgame perfect Nash equilibrium. The subgame perfect equilibrium in addition to the Nash equilibrium requires that the strategy also is a Nash equilibrium in every subgame of that game. This eliminates all non-credible threats, that is, strategies that contain non-rational moves in order to make the counter-player change their strategy.
",The Nash equilibrium is a superset of the subgame perfect Nash equilibrium. The subgame perfect equilibrium in addition to the Nash equilibrium requires that the strategy also is a Nash equilibrium in every subgame of that game.,"[' What is a superset of the subgame perfect Nash equilibrium?', ' What requires that strategy also be a Nash balance in every subgame of a game?']","['The Nash equilibrium', 'subgame perfect equilibrium']"
775,nash equilibrium,NE and non-credible threats,"The image to the right shows a simple sequential game that illustrates the issue with subgame imperfect Nash equilibria. In this game player one chooses left(L) or right(R), which is followed by player two being called upon to be kind (K) or unkind (U) to player one, However, player two only stands to gain from being unkind if player one goes left. If player one goes right the rational player two would de facto be kind to her/him in that subgame. However, The non-credible threat of being unkind at 2(2) is still part of the blue (L, (U,U)) Nash equilibrium. Therefore, if rational behavior can be expected by both parties the subgame perfect Nash equilibrium may be a more meaningful solution concept when such dynamic inconsistencies arise.
","The image to the right shows a simple sequential game that illustrates the issue with subgame imperfect Nash equilibria. In this game player one chooses left(L) or right(R), which is followed by player two being called upon to be kind (K) or unkind (U) to player one, However, player two only stands to gain from being unkind if player one goes left.","[' What is the issue with subgame imperfect Nash equilibria?', ' Which player is called upon to be kind or unkind to player one?', ' Who stands to gain from being unkind to player one?']","['player one chooses left(L) or right(R), which is followed by player two being called upon to be kind (K) or unkind (U) to player one', 'player two', 'player two']"
776,nash equilibrium,Computing Nash equilibria,"If a player A has a dominant strategy 




s

A




{\displaystyle s_{A}}
 then there exists a Nash equilibrium in which A plays 




s

A




{\displaystyle s_{A}}
. In the case of two players A and B, there exists a Nash equilibrium in which A plays 




s

A




{\displaystyle s_{A}}
 and B plays a best response to 




s

A




{\displaystyle s_{A}}
. If 




s

A




{\displaystyle s_{A}}
 is a strictly dominant strategy, A plays 




s

A




{\displaystyle s_{A}}
 in all Nash equilibria. If both A and B have strictly dominant strategies, there exists a unique Nash equilibrium in which each plays their strictly dominant strategy.
","If a player A has a dominant strategy 




s

A




{\displaystyle s_{A}}
 then there exists a Nash equilibrium in which A plays 




s

A




{\displaystyle s_{A}}
. In the case of two players A and B, there exists a Nash equilibrium in which A plays 




s

A




{\displaystyle s_{A}}
 and B plays a best response to 




s

A




{\displaystyle s_{A}}
.","[' What is the dominant strategy of a player A?', ' What is a Nash equilibrium in which A plays s A <unk>displaystyle s_<unk>A<unk>?', ' In the case of two players A and B, what is the best response?', ' What plays a best response to s A <unk>displaystyle s_<unk>A<unk>?']","['A', 'If a player A has a dominant strategy', 'A', 'B']"
777,nash equilibrium,Computing Nash equilibria,"In games with mixed-strategy Nash equilibria, the probability of a player choosing any particular (so pure) strategy can be computed by assigning a variable to each strategy that represents a fixed probability for choosing that strategy. In order for a player to be willing to randomize, their expected payoff for each (pure) strategy should be the same. In addition, the sum of the probabilities for each strategy of a particular player should be 1. This creates a system of equations from which the probabilities of choosing each strategy can be derived.","In games with mixed-strategy Nash equilibria, the probability of a player choosing any particular (so pure) strategy can be computed by assigning a variable to each strategy that represents a fixed probability for choosing that strategy. In order for a player to be willing to randomize, their expected payoff for each (pure) strategy should be the same.","[' How can the probability of a player choosing a particular strategy be computed?', ' What is the variable assigned to each strategy that represents a fixed probability for choosing that strategy?', ' In games with mixed-strategy Nash equilibria, what is the expected payoff?', ' What should players be willing to do?', ' What should the expected payoff be?']","['by assigning a variable to each strategy that represents a fixed probability for choosing that strategy', 'mixed-strategy Nash equilibria', 'the same', 'randomize', 'the same']"
778,nash equilibrium,Oddness of equilibrium points,"In 1971, Robert Wilson came up with the Oddness Theorem,  which says that ""almost all"" finite games have a finite and odd number of Nash equilibria. In 1993, Harsanyi published an alternative proof of the result. ""Almost all"" here means that any game with an infinite or even number of equilibria is very special in the sense that if its payoffs were even slightly randomly perturbed, with probability one it would have an odd number of equilibria instead. 
","In 1971, Robert Wilson came up with the Oddness Theorem,  which says that ""almost all"" finite games have a finite and odd number of Nash equilibria. In 1993, Harsanyi published an alternative proof of the result.","[' In what year did Robert Wilson come up with the Oddness Theorem?', "" What does Wilson's theory say about finite games?"", ' When did Harsanyi publish an alternative proof of his theory?']","['1971', 'almost all"" finite games have a finite and odd number of Nash equilibria', '1993']"
779,nash equilibrium,Oddness of equilibrium points,"The prisoner's dilemma, for example, has one equilibrium, while the battle of the sexes has three-- two pure and one mixed, and this remains true even if the payoffs change slightly. The free money game is an example of a ""special"" game with an even number of equilibria. In it, two players have to both vote ""yes"" rather than ""no"" to get a reward and the votes are simultaneous. There are two pure-strategy Nash equilibria, (yes, yes) and (no, no), and no mixed strategy equilibria, because the strategy ""yes"" weakly dominates ""no"". ""Yes"" is as good as ""no"" regardless of the other player's action, but if there is any chance the other player chooses ""yes"" then ""yes"" is the best reply. Under a small random perturbation of the payoffs, however, the probability that any two payoffs would remain tied, whether at 0 or some other number, is vanishingly small, and the game would have either one or  three equilibria instead.
","The prisoner's dilemma, for example, has one equilibrium, while the battle of the sexes has three-- two pure and one mixed, and this remains true even if the payoffs change slightly. The free money game is an example of a ""special"" game with an even number of equilibria.","[' What is an example of a ""special"" game with an even number of equilibria?', "" How many equilibriums does the prisoner's dilemma have?"", ' The battle of the sexes has how many?']","['The free money game', 'one', 'three']"
780,sensor network,Summary,"Wireless sensor networks (WSNs) refer to networks of spatially dispersed and dedicated sensors that monitor and record the physical conditions of the environment and forward the collected data to a central location. WSNs can measure environmental conditions such as temperature, sound, pollution levels, humidity and wind.","Wireless sensor networks (WSNs) refer to networks of spatially dispersed and dedicated sensors that monitor and record the physical conditions of the environment and forward the collected data to a central location. WSNs can measure environmental conditions such as temperature, sound, pollution levels, humidity and wind.","[' What does WSN stand for?', ' What do WSNs monitor and record?', ' Where can WSN data be sent?']","['Wireless sensor networks', 'the physical conditions of the environment', 'central location']"
781,sensor network,Summary,"These are similar to wireless ad hoc networks in the sense that they rely on wireless connectivity and spontaneous formation of networks so that sensor data can be transported wirelessly. WSNs monitor physical or environmental conditions, such as temperature, sound, and pressure. Modern networks are bi-directional, both collecting data and enabling control of sensor activity.  The development of these networks was motivated by military applications such as battlefield surveillance. Such networks are used in industrial and consumer applications, such as industrial process monitoring and control and machine health monitoring.
","These are similar to wireless ad hoc networks in the sense that they rely on wireless connectivity and spontaneous formation of networks so that sensor data can be transported wirelessly. WSNs monitor physical or environmental conditions, such as temperature, sound, and pressure.","[' What type of networks are similar to wireless ad hoc networks in the sense that they rely on wireless connectivity and spontaneous formation of networks?', ' WSNs monitor physical or environmental conditions, such as temperature, sound, and pressure.']","['WSNs', 'wireless ad hoc networks']"
782,sensor network,Summary,"A WSN is built of ""nodes"" – from a few to hundreds or thousands, where each node is connected to other sensors. Each such node typically has several parts: a radio transceiver with an internal antenna or connection to an external antenna, a microcontroller, an electronic circuit for interfacing with the sensors and an energy source, usually a battery or an embedded form of energy harvesting. A sensor node might vary in size from a shoebox to (theoretically) a grain of dust, although microscopic dimensions have yet to be realized. Sensor node cost is similarly variable, ranging from a few to hundreds of dollars, depending on node sophistication. Size and cost constraints constrain resources such as energy, memory, computational speed and communications bandwidth. The topology of a WSN can vary from a simple star network to an advanced multi-hop wireless mesh network. Propagation can employ routing or flooding.","A WSN is built of ""nodes"" – from a few to hundreds or thousands, where each node is connected to other sensors. Each such node typically has several parts: a radio transceiver with an internal antenna or connection to an external antenna, a microcontroller, an electronic circuit for interfacing with the sensors and an energy source, usually a battery or an embedded form of energy harvesting.","[' What is a WSN built of?', ' How many sensors are connected to each node?', ' What is the name of the electronic circuit for interfacing with other sensors?', ' What is a microcontroller?', ' What is an energy source?']","['nodes', 'hundreds or thousands', 'microcontroller', 'an electronic circuit for interfacing with the sensors', 'a battery']"
783,sensor network,Summary,"In computer science and telecommunications, wireless sensor networks are an active research area supporting many workshops and conferences, including International Workshop on Embedded Networked Sensors (EmNetS), IPSN, SenSys, MobiCom and EWSN. As of 2010, wireless sensor networks had deployed approximately 120 million remote units worldwide.","In computer science and telecommunications, wireless sensor networks are an active research area supporting many workshops and conferences, including International Workshop on Embedded Networked Sensors (EmNetS), IPSN, SenSys, MobiCom and EWSN. As of 2010, wireless sensor networks had deployed approximately 120 million remote units worldwide.","[' In what fields are wireless sensor networks an active research area?', ' What is the International Workshop on Embedded Networked Sensors?', ' As of 2010, how many remote units were deployed worldwide?']","['computer science and telecommunications', 'EmNetS', '120\xa0million']"
784,sensor network,Characteristics,"Cross-layer is becoming an important studying area for wireless communications. In addition, the traditional layered approach presents three main problems:
","Cross-layer is becoming an important studying area for wireless communications. In addition, the traditional layered approach presents three main problems:","[' What is becoming an important studying area for wireless communications?', ' How many main problems does the traditional layered approach present?']","['Cross-layer', 'three']"
785,sensor network,Characteristics,"So the cross-layer can be used to make the optimal modulation to improve the transmission performance, such as data rate, energy efficiency, quality of service (QoS), etc. Sensor nodes can be imagined as small computers which are extremely basic in terms of their interfaces and their components. They usually consist of a processing unit with limited computational power and limited memory, sensors or MEMS (including specific conditioning circuitry), a communication device (usually radio transceivers or alternatively optical), and a power source usually in the form of a battery. Other possible inclusions are energy harvesting modules, secondary ASICs, and possibly secondary communication interface (e.g. RS-232 or USB).
","So the cross-layer can be used to make the optimal modulation to improve the transmission performance, such as data rate, energy efficiency, quality of service (QoS), etc. Sensor nodes can be imagined as small computers which are extremely basic in terms of their interfaces and their components.","[' What can be used to make the optimal modulation to improve the transmission performance?', ' Sensor nodes can be imagined as small computers which are what?']","['the cross-layer', 'extremely basic']"
786,sensor network,Characteristics,"The base stations are one or more components of the WSN with much more computational, energy and communication resources. They act as a gateway between sensor nodes and the end user as they typically forward data from the WSN on to a server. Other special components in routing based networks are routers, designed to compute, calculate and distribute the routing tables.","The base stations are one or more components of the WSN with much more computational, energy and communication resources. They act as a gateway between sensor nodes and the end user as they typically forward data from the WSN on to a server.","[' What are base stations?', ' What do base stations act as between sensor nodes and end user?']","['one or more components of the WSN with much more computational, energy and communication resources', 'gateway']"
787,sensor network,Simulation,"At present, agent-based modeling and simulation is the only paradigm which allows the simulation of complex behavior in the environments of wireless sensors (such as flocking). Agent-based simulation of wireless sensor and ad hoc networks is a relatively new paradigm. Agent-based modelling was originally based on social simulation.
","At present, agent-based modeling and simulation is the only paradigm which allows the simulation of complex behavior in the environments of wireless sensors (such as flocking). Agent-based simulation of wireless sensor and ad hoc networks is a relatively new paradigm.","[' What is the only paradigm that allows the simulation of complex behavior in the environments of wireless sensors?', ' What is a relatively new paradigm?']","['agent-based modeling and simulation', 'Agent-based simulation of wireless sensor and ad hoc networks']"
788,association rule,Summary,"Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness. In any given transaction with a variety of items, association rules are meant to discover the rules that determine how or why certain items are connected.
",Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness.,[' Association rule learning is a machine learning method for discovering interesting relations between variables in large databases.'],['Association']
789,association rule,Summary,"Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule 



{

o
n
i
o
n
s
,
p
o
t
a
t
o
e
s

}
⇒
{

b
u
r
g
e
r

}


{\displaystyle \{\mathrm {onions,potatoes} \}\Rightarrow \{\mathrm {burger} \}}
 found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as, e.g., promotional pricing or product placements.
","Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule 



{

o
n
i
o
n
s
,
p
o
t
a
t
o
e
s

}
⇒
{

b
u
r
g
e
r

}


{\displaystyle \{\mathrm {onions,potatoes} \}\Rightarrow \{\mathrm {burger} \}}
 found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat.","[' Rakesh Agrawal, Tomasz Imieli<unk>ski and Arun Swami introduced strong rules for discovering what between products?', ' What did strong rules introduce for discovering regularities between products in?', ' What would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat?']","['regularities', 'large-scale transaction data', 'sales data of a supermarket']"
790,association rule,Summary,"In addition to the above example from market basket analysis association rules are employed today in many application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.
","In addition to the above example from market basket analysis association rules are employed today in many application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.","[' What is an example of a market basket analysis?', ' In addition to intrusion detection, continuous production, and bioinformatics, what is an application area of association rules?', ' What does association rule learning typically not consider?', ' What is considered the order of items within a transaction or across transactions?']","['association rules', 'Web usage mining', 'the order of items either within a transaction or across transactions', 'association rule learning']"
791,association rule,Summary,"Despite this, association rule learning is a great system for predicting the behavior in data interconnections. This makes it a noteworthy technique for classification, or discovering patterns in data, when implementing machine learning methods.
","Despite this, association rule learning is a great system for predicting the behavior in data interconnections. This makes it a noteworthy technique for classification, or discovering patterns in data, when implementing machine learning methods.","[' Association rule learning is a great system for predicting the behavior in what?', ' What is association rule learning a noteworthy technique for?']","['data interconnections', 'classification']"
792,association rule,Definition,"Every rule is composed by two different sets of items, also known as itemsets, 



X


{\displaystyle X}
 and 



Y


{\displaystyle Y}
, where 



X


{\displaystyle X}
 is called antecedent or left-hand-side (LHS) and 



Y


{\displaystyle Y}
 consequent or right-hand-side (RHS). The antecedent is that item that can be found in the data while the consequent is the item found when combined with the antecedent. The statement 



X
⇒
Y


{\displaystyle X\Rightarrow Y}
 is often read as if 



X


{\displaystyle X}
 then 



Y


{\displaystyle Y}
, where the antecedent (



X


{\displaystyle X}
 ) is the if and the consequent (



Y


{\displaystyle Y}
) is the then. This simply implies that, in theory, whenever 



X


{\displaystyle X}
 occurs in a dataset, then 



Y


{\displaystyle Y}
 will as well.
","Every rule is composed by two different sets of items, also known as itemsets, 



X


{\displaystyle X}
 and 



Y


{\displaystyle Y}
, where 



X


{\displaystyle X}
 is called antecedent or left-hand-side (LHS) and 



Y


{\displaystyle Y}
 consequent or right-hand-side (RHS). The antecedent is that item that can be found in the data while the consequent is the item found when combined with the antecedent.","[' What are itemsets also known as?', "" What is X <unk>displaystyle X' called?"", ' What can be found in the data while the consequent is the item found when combined with the antecedent?']","['X', 'X', 'antecedent']"
793,association rule,Process,"Association rules are made by searching data for frequent if-then patterns and by using a certain criterion under Support and Confidence to define what the most important relationships are. Support is the evidence of how frequent an item appears in the data given, as Confidence is defined by how many times the if-then statements are found true. However, there is a third criteria that can be used, it is called Lift and it can be used to compare the expected Confidence and the actual Confidence. Lift will show how many times the if-then statement is expected to be found to be true.
","Association rules are made by searching data for frequent if-then patterns and by using a certain criterion under Support and Confidence to define what the most important relationships are. Support is the evidence of how frequent an item appears in the data given, as Confidence is defined by how many times the if-then statements are found true.","[' How are association rules made?', ' What is the evidence of how frequent an item appears in the data given?', ' Confidence is defined by how many what?', ' What is Confidence defined by?', ' How many times the if-then statements are found true?']","['by searching data for frequent if-then patterns', 'Support', 'how many times the if-then statements are found true', 'how many times the if-then statements are found true', 'Confidence is defined by how many times the if-then statements are found true.']"
794,association rule,Process,"Association rules are made to calculate from itemsets, which are created by two or more items. If the rules were built from the analyzing from all the possible itemsets from the data then there would be so many rules that they wouldn’t have any meaning. That is why Association rules are typically made from rules that are well represented by the data.
","Association rules are made to calculate from itemsets, which are created by two or more items. If the rules were built from the analyzing from all the possible itemsets from the data then there would be so many rules that they wouldn’t have any meaning.","[' What are association rules made to calculate from?', ' What are itemsets created by two or more items?']","['itemsets', 'Association rules are made to calculate from itemsets']"
795,association rule,Process,"There are many different data mining techniques you could use to find certain analytics and results, for example, there is Classification analysis, Clustering analysis, and Regression analysis. Depending on what you are looking for with your data depends on what technique you should use. Association rules are primarily used to find analytics and a prediction of customer behavior. For Classification analysis, it would most likely be used to question, make decisions, and predict behavior. Clustering analysis is primarily used when there are no assumptions made about the likely relationships within the data. Regression analysis Is used when you want to predict the value of a continuous dependent from a number of independent variables.","There are many different data mining techniques you could use to find certain analytics and results, for example, there is Classification analysis, Clustering analysis, and Regression analysis. Depending on what you are looking for with your data depends on what technique you should use.","[' Classification analysis, Clustering analysis, and Regression analysis are examples of what?']",['data mining techniques']
796,association rule,Process,"There are many benefits of using Association rules like finding the pattern that helps understand the correlations and co-occurrences between data sets. A very good real-world example that uses Association rules would be medicine. Medicine uses Association rules to help diagnose patients. When diagnosing patients there are many variables to consider as many diseases will share similar symptoms. With the use of the Association rules, doctors can determine the conditional probability of an illness by comparing symptom relationships from past cases.",There are many benefits of using Association rules like finding the pattern that helps understand the correlations and co-occurrences between data sets. A very good real-world example that uses Association rules would be medicine.,"[' What are some benefits of using Association rules?', ' What is a real-world example that uses Association rules in medicine?', ' How do Association rules help understand correlations and co-occurrences between data sets?']","['finding the pattern that helps understand the correlations and co-occurrences between data sets', 'medicine', 'finding the pattern']"
797,association rule,Process,"However, Association rules also lead to many different downfalls such as finding the appropriate parameter and threshold settings for the mining algorithm. But there is also the downfall of having a large number of discovered rules. The reason is that this does not guarantee that the rules will be found relevant, but it could also cause the algorithm to have low performance. Sometimes the implemented algorithms will contain too many variables and parameters. For someone that doesn’t have a good concept of data mining, this might cause them to have trouble understanding it.","However, Association rules also lead to many different downfalls such as finding the appropriate parameter and threshold settings for the mining algorithm. But there is also the downfall of having a large number of discovered rules.","[' Association rules lead to many different downfalls such as finding the appropriate parameter and what else?', ' Association rules also lead to what other downfall?']","['threshold settings for the mining algorithm', 'finding the appropriate parameter and threshold settings for the mining algorithm']"
798,association rule,Process,"When using Association rules, you are most likely to only use Support and Confidence. However, this means you have to satisfy a user-specified minimum support and a user-specified minimum confidence at the same time. Usually, the Association rule generation is split into two different steps that needs to be applied:
","When using Association rules, you are most likely to only use Support and Confidence. However, this means you have to satisfy a user-specified minimum support and a user-specified minimum confidence at the same time.","[' When using Association rules, you are most likely to only use what?', ' What do you have to satisfy at the same time?']","['Support and Confidence', 'a user-specified minimum support and a user-specified minimum confidence']"
799,association rule,Process,"The Table on the left is the original unorganized data and the table on the right is organized by the thresholds. In this case Item C is better than the thresholds for both Support and Confidence which is why it is first. Item A is second because its threshold values are spot on. Item D has met the threshold for Support but not Confidence. Item B has not met the threshold for either Support or Confidence and that is why it is last.
",The Table on the left is the original unorganized data and the table on the right is organized by the thresholds. In this case Item C is better than the thresholds for both Support and Confidence which is why it is first.,"[' What is on the left of the table?', ' What is the table on the right of the Table organized by?', ' Item C is better than what?']","['the original unorganized data', 'the thresholds', 'the thresholds for both Support and Confidence']"
800,association rule,Process,"To find all the frequent itemsets in a database is not an easy task since it involves going through all the data to find all possible item combinations from all possible itemsets. The set of possible itemsets is the power set over I and has size 




2

n


−
1


{\displaystyle 2^{n}-1}
 , of course this means to exclude the empty set which is not considered to be a valid itemset. However, the size of the power set will grow exponentially in the number of item n that is within the power set I. An efficient search is possible by using the downward-closure property of support (also called anti-monotonicity). This would guarantee that a frequent itemset and all its subsets are also frequent and thus will have no infrequent itemsets as a subset of a frequent itemset. Exploiting this property, efficient algorithms (e.g., Apriori and Eclat) can find all frequent itemsets.
","To find all the frequent itemsets in a database is not an easy task since it involves going through all the data to find all possible item combinations from all possible itemsets. The set of possible itemsets is the power set over I and has size 




2

n


−
1


{\displaystyle 2^{n}-1}
 , of course this means to exclude the empty set which is not considered to be a valid itemset.","[' How many itemsets are there in a database?', ' What is the power set over I?', ' What does set over I and has size 2 n <unk> 1 <unk>displaystyle 2<unk>n<unk>-1<unk> mean?', ' What does this exclude?']","['2^{n}-1}', 'The set of possible itemsets', 'to exclude the empty set', 'the empty set']"
801,association rule,Useful Concepts,"To illustrate the concepts, we use a small example from the supermarket domain. Table 2 shows a small database containing the items where, in each entry, the value 1 means the presence of the item in the corresponding transaction, and the value 0 represents the absence of an item in that transaction. The set of items is 



I
=
{

m
i
l
k
,
b
r
e
a
d
,
b
u
t
t
e
r
,
b
e
e
r
,
d
i
a
p
e
r
s
,
e
g
g
s
,
f
r
u
i
t

}


{\displaystyle I=\{\mathrm {milk,bread,butter,beer,diapers,eggs,fruit} \}}
.
","To illustrate the concepts, we use a small example from the supermarket domain. Table 2 shows a small database containing the items where, in each entry, the value 1 means the presence of the item in the corresponding transaction, and the value 0 represents the absence of an item in that transaction.","[' To illustrate concepts, we use a small example from what domain?', ' What does Table 2 show?', ' The value 1 means the presence of the item in the corresponding transaction, and the value 0 represents the absence of an item in what transaction?', ' What value represents the absence of an item in a transaction?']","['supermarket', 'a small database containing the items', 'transaction', '0']"
802,association rule,Useful Concepts,"In order to select interesting rules from the set of all possible rules, constraints on various measures of significance and interest are used. The best-known constraints are minimum thresholds on support and confidence.
","In order to select interesting rules from the set of all possible rules, constraints on various measures of significance and interest are used. The best-known constraints are minimum thresholds on support and confidence.","[' What are used to select interesting rules from the set of all possible rules?', ' What are the best-known constraints?']","['constraints on various measures of significance and interest', 'minimum thresholds on support and confidence']"
803,association rule,Useful Concepts,"Note: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.
","Note: this example is extremely small. In practical applications, a rule needs a support of several hundred transactions before it can be considered statistically significant, and datasets often contain thousands or millions of transactions.","[' How many transactions does a rule need to support before it can be considered statistically significant?', ' Datasets often contain thousands or millions of transactions.']","['several hundred', 'datasets']"
804,association rule,History,"The concept of association rules was popularized particularly due to the 1993 article of Agrawal et al., which has acquired more than 23,790 citations according to Google Scholar, as of April 2021, and is thus one of the most cited papers in the Data Mining field. However, what is now called ""association rules"" is introduced already in the 1966 paper on GUHA, a general data mining method developed by Petr Hájek et al.","The concept of association rules was popularized particularly due to the 1993 article of Agrawal et al., which has acquired more than 23,790 citations according to Google Scholar, as of April 2021, and is thus one of the most cited papers in the Data Mining field. However, what is now called ""association rules"" is introduced already in the 1966 paper on GUHA, a general data mining method developed by Petr Hájek et al.","[' How many citations did Agrawal et al. have as of April 2021 according to Google Scholar?', ' What is one of the most cited papers in the Data Mining field?', ' What are association rules?', ' What was the name of the general data mining method?', ' Who developed GUHA?']","['23,790', 'Agrawal et al.,', 'a general data mining method', 'GUHA', 'Petr Hájek et al']"
805,association rule,Statistically sound associations,"One limitation of the standard approach to discovering associations is that by searching massive numbers of possible associations to look for collections of items that appear to be associated, there is a large risk of finding many spurious associations. These are collections of items that co-occur with unexpected frequency in the data, but only do so by chance. For example, suppose we are considering a collection of 10,000 items and looking for rules containing two items in the left-hand-side and 1 item in the right-hand-side. There are approximately 1,000,000,000,000 such rules. If we apply a statistical test for independence with a significance level of 0.05 it means there is only a 5% chance of accepting a rule if there is no association. If we assume there are no associations, we should nonetheless expect to find 50,000,000,000 rules. Statistically sound association discovery controls this risk, in most cases reducing the risk of finding any spurious associations to a user-specified significance level.
","One limitation of the standard approach to discovering associations is that by searching massive numbers of possible associations to look for collections of items that appear to be associated, there is a large risk of finding many spurious associations. These are collections of items that co-occur with unexpected frequency in the data, but only do so by chance.","[' What is one limitation of the standard approach to discovering associations?', ' What is a large risk of finding many spurious associations by searching massive numbers of possible associations to look for collections of items that appear to be associated?', ' What are collections of items that co-occur with unexpected frequency in the data that only do so by chance?']","['there is a large risk of finding many spurious associations', 'One limitation of the standard approach to discovering associations', 'spurious associations']"
806,association rule,Algorithms,"Some well-known algorithms are Apriori, Eclat and FP-Growth, but they only do half the job, since they are algorithms for mining frequent itemsets. Another step needs to be done after to generate rules from frequent itemsets found in a database.
","Some well-known algorithms are Apriori, Eclat and FP-Growth, but they only do half the job, since they are algorithms for mining frequent itemsets. Another step needs to be done after to generate rules from frequent itemsets found in a database.","[' Apriori, Eclat and FP-Growth are examples of what kind of algorithms?', ' What are the algorithms for mining?', ' A step needs to be done after generating rules from frequent itemsets found in a database?']","['well-known algorithms are Apriori, Eclat and FP-Growth, but they only do half the job, since they are algorithms for mining frequent itemsets', 'frequent itemsets', 'algorithms']"
807,association rule,Lore,"A famous story about association rule mining is the ""beer and diaper"" story. A purported survey of behavior of supermarket shoppers discovered that customers (presumably young men) who buy diapers tend also to buy beer. This anecdote became popular as an example of how unexpected association rules might be found from everyday data. There are varying opinions as to how much of the story is true. Daniel Powers says:","A famous story about association rule mining is the ""beer and diaper"" story. A purported survey of behavior of supermarket shoppers discovered that customers (presumably young men) who buy diapers tend also to buy beer.","[' What is a famous story about association rule mining?', ' A purported survey of behavior of supermarket shoppers discovered that customers who buy diapers tend to buy what?']","['beer and diaper"" story', 'beer']"
808,association rule,Lore,"In 1992, Thomas Blischok, manager of a retail consulting group at Teradata, and his staff prepared an analysis of 1.2 million market baskets from about 25 Osco Drug stores. Database queries were developed to identify affinities. The analysis ""did discover that between 5:00 and 7:00 p.m. that consumers bought beer and diapers"". Osco managers did NOT exploit the beer and diapers relationship by moving the products closer together on the shelves.","In 1992, Thomas Blischok, manager of a retail consulting group at Teradata, and his staff prepared an analysis of 1.2 million market baskets from about 25 Osco Drug stores. Database queries were developed to identify affinities.","[' Who was the manager of a retail consulting group at Teradata in 1992?', ' How many market baskets did Blischok and his staff analyze?', ' What were database queries developed to identify?']","['Thomas Blischok', '1.2 million', 'affinities']"
809,association rule,Other types of association rule mining,"Multi-Relation Association Rules: Multi-Relation Association Rules (MRAR) are association rules where each item may have several relations. These relations indicate indirect relationship between the entities. Consider the following MRAR where the first item consists of three relations live in, nearby and humid: “Those who live in a place which is nearby a city with humid climate type and also are younger than 20 -> their health condition is good”. Such association rules are extractable from RDBMS data or semantic web data.",Multi-Relation Association Rules: Multi-Relation Association Rules (MRAR) are association rules where each item may have several relations. These relations indicate indirect relationship between the entities.,"[' What are Multi-Relation Association Rules?', ' What do MRAR stand for?']","['association rules where each item may have several relations', 'Multi-Relation Association Rules']"
810,association rule,Other types of association rule mining,Contrast set learning is a form of associative learning. Contrast set learners use rules that differ meaningfully in their distribution across subsets.,Contrast set learning is a form of associative learning. Contrast set learners use rules that differ meaningfully in their distribution across subsets.,"[' What is a form of associative learning?', ' Contrast set learners use rules that differ meaningfully in what?']","['Contrast set learning', 'their distribution across subsets']"
811,association rule,Other types of association rule mining,"Interval Data Association Rules e.g. partition the age into 5-year-increment ranged
",Interval Data Association Rules e.g. partition the age into 5-year-increment ranged,[' What do Interval Data Association Rules partition the age into?'],['5-year-increment ranged']
812,association rule,Other types of association rule mining,"Sequential pattern mining  discovers subsequences that are common to more than minsup sequences in a sequence database, where minsup is set by the user. A sequence is an ordered list of transactions.","Sequential pattern mining  discovers subsequences that are common to more than minsup sequences in a sequence database, where minsup is set by the user. A sequence is an ordered list of transactions.","[' What sort of mining finds subsequences that are common to more than minsup sequences?', ' What is a sequence an ordered list of?']","['Sequential pattern mining', 'transactions']"
813,association rule,Other types of association rule mining,Warmr is shipped as part of the ACE data mining suite. It allows association rule learning for first order relational rules.,Warmr is shipped as part of the ACE data mining suite. It allows association rule learning for first order relational rules.,"[' Warmr is shipped as part of what suite?', ' Warmr allows association rule learning for what kind of rules?']","['ACE data mining suite', 'first order relational']"
814,description logic,Summary,"Description logics (DL) are a family of formal knowledge representation languages. Many DLs are more expressive than propositional logic but less expressive than first-order logic. In contrast to the latter, the core reasoning problems for DLs are (usually) decidable, and efficient decision procedures have been designed and implemented for these problems. There are general, spatial, temporal, spatiotemporal, and fuzzy description logics, and each description logic features a different balance between expressive power and reasoning complexity by supporting different sets of mathematical constructors.",Description logics (DL) are a family of formal knowledge representation languages. Many DLs are more expressive than propositional logic but less expressive than first-order logic.,"[' Description logics are a family of what?', ' DLs are more expressive than propositional logic but less expressive than first-order logic?']","['formal knowledge representation languages', 'Description logics']"
815,description logic,Summary,"DLs are used in artificial intelligence to describe and reason about the relevant concepts of an application domain (known as terminological knowledge). It is of particular importance in providing a logical formalism for ontologies and the Semantic Web: the Web Ontology Language (OWL) and its profiles are based on DLs. The most notable application of DLs and OWL is in biomedical informatics where DL assists in the codification of biomedical knowledge.
",DLs are used in artificial intelligence to describe and reason about the relevant concepts of an application domain (known as terminological knowledge). It is of particular importance in providing a logical formalism for ontologies and the Semantic Web: the Web Ontology Language (OWL) and its profiles are based on DLs.,"[' DLs are used in artificial intelligence to describe and reason about the relevant concepts of what?', ' What is term for terminological knowledge?', ' The Web Ontology Language (OWL) and its profiles are based on what type of language?']","['an application domain', 'application domain', 'DLs']"
816,description logic,Introduction,The fundamental modeling concept of a DL is the axiom—a logical statement relating roles and/or concepts. This is a key difference from the  frames paradigm where a frame specification declares and completely defines a class.,The fundamental modeling concept of a DL is the axiom—a logical statement relating roles and/or concepts. This is a key difference from the  frames paradigm where a frame specification declares and completely defines a class.,"[' What is the fundamental modeling concept of a DL?', ' A logical statement relating roles and concepts is called what?', ' What paradigm declares and completely defines a class?']","['the axiom', 'axiom', 'frames paradigm']"
817,description logic,History,"Description logic was given its current name in the 1980s. Previous to this it was called (chronologically): terminological systems, and concept languages.
","Description logic was given its current name in the 1980s. Previous to this it was called (chronologically): terminological systems, and concept languages.","[' When was Description logic given its current name?', ' What was the previous name of Description logic?']","['1980s', 'terminological systems, and concept languages']"
818,description logic,Modeling,"In DL, a distinction is drawn between the so-called TBox (terminological box) and the ABox (assertional box). In general, the TBox contains sentences describing concept hierarchies (i.e., relations between concepts) while the ABox contains ground sentences stating where in the hierarchy, individuals belong (i.e., relations between individuals and concepts). For example, the statement:
","In DL, a distinction is drawn between the so-called TBox (terminological box) and the ABox (assertional box). In general, the TBox contains sentences describing concept hierarchies (i.e., relations between concepts) while the ABox contains ground sentences stating where in the hierarchy, individuals belong (i.e., relations between individuals and concepts).","[' In DL, what is the difference between the TBox and the ABox?', ' The TBox contains sentences describing what?', ' What does the Abox contain?']","['sentences describing concept hierarchies', 'concept hierarchies', 'ground sentences stating where in the hierarchy, individuals belong']"
819,description logic,Modeling,"Note that the TBox/ABox distinction is not significant, in the same sense that the two ""kinds"" of sentences are not treated differently in first-order logic (which subsumes most DL). When translated into first-order logic, a subsumption axiom like (1) is simply a conditional restriction to unary predicates (concepts) with only variables appearing in it. Clearly, a sentence of this form is not privileged or special over sentences in which only constants (""grounded"" values) appear like (2).
","Note that the TBox/ABox distinction is not significant, in the same sense that the two ""kinds"" of sentences are not treated differently in first-order logic (which subsumes most DL). When translated into first-order logic, a subsumption axiom like (1) is simply a conditional restriction to unary predicates (concepts) with only variables appearing in it.","[' What is not significant in the same sense that the two ""kinds"" of sentences are not treated differently in first-order logic?', ' What subsumption axiom is simply a conditional restriction to unary predicates?', ' What is simply a conditional restriction to unary predicates?']","['TBox/ABox distinction', '(1)', 'a subsumption axiom like (1)']"
820,description logic,Modeling,"So why was the distinction introduced? The primary reason is that the separation can be useful when describing and formulating decision-procedures for various DL. For example, a reasoner might process the TBox and ABox separately, in part because certain key inference problems are tied to one but not the other one ('classification' is related to the TBox, 'instance checking' to the ABox). Another example is that the complexity of the TBox can greatly affect the performance of a given decision-procedure for a certain DL, independently of the ABox. Thus, it is useful to have a way to talk about that specific part of the knowledge base.
",So why was the distinction introduced? The primary reason is that the separation can be useful when describing and formulating decision-procedures for various DL.,"[' Why was the distinction introduced?', ' What can be useful when describing and formulating decision-procedures?']","['the separation can be useful when describing and formulating decision-procedures for various DL', 'the separation']"
821,description logic,Modeling,"The secondary reason is that the distinction can make sense from the knowledge base modeler's perspective. It is plausible to distinguish between our conception of terms/concepts in the world (class axioms in the TBox) and particular manifestations of those terms/concepts (instance assertions in the ABox). In the above example: when the hierarchy within a company is the same in every branch but the assignment to employees is different in every department (because there are other people working there), it makes sense to reuse the TBox for different branches that do not use the same ABox.
",The secondary reason is that the distinction can make sense from the knowledge base modeler's perspective. It is plausible to distinguish between our conception of terms/concepts in the world (class axioms in the TBox) and particular manifestations of those terms/concepts (instance assertions in the ABox).,"[' What is plausible to distinguish between our conception of terms/concepts in the world and particular manifestations thereof?', "" What can make sense from the knowledge base modeler's perspective?""]","['instance assertions in the ABox', 'the distinction']"
822,description logic,Modeling,"There are two features of description logic that are not shared by most other data description formalisms: DL does not make the unique name assumption (UNA) or the closed-world assumption (CWA). Not having UNA means that two concepts with different names may be allowed by some inference to be shown to be equivalent. Not having CWA, or rather having the open world assumption (OWA) means that lack of knowledge of a fact does not immediately imply knowledge of the negation of a fact.
",There are two features of description logic that are not shared by most other data description formalisms: DL does not make the unique name assumption (UNA) or the closed-world assumption (CWA). Not having UNA means that two concepts with different names may be allowed by some inference to be shown to be equivalent.,"[' What are the two features of description logic that are not shared by most other data description formalisms?', ' What does DL not make the UNA or the closed-world assumption?', ' Names may be allowed by some inference to be shown to be what?']","['DL does not make the unique name assumption (UNA) or the closed-world assumption (CWA).', 'unique name assumption', 'equivalent']"
823,description logic,Formal description,"Like first-order logic (FOL), a syntax defines which collections of symbols are legal expressions in a description logic, and semantics determine meaning. Unlike FOL, a DL may have several well known syntactic variants.","Like first-order logic (FOL), a syntax defines which collections of symbols are legal expressions in a description logic, and semantics determine meaning. Unlike FOL, a DL may have several well known syntactic variants.","[' What defines which collections of symbols are legal expressions in a description logic?', ' What determines meaning of a DL?', ' A DL may have several well known what?']","['a syntax', 'semantics', 'syntactic variants']"
824,genetic programming,Summary,"The operations are: selection of the fittest programs for reproduction (crossover) and mutation according to a predefined fitness measure, usually proficiency at the desired task.  The crossover operation involves swapping random parts of selected pairs (parents) to produce new and different offspring that become part of the new generation of programs.  Mutation involves substitution of some random part of a program with some other random part of a program. Some programs not selected for reproduction are copied from the current generation to the new generation. Then the selection and other operations are recursively applied to the new generation of programs.
","The operations are: selection of the fittest programs for reproduction (crossover) and mutation according to a predefined fitness measure, usually proficiency at the desired task. The crossover operation involves swapping random parts of selected pairs (parents) to produce new and different offspring that become part of the new generation of programs.","[' What are the operations of selection of the fittest programs for reproduction called?', ' What is a predefined fitness measure?', ' Which operation involves swapping random parts of selected pairs?', ' What are the different offspring that become part of the new generation of programs?']","['crossover', 'proficiency at the desired task', 'crossover operation', 'new and different offspring']"
825,genetic programming,Summary,"Typically, members of each new generation are on average more fit than the members of the previous generation, and the best-of-generation program is often better than the best-of-generation programs from previous generations.  Termination of the recursion is when some individual program reaches a predefined proficiency or fitness level.
","Typically, members of each new generation are on average more fit than the members of the previous generation, and the best-of-generation program is often better than the best-of-generation programs from previous generations. Termination of the recursion is when some individual program reaches a predefined proficiency or fitness level.","[' What are members of each new generation on average more fit than?', ' What is the best-of-generation program often better than the best of-generation programs from previous generations?', ' Termination of the recursion is when some individual program reaches what?']","['the members of the previous generation', 'members of each new generation are on average more fit', 'a predefined proficiency or fitness level']"
826,genetic programming,Summary,"It may and often does happen that a particular run of the algorithm results in premature convergence to some local maximum which is not a globally optimal or even good solution.  Multiple runs (dozens to hundreds) are usually necessary to produce a very good result.  It may also be necessary to increase the starting population size and variability of the individuals to avoid pathologies.
",It may and often does happen that a particular run of the algorithm results in premature convergence to some local maximum which is not a globally optimal or even good solution. Multiple runs (dozens to hundreds) are usually necessary to produce a very good result.,"[' How many runs are usually necessary to produce a very good result?', ' What happens when a particular run of the algorithm results in premature convergence to some local maximum?']","['dozens to hundreds', 'not a globally optimal or even good solution']"
827,genetic programming,History,"The first record of the proposal to evolve programs is probably that of Alan Turing in 1950. There was a gap of 25 years before the publication of John Holland's 'Adaptation in Natural and Artificial Systems' laid out the theoretical and empirical foundations of the science. In 1981, Richard Forsyth demonstrated the successful evolution of small programs, represented as trees, to perform classification of crime scene evidence for the UK Home Office.",The first record of the proposal to evolve programs is probably that of Alan Turing in 1950. There was a gap of 25 years before the publication of John Holland's 'Adaptation in Natural and Artificial Systems' laid out the theoretical and empirical foundations of the science.,"[' Who wrote the first proposal to evolve programs?', "" When was Alan Turing's proposal to develop programs first published?"", "" How long did it take for John Holland's 'Adaptation in Natural and Artificial Systems' to lay out the theoretical and empirical foundations of science?""]","['Alan Turing', '1950', '25 years']"
828,genetic programming,History,"Although the idea of evolving programs, initially in the computer language Lisp, was current amongst John Holland’s students, it was not until they organised the first Genetic Algorithms (GA) conference in Pittsburgh that Nichael Cramer published evolved programs in two specially designed languages, which included the first statement of modern ""tree-based"" Genetic Programming (that is, procedural languages organized in tree-based structures and operated on by suitably defined GA-operators). In 1988, John Koza (also a PhD student of John Holland) patented his invention of a GA for program evolution. This was followed by publication in the International Joint Conference on Artificial Intelligence IJCAI-89.","Although the idea of evolving programs, initially in the computer language Lisp, was current amongst John Holland’s students, it was not until they organised the first Genetic Algorithms (GA) conference in Pittsburgh that Nichael Cramer published evolved programs in two specially designed languages, which included the first statement of modern ""tree-based"" Genetic Programming (that is, procedural languages organized in tree-based structures and operated on by suitably defined GA-operators). In 1988, John Koza (also a PhD student of John Holland) patented his invention of a GA for program evolution.","[' What language did John Holland study?', ' What was the name of the first Genetic Algorithms conference?', ' In what city did Nichael Cramer publish the first genetic algorithm?', ' The first statement of modern programming was published in what language?', ' What was the first statement of Genetic Programming?', ' Who patented his invention of a GA for program evolution?']","['Lisp', 'Pittsburgh', 'Pittsburgh', 'Genetic Algorithms', 'Genetic Algorithms (GA) conference in Pittsburgh that Nichael Cramer published evolved programs in two specially designed languages', 'John Koza']"
829,genetic programming,History,"Koza followed this with 205 publications on “Genetic Programming” (GP), name coined by David Goldberg, also a PhD student of John Holland. However, it is the series of 4 books by Koza, starting in 1992 with accompanying videos, that really established GP. Subsequently, there was an enormous expansion of the number of publications with the Genetic Programming Bibliography, surpassing 10,000 entries. In 2010, Koza listed 77 results where Genetic Programming was human competitive.
","Koza followed this with 205 publications on “Genetic Programming” (GP), name coined by David Goldberg, also a PhD student of John Holland. However, it is the series of 4 books by Koza, starting in 1992 with accompanying videos, that really established GP.","[' How many publications did Koza publish on Genetic Programming?', ' Who coined the name GP?', "" What was the name of Koza's series of 4 books?"", ' How many books were published by Koza in 1992?']","['205', 'David Goldberg', 'Genetic Programming', '4']"
830,genetic programming,History,"In 1996, Koza started the annual Genetic Programming conference which was followed in 1998 by the annual EuroGP conference, and the first book in a GP series edited by Koza. 1998 also saw the first GP textbook. GP continued to flourish, leading to the first specialist GP journal and three years later (2003) the annual Genetic Programming Theory and Practice (GPTP) workshop was established by Rick Riolo. Genetic Programming papers continue to be published at a diversity of conferences and associated journals. Today there are nineteen GP books including several for students.","In 1996, Koza started the annual Genetic Programming conference which was followed in 1998 by the annual EuroGP conference, and the first book in a GP series edited by Koza. 1998 also saw the first GP textbook.","[' In what year did Koza start the annual Genetic Programming conference?', ' What was the name of the first book in a GP series edited by Koza?', ' When was the first GP textbook published?']","['1996', 'Genetic Programming', '1998']"
831,genetic programming,Foundational work in GP,"Early work that set the stage for current genetic programming research topics and applications is diverse, and includes software synthesis and repair, predictive modeling, data mining, financial modeling, soft sensors, design, and image processing. Applications in some areas, such as design, often make use of intermediate representations, such as Fred Gruau’s cellular encoding. Industrial uptake has been significant in several areas including finance, the chemical industry, bioinformatics and the steel industry.","Early work that set the stage for current genetic programming research topics and applications is diverse, and includes software synthesis and repair, predictive modeling, data mining, financial modeling, soft sensors, design, and image processing. Applications in some areas, such as design, often make use of intermediate representations, such as Fred Gruau’s cellular encoding.","[' What are some of the early work that set the stage for current genetic programming topics and applications?', ' What is one example of an application that often makes use of intermediate representations?', "" What is Fred Gruau's cellular encoding?""]","['software synthesis and repair, predictive modeling, data mining, financial modeling, soft sensors, design, and image processing', 'design', 'intermediate representations']"
832,genetic programming,Applications,"GP has been successfully used as an automatic programming tool, a machine learning tool and an automatic problem-solving engine. GP is especially useful in the domains where the exact form of the 
solution is not known in advance or an approximate solution is acceptable (possibly because finding the exact solution is very difficult). Some of the applications of GP are curve fitting, data modeling, symbolic regression, feature selection, classification, etc. John R. Koza mentions 76
instances where Genetic Programming has been able to produce results that are competitive with human-produced results (called Human-competitive results). Since 2004, the annual Genetic and Evolutionary Computation Conference (GECCO) holds Human Competitive Awards (called Humies) competition, where cash awards are presented to human-competitive results produced by any form of genetic and evolutionary computation. GP has won many awards in this competition over the years.
","GP has been successfully used as an automatic programming tool, a machine learning tool and an automatic problem-solving engine. GP is especially useful in the domains where the exact form of the 
solution is not known in advance or an approximate solution is acceptable (possibly because finding the exact solution is very difficult).","[' What has been successfully used as an automatic programming tool, a machine learning tool and an automatic problem-solving engine?', ' What is especially useful in the domains where the exact form of the solution is not known in advance?', ' Is an approximate solution acceptable?', ' Why is finding the exact solution difficult?']","['GP', 'GP', 'the exact form of the \nsolution is not known in advance or an approximate solution is acceptable', 'the exact form of the \nsolution is not known in advance or an approximate solution is acceptable']"
833,genetic programming,Meta-genetic programming,"Meta-genetic programming is the proposed meta learning technique of evolving a genetic programming system using genetic programming itself. It suggests that chromosomes, crossover, and mutation were themselves evolved, therefore like their real life counterparts should be allowed to change on their own rather than being determined by a human programmer. Meta-GP was formally proposed by Jürgen Schmidhuber in 1987. Doug Lenat's Eurisko is an earlier effort that may be the same technique. It is a recursive but terminating algorithm, allowing it to avoid infinite recursion. In the ""autoconstructive evolution"" approach to meta-genetic programming, the methods for the production and variation of offspring are encoded within the evolving programs themselves, and programs are executed to produce new programs to be added to the population.","Meta-genetic programming is the proposed meta learning technique of evolving a genetic programming system using genetic programming itself. It suggests that chromosomes, crossover, and mutation were themselves evolved, therefore like their real life counterparts should be allowed to change on their own rather than being determined by a human programmer.","[' What is the meta learning technique of evolving a genetic programming system using genetic programming itself?', ' What suggests that chromosomes, crossover, and mutation were themselves evolved?']","['Meta-genetic programming', 'Meta-genetic programming']"
834,genetic programming,Meta-genetic programming,"Critics of this idea often say this approach is overly broad in scope. However, it might be possible to constrain the fitness criterion onto a general class of results, and so obtain an evolved GP that would more efficiently produce results for sub-classes. This might take the form of a meta evolved GP for producing human walking algorithms which is then used to evolve human running, jumping, etc. The fitness criterion applied to the meta GP would simply be one of efficiency.
","Critics of this idea often say this approach is overly broad in scope. However, it might be possible to constrain the fitness criterion onto a general class of results, and so obtain an evolved GP that would more efficiently produce results for sub-classes.","[' Critics of this idea often say this approach is overly broad in what?', ' What might be possible to constrain the fitness criterion onto a general class of results?']","['scope', 'it might be possible to constrain the fitness criterion onto a general class of results, and so obtain an evolved GP that would more efficiently produce results for sub-classes']"
835,sentiment analysis,Summary,"Sentiment analysis (also known as opinion mining or emotion AI) is the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine. With the rise of deep language models, such as RoBERTa, also more difficult data domains can be analyzed, e.g., news texts where authors typically express their opinion/sentiment less explicitly.","Sentiment analysis (also known as opinion mining or emotion AI) is the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine.","[' What is sentiment analysis also known as?', ' What is the use of natural language processing, text analysis, computational linguistics, and biometrics to study?', ' Reviews, surveys, online and social media, and healthcare materials for applications that range from marketing to what?']","['opinion mining or emotion AI', 'Sentiment analysis', 'customer service']"
836,sentiment analysis,Types,"A basic task in sentiment analysis is classifying the polarity of a given text at the document, sentence, or feature/aspect level—whether the expressed opinion in a document, a sentence or an entity feature/aspect is positive, negative, or neutral. Advanced, ""beyond polarity"" sentiment classification looks, for instance, at emotional states such as enjoyment, anger, disgust, sadness, fear, and surprise.","A basic task in sentiment analysis is classifying the polarity of a given text at the document, sentence, or feature/aspect level—whether the expressed opinion in a document, a sentence or an entity feature/aspect is positive, negative, or neutral. Advanced, ""beyond polarity"" sentiment classification looks, for instance, at emotional states such as enjoyment, anger, disgust, sadness, fear, and surprise.","[' What is a basic task in sentiment analysis?', ' What does advanced sentiment classification look at?', ' What sort of sentiment classification looks at emotional states such as enjoyment, anger, disgust, sadness, fear, and surprise?']","['classifying the polarity of a given text at the document, sentence, or feature/aspect level', 'emotional states such as enjoyment, anger, disgust, sadness, fear, and surprise', 'Advanced, ""beyond polarity""']"
837,sentiment analysis,Types,"Subsequently, the method described in a patent by Volcani and Fogel, looked specifically at sentiment and identified individual words and phrases in text with respect to different emotional scales. A current system based on their work, called EffectCheck, presents synonyms that can be used to increase or decrease the level of evoked emotion in each scale.
","Subsequently, the method described in a patent by Volcani and Fogel, looked specifically at sentiment and identified individual words and phrases in text with respect to different emotional scales. A current system based on their work, called EffectCheck, presents synonyms that can be used to increase or decrease the level of evoked emotion in each scale.","["" What did Volcani and Fogel's method look specifically at?"", ' What is the current system based on their work called?', ' EffectCheck presents synonyms that can be used to increase or decrease what level of emotion?', ' What can be used to increase or decrease the level of evoked emotion in each scale?']","['sentiment', 'EffectCheck', 'evoked', 'synonyms']"
838,sentiment analysis,Types,"Many other subsequent efforts were less sophisticated, using a mere polar view of sentiment, from positive to negative, such as work by Turney, and Pang who applied different methods for detecting the polarity of product reviews and movie reviews respectively. This work is at the document level. One can also classify a document's polarity on a multi-way scale, which was attempted by Pang and Snyder among others: Pang and Lee expanded the basic task of classifying a movie review as either positive or negative to predict star ratings on either a 3- or a 4-star scale, while Snyder performed an in-depth analysis of restaurant reviews, predicting ratings for various aspects of the given restaurant, such as the food and atmosphere (on a five-star scale).
","Many other subsequent efforts were less sophisticated, using a mere polar view of sentiment, from positive to negative, such as work by Turney, and Pang who applied different methods for detecting the polarity of product reviews and movie reviews respectively. This work is at the document level.",[' What did Turney and Pang use to detect the polarity of product reviews and movie reviews?'],['different methods']
839,sentiment analysis,Types,"Even though in most statistical classification methods, the neutral class is ignored under the assumption that neutral texts lie near the boundary of the binary classifier, several researchers suggest that, as in every polarity problem, three categories must be identified. Moreover, it can be proven that specific classifiers such as the Max Entropy and SVMs can benefit from the introduction of a neutral class and improve the overall accuracy of the classification. There are in principle two ways for operating with a neutral class. Either, the algorithm proceeds by first identifying the neutral language, filtering it out and then assessing the rest in terms of positive and negative sentiments, or it builds a three-way classification in one step. This second approach often involves estimating a probability distribution over all categories (e.g. naive Bayes classifiers as implemented by the NLTK). Whether and how to use a neutral class depends on the nature of the data: if the data is clearly clustered into neutral, negative and positive language, it makes sense to filter the neutral language out and focus on the polarity between positive and negative sentiments. If, in contrast, the data are mostly neutral with small deviations towards positive and negative affect, this strategy would make it harder to clearly distinguish between the two poles.
","Even though in most statistical classification methods, the neutral class is ignored under the assumption that neutral texts lie near the boundary of the binary classifier, several researchers suggest that, as in every polarity problem, three categories must be identified. Moreover, it can be proven that specific classifiers such as the Max Entropy and SVMs can benefit from the introduction of a neutral class and improve the overall accuracy of the classification.","[' What is ignored in most statistical classification methods?', ' Why is the neutral class ignored?', ' How many categories must be identified in every polarity problem?', ' What classifiers can benefit from the introduction of a neutral class?', ' What can improve the overall accuracy of classification?']","['the neutral class', 'neutral texts lie near the boundary of the binary classifier', 'three', 'Max Entropy and SVMs', 'specific classifiers such as the Max Entropy and SVMs can benefit from the introduction of a neutral class']"
840,sentiment analysis,Types,"A different method for determining sentiment is the use of a scaling system whereby words commonly associated with having a negative, neutral, or positive sentiment with them are given an associated number on a −10 to +10 scale (most negative up to most positive) or simply from 0 to a positive upper limit such as +4. This makes it possible to adjust the sentiment of a given term relative to its environment (usually on the level of the sentence). When a piece of unstructured text is analyzed using natural language processing, each concept in the specified environment is given a score based on the way sentiment words relate to the concept and its associated score. This allows movement to a more sophisticated understanding of sentiment, because it is now possible to adjust the sentiment value of a concept relative to modifications that may surround it. Words, for example, that intensify, relax or negate the sentiment expressed by the concept can affect its score. Alternatively, texts can be given a positive and negative sentiment strength score if the goal is to determine the sentiment in a text rather than the overall polarity and strength of the text.","A different method for determining sentiment is the use of a scaling system whereby words commonly associated with having a negative, neutral, or positive sentiment with them are given an associated number on a −10 to +10 scale (most negative up to most positive) or simply from 0 to a positive upper limit such as +4. This makes it possible to adjust the sentiment of a given term relative to its environment (usually on the level of the sentence).","[' What is a different method for determining sentiment?', ' How are words commonly associated with having a negative, neutral, or positive sentiment with them given an associated number on a <unk>10 to +10 scale?', ' What is a positive upper limit?', ' What makes it possible to adjust the sentiment of a given term relative to its environment?']","['a scaling system', 'a scaling system', '+4', 'a scaling system']"
841,sentiment analysis,Methods and features,"Existing approaches to sentiment analysis can be grouped into three main categories: knowledge-based techniques, statistical methods, and hybrid approaches. Knowledge-based techniques classify text by affect categories based on the presence of unambiguous affect words such as happy, sad, afraid, and bored. Some knowledge bases not only list obvious affect words, but also assign arbitrary words a probable ""affinity"" to particular emotions. Statistical methods leverage elements from machine learning such as latent semantic analysis, support vector machines, ""bag of words"", ""Pointwise Mutual Information"" for Semantic Orientation, and deep learning. More sophisticated methods try to detect the holder of a sentiment (i.e., the person who maintains that affective state) and the target (i.e., the entity about which the affect is felt). To mine the opinion in context and get the feature about which the speaker has opined, the grammatical relationships of words are used. Grammatical dependency relations are obtained by deep parsing of the text. Hybrid approaches leverage both machine learning and elements from knowledge representation such as ontologies and semantic networks in order to detect semantics that are expressed in a subtle manner, e.g., through the analysis of concepts that do not explicitly convey relevant information, but which are implicitly linked to other concepts that do so.","Existing approaches to sentiment analysis can be grouped into three main categories: knowledge-based techniques, statistical methods, and hybrid approaches. Knowledge-based techniques classify text by affect categories based on the presence of unambiguous affect words such as happy, sad, afraid, and bored.","[' What are the three main categories of approaches to sentiment analysis?', ' What do knowledge-based techniques classify text by?']","['knowledge-based techniques, statistical methods, and hybrid approaches', 'affect categories']"
842,sentiment analysis,Methods and features,"Open source software tools as well as range of free and paid sentiment analysis tools deploy machine learning, statistics, and natural language processing techniques to automate sentiment analysis on large collections of texts, including web pages, online news, internet discussion groups, online reviews, web blogs, and social media. Knowledge-based systems, on the other hand, make use of publicly available resources, to extract the semantic and affective information associated with natural language concepts. The system can help perform affective commonsense reasoning. Sentiment analysis can also be performed on visual content, i.e., images and videos (see Multimodal sentiment analysis). One of the first approaches in this direction is SentiBank utilizing an adjective noun pair representation of visual content. In addition, the vast majority of sentiment classification approaches rely on the bag-of-words model, which disregards context, grammar and even word order. Approaches that analyses the sentiment based on how words compose the meaning of longer phrases have shown better result, but they incur an additional annotation overhead.
","Open source software tools as well as range of free and paid sentiment analysis tools deploy machine learning, statistics, and natural language processing techniques to automate sentiment analysis on large collections of texts, including web pages, online news, internet discussion groups, online reviews, web blogs, and social media. Knowledge-based systems, on the other hand, make use of publicly available resources, to extract the semantic and affective information associated with natural language concepts.","[' Machine learning, statistics, and natural language processing techniques are used to automate sentiment analysis on what?', ' What types of systems are knowledge-based systems built on?', ' What do knowledge-based systems use to extract the semantic and affective information associated with natural language concepts?']","['large collections of texts', 'publicly available resources', 'publicly available resources']"
843,sentiment analysis,Methods and features,"A human analysis component is required in sentiment analysis, as automated systems are not able to analyze historical tendencies of the individual commenter, or the platform and are often classified incorrectly in their expressed sentiment. Automation impacts approximately 23% of comments that are correctly classified by humans. However, humans often disagree, and it is argued that the inter-human agreement provides an upper bound that automated sentiment classifiers can eventually reach.","A human analysis component is required in sentiment analysis, as automated systems are not able to analyze historical tendencies of the individual commenter, or the platform and are often classified incorrectly in their expressed sentiment. Automation impacts approximately 23% of comments that are correctly classified by humans.","[' What is required in sentiment analysis?', ' Automated systems are not able to analyze historical tendencies of the individual commenter or what?', ' Automation impacts how much of comments that are correctly classified by humans?']","['human analysis component', 'the platform', '23%']"
844,sentiment analysis,Evaluation,"The accuracy of a sentiment analysis system is, in principle, how well it agrees with human judgments. This is usually measured by variant measures based on precision and recall over the two target categories of negative and positive texts. However, according to research human raters typically only agree about 80% of the time (see Inter-rater reliability). Thus, a program that achieves 70% accuracy in classifying sentiment is doing nearly as well as humans, even though such accuracy may not sound impressive. If a program were ""right"" 100% of the time, humans would still disagree with it about 20% of the time, since they disagree that much about any answer.","The accuracy of a sentiment analysis system is, in principle, how well it agrees with human judgments. This is usually measured by variant measures based on precision and recall over the two target categories of negative and positive texts.","[' How well does a sentiment analysis system agree with human judgments?', ' Variant measures are based on what two target categories of negative and positive texts?']","['The accuracy', 'precision and recall']"
845,sentiment analysis,Evaluation,"On the other hand, computer systems will make very different errors than human assessors, and thus the figures are not entirely comparable. For instance, a computer system will have trouble with negations, exaggerations, jokes, or sarcasm, which typically are easy to handle for a human reader: some errors a computer system makes will seem overly naive to a human. In general, the utility for practical commercial tasks of sentiment analysis as it is defined in academic research has been called into question, mostly since the simple one-dimensional model of sentiment from negative to positive yields rather little actionable information for a client worrying about the effect of public discourse on e.g. brand or corporate reputation.","On the other hand, computer systems will make very different errors than human assessors, and thus the figures are not entirely comparable. For instance, a computer system will have trouble with negations, exaggerations, jokes, or sarcasm, which typically are easy to handle for a human reader: some errors a computer system makes will seem overly naive to a human.","[' Computer systems will make what kind of errors than human assessors?', ' A computer system will have trouble with negations, exaggerations, jokes, and what else?', ' What will a computer system make a human read?']","['very different', 'sarcasm', 'negations, exaggerations, jokes, or sarcasm']"
846,sentiment analysis,Evaluation,"To better fit market needs, evaluation of sentiment analysis has moved to more task-based measures, formulated together with representatives from PR agencies and market research professionals. The focus in e.g. the RepLab evaluation data set is less on the content of the text under consideration and more on the effect of the text in question on brand reputation.","To better fit market needs, evaluation of sentiment analysis has moved to more task-based measures, formulated together with representatives from PR agencies and market research professionals. The focus in e.g.","[' To better fit market needs, sentiment analysis has moved to more what?', ' Along with PR agencies and market research professionals, what has been the focus of sentiment analysis?']","['task-based measures', 'task-based measures']"
847,sentiment analysis,Web 2.0,"The rise of social media such as blogs and social networks has fueled interest in sentiment analysis.  With the proliferation of reviews, ratings, recommendations and other forms of online expression, online opinion has turned into a kind of virtual currency for businesses looking to market their products, identify new opportunities and manage their reputations.  As businesses look to automate the process of filtering out the noise, understanding the conversations, identifying the relevant content and actioning it appropriately, many are now looking to the field of sentiment analysis. Further complicating the matter, is the rise of anonymous social media platforms such as 4chan and Reddit. If web 2.0 was all about democratizing publishing, then the next stage of the web may well be based on democratizing data mining of all the content that is getting published.","The rise of social media such as blogs and social networks has fueled interest in sentiment analysis. With the proliferation of reviews, ratings, recommendations and other forms of online expression, online opinion has turned into a kind of virtual currency for businesses looking to market their products, identify new opportunities and manage their reputations.","[' What has fueled interest in sentiment analysis?', ' With the proliferation of reviews, ratings, recommendations, and other forms of online expression, what has turned into a virtual currency for businesses?', ' What are businesses looking to market their products, identify new opportunities and manage their reputation?']","['The rise of social media', 'online opinion', 'online opinion has turned into a kind of virtual currency']"
848,sentiment analysis,Web 2.0,"One step towards this aim is accomplished in research. Several research teams in universities around the world currently focus on understanding the dynamics of sentiment in e-communities through sentiment analysis. The CyberEmotions project, for instance, recently identified the role of negative emotions in driving social networks discussions.",One step towards this aim is accomplished in research. Several research teams in universities around the world currently focus on understanding the dynamics of sentiment in e-communities through sentiment analysis.,"[' What is the first step towards understanding sentiment in e-communities?', ' What do research teams in universities around the world focus on?']","['research', 'understanding the dynamics of sentiment in e-communities through sentiment analysis']"
849,sentiment analysis,Web 2.0,"The problem is that most sentiment analysis algorithms use simple terms to express sentiment about a product or service.  However, cultural factors, linguistic nuances, and differing contexts make it extremely difficult to turn a string of written text into a simple pro or con sentiment.  The fact that humans often disagree on the sentiment of text illustrates how big a task it is for computers to get this right.  The shorter the string of text, the harder it becomes.
","The problem is that most sentiment analysis algorithms use simple terms to express sentiment about a product or service. However, cultural factors, linguistic nuances, and differing contexts make it extremely difficult to turn a string of written text into a simple pro or con sentiment.","[' What do most sentiment analysis algorithms use to express sentiment about a product or service?', ' What makes it extremely difficult to turn a string of written text into a simple pro or con sentiment?']","['simple terms', 'cultural factors, linguistic nuances, and differing contexts']"
850,sentiment analysis,Web 2.0,"Even though short text strings might be a problem, sentiment analysis within microblogging has shown that Twitter can be seen as a valid online indicator of political sentiment. Tweets' political sentiment demonstrates close correspondence to parties' and politicians' political positions, indicating that the content of Twitter messages plausibly reflects the offline political landscape. Furthermore, sentiment analysis on Twitter has also been shown to capture the public mood behind human reproduction cycles globally, as well as other problems of public-health relevance such as adverse drug reactions.","Even though short text strings might be a problem, sentiment analysis within microblogging has shown that Twitter can be seen as a valid online indicator of political sentiment. Tweets' political sentiment demonstrates close correspondence to parties' and politicians' political positions, indicating that the content of Twitter messages plausibly reflects the offline political landscape.","[' What has shown that Twitter can be a valid online indicator of political sentiment?', "" What demonstrates close correspondence to parties' and politicians' political positions?"", ' What reflects the offline political landscape on Twitter?']","['sentiment analysis within microblogging', ""Tweets' political sentiment"", 'the content of Twitter messages']"
851,sentiment analysis,Web 2.0,"While sentiment analysis has been popular for domains where authors express their opinion rather explicitly (""the movie is awesome""), such as social media and product reviews, only recently robust methods were devised for other domains where sentiment is strongly implicit or indirect. For example, in news articles - mostly due to the expected journalistic objectivity - journalists often describe actions or events rather than directly stating the polarity of a piece of information. Earlier approaches using dictionaries or shallow machine learning features were unable to catch the ""meaning between the lines"", but recently researchers have proposed a deep learning based approach and dataset that is able to analyze sentiment in news articles.","While sentiment analysis has been popular for domains where authors express their opinion rather explicitly (""the movie is awesome""), such as social media and product reviews, only recently robust methods were devised for other domains where sentiment is strongly implicit or indirect. For example, in news articles - mostly due to the expected journalistic objectivity - journalists often describe actions or events rather than directly stating the polarity of a piece of information.","[' What has been popular for domains where authors express their opinion rather explicitly?', ' Social media and product reviews are examples of what kind of analysis?', ' What kind of methods have been devised for other domains?', ' What do journalists often describe rather than directly stating the polarity of a piece of information?']","['sentiment analysis', 'sentiment', 'robust', 'actions or events']"
852,sentiment analysis,Application in recommender systems,"For a recommender system, sentiment analysis has been proven to be a valuable technique. A recommender system aims to predict the preference for an item of a target user. Mainstream recommender systems work on explicit data set. For example, collaborative filtering works on the rating matrix, and content-based filtering works on the meta-data of the items.
","For a recommender system, sentiment analysis has been proven to be a valuable technique. A recommender system aims to predict the preference for an item of a target user.","[' What has been proven to be a valuable technique for a recommender system?', ' What aims to predict the preference for an item of a target user?']","['sentiment analysis', 'A recommender system']"
853,sentiment analysis,Application in recommender systems,"In many social networking services or e-commerce websites, users can provide text review, comment or feedback to the items. These user-generated text provide a rich source of user's sentiment opinions about numerous products and items. Potentially, for an item, such text can reveal both the related feature/aspects of the item and the users' sentiments on each feature. The item's feature/aspects described in the text play the same role with the meta-data in content-based filtering, but the former are more valuable for the recommender system. Since these features are broadly mentioned by users in their reviews, they can be seen as the most crucial features that can significantly influence the user's experience on the item, while the meta-data of the item (usually provided by the producers instead of consumers) may ignore features that are concerned by the users. For different items with common features, a user may give different sentiments. Also, a feature of the same item may receive different sentiments from different users. Users' sentiments on the features can be regarded as a multi-dimensional rating score, reflecting their preference on the items.
","In many social networking services or e-commerce websites, users can provide text review, comment or feedback to the items. These user-generated text provide a rich source of user's sentiment opinions about numerous products and items.","[' What can users provide in social networking services or e-commerce websites?', ' What can user-generated text provide a rich source of?']","['text review, comment or feedback to the items', ""user's sentiment opinions about numerous products and items""]"
854,sentiment analysis,Application in recommender systems,"Based on the feature/aspects and the sentiments extracted from the user-generated text, a hybrid recommender system can be constructed. There are two types of motivation to recommend a candidate item to a user. The first motivation is the candidate item have numerous common features with the user's preferred items, while the second motivation is that the candidate item receives a high sentiment on its features. For a preferred item, it is reasonable to believe that items with the same features will have a similar function or utility. So, these items will also likely to be preferred by the user. On the other hand, for a shared feature of two candidate items, other users may give positive sentiment to one of them while giving negative sentiment to another. Clearly, the high evaluated item should be recommended to the user. Based on these two motivations, a combination ranking score of similarity and sentiment rating can be constructed for each candidate item.","Based on the feature/aspects and the sentiments extracted from the user-generated text, a hybrid recommender system can be constructed. There are two types of motivation to recommend a candidate item to a user.","[' What can be constructed based on the feature/aspects and sentiments extracted from user-generated text?', ' How many types of motivation are there to recommend a candidate item to a user?']","['a hybrid recommender system', 'two']"
855,sentiment analysis,Application in recommender systems,"Except for the difficulty of the sentiment analysis itself, applying sentiment analysis on reviews or feedback also faces the challenge of spam and biased reviews. One direction of work is focused on evaluating the helpfulness of each review. Review or feedback poorly written is hardly helpful for recommender system. Besides, a review can be designed to hinder sales of a target product, thus be harmful to the recommender system even it is well written.
","Except for the difficulty of the sentiment analysis itself, applying sentiment analysis on reviews or feedback also faces the challenge of spam and biased reviews. One direction of work is focused on evaluating the helpfulness of each review.","[' What is another challenge of sentiment analysis?', ' What is one direction of work focused on evaluating?']","['spam and biased reviews', 'the helpfulness of each review']"
856,sentiment analysis,Application in recommender systems,"Researchers also found that long and short forms of user-generated text should be treated differently. An interesting result shows that short-form reviews are sometimes more helpful than long-form, because it is easier to filter out the noise in a short-form text. For the long-form text, the growing length of the text does not always bring a proportionate increase in the number of features or sentiments in the text.
","Researchers also found that long and short forms of user-generated text should be treated differently. An interesting result shows that short-form reviews are sometimes more helpful than long-form, because it is easier to filter out the noise in a short-form text.","[' Researchers found that long and short forms of user-generated text should be treated differently?', ' What is easier to filter out in a short-form text?']","['short-form reviews are sometimes more helpful than long-form, because it is easier to filter out the noise in a short-form text', 'noise']"
857,sentiment analysis,Application in recommender systems,"Lamba & Madhusudhan introduce a nascent way to cater the information needs of today's library users by repackaging the results from sentiment analysis of social media platforms like Twitter and provide it as a consolidated time-based service in different formats. Further, they propose a new way of conducting marketing in libraries using social media mining and sentiment analysis.
","Lamba & Madhusudhan introduce a nascent way to cater the information needs of today's library users by repackaging the results from sentiment analysis of social media platforms like Twitter and provide it as a consolidated time-based service in different formats. Further, they propose a new way of conducting marketing in libraries using social media mining and sentiment analysis.","["" Lamba & Madhusudhan introduce a new way to cater the information needs of today's library users by repackaging the results from sentiment analysis of social media platforms like Twitter and providing it as a consolidated time-based service in different formats?"", ' What do they propose a new way of doing in libraries using social media mining and sentiment analysis?']","[""Lamba & Madhusudhan introduce a nascent way to cater the information needs of today's library users by repackaging the results from sentiment analysis of social media platforms like Twitter and provide it as a consolidated time-based service in different formats."", 'conducting marketing']"
858,authentication,Summary,"Authentication (from Greek: αὐθεντικός authentikos, ""real, genuine"", from αὐθέντης authentes, ""author"") is the act of proving an assertion, such as the identity  of a computer system user. In contrast with identification, the act of indicating  a person or thing's identity, authentication is the process of verifying that identity. It might involve validating personal identity documents, verifying the authenticity of a website with a digital certificate, determining the age of an artifact by carbon dating, or ensuring that a product or document is not counterfeit.
","Authentication (from Greek: αὐθεντικός authentikos, ""real, genuine"", from αὐθέντης authentes, ""author"") is the act of proving an assertion, such as the identity  of a computer system user. In contrast with identification, the act of indicating  a person or thing's identity, authentication is the process of verifying that identity.","[' What is the Greek word for authentication?', ' What is an example of an assertion that authentication is the act of proving?']","['αὐθεντικός authentikos', 'the identity  of a computer system user']"
859,authentication,Methods,"Authentication is relevant to multiple fields. In art, antiques and anthropology, a common problem is verifying that a given artifact was produced by a certain person or in a certain place or period of history.  In computer science, verifying a user's identity is often required to allow access to confidential data or systems.","Authentication is relevant to multiple fields. In art, antiques and anthropology, a common problem is verifying that a given artifact was produced by a certain person or in a certain place or period of history.","[' Authentication is relevant to what fields?', ' What is a common problem in art, antiques and anthropology?']","['art, antiques and anthropology', 'verifying that a given artifact was produced by a certain person or in a certain place or period of history']"
860,authentication,Methods,"The first type of authentication is accepting proof of identity given by a credible person who has first-hand evidence that the identity is genuine.  When authentication is required of art or physical objects, this proof could be a friend, family member or colleague attesting to the item's provenance, perhaps by having witnessed the item in its creator's possession.  With autographed sports memorabilia, this could involve someone attesting that they witnessed the object being signed.  A vendor selling branded items implies authenticity, while he or she may not have evidence that every step in the supply chain was authenticated. Centralized authority-based trust relationships back most secure internet communication through known public certificate authorities; decentralized peer-based trust, also known as a web of trust, is used for personal services such as email or files (Pretty Good Privacy, GNU Privacy Guard) and trust is established by known individuals signing each other's cryptographic key at Key signing parties, for instance.
","The first type of authentication is accepting proof of identity given by a credible person who has first-hand evidence that the identity is genuine. When authentication is required of art or physical objects, this proof could be a friend, family member or colleague attesting to the item's provenance, perhaps by having witnessed the item in its creator's possession.","[' What is the first type of authentication?', ' Who has first-hand evidence that the identity is genuine?', ' What is required when authentication is required of art or physical objects?', "" Who attests to the item's provenance?"", "" Who witnessed the item in its creator's possession?""]","['accepting proof of identity', 'a credible person', ""a friend, family member or colleague attesting to the item's provenance"", 'a friend, family member or colleague', 'a friend, family member or colleague']"
861,authentication,Methods,"The second type of authentication is comparing the attributes of the object itself to what is known about objects of that origin.  For example, an art expert might look for similarities in the style of painting, check the location and form of a signature, or compare the object to an old photograph.  An archaeologist, on the other hand, might use carbon dating to verify the age of an artifact, do a chemical and spectroscopic analysis of the materials used, or compare the style of construction or decoration to other artifacts of similar origin.  The physics of sound and light, and comparison with a known physical environment, can be used to examine the authenticity of audio recordings, photographs, or videos.  Documents can be verified as being created on ink or paper readily available at the time of the item's implied creation.
","The second type of authentication is comparing the attributes of the object itself to what is known about objects of that origin. For example, an art expert might look for similarities in the style of painting, check the location and form of a signature, or compare the object to an old photograph.","[' What is the second type of authentication?', ' What might an art expert look for in the style of painting?', ' What is a form of a signature?', ' What is an object compared to an old photograph?']","['comparing the attributes of the object itself to what is known about objects of that origin', 'similarities', 'location', 'attributes of the object itself to what is known about objects of that origin. For example, an art expert might look for similarities in the style of painting, check the location and form of a signature']"
862,authentication,Methods,"Attribute comparison may be vulnerable to forgery.  In general, it relies on the facts that creating a forgery indistinguishable from a genuine artifact requires expert knowledge, that mistakes are easily made, and that the amount of effort required to do so is considerably greater than the amount of profit that can be gained from the forgery.
","Attribute comparison may be vulnerable to forgery. In general, it relies on the facts that creating a forgery indistinguishable from a genuine artifact requires expert knowledge, that mistakes are easily made, and that the amount of effort required to do so is considerably greater than the amount of profit that can be gained from the forgery.","[' Attribute comparison may be vulnerable to what?', ' What does forgery require?', ' How can mistakes be easily made?', ' How much greater is the amount of profit that can be gained from the forgery?']","['forgery', 'expert knowledge', 'creating a forgery indistinguishable from a genuine artifact requires expert knowledge', 'considerably greater']"
863,authentication,Methods,"In art and antiques, certificates are of great importance for authenticating an object of interest and value. Certificates can, however, also be forged, and the authentication of these poses a problem. For instance, the son of Han van Meegeren, the well-known art-forger, forged the work of his father and provided a certificate for its provenance as well; see the article Jacques van Meegeren.
","In art and antiques, certificates are of great importance for authenticating an object of interest and value. Certificates can, however, also be forged, and the authentication of these poses a problem.","[' What is of great importance for authenticating an object of interest and value?', ' Certificates can be forged, and what poses a problem?']","['certificates', 'the authentication']"
864,authentication,Methods,"Currency and other financial instruments commonly use this second type of authentication method.  Bills, coins, and cheques incorporate hard-to-duplicate physical features, such as fine printing or engraving, distinctive feel, watermarks, and holographic imagery, which are easy for trained receivers to verify.
","Currency and other financial instruments commonly use this second type of authentication method. Bills, coins, and cheques incorporate hard-to-duplicate physical features, such as fine printing or engraving, distinctive feel, watermarks, and holographic imagery, which are easy for trained receivers to verify.","[' Currency and other financial instruments commonly use what second type of authentication method?', ' Bills, coins, and cheques incorporate what hard-to-duplicate physical features?', ' What are easy for trained receivers to verify?']","['Bills', 'fine printing or engraving, distinctive feel, watermarks, and holographic imagery', 'Bills, coins, and cheques incorporate hard-to-duplicate physical features, such as fine printing or engraving, distinctive feel, watermarks, and holographic imagery']"
865,authentication,Methods,"The third type of authentication relies on documentation or other external affirmations.  In criminal courts, the rules of evidence often require establishing the chain of custody of evidence presented.  This can be accomplished through a written evidence log, or by testimony from the police detectives and forensics staff that handled it.  Some antiques are accompanied by certificates attesting to their authenticity.  Signed sports memorabilia is usually accompanied by a certificate of authenticity.  These external records have their own problems of forgery and perjury, and are also vulnerable to being separated from the artifact and lost.
","The third type of authentication relies on documentation or other external affirmations. In criminal courts, the rules of evidence often require establishing the chain of custody of evidence presented.","[' What type of authentication relies on documentation or other external affirmations?', ' In criminal courts, the rules of evidence often require establishing what?']","['third', 'the chain of custody of evidence presented']"
866,authentication,Methods,"In computer science, a user can be given access to secure systems based on user credentials that imply authenticity.  A network administrator can give a user a password, or provide the user with a key card or other access device to allow system access.  In this case, authenticity is implied but not guaranteed.
","In computer science, a user can be given access to secure systems based on user credentials that imply authenticity. A network administrator can give a user a password, or provide the user with a key card or other access device to allow system access.","[' In computer science, a user can be given access to secure systems based on user credentials that imply what?', ' A network administrator can give the user a password, or provide the user with a key card or other access device to allow system access?']","['authenticity', 'computer science, a user can be given access to secure systems based on user credentials that imply authenticity.']"
867,authentication,Methods,"Consumer goods such as pharmaceuticals, perfume, fashion clothing can use all three forms of authentication to prevent counterfeit goods from taking advantage of a popular brand's reputation (damaging the brand owner's sales and reputation).  As mentioned above, having an item for sale in a reputable store implicitly attests to it being genuine, the first type of authentication.  The second type of authentication might involve comparing the quality and craftsmanship of an item, such as an expensive handbag, to genuine articles.  The third type of authentication could be the presence of a trademark on the item, which is a legally protected marking, or any other identifying feature which aids consumers in the identification of genuine brand-name goods.  With software, companies have taken great steps to protect from counterfeiters, including adding holograms, security rings, security threads and color shifting ink.","Consumer goods such as pharmaceuticals, perfume, fashion clothing can use all three forms of authentication to prevent counterfeit goods from taking advantage of a popular brand's reputation (damaging the brand owner's sales and reputation). As mentioned above, having an item for sale in a reputable store implicitly attests to it being genuine, the first type of authentication.","["" How many forms of authentication can consumer goods use to prevent counterfeit goods from taking advantage of a popular brand's reputation?"", ' What does having an item for sale in a reputable store implicitly attest to?', ' For sale in a reputable store implicitly attests to it being what?']","['all three', 'it being genuine', 'genuine']"
868,authentication,Authentication factors,"The ways in which someone may be authenticated fall into three categories, based on what are known as the factors of authentication: something the user knows, something the user has, and something the user is. Each authentication factor covers a range of elements used to authenticate or verify a person's identity prior to being granted access, approving a transaction request, signing a document or other work product, granting authority to others, and establishing a chain of authority.
","The ways in which someone may be authenticated fall into three categories, based on what are known as the factors of authentication: something the user knows, something the user has, and something the user is. Each authentication factor covers a range of elements used to authenticate or verify a person's identity prior to being granted access, approving a transaction request, signing a document or other work product, granting authority to others, and establishing a chain of authority.","[' How many categories do the ways in which someone may be authenticated fall into?', ' What are known as the factors of authentication?', "" What is used to authenticate or verify a person's identity prior to being granted access?"", ' What is the range of elements used to approving a transaction request?']","['three', 'something the user knows, something the user has, and something the user is', 'Each authentication factor', 'Each authentication factor']"
869,authentication,Authentication factors,"Security research has determined that for a positive authentication, elements from at least two, and preferably all three, factors should be verified. The three factors (classes) and some of elements of each factor are:
","Security research has determined that for a positive authentication, elements from at least two, and preferably all three, factors should be verified. The three factors (classes) and some of elements of each factor are:","[' How many factors should be verified for a positive authentication?', ' What are the three classes of factors?']","['at least two, and preferably all three', 'classes']"
870,authentication,Information content,"Literary forgery can involve imitating the style of a famous author.  If an original manuscript, typewritten text, or recording is available, then the medium itself (or its packaging – anything from a box to e-mail headers) can help prove or disprove the authenticity of the document. However, text, audio, and video can be copied into new media, possibly leaving only the informational content itself to use in authentication. Various systems have been invented to allow authors to provide a means for readers to reliably authenticate that a given message originated from or was relayed by them.  These involve authentication factors like:
","Literary forgery can involve imitating the style of a famous author. If an original manuscript, typewritten text, or recording is available, then the medium itself (or its packaging – anything from a box to e-mail headers) can help prove or disprove the authenticity of the document.","[' Literary forgery can involve imitating the style of what famous author?', ' If an original manuscript, typewritten text, or recording is available, what can help prove the authenticity of the document?']","['Literary', 'the medium itself']"
871,authentication,Information content,"The opposite problem is detection of plagiarism, where information from a different author is passed off as a person's own work.  A common technique for proving plagiarism is the discovery of another copy of the same or very similar text, which has different attribution.  In some cases, excessively high quality or a style mismatch may raise suspicion of plagiarism.
","The opposite problem is detection of plagiarism, where information from a different author is passed off as a person's own work. A common technique for proving plagiarism is the discovery of another copy of the same or very similar text, which has different attribution.","[' What is a common technique for proving plagiarism?', ' What is the opposite problem of plagiarism detection?', ' How is information from a different author passed off?']","['the discovery of another copy of the same or very similar text', ""information from a different author is passed off as a person's own work"", ""as a person's own work""]"
872,authentication,History and state-of-the-art,"Historically, fingerprints have been used as the most authoritative method of authentication, but court cases in the US and elsewhere have raised fundamental doubts about fingerprint reliability. Outside of the legal system as well, fingerprints have been shown to be easily spoofable, with British Telecom's top computer-security official noting that ""few"" fingerprint readers have not already been tricked by one spoof or another. Hybrid or two-tiered authentication methods offer a compelling solution, such as private keys encrypted by fingerprint inside of a USB device.
","Historically, fingerprints have been used as the most authoritative method of authentication, but court cases in the US and elsewhere have raised fundamental doubts about fingerprint reliability. Outside of the legal system as well, fingerprints have been shown to be easily spoofable, with British Telecom's top computer-security official noting that ""few"" fingerprint readers have not already been tricked by one spoof or another.","[' What has historically been used as the most authoritative method of authentication?', ' What have court cases in the US and elsewhere raised fundamental doubts about?', ' Outside of the legal system, fingerprints have been shown to be what?', ' Who noted that fingerprints were easily spoofable?', ' What is easily spoofable?', "" What has British Telecom's top computer-security official noted?"", ' How many fingerprint readers have not already been tricked?']","['fingerprints', 'fingerprint reliability', 'easily spoofable', ""British Telecom's top computer-security official"", 'fingerprints', '""few"" fingerprint readers have not already been tricked by one spoof or another', 'few']"
873,authentication,History and state-of-the-art,"In a computer data context, cryptographic methods have been developed (see digital signature and challenge–response authentication) which are currently not spoofable if and only if the originator's key has not been compromised. That the originator (or anyone other than an attacker) knows (or doesn't know) about a compromise is irrelevant. It is not known whether these cryptographically based authentication methods are provably secure, since unanticipated mathematical developments may make them vulnerable to attack in future. If that were to occur, it may call into question much of the authentication in the past. In particular, a digitally signed contract may be questioned when a new attack on the cryptography underlying the signature is discovered.","In a computer data context, cryptographic methods have been developed (see digital signature and challenge–response authentication) which are currently not spoofable if and only if the originator's key has not been compromised. That the originator (or anyone other than an attacker) knows (or doesn't know) about a compromise is irrelevant.","[' What cryptographic methods have been developed in a computer data context?', "" What are currently not spoofable if and only if the originator's key has not been compromised?""]","['digital signature and challenge–response authentication', 'cryptographic methods have been developed (see digital signature and challenge–response authentication']"
874,authentication,Authorization,"The process of authorization is distinct from that of authentication. Whereas authentication is the process of verifying that ""you are who you say you are"", authorization is the process of verifying that ""you are permitted to do what you are trying to do"". While authorization often happens immediately after authentication (e.g., when logging into a computer system), this does not mean authorization presupposes authentication: an anonymous agent could be authorized to a limited action set.","The process of authorization is distinct from that of authentication. Whereas authentication is the process of verifying that ""you are who you say you are"", authorization is the process of verifying that ""you are permitted to do what you are trying to do"".","[' What is the process of verifying that ""you are who you say you are?""', ' What is authorization?']","['authentication', 'the process of verifying that ""you are permitted to do what you are trying to do"".']"
875,authentication,Access control,"One familiar use of authentication and authorization is access control. A computer system that is supposed to be used only by those authorized must attempt to detect and exclude the unauthorized. Access to it is therefore usually controlled by insisting on an authentication procedure to establish with some degree of confidence the identity of the user, granting privileges established for that identity.
",One familiar use of authentication and authorization is access control. A computer system that is supposed to be used only by those authorized must attempt to detect and exclude the unauthorized.,"[' What is one common use of authentication and authorization?', ' A computer system that is supposed to be used only by those authorized must try to detect and exclude who?']","['access control', 'the unauthorized']"
876,boolean function,Summary,"In mathematics, a Boolean function is a function whose arguments and result assume values from a two-element set (usually {true, false}, {0,1} or {-1,1}). Alternative names are switching function, used especially in older computer science literature, and truth function (or logical function), used in logic. Boolean functions are the subject of Boolean algebra and switching theory.","In mathematics, a Boolean function is a function whose arguments and result assume values from a two-element set (usually {true, false}, {0,1} or {-1,1}). Alternative names are switching function, used especially in older computer science literature, and truth function (or logical function), used in logic.","["" What is a Boolean function's arguments and result assumed values from?"", ' What are two alternative names to switching function?', ' Switching function is especially used in older computer science literature, what is it?']","['a two-element set', 'truth function', 'switching function']"
877,boolean function,Summary,"A Boolean function takes the form 



f
:
{
0
,
1

}

k


→
{
0
,
1
}


{\displaystyle f:\{0,1\}^{k}\to \{0,1\}}
, where 



{
0
,
1
}


{\displaystyle \{0,1\}}
 is known as the Boolean domain and 



k


{\displaystyle k}
 is a non-negative integer called the arity of the function. In the case where 



k
=
0


{\displaystyle k=0}
, the ""function"" is a constant element of 



{
0
,
1
}


{\displaystyle \{0,1\}}
. A Boolean function with multiple outputs, 



f
:
{
0
,
1

}

k


→
{
0
,
1

}

m




{\displaystyle f:\{0,1\}^{k}\to \{0,1\}^{m}}
 with 



m
>
1


{\displaystyle m>1}
 is a vectorial or vector-valued Boolean function (an S-box in cryptography).","A Boolean function takes the form 



f
:
{
0
,
1

}

k


→
{
0
,
1
}


{\displaystyle f:\{0,1\}^{k}\to \{0,1\}}
, where 



{
0
,
1
}


{\displaystyle \{0,1\}}
 is known as the Boolean domain and 



k


{\displaystyle k}
 is a non-negative integer called the arity of the function. In the case where 



k
=
0


{\displaystyle k=0}
, the ""function"" is a constant element of 



{
0
,
1
}


{\displaystyle \{0,1\}}
.","[' What is known as the Boolean domain?', ' What is a non-negative integer called?']","['{\n0\n,\n1\n}\n\n\n{\\displaystyle \\{0,1\\}}', 'the arity']"
878,boolean function,Summary,"Every 



k


{\displaystyle k}
-ary Boolean function can be expressed as a propositional formula in 



k


{\displaystyle k}
 variables 




x

1


,
.
.
.
,

x

k




{\displaystyle x_{1},...,x_{k}}
, and two propositional formulas are logically equivalent if and only if they express the same Boolean function.
","Every 



k


{\displaystyle k}
-ary Boolean function can be expressed as a propositional formula in 



k


{\displaystyle k}
 variables 




x

1


,
. .",[' Every k <unk>displaystyle k<unk> -ary Boolean function can be expressed as what?'],['a propositional formula']
879,approximation algorithm,Summary,"In computer science and operations research, approximation algorithms are efficient algorithms that find approximate solutions to optimization problems (in particular NP-hard problems) with provable guarantees on the distance of the returned solution to the optimal one. Approximation algorithms naturally arise in the field of theoretical computer science as a consequence of the widely believed P ≠ NP conjecture. Under this conjecture, a wide class of optimization problems cannot be solved exactly in polynomial time. The field of approximation algorithms, therefore, tries to understand how closely it is possible to approximate optimal solutions to such problems in polynomial time. In an overwhelming majority of the cases, the guarantee of such algorithms is a multiplicative one expressed as an approximation ratio or approximation factor i.e., the optimal solution is always guaranteed to be within a (predetermined) multiplicative factor of the returned solution. However, there are also many approximation algorithms that provide an additive guarantee on the quality of the returned solution. A notable example of an approximation algorithm that provides both is the classic approximation algorithm of Lenstra, Shmoys and Tardos for scheduling on unrelated parallel machines.
","In computer science and operations research, approximation algorithms are efficient algorithms that find approximate solutions to optimization problems (in particular NP-hard problems) with provable guarantees on the distance of the returned solution to the optimal one. Approximation algorithms naturally arise in the field of theoretical computer science as a consequence of the widely believed P ≠ NP conjecture.","[' What are efficient algorithms that find approximate solutions to optimization problems?', ' What do approximation algorithms find with provable guarantees on the distance of the returned solution to the optimal one?', ' What conjecture is widely believed in the field of theoretical computer science?']","['approximation algorithms', 'approximate solutions to optimization problems', 'P ≠ NP']"
880,approximation algorithm,Summary,"The design and analysis of approximation algorithms crucially involves a mathematical proof certifying the quality of the returned solutions in the worst case. This distinguishes them from heuristics such as annealing or genetic algorithms, which find reasonably good solutions on some inputs, but provide no clear indication at the outset on when they may succeed or fail.
","The design and analysis of approximation algorithms crucially involves a mathematical proof certifying the quality of the returned solutions in the worst case. This distinguishes them from heuristics such as annealing or genetic algorithms, which find reasonably good solutions on some inputs, but provide no clear indication at the outset on when they may succeed or fail.","[' What does the design and analysis of approximation algorithms involve?', ' What distinguishes them from heuristics such as annealing or genetic algorithms?', ' What does not provide a clear indication at the outset on when inputs may succeed or fail?']","['a mathematical proof certifying the quality of the returned solutions in the worst case', 'mathematical proof certifying the quality of the returned solutions in the worst case', 'genetic algorithms']"
881,approximation algorithm,Summary,"There is widespread interest in theoretical computer science to better understand the limits to which we can approximate certain famous optimization problems. For example, one of the long-standing open questions in computer science is to determine whether there is an algorithm that outperforms the 1.5 approximation algorithm of Christofides to the metric traveling salesman problem. The desire to understand hard optimization problems from the perspective of approximability is motivated by the discovery of surprising mathematical connections and broadly applicable techniques to design algorithms for hard optimization problems. One well-known example of the former is the Goemans–Williamson algorithm for maximum cut, which solves a graph theoretic problem using high dimensional geometry.","There is widespread interest in theoretical computer science to better understand the limits to which we can approximate certain famous optimization problems. For example, one of the long-standing open questions in computer science is to determine whether there is an algorithm that outperforms the 1.5 approximation algorithm of Christofides to the metric traveling salesman problem.","[' What is a long-standing open question in computer science?', ' What algorithm outperforms the 1.5 approximation algorithm?', ' What algorithm outperforms the 1.5 approximation algorithm of Christofides to the metric traveling salesman problem?']","['to determine whether there is an algorithm that outperforms the 1.5 approximation algorithm of Christofides to the metric traveling salesman problem', 'Christofides', 'algorithm']"
882,approximation algorithm,Introduction,"A simple example of an approximation algorithm is one for the minimum vertex cover problem, where the goal is to choose the smallest set of vertices such that every edge in the input graph contains at least one chosen vertex. One way to find a vertex cover is to repeat the following process: find an uncovered edge, add both its endpoints to the cover, and remove all edges incident to either vertex from the graph. As any vertex cover of the input graph must use a distinct vertex to cover each edge that was considered in the process (since it forms a matching), the vertex cover produced, therefore, is at most twice as large as the optimal one. In other words, this is a constant factor approximation algorithm with an approximation factor of 2. Under the recent unique games conjecture, this factor is even the best possible one.","A simple example of an approximation algorithm is one for the minimum vertex cover problem, where the goal is to choose the smallest set of vertices such that every edge in the input graph contains at least one chosen vertex. One way to find a vertex cover is to repeat the following process: find an uncovered edge, add both its endpoints to the cover, and remove all edges incident to either vertex from the graph.","[' What is the goal of an approximation algorithm?', ' What is one way to find a vertex cover?', ' How do you find an uncovered edge, add both its endpoints to the cover and remove all edges incident to either vertex from the graph?']","['to choose the smallest set of vertices', 'to repeat the following process', 'repeat the following process']"
883,approximation algorithm,Introduction,"NP-hard problems vary greatly in their approximability; some, such as the knapsack problem, can be approximated within a multiplicative factor 



1
+
ϵ


{\displaystyle 1+\epsilon }
, for any fixed 



ϵ
>
0


{\displaystyle \epsilon >0}
, and therefore produce solutions arbitrarily close to the optimum (such a family of approximation algorithms is called a polynomial time approximation scheme or PTAS). Others are impossible to approximate within any constant, or even polynomial, factor unless P = NP, as in the case of the maximum clique problem. Therefore, an important benefit of studying approximation algorithms is a fine-grained classification of the difficulty of various NP-hard problems beyond the one afforded by the theory of NP-completeness. In other words, although NP-complete problems may be equivalent (under polynomial time reductions) to each other from the perspective of exact solutions, the corresponding optimization problems behave very differently from the perspective of approximate solutions.
","NP-hard problems vary greatly in their approximability; some, such as the knapsack problem, can be approximated within a multiplicative factor 



1
+
ϵ


{\displaystyle 1+\epsilon }
, for any fixed 



ϵ
>
0


{\displaystyle \epsilon >0}
, and therefore produce solutions arbitrarily close to the optimum (such a family of approximation algorithms is called a polynomial time approximation scheme or PTAS). Others are impossible to approximate within any constant, or even polynomial, factor unless P = NP, as in the case of the maximum clique problem.","[' What type of problem can be approximated within a multiplicative factor 1 + <unk> <unk>displaystyle 1+<unk>epsilon <unk>?', ' What is a family of approximation algorithms called?', ' What is the family of approximation algorithms called?', ' What is PTAS?']","['knapsack problem', 'polynomial time approximation scheme', 'polynomial time approximation scheme', 'polynomial time approximation scheme']"
884,approximation algorithm,Algorithm design techniques,"By now there are several established techniques to design approximation algorithms. These include the following ones.
",By now there are several established techniques to design approximation algorithms. These include the following ones.,[' How many established techniques are there to design approximation algorithms?'],['several']
885,approximation algorithm,A posteriori guarantees,"While approximation algorithms always provide an a priori worst case guarantee (be it additive or multiplicative), in some cases they also provide an a posteriori guarantee that is often much better. This is often the case for algorithms that work by solving a convex relaxation of the optimization problem on the given input. For example, there is a different approximation algorithm for minimum vertex cover that solves a linear programming relaxation to find a vertex cover that is at most twice the value of the relaxation. Since the value of the relaxation is never larger than the size of the optimal vertex cover, this yields another 2-approximation algorithm. While this is similar to the a priori guarantee of the previous approximation algorithm, the guarantee of the latter can be much better (indeed when the value of the LP relaxation is far from the size of the optimal vertex cover).
","While approximation algorithms always provide an a priori worst case guarantee (be it additive or multiplicative), in some cases they also provide an a posteriori guarantee that is often much better. This is often the case for algorithms that work by solving a convex relaxation of the optimization problem on the given input.","[' What guarantee do approximation algorithms always provide?', ' What is often the case for algorithms that work by solving a convex relaxation of the optimization problem?', ' By solving a convex relaxation of the optimization problem on the given input?']","['a priori worst case guarantee', 'a posteriori guarantee', 'approximation algorithms']"
886,approximation algorithm,Hardness of approximation,"Approximation algorithms as a research area is closely related to and informed by inapproximability theory where the non-existence of efficient algorithms with certain approximation ratios is proved (conditioned on widely believed hypotheses such as the P ≠ NP conjecture) by means of reductions. In the case of the metric traveling salesman problem, the best known inapproximability result rules out algorithms with an approximation ratio less than 123/122 ≈ 1.008196 unless P = NP, Karpinski, Lampis, Schmied. Coupled with the knowledge of the existence of Christofides' 1.5 approximation algorithm, this tells us that the threshold of approximability for metric traveling salesman (if it exists) is somewhere between 123/122 and 1.5.
","Approximation algorithms as a research area is closely related to and informed by inapproximability theory where the non-existence of efficient algorithms with certain approximation ratios is proved (conditioned on widely believed hypotheses such as the P ≠ NP conjecture) by means of reductions. In the case of the metric traveling salesman problem, the best known inapproximability result rules out algorithms with an approximation ratio less than 123/122 ≈ 1.008196 unless P = NP, Karpinski, Lampis, Schmied.","[' Approximation algorithms are closely related to and informed by what theory?', ' The non-existence of efficient algorithms with certain approximations ratios is proved by means of what?', ' What is the best known inapproximability result in the case of the metric traveling salesman problem?', ' What rules out algorithms with an approximation ratio less than 123/122 <unk> 1.008196?']","['inapproximability', 'reductions', 'rules out algorithms with an approximation ratio less than 123/122 ≈ 1.008196', 'inapproximability result']"
887,approximation algorithm,Hardness of approximation,"While inapproximability results have been proved since the 1970s, such results were obtained by ad hoc means and no systematic understanding was available at the time. It is only since the 1990 result of Feige, Goldwasser, Lovász, Safra and Szegedy on the inapproximability of Independent Set and the famous PCP theorem, that modern tools for proving inapproximability results were uncovered. The PCP theorem, for example, shows that Johnson's 1974 approximation algorithms for Max SAT, set cover, independent set and coloring all achieve the optimal approximation ratio, assuming P ≠ NP.","While inapproximability results have been proved since the 1970s, such results were obtained by ad hoc means and no systematic understanding was available at the time. It is only since the 1990 result of Feige, Goldwasser, Lovász, Safra and Szegedy on the inapproximability of Independent Set and the famous PCP theorem, that modern tools for proving inapproximability results were uncovered.","[' Since when have inapproximability results been proved?', ' What was the only way to obtain the inapprovability results?', ' When did Feige, Goldwasser, Lovász, Safra and Szegedy prove the inprovability of Independent Set and PCP?', ' What is the famous PCP theorem?', ' Modern tools for proving inapproximability results were uncovered?']","['since the 1970s', 'ad hoc means', '1990', 'inapproximability of Independent Set', '1990 result of Feige, Goldwasser, Lovász, Safra and Szegedy on the inapproximability of Independent Set and the famous PCP theorem, that modern']"
888,approximation algorithm,Practicality,"Not all approximation algorithms are suitable for direct practical applications. Some involve solving non-trivial linear programming/semidefinite relaxations (which may themselves invoke the ellipsoid algorithm), complex data structures, or sophisticated algorithmic techniques, leading to difficult implementation issues or improved running time performance (over exact algorithms) only on impractically large inputs. Implementation and running time issues aside, the guarantees provided by approximation algorithms may themselves not be strong enough to justify their consideration in practice. Despite their inability to be used ""out of the box"" in practical applications, the ideas and insights behind the design of such algorithms can often be incorporated in other ways in practical algorithms. In this way, the study of even very expensive algorithms is not a completely theoretical pursuit as they can yield valuable insights.
","Not all approximation algorithms are suitable for direct practical applications. Some involve solving non-trivial linear programming/semidefinite relaxations (which may themselves invoke the ellipsoid algorithm), complex data structures, or sophisticated algorithmic techniques, leading to difficult implementation issues or improved running time performance (over exact algorithms) only on impractically large inputs.","[' Not all approximation algorithms are suitable for what kind of applications?', ' Non-trivial linear programming/semidefinite relaxations may invoke what?']","['direct practical', 'the ellipsoid algorithm']"
889,approximation algorithm,Practicality,"In other cases, even if the initial results are of purely theoretical interest, over time, with an improved understanding, the algorithms may be refined to become more practical. One such example is the initial PTAS for Euclidean TSP by Sanjeev Arora (and independently by Joseph Mitchell) which had a prohibitive running time of 




n

O
(
1

/

ϵ
)




{\displaystyle n^{O(1/\epsilon )}}
 for a 



1
+
ϵ


{\displaystyle 1+\epsilon }
 approximation. Yet, within a year these ideas were incorporated into a near-linear time 



O
(
n
log
⁡
n
)


{\displaystyle O(n\log n)}
 algorithm for any constant 



ϵ
>
0


{\displaystyle \epsilon >0}
.","In other cases, even if the initial results are of purely theoretical interest, over time, with an improved understanding, the algorithms may be refined to become more practical. One such example is the initial PTAS for Euclidean TSP by Sanjeev Arora (and independently by Joseph Mitchell) which had a prohibitive running time of 




n

O
(
1

/

ϵ
)




{\displaystyle n^{O(1/\epsilon )}}
 for a 



1
+
ϵ


{\displaystyle 1+\epsilon }
 approximation.","[' What may be refined over time to become more practical?', "" What was Sanjeev Arora's initial PTAS for Euclidean TSP?"", ' What was the prohibitive running time of Arora?', "" What was Joseph Mitchell's running time?""]","['algorithms', 'n\n\nO\n(\n1\n\n/\n\nϵ\n)\n\n\n\n\n{\\displaystyle n^{O(1/\\epsilon )}}\n for a \n\n\n\n1\n+\nϵ', 'n\n\nO\n(\n1\n\n/\n\nϵ\n)\n\n\n\n\n{\\displaystyle n^{O(1/\\epsilon )}}\n for a \n\n\n\n1\n+\nϵ', 'n^{O(1/\\epsilon )}}\n for a \n\n\n\n1\n+\nϵ']"
890,approximation algorithm,Performance guarantees,"For some approximation algorithms it is possible to prove certain properties about the approximation of the optimum result. For example, a ρ-approximation algorithm A is defined to be an algorithm for which it has been proven that the value/cost, f(x), of the approximate solution A(x) to an instance x will not be more (or less, depending on the situation) than a factor ρ times the value, OPT, of an optimum solution.
","For some approximation algorithms it is possible to prove certain properties about the approximation of the optimum result. For example, a ρ-approximation algorithm A is defined to be an algorithm for which it has been proven that the value/cost, f(x), of the approximate solution A(x) to an instance x will not be more (or less, depending on the situation) than a factor ρ times the value, OPT, of an optimum solution.","[' What is it possible to prove about the approximation of the optimum result?', ' What is defined to be an algorithm for which it has been proven that the value/cost, f(x), of the approximate solution A(x) to an instance x will be proved?', ' What is a factor <unk> times the value of an optimum solution?']","['certain properties', 'ρ-approximation algorithm A', 'ρ']"
891,approximation algorithm,Performance guarantees,"The factor ρ is called the relative performance guarantee. An approximation algorithm has an absolute performance guarantee or bounded error c, if it has been proven for every instance x that
","The factor ρ is called the relative performance guarantee. An approximation algorithm has an absolute performance guarantee or bounded error c, if it has been proven for every instance x that","[' What is the factor <unk> called?', ' An approximation algorithm has an absolute performance guarantee or what else?']","['relative performance guarantee', 'bounded error c']"
892,approximation algorithm,Performance guarantees,"where f(y) is the value/cost of the solution y for the instance x. Clearly, the performance guarantee is greater than or equal to 1 and equal to 1 if and only if y is an optimal solution. If an algorithm A guarantees to return solutions with a performance guarantee of at most r(n), then A is said to be an r(n)-approximation algorithm and has an approximation ratio of r(n). Likewise, a problem with an r(n)-approximation algorithm is said to be r(n)-approximable or have an approximation ratio of r(n).","where f(y) is the value/cost of the solution y for the instance x. Clearly, the performance guarantee is greater than or equal to 1 and equal to 1 if and only if y is an optimal solution.","[' Where f(y) is the value/cost of the solution y for the instance x?', ' The performance guarantee is greater than or equal to what?']","['performance guarantee is greater than or equal to 1 and equal to 1 if and only if y is an optimal solution', '1']"
893,approximation algorithm,Performance guarantees,"For minimization problems, the two different guarantees provide the same result and that for maximization problems, a relative performance guarantee of ρ is equivalent to a performance guarantee of 



r
=

ρ

−
1




{\displaystyle r=\rho ^{-1}}
. In the literature, both definitions are common but it is clear which definition is used since, for maximization problems, as ρ ≤ 1 while r ≥ 1.
","For minimization problems, the two different guarantees provide the same result and that for maximization problems, a relative performance guarantee of ρ is equivalent to a performance guarantee of 



r
=

ρ

−
1




{\displaystyle r=\rho ^{-1}}
. In the literature, both definitions are common but it is clear which definition is used since, for maximization problems, as ρ ≤ 1 while r ≥ 1.","[' How many different guarantees provide the same result for minimization problems?', ' What is equivalent to a performance guarantee of r = <unk> <unk> 1?', ' In the literature, both definitions are common but it is unclear which definition is used?', ' What is used for maximization problems?', ' What is the definition used for?']","['two', '\\rho', 'r=\\rho ^{-1', 'r=\\rho ^{-1', 'maximization problems']"
894,approximation algorithm,Performance guarantees,"The absolute performance guarantee 





P


A




{\displaystyle \mathrm {P} _{A}}
 of some approximation algorithm A, where x refers to an instance of a problem, and where 




R

A


(
x
)


{\displaystyle R_{A}(x)}
 is the performance guarantee of A on x (i.e. ρ for problem instance x) is:
","The absolute performance guarantee 





P


A




{\displaystyle \mathrm {P} _{A}}
 of some approximation algorithm A, where x refers to an instance of a problem, and where 




R

A


(
x
)


{\displaystyle R_{A}(x)}
 is the performance guarantee of A on x (i.e. ρ for problem instance x) is:","[' What is the absolute performance guarantee of some approximation algorithm A?', ' What refers to an instance of a problem?']","['\\mathrm', 'x']"
895,approximation algorithm,Performance guarantees,"That is to say that 





P


A




{\displaystyle \mathrm {P} _{A}}
 is the largest bound on the approximation ratio, r, that one sees over all possible instances of the problem. Likewise, the asymptotic performance ratio 




R

A


∞




{\displaystyle R_{A}^{\infty }}
 is:
","That is to say that 





P


A




{\displaystyle \mathrm {P} _{A}}
 is the largest bound on the approximation ratio, r, that one sees over all possible instances of the problem. Likewise, the asymptotic performance ratio 




R

A


∞




{\displaystyle R_{A}^{\infty }}
 is:","[' What is the largest bound on the approximation ratio that one sees over all possible instances of the problem?', ' The asymptotic performance ratio R A <unk> <unk>displaystyle R_<unk>A<unk>infty <unk> is:']","['P\n\n\nA\n\n\n\n\n{\\displaystyle \\mathrm {P} _{A}}\n is the largest bound on the approximation ratio, r', 'R\n\nA\n\n\n∞']"
896,approximation algorithm,Performance guarantees,"That is to say that it is the same as the absolute performance ratio, with a lower bound n on the size of problem instances. These two types of ratios are used because there exist algorithms where the difference between these two is significant.
","That is to say that it is the same as the absolute performance ratio, with a lower bound n on the size of problem instances. These two types of ratios are used because there exist algorithms where the difference between these two is significant.","[' What is the same as the absolute performance ratio?', ' What is a lower bound on the size of problem instances?', ' Why are these two types of ratios used?']","['it is the same as the absolute performance ratio, with a lower bound n on the size of problem instances', 'n', 'because there exist algorithms where the difference between these two is significant']"
897,approximation algorithm,Epsilon terms,"In the literature, an approximation ratio for a maximization (minimization) problem of c - ϵ (min: c + ϵ) means that the algorithm has an approximation ratio of c ∓ ϵ  for arbitrary ϵ > 0 but that the ratio has not (or cannot) be shown for ϵ = 0. An example of this is the optimal inapproximability — inexistence of approximation — ratio of 7 / 8 + ϵ for satisfiable MAX-3SAT instances due to Johan Håstad. As mentioned previously, when c = 1, the problem is said to have a polynomial-time approximation scheme.
","In the literature, an approximation ratio for a maximization (minimization) problem of c - ϵ (min: c + ϵ) means that the algorithm has an approximation ratio of c ∓ ϵ  for arbitrary ϵ > 0 but that the ratio has not (or cannot) be shown for ϵ = 0. An example of this is the optimal inapproximability — inexistence of approximation — ratio of 7 / 8 + ϵ for satisfiable MAX-3SAT instances due to Johan Håstad.","[' What is an approximation ratio for a maximization (minimization) problem of c - <unk> (min: c + <unk>)?', ' What does the literature say the algorithm has a ratio of for arbitrary <unk> > 0 but cannot be shown?', ' What is the ratio of 7 / 8 + <unk> for satisfiable MAX-3SAT instances due to Johan H<unk>stad?', ' What is an example of an optimal inapproximability — inexistence of approximation?']","['c ∓ ϵ  for arbitrary ϵ > 0', 'c ∓ ϵ', 'optimal inapproximability', '7 / 8 + ϵ for satisfiable MAX-3SAT instances due to Johan Håstad']"
898,approximation algorithm,Epsilon terms,"An ϵ-term may appear when an approximation algorithm introduces a multiplicative error and a constant error while the minimum optimum of instances of size n goes to infinity as n does. In this case, the approximation ratio is c ∓ k / OPT = c ∓ o(1) for some constants c and k. Given arbitrary ϵ > 0, one can choose a large enough N such that the term k / OPT < ϵ for every n ≥ N. For every fixed ϵ, instances of size n < N can be solved by brute force, thereby showing an approximation ratio — existence of approximation algorithms with a guarantee — of c ∓ ϵ for every ϵ > 0.
","An ϵ-term may appear when an approximation algorithm introduces a multiplicative error and a constant error while the minimum optimum of instances of size n goes to infinity as n does. In this case, the approximation ratio is c ∓ k / OPT = c ∓ o(1) for some constants c and k. Given arbitrary ϵ > 0, one can choose a large enough N such that the term k / OPT < ϵ for every n ≥ N. For every fixed ϵ, instances of size n < N can be solved by brute force, thereby showing an approximation ratio — existence of approximation algorithms with a guarantee — of c ∓ ϵ for every ϵ > 0.","[' What may appear when an approximation algorithm introduces a multiplicative error and a constant error?', ' The minimum optimum of instances of size n goes to what as n does?', ' What does k / OPT = c <unk> o(1) for some constants c and k?', ' What can one choose given arbitrary <unk> > 0, given a large enough N?', ' For every fixed <unk>, instances of size n <unk> N can be what?', ' How can every fixed <unk>, instances of size n <unk> N be solved by brute force?', ' What is the existence of approximation algorithms with a guarantee?']","['An ϵ-term', 'infinity', 'approximation ratio', 'k / OPT < ϵ', 'solved by brute force', 'an approximation ratio', 'approximation ratio']"
899,image retrieval,Summary,"An image retrieval system is a computer system used for browsing, searching and retrieving images from a large database of digital images. Most traditional and common methods of image retrieval utilize some method of adding metadata such as captioning, keywords, title or descriptions to the images so that retrieval can be performed over the annotation words. Manual image annotation is time-consuming, laborious and expensive; to address this, there has been a large amount of research done on automatic image annotation. Additionally, the increase in social web applications and the semantic web have inspired the development of several web-based image annotation tools. 
","An image retrieval system is a computer system used for browsing, searching and retrieving images from a large database of digital images. Most traditional and common methods of image retrieval utilize some method of adding metadata such as captioning, keywords, title or descriptions to the images so that retrieval can be performed over the annotation words.","[' What is a computer system used for browsing, searching and retrieving images from a large database of digital images?', ' Most traditional and common methods of image retrieval use some method of what?', ' How can retrieval be performed over the annotation words?']","['An image retrieval system', 'adding metadata', 'adding metadata']"
900,image retrieval,Search methods,"Image search is a specialized data search used to find images. To search for images, a user may provide query terms such as keyword, image file/link, or click on some image, and the system will return images ""similar"" to the query. The similarity used for search criteria could be meta tags, color distribution in images, region/shape attributes, etc.
","Image search is a specialized data search used to find images. To search for images, a user may provide query terms such as keyword, image file/link, or click on some image, and the system will return images ""similar"" to the query.","[' What is a specialized data search used to find?', ' What may a user provide to search for images?', ' How will the system return images similar to the query?']","['images', 'query terms such as keyword, image file/link, or click on some image', 'Image search']"
901,image retrieval,Data scope,"It is crucial to understand the scope and nature of image data in order to determine the complexity of image search system design. The design is also largely influenced by factors such as the diversity of user-base and expected user traffic for a search system. Along this dimension, search data can be classified into the following categories:
",It is crucial to understand the scope and nature of image data in order to determine the complexity of image search system design. The design is also largely influenced by factors such as the diversity of user-base and expected user traffic for a search system.,"[' What is important to understand in order to determine the complexity of image search system design?', ' What is largely influenced by the diversity of user-base and expected user traffic?']","['the scope and nature of image data', 'The design']"
902,virtual machine,Summary,"In computing, a virtual machine (VM) is the virtualization/emulation of a computer system.  Virtual machines are based on computer architectures and provide functionality of a physical computer. Their implementations may involve specialized hardware, software, or a combination.
","In computing, a virtual machine (VM) is the virtualization/emulation of a computer system. Virtual machines are based on computer architectures and provide functionality of a physical computer.","[' What is a VM?', ' What are virtual machines based on?']","['virtual machine (VM) is the virtualization/emulation of a computer system', 'computer architectures']"
903,virtual machine,Summary,"Some virtual machine emulators, such as QEMU and video game console emulators, are designed to also emulate (or ""virtually imitate"") different system architectures thus allowing execution of software applications and operating systems written for another CPU or architecture. Operating-system-level virtualization allows the resources of a computer to be partitioned via the kernel. The terms are not universally interchangeable.
","Some virtual machine emulators, such as QEMU and video game console emulators, are designed to also emulate (or ""virtually imitate"") different system architectures thus allowing execution of software applications and operating systems written for another CPU or architecture. Operating-system-level virtualization allows the resources of a computer to be partitioned via the kernel.","[' What are some virtual machine emulators designed to emulate?', ' What allows the execution of software applications and operating systems written for another CPU or architecture?', ' What allows the resources of a computer to be partitioned via the kernel?']","['different system architectures', 'virtual machine emulators', 'Operating-system-level virtualization']"
904,virtual machine,History,"System virtual machines grew out of time-sharing, as notably implemented in the Compatible Time-Sharing System (CTSS). Time-sharing allowed multiple users to use a computer concurrently: each program appeared to have full access to the machine, but only one program was executed at the time, with the system switching between programs in time slices, saving and restoring state each time. This evolved into virtual machines, notably via IBM's research systems: the M44/44X, which used partial virtualization, and the CP-40 and SIMMON, which used full virtualization, and were early examples of hypervisors. The first widely available virtual machine architecture was the CP-67/CMS (see History of CP/CMS for details). An important distinction was between using multiple virtual machines on one host system for time-sharing, as in M44/44X and CP-40, and using one virtual machine on a host system for prototyping, as in SIMMON. Emulators, with hardware emulation of earlier systems for compatibility, date back to the IBM System/360 in 1963, while the software emulation (then-called ""simulation"") predates it.
","System virtual machines grew out of time-sharing, as notably implemented in the Compatible Time-Sharing System (CTSS). Time-sharing allowed multiple users to use a computer concurrently: each program appeared to have full access to the machine, but only one program was executed at the time, with the system switching between programs in time slices, saving and restoring state each time.","[' What grew out of time-sharing?', ' What was the Compatible Time-Sharing System (CTSS) implemented in?', ' How many users could use a computer concurrently?', ' What is executed at the time, with the system switching between programs in time slices?']","['System virtual machines', 'System virtual machines', 'multiple', 'one program']"
905,virtual machine,History,"Process virtual machines arose originally as abstract platforms for an intermediate language used as the intermediate representation of a program by a compiler; early examples date to around 1966. An early 1966 example was the O-code machine, a virtual machine that executes O-code (object code) emitted by the front end of the BCPL compiler. This abstraction allowed the compiler to be easily ported to a new architecture by implementing a new back end that took the existing O-code and compiled it to machine code for the underlying physical machine. The Euler language used a similar design, with the intermediate language named P (portable). This was popularized around 1970 by Pascal, notably in the Pascal-P system (1973) and Pascal-S compiler (1975), in which it was termed p-code and the resulting machine as a p-code machine. This has been influential, and virtual machines in this sense have been often generally called p-code machines. In addition to being an intermediate language, Pascal p-code was also executed directly by an interpreter implementing the virtual machine, notably in UCSD Pascal (1978); this influenced later interpreters, notably the Java virtual machine (JVM). Another early example was SNOBOL4 (1967), which was written in the SNOBOL Implementation Language (SIL), an assembly language for a virtual machine, which was then targeted to physical machines by transpiling to their native assembler via a macro assembler. Macros have since fallen out of favor, however, so this approach has been less influential. Process virtual machines were a popular approach to implementing early microcomputer software, including Tiny BASIC and adventure games, from one-off implementations such as Pyramid 2000 to a general-purpose engine like Infocom's z-machine, which Graham Nelson argues is ""possibly the most portable virtual machine ever created"".","Process virtual machines arose originally as abstract platforms for an intermediate language used as the intermediate representation of a program by a compiler; early examples date to around 1966. An early 1966 example was the O-code machine, a virtual machine that executes O-code (object code) emitted by the front end of the BCPL compiler.","[' When did the O-code machine first appear?', ' What was the origin of process virtual machines?', ' When were the first examples of a process virtual machine?', ' What does O-code stand for?', ' What is the front end of the BCPL compiler called?']","['1966', 'abstract platforms for an intermediate language', '1966', 'object code', 'O-code machine']"
906,virtual machine,History,"Significant advances occurred in the implementation of Smalltalk-80,
particularly the Deutsch/Schiffmann implementation
which pushed just-in-time (JIT) compilation forward as an implementation approach that uses process virtual machine.
Later notable Smalltalk VMs were VisualWorks, the Squeak Virtual Machine,
and Strongtalk.
A related language that produced a lot of virtual machine innovation was the Self programming language, which pioneered adaptive optimization and generational garbage collection. These techniques proved commercially successful in 1999 in the HotSpot Java virtual machine.
Other innovations include having a register-based virtual machine, to better match the underlying hardware, rather than a stack-based virtual machine, which is a closer match for the programming language; in 1995, this was pioneered by the Dis virtual machine for the Limbo language. OpenJ9 is an alternative for HotSpot JVM in OpenJDK and is an open source eclipse project claiming better startup and less resource consumption compared to HotSpot.
","Significant advances occurred in the implementation of Smalltalk-80,
particularly the Deutsch/Schiffmann implementation
which pushed just-in-time (JIT) compilation forward as an implementation approach that uses process virtual machine. Later notable Smalltalk VMs were VisualWorks, the Squeak Virtual Machine,
and Strongtalk.","[' What was the name of the implementation of Smalltalk-80?', ' What did the Deutsch/Schiffmann implementation push forward as an implementation approach that uses process virtual machine?']","['Deutsch/Schiffmann', 'just-in-time (JIT) compilation']"
907,virtual machine,Full virtualization,"In full virtualization, the virtual machine simulates enough hardware to allow an unmodified ""guest"" OS (one designed for the same instruction set) to be run in isolation. This approach was pioneered in 1966 with the IBM CP-40 and CP-67, predecessors of the VM family.
","In full virtualization, the virtual machine simulates enough hardware to allow an unmodified ""guest"" OS (one designed for the same instruction set) to be run in isolation. This approach was pioneered in 1966 with the IBM CP-40 and CP-67, predecessors of the VM family.","[' What is a ""guest"" OS?', ' When was full virtualization pioneered?', ' What were the predecessors of the VM family?']","['one designed for the same instruction set', '1966', 'IBM CP-40 and CP-67']"
908,virtual machine,Operating-system-level virtualization,"In operating-system-level virtualization, a physical server is virtualized at the operating system level, enabling multiple isolated and secure virtualized servers to run on a single physical server.  The ""guest"" operating system environments share the same running instance of the operating system as the host system.  Thus, the same operating system kernel is also used to implement the ""guest"" environments, and applications running in a given ""guest"" environment view it as a stand-alone system. The pioneer implementation was FreeBSD jails; other examples include Docker, Solaris Containers, OpenVZ, Linux-VServer, LXC, AIX Workload Partitions, Parallels Virtuozzo Containers, and iCore Virtual Accounts.
","In operating-system-level virtualization, a physical server is virtualized at the operating system level, enabling multiple isolated and secure virtualized servers to run on a single physical server. The ""guest"" operating system environments share the same running instance of the operating system as the host system.","[' What type of virtualization allows multiple isolated and secure virtualized servers to run on a single physical server?', ' What do ""guest"" operating system environments share?']","['operating-system-level virtualization', 'the same running instance of the operating system as the host system']"
909,information system,Summary,"An information system (IS) is a formal, sociotechnical, organizational system designed to collect, process, store, and distribute information. From a sociotechnical perspective, information systems are composed by four components: task, people, structure (or roles), and technology. Information systems can be defined as an integration of components for collection, storage and processing of data of which the data is used to provide information, contribute to knowledge as well as digital products that facilitate decision making.","An information system (IS) is a formal, sociotechnical, organizational system designed to collect, process, store, and distribute information. From a sociotechnical perspective, information systems are composed by four components: task, people, structure (or roles), and technology.","[' What is an IS?', ' What are the four components of an information system?']","['a formal, sociotechnical, organizational system designed to collect, process, store, and distribute information', 'task, people, structure (or roles), and technology']"
910,information system,Summary,"A computer information system is a system that is composed of people and computers that processes or interprets information. The term is also sometimes used to simply refer to a computer system with software installed.
",A computer information system is a system that is composed of people and computers that processes or interprets information. The term is also sometimes used to simply refer to a computer system with software installed.,"[' What is a computer information system composed of?', ' What is the term used to refer to a system with software installed?']","['people and computers', 'computer information system']"
911,information system,Summary,"""Information systems"" is also an academic field study about systems with a specific reference to information and the complementary networks of computer hardware and software that people and organizations use to collect, filter, process, create and also distribute data. An emphasis is placed on an information system having a definitive boundary, users, processors, storage, inputs, outputs and the aforementioned communication networks.","""Information systems"" is also an academic field study about systems with a specific reference to information and the complementary networks of computer hardware and software that people and organizations use to collect, filter, process, create and also distribute data. An emphasis is placed on an information system having a definitive boundary, users, processors, storage, inputs, outputs and the aforementioned communication networks.","[' What is an academic field study about systems with a specific reference to information and the complementary networks of computer hardware and software?', ' An emphasis is placed on an information system having a definitive what?', ' What is an information system with a definitive boundary?', ' What is a system with users, processors, storage, inputs, outputs, and communication networks?']","['Information systems', 'boundary', 'users, processors, storage, inputs, outputs and the aforementioned communication networks', 'a definitive boundary']"
912,information system,Summary,"Any specific information system aims to support operations, management and decision-making. An information system is the information and communication technology (ICT) that an organization uses, and also the way in which people interact with this technology in support of business processes.","Any specific information system aims to support operations, management and decision-making. An information system is the information and communication technology (ICT) that an organization uses, and also the way in which people interact with this technology in support of business processes.","[' What does any specific information system aim to support?', ' What is the name of the information and communication technology that an organization uses?']","['operations, management and decision-making', 'An information system']"
913,information system,Summary,"Some authors make a clear distinction between information systems, computer systems, and business processes. Information systems typically include an ICT component but are not purely concerned with ICT, focusing instead on the end-use of information technology. Information systems are also different from business processes. Information systems help to control the performance of business processes.","Some authors make a clear distinction between information systems, computer systems, and business processes. Information systems typically include an ICT component but are not purely concerned with ICT, focusing instead on the end-use of information technology.","[' Some authors make a clear distinction between what?', ' Information systems typically include an ICT component but are not purely concerned with ICT, what do they focus instead on?']","['information systems, computer systems, and business processes', 'the end-use of information technology']"
914,information system,Summary,"Alter argues for advantages of viewing an information system as a special type of work system. A work system is a system in which humans or machines perform processes and activities using resources to produce specific products or services for customers. An information system is a work system whose activities are devoted to capturing, transmitting, storing, retrieving, manipulating and displaying information.",Alter argues for advantages of viewing an information system as a special type of work system. A work system is a system in which humans or machines perform processes and activities using resources to produce specific products or services for customers.,"[' What does Alter argue for?', ' What is a work system?']","['advantages of viewing an information system as a special type of work system', 'a system in which humans or machines perform processes and activities using resources to produce specific products or services for customers']"
915,information system,Summary,"As such, information systems inter-relate with data systems on the one hand and activity systems on the other. An information system is a form of communication system in which data represent and are processed as a form of social memory. An information system can also be considered a semi-formal language which supports human decision making and action.
","As such, information systems inter-relate with data systems on the one hand and activity systems on the other. An information system is a form of communication system in which data represent and are processed as a form of social memory.",[' What is an information system a form of communication system in which data represent and are processed as?'],['a form of social memory']
916,information system,Overview,"Silver et al. (1995) provided two views on IS that includes software, hardware, data, people, and procedures.","Silver et al. (1995) provided two views on IS that includes software, hardware, data, people, and procedures.",[' How many views did Silver et al. provide on IS?'],['two']
917,information system,Overview,"There are various types of information systems, for example: transaction processing systems, decision support systems, knowledge management systems, learning management systems, database management systems, and office information systems. Critical to most information systems are information technologies, which are typically designed to enable humans to perform tasks for which the human brain is not well suited, such as: handling large amounts of information, performing complex calculations, and controlling many simultaneous processes.
","There are various types of information systems, for example: transaction processing systems, decision support systems, knowledge management systems, learning management systems, database management systems, and office information systems. Critical to most information systems are information technologies, which are typically designed to enable humans to perform tasks for which the human brain is not well suited, such as: handling large amounts of information, performing complex calculations, and controlling many simultaneous processes.","[' What are some types of information systems?', ' What type of systems are transaction processing systems, decision support systems, knowledge management systems, learning management systems and database management systems all considered to be?', ' To enable humans to perform tasks for which the human brain is not well suited, such as handling large amounts of information, performing complex calculations, and controlling many simultaneous processes?']","['transaction processing systems, decision support systems, knowledge management systems, learning management systems, database management systems, and office information systems', 'information systems', 'information technologies']"
918,information system,Overview,"Information technologies are a very important and malleable resource available to executives. Many companies have created a position of chief information officer (CIO) that sits on the executive board with the chief executive officer (CEO), chief financial officer (CFO), chief operating officer (COO), and chief technical officer (CTO). The CTO may also serve as CIO, and vice versa. The chief information security officer (CISO) focuses on information security management.
","Information technologies are a very important and malleable resource available to executives. Many companies have created a position of chief information officer (CIO) that sits on the executive board with the chief executive officer (CEO), chief financial officer (CFO), chief operating officer (COO), and chief technical officer (CTO).","[' What is a very important and malleable resource available to executives?', ' What position sits on the executive board with the chief executive officer?']","['Information technologies', 'chief information officer']"
919,information system,Overview,"Data is the bridge between hardware and people. This means that the data we collect is only data until we involve people. At that point, data is now information.
",Data is the bridge between hardware and people. This means that the data we collect is only data until we involve people.,"[' What is the bridge between hardware and people?', ' Data is only data until we involve what?']","['Data', 'people']"
920,information system,Types of information system,"The ""classic"" view of Information systems found in textbooks in the 1980s was a pyramid of systems that reflected the hierarchy of the organization, usually transaction processing systems at the bottom of the pyramid, followed by management information systems, decision support systems, and ending with executive information systems at the top. Although the pyramid model remains useful since it was first formulated, a number of new technologies have been developed and new categories of information systems have emerged, some of which no longer fit easily into the original pyramid model.
","The ""classic"" view of Information systems found in textbooks in the 1980s was a pyramid of systems that reflected the hierarchy of the organization, usually transaction processing systems at the bottom of the pyramid, followed by management information systems, decision support systems, and ending with executive information systems at the top. Although the pyramid model remains useful since it was first formulated, a number of new technologies have been developed and new categories of information systems have emerged, some of which no longer fit easily into the original pyramid model.","[' What was the ""classic"" view of Information systems found in textbooks in the 1980s?', ' Transaction processing systems were usually at the bottom of what pyramid?', ' Decision support systems were followed by management information systems and what other system?', ' What type of systems are at the top of the pyramid?', ' What is the pyramid model still useful for?']","['a pyramid of systems that reflected the hierarchy of the organization', 'Information systems', 'executive information systems', 'executive information systems', 'since it was first formulated']"
921,information system,Types of information system,"A computer(-based) information system is essentially an IS using computer technology to carry out some or all of its planned tasks. The basic components of computer-based information systems are:
",A computer(-based) information system is essentially an IS using computer technology to carry out some or all of its planned tasks. The basic components of computer-based information systems are:,"[' What is a computer(-based) information system essentially?', ' What does an IS use computer technology to carry out?']","['an IS using computer technology to carry out some or all of its planned tasks', 'some or all of its planned tasks']"
922,information system,Types of information system,"The first four components (hardware, software, database, and network) make up what is known as the information technology platform.
Information technology workers could then use these components to create information systems that watch over safety measures, risk and the management of data. These actions are known as information technology services.","The first four components (hardware, software, database, and network) make up what is known as the information technology platform. Information technology workers could then use these components to create information systems that watch over safety measures, risk and the management of data.","[' What are the first four components of an information technology platform called?', ' What could information technology workers use these components to create?']","['hardware, software, database, and network', 'information systems']"
923,information system,Types of information system,"Certain information systems support parts of organizations, others support entire organizations, and still others, support groups of organizations. Recall that each department or functional area within an organization has its own collection of application programs or information systems. These functional area information systems (FAIS) are supporting pillars for more general IS namely, business intelligence systems and dashboards. As the name suggests, each FAIS supports a particular function within the organization, e.g.: accounting IS, finance IS, production-operation management (POM) IS, marketing IS, and human resources IS. In finance and accounting, managers use IT systems to forecast revenues and business activity, to determine the best sources and uses of funds, and to perform audits to ensure that the organization is fundamentally sound and that all financial reports and documents are accurate. Other types of organizational information systems are FAIS, Transaction processing systems, enterprise resource planning, office automation system, management information system, decision support system, expert system, executive dashboard, supply chain management system, and electronic commerce system. Dashboards are a special form of IS that support all managers of the organization. They provide rapid access to timely information and direct access to structured information in the form of reports. Expert systems attempt to duplicate the work of human experts by applying reasoning capabilities, knowledge, and expertise within a specific domain.
","Certain information systems support parts of organizations, others support entire organizations, and still others, support groups of organizations. Recall that each department or functional area within an organization has its own collection of application programs or information systems.","[' What do some information systems support?', ' What does each department or functional area within an organization have?']","['parts of organizations', 'its own collection of application programs or information systems']"
924,information system,Information system development,"Information technology departments in larger organizations tend to strongly influence the development, use, and application of information technology in the business.
A series of methodologies and processes can be used to develop and use an information system. Many developers use a systems engineering approach such as the system development life cycle (SDLC), to systematically develop an information system in stages. The stages of the system development lifecycle are planning, system analysis, and requirements, system design, development, integration and testing, implementation and operations, and maintenance.
Recent research aims at enabling and measuring the ongoing, collective development of such systems within an organization by the entirety of human actors themselves.
An information system can be developed in house (within the organization) or outsourced. This can be accomplished by outsourcing certain components or the entire system. A specific case is the geographical distribution of the development team (offshoring, global information system).
","Information technology departments in larger organizations tend to strongly influence the development, use, and application of information technology in the business. A series of methodologies and processes can be used to develop and use an information system.","[' What departments in larger organizations tend to strongly influence the development, use, and application of information technology in the business?', ' A series of methodologies and processes can be used to develop and use what?']","['Information technology departments', 'an information system']"
925,information system,Information system development,"Geographic information systems, land information systems, and disaster information systems are examples of emerging information systems, but they can be broadly considered as spatial information systems.
System development is done in stages which include:
","Geographic information systems, land information systems, and disaster information systems are examples of emerging information systems, but they can be broadly considered as spatial information systems. System development is done in stages which include:","[' What are examples of emerging information systems?', ' What can be broadly considered as a spatial information system?', ' How is system development done?']","['Geographic information systems, land information systems, and disaster information systems', 'Geographic information systems, land information systems, and disaster information systems', 'in stages']"
926,information system,As an academic discipline,"The field of study called information systems encompasses a variety of topics including systems analysis and design, computer networking, information security, database management, and decision support systems. Information management deals with the practical and theoretical problems of collecting and analyzing information in a business function area including business productivity tools, applications programming and implementation, electronic commerce, digital media production, data mining, and decision support. Communications and networking deals with telecommunication technologies.
Information systems bridges business and computer science using the theoretical foundations of information and computation to study various business models and related algorithmic processes  on building the IT systems  within a computer science discipline. Computer information system(s) (CIS) is a field studying computers and algorithmic processes, including their principles, their software and hardware designs, their applications, and their impact on society, whereas IS emphasizes functionality over design.","The field of study called information systems encompasses a variety of topics including systems analysis and design, computer networking, information security, database management, and decision support systems. Information management deals with the practical and theoretical problems of collecting and analyzing information in a business function area including business productivity tools, applications programming and implementation, electronic commerce, digital media production, data mining, and decision support.","[' What is the field of study called that covers a variety of topics?', ' Information management deals with the practical and theoretical problems of what?', ' Business productivity tools, applications programming and implementation, electronic commerce, digital media production, data mining and decision support are examples of what?']","['information systems', 'collecting and analyzing information', 'Information management']"
927,information system,As an academic discipline,"Several IS scholars have debated the nature and foundations of Information Systems which have its roots in other reference disciplines such as Computer Science, Engineering, Mathematics, Management Science, Cybernetics, and others. Information systems also can be defined as a collection of hardware, software, data, people, and procedures that work together to produce quality information.
","Several IS scholars have debated the nature and foundations of Information Systems which have its roots in other reference disciplines such as Computer Science, Engineering, Mathematics, Management Science, Cybernetics, and others. Information systems also can be defined as a collection of hardware, software, data, people, and procedures that work together to produce quality information.","[' What do IS scholars debate about the nature and foundations of Information Systems?', ' What can be defined as a collection of hardware, software, data, people, and procedures?', ' Hardware, software, data, people, and procedures work together to produce what?']","['roots in other reference disciplines', 'Information systems', 'quality information']"
928,information system,Career pathways,"There is a wide variety of career paths in the information systems discipline. ""Workers with specialized technical knowledge and strong communications skills will have the best prospects. Workers with management skills and an understanding of business practices and principles will have excellent opportunities, as companies are increasingly looking to technology to drive their revenue.""","There is a wide variety of career paths in the information systems discipline. ""Workers with specialized technical knowledge and strong communications skills will have the best prospects.",[' What type of knowledge and strong communication skills will have the best prospects in the information systems field?'],['specialized technical']
929,information system,Career pathways,"Information technology is important to the operation of contemporary businesses, it offers many employment opportunities. The information systems field includes the people in organizations who design and build information systems, the people who use those systems, and the people responsible for managing those systems.
The demand for traditional IT staff such as programmers, business analysts, systems analysts, and designer is significant. Many well-paid jobs exist in areas of Information technology. At the top of the list is the chief information officer (CIO).
","Information technology is important to the operation of contemporary businesses, it offers many employment opportunities. The information systems field includes the people in organizations who design and build information systems, the people who use those systems, and the people responsible for managing those systems.","[' What is important to the operation of modern businesses?', ' What field includes people in organizations who design and build information systems, the people who use them, and the people responsible for managing them?']","['Information technology', 'information systems']"
930,information system,Career pathways,"The CIO is the executive who is in charge of the IS function. In most organizations, the CIO works with the chief executive officer (CEO), the chief financial officer (CFO), and other senior executives. Therefore, he or she actively participates in the organization's strategic planning process.
","The CIO is the executive who is in charge of the IS function. In most organizations, the CIO works with the chief executive officer (CEO), the chief financial officer (CFO), and other senior executives.","[' The CIO is the executive who is in charge of what function?', ' In most organizations, the CIO works with whom?']","['IS', 'the chief executive officer (CEO), the chief financial officer (CFO), and other senior executives']"
931,information system,Research,"Information systems research is generally interdisciplinary concerned with the study of the effects of information systems on the behaviour of individuals, groups, and organizations. Hevner et al. (2004) categorized research in IS into two scientific paradigms including behavioural science which is to develop and verify theories that explain or predict human or organizational behavior and design science which extends the boundaries of human and organizational capabilities by creating new and innovative artifacts.
","Information systems research is generally interdisciplinary concerned with the study of the effects of information systems on the behaviour of individuals, groups, and organizations. Hevner et al.","[' What type of research is concerned with the study of the effects of information systems on the behavior of individuals, groups, and organizations?']",['interdisciplinary']
932,information system,Research,"Salvatore March and Gerald Smith proposed a framework for researching different aspects of Information Technology including outputs of the research (research outputs) and activities to carry out this research (research activities). They identified research outputs as follows:
",Salvatore March and Gerald Smith proposed a framework for researching different aspects of Information Technology including outputs of the research (research outputs) and activities to carry out this research (research activities). They identified research outputs as follows:,"[' Who proposed a framework for researching different aspects of Information Technology?', ' What did Salvatore March and Gerald Smith identify as research outputs?']","['Salvatore March and Gerald Smith', 'outputs of the research']"
933,information system,Research,"Although Information Systems as a discipline has been evolving for over 30 years now, the core focus or identity of IS research is still subject to debate among scholars. There are two main views around this debate: a narrow view focusing on the IT artifact as the core subject matter of IS research, and a broad view that focuses on the interplay between social and technical aspects of IT that is embedded into a dynamic evolving context. A third view calls on IS scholars to pay balanced attention to both the IT artifact and its context.
","Although Information Systems as a discipline has been evolving for over 30 years now, the core focus or identity of IS research is still subject to debate among scholars. There are two main views around this debate: a narrow view focusing on the IT artifact as the core subject matter of IS research, and a broad view that focuses on the interplay between social and technical aspects of IT that is embedded into a dynamic evolving context.","[' How long has Information Systems been evolving?', ' What is the core focus of IS research subject to debate among scholars?', ' There are two main views around what debate?', ' A narrow view focuses on what as the core subject matter?', ' What is the core subject matter of IS research?', ' What is a broad view that focuses on the interplay between social and technical aspects of IT?']","['over 30 years', 'identity', 'the core focus or identity of IS research', 'IT artifact', 'IT artifact', 'a narrow view focusing on the IT artifact as the core subject matter of IS research, and a broad view that focuses on the interplay between social and technical aspects of IT that is embedded into a dynamic evolving context']"
934,information system,Research,"Since the study of information systems is an applied field, industry practitioners expect information systems research to generate findings that are immediately applicable in practice. This is not always the case however, as information systems researchers often explore behavioral issues in much more depth than practitioners would expect them to do. This may render information systems research results difficult to understand, and has led to criticism.","Since the study of information systems is an applied field, industry practitioners expect information systems research to generate findings that are immediately applicable in practice. This is not always the case however, as information systems researchers often explore behavioral issues in much more depth than practitioners would expect them to do.","[' What is the study of information systems an applied field?', ' Industry practitioners expect information systems research to generate findings that are immediately applicable in what?', ' Information systems researchers explore behavioral issues in much more depth than practitioners would expect them to?', ' In depth than practitioners would expect them to do?']","['information systems research to generate findings that are immediately applicable in practice', 'practice', 'information systems is an applied field, industry practitioners expect information systems research to generate findings that are immediately applicable in practice', 'information systems researchers often explore behavioral issues in much more depth']"
935,information system,Research,"In the last ten years, the business trend is represented by the considerable increase of Information Systems Function (ISF) role, especially with regard to the enterprise strategies and operations supporting. It became a key factor to increase productivity and to support value creation. To study an information system itself, rather than its effects, information systems models are used, such as EATPUT.
","In the last ten years, the business trend is represented by the considerable increase of Information Systems Function (ISF) role, especially with regard to the enterprise strategies and operations supporting. It became a key factor to increase productivity and to support value creation.","[' In the last ten years, the business trend is represented by the considerable increase of what?', ' What became a key factor to increase productivity and to support value creation?']","['Information Systems Function (ISF) role', 'Information Systems Function (ISF)']"
936,information system,Research,"A number of annual information systems conferences are run in various parts of the world, the majority of which are peer reviewed. The AIS directly runs the International Conference on Information Systems (ICIS) and the Americas Conference on Information Systems (AMCIS), while AIS affiliated conferences include the Pacific Asia Conference on Information Systems (PACIS), European Conference on Information Systems (ECIS), the Mediterranean Conference on Information Systems (MCIS), the International Conference on Information Resources Management (Conf-IRM) and the Wuhan International Conference on E-Business (WHICEB). AIS chapter conferences include Australasian Conference on Information Systems (ACIS), Information Systems Research Conference in Scandinavia (IRIS), Information Systems International Conference (ISICO), Conference of the Italian Chapter of AIS (itAIS), Annual Mid-Western AIS Conference (MWAIS) and Annual Conference of the Southern AIS (SAIS). EDSIG, which is the special interest group on education of the AITP, organizes the Conference on Information Systems and Computing Education and the Conference on Information Systems Applied Research which are both held annually in November.
","A number of annual information systems conferences are run in various parts of the world, the majority of which are peer reviewed. The AIS directly runs the International Conference on Information Systems (ICIS) and the Americas Conference on Information Systems (AMCIS), while AIS affiliated conferences include the Pacific Asia Conference on Information Systems (PACIS), European Conference on Information Systems (ECIS), the Mediterranean Conference on Information Systems (MCIS), the International Conference on Information Resources Management (Conf-IRM) and the Wuhan International Conference on E-Business (WHICEB).","[' What is the name of the international conference on information systems?', ' Who runs the Americas Conference on Information Systems?', ' What is one of the AIS affiliated conferences?', ' What is the acronym for the Pacific Asia Conference on Information Systems?', ' What is ECIS?']","['ICIS', 'The AIS', 'Pacific Asia Conference on Information Systems', 'PACIS', 'European Conference on Information Systems']"
937,knowledge management,Summary,"Knowledge management (KM) is the collection of methods relating to creating, sharing, using and managing the knowledge and information of an organization. It refers to a multidisciplinary approach to achieve organisational objectives by making the best use of knowledge.","Knowledge management (KM) is the collection of methods relating to creating, sharing, using and managing the knowledge and information of an organization. It refers to a multidisciplinary approach to achieve organisational objectives by making the best use of knowledge.","[' What does KM stand for?', ' What is the collection of methods relating to creating, sharing, using and managing the knowledge and information of an organization?']","['Knowledge management', 'Knowledge management (KM)']"
938,knowledge management,Summary,"An established discipline since 1991, KM includes courses taught in the fields of business administration, information systems, management, library, and information science. Other fields may contribute to KM research, including information and media, computer science, public health and public policy. Several universities offer dedicated master's degrees in knowledge management.
","An established discipline since 1991, KM includes courses taught in the fields of business administration, information systems, management, library, and information science. Other fields may contribute to KM research, including information and media, computer science, public health and public policy.","[' Since what year has KM been an established discipline?', ' Business administration, information systems, management, library, and information science are courses taught in what fields?', ' Information and media, computer science, public health and public policy contribute to what?']","['1991', 'KM', 'KM research']"
939,knowledge management,Summary,"Many large companies, public institutions and non-profit organisations have resources dedicated to internal KM efforts, often as a part of their business strategy, IT, or human resource management departments. Several consulting companies provide advice regarding KM to these organizations.","Many large companies, public institutions and non-profit organisations have resources dedicated to internal KM efforts, often as a part of their business strategy, IT, or human resource management departments. Several consulting companies provide advice regarding KM to these organizations.","[' Many large companies, public institutions and non-profit organisations have resources dedicated to internal KM efforts.', ' What do many consulting companies provide to these organizations regarding KM?']","['business strategy, IT, or human resource management departments', 'advice']"
940,knowledge management,Summary,"Knowledge management efforts typically focus on organisational objectives such as improved performance, competitive advantage, innovation, the sharing of lessons learned, integration and continuous improvement of the organisation. These efforts overlap with organisational learning and may be distinguished from that by a greater focus on the management of knowledge as a strategic asset and on encouraging the sharing of knowledge. KM is an enabler of organizational learning.","Knowledge management efforts typically focus on organisational objectives such as improved performance, competitive advantage, innovation, the sharing of lessons learned, integration and continuous improvement of the organisation. These efforts overlap with organisational learning and may be distinguished from that by a greater focus on the management of knowledge as a strategic asset and on encouraging the sharing of knowledge.","[' What do knowledge management efforts typically focus on?', ' What do these efforts overlap with?', ' How are these efforts distinguished from organizational learning?', ' What does a greater focus on the management of knowledge as a strategic asset and on encouraging the sharing of knowledge?']","['organisational objectives', 'organisational learning', 'a greater focus on the management of knowledge as a strategic asset and on encouraging the sharing of knowledge', 'organisational learning']"
941,knowledge management,Summary,"The most complex scenario for knowledge management may be found in the context of supply chain as it involves multiple companies without an ownership relationship or hierarchy between them, being called by some authors as transorganizational or interorganizational knowledge. That complexity is additionally increased by industry 4.0 (or 4th industrial revolution) and digital transformation, as new challenges emerge from both the volume and speed of information flows and knowledge generation.","The most complex scenario for knowledge management may be found in the context of supply chain as it involves multiple companies without an ownership relationship or hierarchy between them, being called by some authors as transorganizational or interorganizational knowledge. That complexity is additionally increased by industry 4.0 (or 4th industrial revolution) and digital transformation, as new challenges emerge from both the volume and speed of information flows and knowledge generation.","[' What is the most complex scenario for knowledge management found in the context of supply chain?', ' What are some authors calling supply chain knowledge?', ' Industry 4.0 (or 4th industrial revolution) increases what?', ' What is industry 4.0?', ' What is the 4th industrial revolution?']","['multiple companies without an ownership relationship or hierarchy between them', 'transorganizational or interorganizational knowledge', 'complexity', '4th industrial revolution', 'industry 4.0']"
942,knowledge management,History,"Knowledge management efforts have a long history, including on-the-job discussions, formal apprenticeship, discussion forums, corporate libraries, professional training, and mentoring programs. With increased use of computers in the second half of the 20th century, specific adaptations of technologies such as knowledge bases, expert systems, information repositories, group decision support systems, intranets, and computer-supported cooperative work have been introduced to further enhance such efforts.","Knowledge management efforts have a long history, including on-the-job discussions, formal apprenticeship, discussion forums, corporate libraries, professional training, and mentoring programs. With increased use of computers in the second half of the 20th century, specific adaptations of technologies such as knowledge bases, expert systems, information repositories, group decision support systems, intranets, and computer-supported cooperative work have been introduced to further enhance such efforts.","[' What has a long history of knowledge management efforts?', ' What was increased use of computers in the second half of the 20th century?', ' Knowledge bases, expert systems, information repositories and group decision support systems are examples of what?', ' What have knowledge bases, expert systems, information repositories, group decision support systems, intranets, and computer-supported cooperative work been introduced to enhance?']","['on-the-job discussions, formal apprenticeship, discussion forums, corporate libraries, professional training, and mentoring programs', 'specific adaptations of technologies such as knowledge bases, expert systems, information repositories, group decision support systems, intranets, and computer-supported cooperative work have been introduced to further enhance such efforts', 'specific adaptations of technologies', 'Knowledge management efforts have a long history, including on-the-job discussions, formal apprenticeship, discussion forums, corporate libraries, professional training, and mentoring programs. With increased use of computers in the second half of the 20th century, specific adaptations of technologies such as knowledge bases, expert systems, information repositories, group decision support systems, intranets, and computer-supported cooperative work have been introduced to further enhance such efforts']"
943,knowledge management,History,"In the enterprise, early collections of case studies recognised the importance of knowledge management dimensions of strategy, process and measurement. Key lessons learned include people and the cultural norms which influence their behaviors are the most critical resources for successful knowledge creation, dissemination and application; cognitive, social and organisational learning processes are essential to the success of a knowledge management strategy; and measurement, benchmarking and incentives are essential to accelerate the learning process and to drive cultural change. In short, knowledge management programs can yield impressive benefits to individuals and organisations if they are purposeful, concrete and action-orientated.
","In the enterprise, early collections of case studies recognised the importance of knowledge management dimensions of strategy, process and measurement. Key lessons learned include people and the cultural norms which influence their behaviors are the most critical resources for successful knowledge creation, dissemination and application; cognitive, social and organisational learning processes are essential to the success of a knowledge management strategy; and measurement, benchmarking and incentives are essential to accelerate the learning process and to drive cultural change.","[' What are the most critical resources for successful knowledge creation, dissemination and application?', ' What are three dimensions of knowledge management in the enterprise?', ' What are essential to the success of a knowledge management strategy?', ' What are important to accelerate the learning process and drive cultural change?']","['people and the cultural norms which influence their behaviors', 'strategy, process and measurement', 'cognitive, social and organisational learning processes', 'measurement, benchmarking and incentives']"
944,knowledge management,Research,"KM emerged as a scientific discipline in the early 1990s. It was initially supported by individual practitioners, when Skandia hired Leif Edvinsson of Sweden as the world's first Chief Knowledge Officer (CKO). Hubert Saint-Onge (formerly of CIBC, Canada), started investigating KM long before that. The objective of CKOs is to manage and maximise the intangible assets of their organizations. Gradually, CKOs became interested in practical and theoretical aspects of KM, and the new research field was formed. The KM idea has been taken up by academics, such as Ikujiro Nonaka (Hitotsubashi University), Hirotaka Takeuchi (Hitotsubashi University), Thomas H. Davenport (Babson College) and Baruch Lev (New York University).","KM emerged as a scientific discipline in the early 1990s. It was initially supported by individual practitioners, when Skandia hired Leif Edvinsson of Sweden as the world's first Chief Knowledge Officer (CKO).","[' When did KM emerge as a scientific discipline?', "" Who was the world's first Chief Knowledge Officer?""]","['early 1990s', 'Leif Edvinsson']"
945,knowledge management,Research,"In 2001, Thomas A. Stewart, former editor at Fortune magazine and subsequently the editor of Harvard Business Review, published a cover story highlighting the importance of intellectual capital in organizations. The KM discipline has been gradually moving towards academic maturity. First, is a trend toward higher cooperation among academics; single-author publications are less common. Second, the role of practitioners has changed. Their contribution to academic research declined from 30% of overall contributions up to 2002, to only 10% by 2009. Third, the number of academic knowledge management journals has been steadily growing, currently reaching 27 outlets.","In 2001, Thomas A. Stewart, former editor at Fortune magazine and subsequently the editor of Harvard Business Review, published a cover story highlighting the importance of intellectual capital in organizations. The KM discipline has been gradually moving towards academic maturity.","[' In what year did Thomas A. Stewart publish a cover story about the importance of intellectual capital in organizations?', ' Who was the former editor of Fortune magazine?', "" What was Stewart's position at Harvard Business Review?""]","['2001', 'Thomas A. Stewart', 'editor']"
946,knowledge management,Research,"Multiple KM disciplines exist; approaches vary by author and school. As the discipline matured, academic debates increased regarding theory and practice, including:
","Multiple KM disciplines exist; approaches vary by author and school. As the discipline matured, academic debates increased regarding theory and practice, including:","[' How many KM disciplines exist?', ' How do KM approaches vary by author and school?', ' As KM matured, academic debates increased regarding what?']","['Multiple', 'Multiple KM disciplines', 'theory and practice']"
947,knowledge management,Research,"Regardless of the school of thought, core components of KM roughly include people/culture, processes/structure and technology.  The details depend on the  perspective. KM perspectives include:
","Regardless of the school of thought, core components of KM roughly include people/culture, processes/structure and technology. The details depend on the  perspective.","[' What are the core components of KM?', ' What are people/culture, processes/structure and technology?']","['people/culture, processes/structure and technology', 'core components of KM']"
948,knowledge management,KM technologies,"These categories overlap. Workflow, for example, is a significant aspect of a content or document management systems, most of which have tools for developing enterprise portals.","These categories overlap. Workflow, for example, is a significant aspect of a content or document management systems, most of which have tools for developing enterprise portals.","[' What is a significant aspect of a content or document management system?', ' What do most document management systems have tools for developing?']","['Workflow', 'enterprise portals']"
949,knowledge management,KM technologies,"Proprietary KM technology products such as Lotus Notes defined proprietary formats for email, documents, forms, etc. The Internet drove most vendors to adopt Internet formats. Open-source and freeware tools for the creation of blogs and wikis now enable capabilities that used to require expensive commercial tools.","Proprietary KM technology products such as Lotus Notes defined proprietary formats for email, documents, forms, etc. The Internet drove most vendors to adopt Internet formats.","[' Lotus Notes defined what kind of formats for email, documents, forms, etc.?', ' What drove most vendors to adopt Internet formats?']","['proprietary', 'The Internet']"
950,knowledge management,KM technologies,"KM is driving the adoption of tools that enable organisations to work at the semantic level, as part of the Semantic Web. Some commentators have argued that after many years the Semantic Web has failed to see widespread adoption, while other commentators have argued that it has been a success.","KM is driving the adoption of tools that enable organisations to work at the semantic level, as part of the Semantic Web. Some commentators have argued that after many years the Semantic Web has failed to see widespread adoption, while other commentators have argued that it has been a success.","[' What is driving the adoption of tools that enable organisations to work at the semantic level?', ' Some commentators have argued that the Semantic Web has failed to see widespread adoption?']","['KM', 'Some commentators have argued that after many years the Semantic Web has failed to see widespread adoption, while other commentators have argued that it has been a success']"
951,knowledge management,Knowledge barriers,"Just like knowledge transfer and knowledge sharing, the term ""knowledge barriers"" is not a uniformly defined term and differs in its meaning depending on the author. Knowledge barriers can be associated with high costs for both companies and individuals.","Just like knowledge transfer and knowledge sharing, the term ""knowledge barriers"" is not a uniformly defined term and differs in its meaning depending on the author. Knowledge barriers can be associated with high costs for both companies and individuals.","[' What does the term ""knowledge barriers"" differ from?', ' What can knowledge barriers be associated with for companies and individuals?']","['a uniformly defined term', 'high costs']"
952,knowledge management,Knowledge retention,Knowledge retention is part of knowledge management. Knowledge retention is needed when expert knowledge workers leave the organization after a long career.  Retaining knowledge prevents losing intellectual capital. ,Knowledge retention is part of knowledge management. Knowledge retention is needed when expert knowledge workers leave the organization after a long career.,"[' What is part of knowledge management?', ' What is needed when expert knowledge workers leave the organization after a long career?']","['Knowledge retention', 'Knowledge retention']"
953,knowledge management,Knowledge retention,"Knowledge retention projects are usually introduced in three stages: decision making, planning and implementation. There are differences among researchers on the terms of the stages. For example, Dalkir talks about knowledge capture, sharing and acquisition and Doan et al. introduces initiation, implementation and evaluation.  Furthermore, Levy introduces three steps (scope, transfer, integration) but also recognizes a “zero stage” for initiation of the project.","Knowledge retention projects are usually introduced in three stages: decision making, planning and implementation. There are differences among researchers on the terms of the stages.","[' How many stages are knowledge retention projects usually introduced in?', ' Decision making, planning and implementation are three of the three stages of what type of project?']","['three', 'Knowledge retention']"
954,social networks,Summary,"This is a list of notable active social network services, excluding online dating services, that have Wikipedia articles. For defunct social networking websites, see List of defunct social networking services.
","This is a list of notable active social network services, excluding online dating services, that have Wikipedia articles. For defunct social networking websites, see List of defunct social networking services.","[' What is a list of notable active social network services that have Wikipedia articles?', ' What are defunct social networking websites?']","['online dating services', 'List of defunct social networking services']"
955,intrusion detection,Summary,An intrusion detection system (IDS; also intrusion prevention system or IPS) is a device or software application that monitors a network or systems for malicious activity or policy violations. Any intrusion activity or violation is typically reported either to an administrator or collected centrally using a security information and event management (SIEM) system. A SIEM system combines outputs from multiple sources and uses alarm filtering techniques to distinguish malicious activity from false alarms.,An intrusion detection system (IDS; also intrusion prevention system or IPS) is a device or software application that monitors a network or systems for malicious activity or policy violations. Any intrusion activity or violation is typically reported either to an administrator or collected centrally using a security information and event management (SIEM) system.,"[' What is a device or software application that monitors a network or systems for malicious activity or policy violations?', ' What is an intrusion detection system also called?', ' What does SIEM stand for?', ' What is a security information and event management system?']","['An intrusion detection system', 'IDS', 'security information and event management', 'SIEM']"
956,intrusion detection,Summary,"IDS types range in scope from single computers to large networks. The most common classifications are network intrusion detection systems (NIDS) and host-based intrusion detection systems (HIDS). A system that monitors important operating system files is an example of an HIDS, while a system that analyzes incoming network traffic is an example of an NIDS. It is also possible to classify IDS by detection approach. The most well-known variants are signature-based detection (recognizing bad patterns, such as malware) and anomaly-based detection (detecting deviations from a model of ""good"" traffic, which often relies on machine learning). Another common variant is reputation-based detection (recognizing the potential threat according to the reputation scores). Some IDS products have the ability to respond to detected intrusions. Systems with response capabilities are typically referred to as an intrusion prevention system. Intrusion detection systems can also serve specific purposes by augmenting them with custom tools, such as using a honeypot to attract and characterize malicious traffic.",IDS types range in scope from single computers to large networks. The most common classifications are network intrusion detection systems (NIDS) and host-based intrusion detection systems (HIDS).,"[' What are the most common classifications for network intrusion detection systems?', ' What is HIDS?']","['NIDS) and host-based intrusion detection systems (HIDS).', 'host-based intrusion detection systems']"
957,intrusion detection,Comparison with firewalls,"Although they both relate to network security, an IDS differs from a firewall in that a traditional network firewall (distinct from a Next-Generation Firewall) uses a static set of rules to permit or deny network connections. It implicitly prevents intrusions, assuming an appropriate set of rules have been defined. Essentially, firewalls limit access between networks to prevent intrusion and do not signal an attack from inside the network. An IDS describes a suspected intrusion once it has taken place and signals an alarm. An IDS also watches for attacks that originate from within a system. This is traditionally achieved by examining network communications, identifying heuristics and patterns (often known as signatures) of common computer attacks, and taking action to alert operators.  A system that terminates connections is called an intrusion prevention system, and performs access control like an application layer firewall.","Although they both relate to network security, an IDS differs from a firewall in that a traditional network firewall (distinct from a Next-Generation Firewall) uses a static set of rules to permit or deny network connections. It implicitly prevents intrusions, assuming an appropriate set of rules have been defined.","[' What does an IDS differ from a firewall in that it uses a static set of rules to permit or deny network connections?', ' An IDS implicitly prevents intrusions, assuming what has been defined?']","['traditional network firewall', 'an appropriate set of rules']"
958,intrusion detection,Intrusion prevention,"Some systems may attempt to stop an intrusion attempt but this is neither required nor expected of a monitoring system. Intrusion detection and prevention systems (IDPS) are primarily focused on identifying possible incidents, logging information about them, and reporting attempts.  In addition, organizations use IDPS for other purposes, such as identifying problems with security policies, documenting existing threats and deterring individuals from violating security policies. IDPS have become a necessary addition to the security infrastructure of nearly every organization.","Some systems may attempt to stop an intrusion attempt but this is neither required nor expected of a monitoring system. Intrusion detection and prevention systems (IDPS) are primarily focused on identifying possible incidents, logging information about them, and reporting attempts.","[' What are intrusion detection and prevention systems primarily focused on?', ' What does IDPS stand for?']","['identifying possible incidents, logging information about them, and reporting attempts', 'Intrusion detection and prevention systems']"
959,intrusion detection,Intrusion prevention,"IDPS typically record information related to observed events, notify security administrators of important observed events and produce reports. Many IDPS can also respond to a detected threat by attempting to prevent it from succeeding. They use several response techniques, which involve the IDPS stopping the attack itself, changing the security environment (e.g. reconfiguring a firewall) or changing the attack's content.","IDPS typically record information related to observed events, notify security administrators of important observed events and produce reports. Many IDPS can also respond to a detected threat by attempting to prevent it from succeeding.","[' IDPS typically record information related to what?', ' IDPS notify security administrators of important observed events and produce what kind of reports?']","['observed events', 'reports']"
960,intrusion detection,Intrusion prevention,"Intrusion prevention systems (IPS), also known as intrusion detection and prevention systems (IDPS), are network security appliances that monitor network or system activities for malicious activity. The main functions of intrusion prevention systems are to identify malicious activity, log information about this activity, report it and attempt to block or stop it..
","Intrusion prevention systems (IPS), also known as intrusion detection and prevention systems (IDPS), are network security appliances that monitor network or system activities for malicious activity. The main functions of intrusion prevention systems are to identify malicious activity, log information about this activity, report it and attempt to block or stop it..","[' What is IPS also known as?', ' What are intrusion prevention systems used for?', ' Report the activity to the police and try to stop it.']","['Intrusion prevention systems', 'to identify malicious activity, log information about this activity, report it and attempt to block or stop it', 'Intrusion prevention systems']"
961,intrusion detection,Intrusion prevention,"Intrusion prevention systems are considered extensions of intrusion detection systems because they both monitor network traffic and/or system activities for malicious activity. The main differences are, unlike intrusion detection systems, intrusion prevention systems are placed in-line and are able to actively prevent or block intrusions that are detected.: 273 : 289  IPS can take such actions as sending an alarm, dropping detected malicious packets, resetting a connection or blocking traffic from the offending IP address. An IPS also can correct cyclic redundancy check (CRC) errors, defragment packet streams, mitigate TCP sequencing issues, and clean up unwanted transport and network layer options.: 278 .
","Intrusion prevention systems are considered extensions of intrusion detection systems because they both monitor network traffic and/or system activities for malicious activity. The main differences are, unlike intrusion detection systems, intrusion prevention systems are placed in-line and are able to actively prevent or block intrusions that are detected.","[' What are intrusion prevention systems considered extensions of?', ' What do intrusion detection systems monitor for malicious activity?', ' In contrast to a detection system, what is the main difference?']","['intrusion detection systems', 'network traffic and/or system activities', 'intrusion prevention systems are placed in-line and are able to actively prevent or block intrusions that are detected']"
962,intrusion detection,Placement,"The correct placement of intrusion detection systems is critical and varies depending on the network. The most common placement is behind the firewall, on the edge of a network. This practice provides the IDS with high visibility of traffic entering your network and will not receive any traffic between users on the network. The edge of the network is the point in which a network connects to the extranet. Another practice that can be accomplished if more resources are available is a strategy where a technician will place their first IDS at the point of highest visibility and depending on resource availability will place another at the next highest point, continuing that process until all points of the network are covered.","The correct placement of intrusion detection systems is critical and varies depending on the network. The most common placement is behind the firewall, on the edge of a network.",[' What is the most common location for intrusion detection systems?'],['behind the firewall']
963,intrusion detection,Placement,"If an IDS is placed beyond a network's firewall, its main purpose would be to defend against noise from the internet but, more importantly, defend against common attacks, such as port scans and network mapper. An IDS in this position would monitor layers 4 through 7 of the OSI model and would be signature-based. This is a very useful practice, because rather than showing actual breaches into the network that made it through the firewall, attempted breaches will be shown which reduces the amount of false positives. The IDS in this position also assists in decreasing the amount of time it takes to discover successful attacks against a network.","If an IDS is placed beyond a network's firewall, its main purpose would be to defend against noise from the internet but, more importantly, defend against common attacks, such as port scans and network mapper. An IDS in this position would monitor layers 4 through 7 of the OSI model and would be signature-based.","["" What is the main purpose of an IDS if it is placed beyond a network's firewall?"", ' What are two common attacks that IDSs protect against?', ' An IDS in this position would monitor layers 4-7 of what model?', ' What would monitor layers 4 through 7 of the OSI model?', ' What would be signature-based?']","['to defend against noise from the internet', 'port scans and network mapper', 'OSI', 'An IDS', 'An IDS in this position would monitor layers 4 through 7 of the OSI model']"
964,intrusion detection,Placement,Sometimes an IDS with more advanced features will be integrated with a firewall in order to be able to intercept sophisticated attacks entering the network. Examples of advanced features would include multiple security contexts in the routing level and bridging mode. All of this in turn potentially reduces cost and operational complexity.,Sometimes an IDS with more advanced features will be integrated with a firewall in order to be able to intercept sophisticated attacks entering the network. Examples of advanced features would include multiple security contexts in the routing level and bridging mode.,"[' What is an IDS with more advanced features sometimes integrated with?', ' What is one example of advanced features?']","['a firewall', 'multiple security contexts in the routing level and bridging mode']"
965,intrusion detection,Placement,"Another option for IDS placement is within the actual network. These will reveal attacks or suspicious activity within the network. Ignoring the security within a network can cause many problems, it will either allow users to bring about security risks or allow an attacker who has already broken into the network to roam around freely. Intense intranet security makes it difficult for even those hackers within the network to maneuver around and escalate their privileges.",Another option for IDS placement is within the actual network. These will reveal attacks or suspicious activity within the network.,"[' What is another option for IDS placement?', ' What will reveal attacks or suspicious activity within the network?']","['within the actual network', 'within the actual network']"
966,intrusion detection,Development,"The earliest preliminary IDS concept was delineated in 1980 by James Anderson at the National Security Agency and consisted of a set of tools intended to help administrators review audit trails. User access logs, file access logs, and system event logs are examples of audit trails.
","The earliest preliminary IDS concept was delineated in 1980 by James Anderson at the National Security Agency and consisted of a set of tools intended to help administrators review audit trails. User access logs, file access logs, and system event logs are examples of audit trails.","[' When was the earliest preliminary IDS concept delineated?', ' What did James Anderson at the National Security Agency delineate?']","['1980', 'IDS concept']"
967,intrusion detection,Development,"Dorothy E. Denning, assisted by Peter G. Neumann, published a model of an IDS in 1986 that formed the basis for many systems today.  Her model used statistics for anomaly detection, and resulted in an early IDS at SRI International named the Intrusion Detection Expert System (IDES), which ran on Sun workstations and could consider both user and network level data.  IDES had a dual approach with a rule-based Expert System to detect known types of intrusions plus a statistical anomaly detection component based on profiles of users, host systems, and target systems. The author of ""IDES: An Intelligent System for Detecting Intruders,"" Teresa F. Lunt, proposed adding an Artificial neural network as a third component.  She said all three components could then report to a resolver.  SRI followed IDES in 1993 with the Next-generation Intrusion Detection Expert System (NIDES).","Dorothy E. Denning, assisted by Peter G. Neumann, published a model of an IDS in 1986 that formed the basis for many systems today. Her model used statistics for anomaly detection, and resulted in an early IDS at SRI International named the Intrusion Detection Expert System (IDES), which ran on Sun workstations and could consider both user and network level data.","[' Who published a model of an IDS in 1986?', "" What did Dorothy Denning's model use for anomaly detection?"", ' At what SRI International organization was the IDS named?', ' What was the Intrusion Detection Expert System (IDES) named?', ' On what workstations did the IDES run?', ' What could IDES consider user and network data?']","['Dorothy E. Denning', 'statistics', 'Intrusion Detection Expert System', 'SRI International', 'Sun workstations', 'both user and network level']"
968,intrusion detection,Development,"The Multics intrusion detection and alerting system (MIDAS), an expert system using P-BEST and Lisp, was developed in 1988 based on the work of Denning and Neumann.  Haystack was also developed in that year using statistics to reduce audit trails.","The Multics intrusion detection and alerting system (MIDAS), an expert system using P-BEST and Lisp, was developed in 1988 based on the work of Denning and Neumann. Haystack was also developed in that year using statistics to reduce audit trails.","[' What is MIDAS?', ' When was the Multics intrusion detection and alerting system developed?', ' What was developed in 1988 based on the work of Denning and Neumann?']","['Multics intrusion detection and alerting system', '1988', 'The Multics intrusion detection and alerting system']"
969,intrusion detection,Development,"In 1986 the National Security Agency started an IDS research transfer program under Rebecca Bace. Bace later published the seminal text on the subject, Intrusion Detection, in 2000.","In 1986 the National Security Agency started an IDS research transfer program under Rebecca Bace. Bace later published the seminal text on the subject, Intrusion Detection, in 2000.","[' In what year did the National Security Agency start an IDS research transfer program?', ' What was the name of the program that was started by Rebecca Bace?', ' Who published the seminal text on intrusion detection in 2000?']","['1986', 'IDS research transfer program', 'Rebecca Bace']"
970,intrusion detection,Development,"Wisdom & Sense (W&S) was a statistics-based anomaly detector developed in 1989 at the Los Alamos National Laboratory.  W&S created rules based on statistical analysis, and then used those rules for anomaly detection.
","Wisdom & Sense (W&S) was a statistics-based anomaly detector developed in 1989 at the Los Alamos National Laboratory. W&S created rules based on statistical analysis, and then used those rules for anomaly detection.","[' What was the name of the statistics-based anomaly detector developed in 1989 at the Los Alamos National Laboratory?', ' What did Wisdom & Sense use to create rules for anomaly detection?']","['Wisdom & Sense', 'statistical analysis']"
971,intrusion detection,Development,"In 1990, the Time-based Inductive Machine (TIM) did anomaly detection using inductive learning of sequential user patterns in Common Lisp on a VAX 3500 computer.  The Network Security Monitor (NSM) performed masking on access matrices for anomaly detection on a Sun-3/50 workstation.  The Information Security Officer's Assistant (ISOA) was a 1990 prototype that considered a variety of strategies including statistics, a profile checker, and an expert system.  ComputerWatch at AT&T Bell Labs used statistics and rules for audit data reduction and intrusion detection.","In 1990, the Time-based Inductive Machine (TIM) did anomaly detection using inductive learning of sequential user patterns in Common Lisp on a VAX 3500 computer. The Network Security Monitor (NSM) performed masking on access matrices for anomaly detection on a Sun-3/50 workstation.","[' In what year did the Time-based Inductive Machine do anomaly detection?', ' What did the TIM do on a VAX 3500 computer?', ' The Network Security Monitor performed masking on access matrices for what purpose?']","['1990', 'anomaly detection', 'anomaly detection']"
972,intrusion detection,Development,"Then, in 1991, researchers at the University of California, Davis created a prototype Distributed Intrusion Detection System (DIDS), which was also an expert system.  The Network Anomaly Detection and Intrusion Reporter (NADIR), also in 1991, was a prototype IDS developed at the Los Alamos National Laboratory's Integrated Computing Network (ICN), and was heavily influenced by the work of Denning and Lunt.  NADIR used a statistics-based anomaly detector and an expert system.
","Then, in 1991, researchers at the University of California, Davis created a prototype Distributed Intrusion Detection System (DIDS), which was also an expert system. The Network Anomaly Detection and Intrusion Reporter (NADIR), also in 1991, was a prototype IDS developed at the Los Alamos National Laboratory's Integrated Computing Network (ICN), and was heavily influenced by the work of Denning and Lunt.","[' In what year did researchers at the University of California, Davis create a prototype Distributed Intrusion Detection System (DIDS)?', "" What was the prototype of the Network Anomaly Detect and Intrusion Reporter (NADIR) developed at the Los Alamos National Laboratory's Integrated Computing Network?"", ' What was the name of the Integrated Computing Network at the Los Alamos National Laboratory?']","['1991', 'IDS', 'ICN']"
973,intrusion detection,Development,"The Lawrence Berkeley National Laboratory announced Bro in 1998, which used its own rule language for packet analysis from libpcap data.  Network Flight Recorder (NFR) in 1999 also used libpcap.","The Lawrence Berkeley National Laboratory announced Bro in 1998, which used its own rule language for packet analysis from libpcap data. Network Flight Recorder (NFR) in 1999 also used libpcap.","[' When did the Lawrence Berkeley National Laboratory announce Bro?', ' What did Bro use for packet analysis?', ' When did NFR use libpcap?']","['1998', 'rule language', '1999']"
974,intrusion detection,Development,"APE was developed as a packet sniffer, also using libpcap, in November, 1998, and was renamed Snort one month later. Snort has since become the world's largest used IDS/IPS system with over 300,000 active users. It can monitor both local systems, and remote capture points using the TZSP protocol.
","APE was developed as a packet sniffer, also using libpcap, in November, 1998, and was renamed Snort one month later. Snort has since become the world's largest used IDS/IPS system with over 300,000 active users.","[' When was APE developed?', ' What was the original name of APE?', "" Snort became the world's largest used IDS/IPS system with how many users?""]","['November, 1998', 'Snort', '300,000']"
975,intrusion detection,Development,"The Audit Data Analysis and Mining (ADAM) IDS in 2001 used tcpdump to build profiles of rules for classifications. In 2003, Yongguang Zhang and Wenke Lee argue for the importance of IDS in networks with mobile nodes.","The Audit Data Analysis and Mining (ADAM) IDS in 2001 used tcpdump to build profiles of rules for classifications. In 2003, Yongguang Zhang and Wenke Lee argue for the importance of IDS in networks with mobile nodes.","[' When did the Audit Data Analysis and Mining (ADAM) IDS use tcpdump?', ' Who argue for the importance of IDS in networks with mobile nodes?']","['2001', 'Yongguang Zhang and Wenke Lee']"
976,intrusion detection,Development,"In 2015, Viegas and his colleagues  proposed an anomaly-based intrusion detection engine, aiming System-on-Chip (SoC) for applications in Internet of Things (IoT), for instance. The proposal applies machine learning for anomaly detection, providing energy-efficiency to a Decision Tree, Naive-Bayes, and k-Nearest Neighbors classifiers implementation in an Atom CPU and its hardware-friendly implementation in a FPGA. In the literature, this was the first work that implement each classifier equivalently in software and hardware and measures its energy consumption on both. Additionally, it was the first time that was measured the energy consumption for extracting each features used to make the network packet classification, implemented in software and hardware.","In 2015, Viegas and his colleagues  proposed an anomaly-based intrusion detection engine, aiming System-on-Chip (SoC) for applications in Internet of Things (IoT), for instance. The proposal applies machine learning for anomaly detection, providing energy-efficiency to a Decision Tree, Naive-Bayes, and k-Nearest Neighbors classifiers implementation in an Atom CPU and its hardware-friendly implementation in a FPGA.","[' In what year did Viegas and his colleagues propose an anomaly-based intrusion detection engine?', ' What was the SoC engine aimed at?', ' How does SoC apply machine learning for anomaly detection?', ' What is the name of the classifier implemented in an Atom CPU?', ' Where is the k-Nearest Neighbors implementation implemented?']","['2015', 'Internet of Things', 'providing energy-efficiency to a Decision Tree, Naive-Bayes, and k-Nearest Neighbors classifiers implementation in an Atom CPU and its hardware-friendly implementation in a FPGA', 'k-Nearest Neighbors', 'Atom CPU']"
977,multi-agent systems,Summary,"A multi-agent system (MAS or ""self-organized system"") is a computerized system composed of multiple interacting intelligent agents. Multi-agent systems can solve problems that are difficult or impossible for an individual agent or a monolithic system to solve. Intelligence may include methodic, functional, procedural approaches, algorithmic search or reinforcement learning.","A multi-agent system (MAS or ""self-organized system"") is a computerized system composed of multiple interacting intelligent agents. Multi-agent systems can solve problems that are difficult or impossible for an individual agent or a monolithic system to solve.","[' What does MAS stand for?', ' What is a multi-agent system composed of?']","['multi-agent system', 'multiple interacting intelligent agents']"
978,multi-agent systems,Summary,"Despite considerable overlap, a multi-agent system is not always the same as an agent-based model (ABM).  The goal of an ABM is to search for explanatory insight into the collective behavior of agents (which don't necessarily need to be ""intelligent"") obeying simple rules, typically in natural systems, rather than in solving specific practical or engineering problems. The terminology of ABM tends to be used more often in the science, and MAS in engineering and technology. Applications where multi-agent systems research may deliver an appropriate approach include online trading, disaster response, target surveillance   and social structure modelling.","Despite considerable overlap, a multi-agent system is not always the same as an agent-based model (ABM). The goal of an ABM is to search for explanatory insight into the collective behavior of agents (which don't necessarily need to be ""intelligent"") obeying simple rules, typically in natural systems, rather than in solving specific practical or engineering problems.","[' A multi-agent system is not always the same as what?', ' What is the goal of an ABM?', ' ABM is to search for explanatory insight into the collective behavior of agents.', ' What is the most common way to obey simple rules?', ' What is more common in natural systems?']","['an agent-based model', 'to search for explanatory insight into the collective behavior of agents', 'agent-based model', 'natural systems', 'simple rules']"
979,multi-agent systems,Concept,"Multi-agent systems consist of agents and their environment. Typically multi-agent systems research refers to software agents. However, the agents in a multi-agent system could equally well be robots, humans or human teams. A multi-agent system may contain combined human-agent teams.
",Multi-agent systems consist of agents and their environment. Typically multi-agent systems research refers to software agents.,"[' What do multi-agent systems consist of?', ' What does research refer to?']","['agents and their environment', 'software agents']"
980,multi-agent systems,Concept,"Agents can be divided into types spanning simple to complex. Categories include:
",Agents can be divided into types spanning simple to complex. Categories include:,"[' What can agents be divided into?', ' What types of agents are there?']","['simple to complex', 'simple to complex']"
981,multi-agent systems,Concept,"Agent environments can also be organized according to properties such as accessibility (whether it is possible to gather complete information about the environment), determinism (whether an action causes a definite effect), dynamics (how many entities influence the environment in the moment), discreteness (whether the number of possible actions in the environment is finite), episodicity (whether agent actions in certain time periods influence other periods), and dimensionality (whether spatial characteristics are important factors of the environment and the agent considers space in its decision making). Agent actions are typically mediated via an appropriate middleware. This middleware offers a first-class design abstraction for multi-agent systems, providing means to govern resource access and agent coordination.","Agent environments can also be organized according to properties such as accessibility (whether it is possible to gather complete information about the environment), determinism (whether an action causes a definite effect), dynamics (how many entities influence the environment in the moment), discreteness (whether the number of possible actions in the environment is finite), episodicity (whether agent actions in certain time periods influence other periods), and dimensionality (whether spatial characteristics are important factors of the environment and the agent considers space in its decision making). Agent actions are typically mediated via an appropriate middleware.","[' What is accessibility?', ' What is determinism?', ' How many entities influence the environment in the moment?', ' What is discreteness?', ' What is episodicity?', ' How are agent actions typically mediated?', ' What type of middleware is used?']","['whether it is possible to gather complete information about the environment', 'whether an action causes a definite effect', 'dynamics (how many', 'whether the number of possible actions in the environment is finite', 'whether agent actions in certain time periods influence other periods', 'via an appropriate middleware', 'appropriate']"
982,multi-agent systems,Research,"The study of multi-agent systems is ""concerned with the development and analysis of sophisticated AI problem-solving and control architectures for both single-agent and multiple-agent systems."" Research topics include:
","The study of multi-agent systems is ""concerned with the development and analysis of sophisticated AI problem-solving and control architectures for both single-agent and multiple-agent systems."" Research topics include:","[' What is the study of multi-agent systems concerned with?', ' What is one of the research topics?']","['the development and analysis of sophisticated AI problem-solving and control architectures for both single-agent and multiple-agent systems', 'multi-agent systems']"
983,multi-agent systems,Frameworks,"Frameworks have emerged that implement common standards (such as the FIPA and OMG MASIF standards). These frameworks e.g. JADE, save time and aid in the standardization of MAS development.",Frameworks have emerged that implement common standards (such as the FIPA and OMG MASIF standards). These frameworks e.g.,"[' What are examples of common standards?', ' What are two common standards implemented by frameworks?']","['FIPA and OMG MASIF standards', 'FIPA and OMG MASIF standards']"
984,multi-agent systems,Frameworks,"Currently though, no standard is actively maintained from FIPA or OMG. Efforts for further development of software agents in industrial context are carried out in IEEE IES technical committee on Industrial Agents.","Currently though, no standard is actively maintained from FIPA or OMG. Efforts for further development of software agents in industrial context are carried out in IEEE IES technical committee on Industrial Agents.","[' What is the current status of FIPA or OMG?', ' Who is responsible for further development of software agents in industrial context?']","['no standard is actively maintained', 'IEEE IES technical committee on Industrial Agents']"
985,multi-agent systems,Applications,"MAS have not only been applied in academic research, but also in industry. MAS are applied in the real world to graphical applications such as computer games. Agent systems have been used in films. It is widely advocated for use in networking and mobile technologies, to achieve automatic and dynamic load balancing, high scalability and self-healing networks. They are being used for coordinated defence systems.
","MAS have not only been applied in academic research, but also in industry. MAS are applied in the real world to graphical applications such as computer games.","[' MAS have not only been applied in academic research but also in what industry?', ' MAS are applied in the real world to what kind of applications?']","['industry', 'graphical']"
986,multi-agent systems,Applications,"Also, Multi-agent Systems Artificial Intelligence (MAAI) are used for simulating societies, the purpose thereof being helpful in the fields of climate, energy, epidemiology, conflict management, child abuse, .... Some organisations working on using multi-agent system models include Center for Modelling Social Systems, Centre for Research in Social Simulation, Centre for Policy Modelling, Society for Modelling and Simulation International. Hallerbach et al. discussed the application of agent-based approaches for the development and validation of automated driving systems via a digital twin of the vehicle-under-test and microscopic traffic simulation based on independent agents. Waymo has created a multi-agent simulation environment Carcraft to test algorithms for self-driving cars. It simulates traffic interactions between human drivers, pedestrians and automated vehicles. People's behavior is imitated by artificial agents based on data of real human behavior.
","Also, Multi-agent Systems Artificial Intelligence (MAAI) are used for simulating societies, the purpose thereof being helpful in the fields of climate, energy, epidemiology, conflict management, child abuse, .... Some organisations working on using multi-agent system models include Center for Modelling Social Systems, Centre for Research in Social Simulation, Centre for Policy Modelling, Society for Modelling and Simulation International.","[' What are Multi-agent Systems Artificial Intelligence (MAAI) used for?', ' What is the purpose of MAAI?', ' Centre for Research in Social Simulation, Centre for Policy Modelling, Society for Modelling and Simulation International.']","['simulating societies', 'simulating societies', 'multi-agent system models']"
987,pattern recognition,Summary,"Pattern recognition is the automated recognition of patterns and regularities in data. It has applications in statistical data analysis, signal processing, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning. Pattern recognition has its origins in statistics and engineering; some modern approaches to pattern recognition include the use of machine learning, due to the increased availability of big data and a new abundance of processing power. These activities can be viewed as two facets of the same field of application, and they have undergone substantial development over the past few decades.
","Pattern recognition is the automated recognition of patterns and regularities in data. It has applications in statistical data analysis, signal processing, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning.","[' What is the automatic recognition of patterns and regularities in data?', ' Pattern recognition has applications in what?']","['Pattern recognition', 'statistical data analysis, signal processing, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning']"
988,pattern recognition,Summary,"Pattern recognition systems are commonly trained from labeled ""training"" data. When no labeled data are available, other algorithms can be used to discover previously unknown patterns. KDD and data mining have a larger focus on unsupervised methods and stronger connection to business use. Pattern recognition focuses more on the signal and also takes acquisition and Signal Processing into consideration. It originated in engineering, and the term is popular in the context of computer vision: a leading computer vision conference is named Conference on Computer Vision and Pattern Recognition.
","Pattern recognition systems are commonly trained from labeled ""training"" data. When no labeled data are available, other algorithms can be used to discover previously unknown patterns.","[' Pattern recognition systems are typically trained from what?', ' When no labeled data are available, what can be used to discover previously unknown patterns?']","['labeled ""training"" data', 'other algorithms']"
989,pattern recognition,Summary,"In machine learning, pattern recognition is the assignment of a label to a given input value. In statistics, discriminant analysis was introduced for this same purpose in 1936. An example of pattern recognition is classification, which attempts to assign each input value to one of a given set of classes (for example, determine whether a given email is ""spam"" or ""non-spam""). Pattern recognition is a more general problem that encompasses other types of output as well. Other examples are regression, which assigns a real-valued output to each input; sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); and parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence.","In machine learning, pattern recognition is the assignment of a label to a given input value. In statistics, discriminant analysis was introduced for this same purpose in 1936.","[' In machine learning, what is the assignment of a label to a given input value?', ' When was discriminant analysis introduced?']","['pattern recognition', '1936']"
990,pattern recognition,Summary,"Pattern recognition algorithms generally aim to provide a reasonable answer for all possible inputs and to perform ""most likely"" matching of the inputs, taking into account their statistical variation. This is opposed to pattern matching algorithms, which look for exact matches in the input with pre-existing patterns. A common example of a pattern-matching algorithm is regular expression matching, which looks for patterns of a given sort in textual data and is included in the search capabilities of many text editors and word processors.
","Pattern recognition algorithms generally aim to provide a reasonable answer for all possible inputs and to perform ""most likely"" matching of the inputs, taking into account their statistical variation. This is opposed to pattern matching algorithms, which look for exact matches in the input with pre-existing patterns.","[' Pattern recognition algorithms aim to provide a reasonable answer for what?', ' Pattern matching algorithms look for what in the input with pre-existing patterns?']","['all possible inputs', 'exact matches']"
991,pattern recognition,Overview,"Pattern recognition is generally categorized according to the type of learning procedure used to generate the output value. Supervised learning assumes that a set of training data (the training set) has been provided, consisting of a set of instances that have been properly labeled by hand with the correct output. A learning procedure then generates a model that attempts to meet two sometimes conflicting objectives: Perform as well as possible on the training data, and generalize as well as possible to new data (usually, this means being as simple as possible, for some technical definition of ""simple"", in accordance with Occam's Razor, discussed below). Unsupervised learning, on the other hand, assumes training data that has not been hand-labeled, and attempts to find inherent patterns in the data that can then be used to determine the correct output value for new data instances. A combination of the two that has been explored is semi-supervised learning, which uses a combination of labeled and unlabeled data (typically a small set of labeled data combined with a large amount of unlabeled data). In cases of unsupervised learning, there may be no training data at all.
","Pattern recognition is generally categorized according to the type of learning procedure used to generate the output value. Supervised learning assumes that a set of training data (the training set) has been provided, consisting of a set of instances that have been properly labeled by hand with the correct output.","[' Pattern recognition is categorized according to what?', ' What assumes that a set of training data has been provided?']","['the type of learning procedure used to generate the output value', 'Supervised learning']"
992,pattern recognition,Overview,"Sometimes different terms are used to describe the corresponding supervised and unsupervised learning procedures for the same type of output. The unsupervised equivalent of classification is normally known as clustering, based on the common perception of the task as involving no training data to speak of, and of grouping the input data into clusters based on some inherent similarity measure (e.g. the distance between instances, considered as vectors in a multi-dimensional vector space), rather than assigning each input instance into one of a set of pre-defined classes. In some fields, the terminology is different. In community ecology, the term classification is used to refer to what is commonly known as ""clustering"".
","Sometimes different terms are used to describe the corresponding supervised and unsupervised learning procedures for the same type of output. The unsupervised equivalent of classification is normally known as clustering, based on the common perception of the task as involving no training data to speak of, and of grouping the input data into clusters based on some inherent similarity measure (e.g.","[' What is the unsupervised equivalent of classification?', ' What is clustering based on?', ' No training data to speak of, and of grouping input data into clusters based on some inherent similarity measure?']","['clustering', 'the common perception of the task as involving no training data to speak of, and of grouping the input data into clusters based on some inherent similarity measure', 'clustering']"
993,pattern recognition,Overview,"The piece of input data for which an output value is generated is formally termed an instance. The instance is formally described by a vector of features, which together constitute a description of all known characteristics of the instance. These feature vectors can be seen as defining points in an appropriate multidimensional space, and methods for manipulating vectors in vector spaces can be correspondingly applied to them, such as computing the dot product or the angle between two vectors. Features typically are either categorical (also known as nominal, i.e., consisting of one of a set of unordered items, such as a gender of ""male"" or ""female"", or a blood type of ""A"", ""B"", ""AB"" or ""O""), ordinal (consisting of one of a set of ordered items, e.g., ""large"", ""medium"" or ""small""), integer-valued (e.g., a count of the number of occurrences of a particular word in an email) or real-valued (e.g., a measurement of blood pressure). Often, categorical and ordinal data are grouped together, and this is also the case for integer-valued and real-valued data. Many algorithms work only in terms of categorical data and require that real-valued or integer-valued data be discretized into groups (e.g., less than 5, between 5 and 10, or greater than 10).
","The piece of input data for which an output value is generated is formally termed an instance. The instance is formally described by a vector of features, which together constitute a description of all known characteristics of the instance.","[' What is the piece of input data for which an output value is generated formally termed?', ' What is formally described by a vector of features?']","['an instance', 'instance']"
994,pattern recognition,Problem statement,"The problem of pattern recognition can be stated as follows: Given an unknown function 



g
:


X


→


Y




{\displaystyle g:{\mathcal {X}}\rightarrow {\mathcal {Y}}}
 (the ground truth) that maps input instances 




x

∈


X




{\displaystyle {\boldsymbol {x}}\in {\mathcal {X}}}
 to output labels 



y
∈


Y




{\displaystyle y\in {\mathcal {Y}}}
, along with training data 




D

=
{
(


x


1


,

y

1


)
,
…
,
(


x


n


,

y

n


)
}


{\displaystyle \mathbf {D} =\{({\boldsymbol {x}}_{1},y_{1}),\dots ,({\boldsymbol {x}}_{n},y_{n})\}}
 assumed to represent accurate examples of the mapping, produce a function 



h
:


X


→


Y




{\displaystyle h:{\mathcal {X}}\rightarrow {\mathcal {Y}}}
 that approximates as closely as possible the correct mapping 



g


{\displaystyle g}
. (For example, if the problem is filtering spam, then 





x


i




{\displaystyle {\boldsymbol {x}}_{i}}
 is some representation of an email and 



y


{\displaystyle y}
 is either ""spam"" or ""non-spam""). In order for this to be a well-defined problem, ""approximates as closely as possible"" needs to be defined rigorously. In decision theory, this is defined by specifying a loss function or cost function that assigns a specific value to ""loss"" resulting from producing an incorrect label. The goal then is to minimize the expected loss, with the expectation taken over the probability distribution of 





X




{\displaystyle {\mathcal {X}}}
. In practice, neither the distribution of 





X




{\displaystyle {\mathcal {X}}}
 nor the ground truth function 



g
:


X


→


Y




{\displaystyle g:{\mathcal {X}}\rightarrow {\mathcal {Y}}}
 are known exactly, but can be computed only empirically by collecting a large number of samples of 





X




{\displaystyle {\mathcal {X}}}
 and hand-labeling them using the correct value of 





Y




{\displaystyle {\mathcal {Y}}}
 (a time-consuming process, which is typically the limiting factor in the amount of data of this sort that can be collected). The particular loss function depends on the type of label being predicted. For example, in the case of classification, the simple zero-one loss function is often sufficient. This corresponds simply to assigning a loss of 1 to any incorrect labeling and implies that the optimal classifier minimizes the error rate on independent test data (i.e. counting up the fraction of instances that the learned function 



h
:


X


→


Y




{\displaystyle h:{\mathcal {X}}\rightarrow {\mathcal {Y}}}
 labels wrongly, which is equivalent to maximizing the number of correctly classified instances). The goal of the learning procedure is then to minimize the error rate (maximize the correctness) on a ""typical"" test set.
","The problem of pattern recognition can be stated as follows: Given an unknown function 



g
:


X


→


Y




{\displaystyle g:{\mathcal {X}}\rightarrow {\mathcal {Y}}}
 (the ground truth) that maps input instances 




x

∈


X




{\displaystyle {\boldsymbol {x}}\in {\mathcal {X}}}
 to output labels 



y
∈


Y




{\displaystyle y\in {\mathcal {Y}}}
, along with training data 




D

=
{
(


x


1


,

y

1


)
,
…
,
(


x


n


,

y

n


)
}


{\displaystyle \mathbf {D} =\{({\boldsymbol {x}}_{1},y_{1}),\dots ,({\boldsymbol {x}}_{n},y_{n})\}}
 assumed to represent accurate examples of the mapping, produce a function 



h
:


X


→


Y




{\displaystyle h:{\mathcal {X}}\rightarrow {\mathcal {Y}}}
 that approximates as closely as possible the correct mapping 



g


{\displaystyle g}
. (For example, if the problem is filtering spam, then 





x


i




{\displaystyle {\boldsymbol {x}}_{i}}
 is some representation of an email and 



y


{\displaystyle y}
 is either ""spam"" or ""non-spam"").","[' What is the problem of pattern recognition?', ' What is given an unknown function that maps input instances x <unk> X to output labels?', ' What is assumed to represent accurate examples of the mapping?', ' What approximates as closely as possible the correct mapping g?', ' What is the problem with filtering spam?', ' What is some representation of an email?']","['filtering spam', 'the ground truth', 'g', 'h\n:\n\n\nX\n\n\n→\n\n\nY', '\\mathbf', '\\mathbf']"
995,pattern recognition,Problem statement,"where the feature vector input is 




x



{\displaystyle {\boldsymbol {x}}}
, and the function f is typically parameterized by some parameters 




θ



{\displaystyle {\boldsymbol {\theta }}}
. In a discriminative approach to the problem, f is estimated directly. In a generative approach, however, the inverse probability 



p
(


x


|



l
a
b
e
l



)


{\displaystyle p({{\boldsymbol {x}}|{\rm {label}}})}
 is instead estimated and combined with the prior probability 



p
(


l
a
b
e
l



|


θ

)


{\displaystyle p({\rm {label}}|{\boldsymbol {\theta }})}
 using Bayes' rule, as follows:
","where the feature vector input is 




x



{\displaystyle {\boldsymbol {x}}}
, and the function f is typically parameterized by some parameters 




θ



{\displaystyle {\boldsymbol {\theta }}}
. In a discriminative approach to the problem, f is estimated directly.","[' Where the feature vector input is x <unk>displaystyle <unk>boldsymbol <unk>x<unk>, and the function f is typically parameterized by some parameters?', ' In a discriminative approach to the problem, how is f estimated?']","['θ', 'directly']"
996,pattern recognition,Problem statement,"The value of 




θ



{\displaystyle {\boldsymbol {\theta }}}
 is typically learned using maximum a posteriori (MAP) estimation. This finds the best value that simultaneously meets two conflicting objects: To perform as well as possible on the training data (smallest error-rate) and to find the simplest possible model. Essentially, this combines maximum likelihood estimation with a regularization procedure that favors simpler models over more complex models. In a Bayesian context, the regularization procedure can be viewed as placing a prior probability 



p
(

θ

)


{\displaystyle p({\boldsymbol {\theta }})}
 on different values of 




θ



{\displaystyle {\boldsymbol {\theta }}}
. Mathematically:
","The value of 




θ



{\displaystyle {\boldsymbol {\theta }}}
 is typically learned using maximum a posteriori (MAP) estimation. This finds the best value that simultaneously meets two conflicting objects: To perform as well as possible on the training data (smallest error-rate) and to find the simplest possible model.","[' How is the value of <unk> <unk>displaystyle <unk>boldsymbol <unk>theta learned?', ' What is MAP?', ' How does MAP estimation find the best value?']","['using maximum a posteriori (MAP) estimation', 'maximum a posteriori', 'simultaneously meets two conflicting objects']"
997,pattern recognition,Uses,"Within medical science, pattern recognition is the basis for computer-aided diagnosis (CAD) systems. CAD describes a procedure that supports the doctor's interpretations and findings. Other typical applications of pattern recognition techniques are automatic speech recognition, speaker identification, classification of text into several categories (e.g., spam or non-spam email messages), the automatic recognition of handwriting on postal envelopes, automatic recognition of images of human faces, or handwriting image extraction from medical forms. The last two examples form the subtopic image analysis of pattern recognition that deals with digital images as input to pattern recognition systems.","Within medical science, pattern recognition is the basis for computer-aided diagnosis (CAD) systems. CAD describes a procedure that supports the doctor's interpretations and findings.","[' What is the basis for computer-aided diagnosis systems?', ' What does CAD stand for?']","['pattern recognition', 'computer-aided diagnosis']"
998,pattern recognition,Uses,"Optical character recognition is an example of the application of a pattern classifier. The method of signing one's name was captured with stylus and overlay starting in 1990. The strokes, speed, relative min, relative max, acceleration and pressure is used to uniquely identify and confirm identity. Banks were first offered this technology, but were content to collect from the FDIC for any bank fraud and did not want to inconvenience customers.",Optical character recognition is an example of the application of a pattern classifier. The method of signing one's name was captured with stylus and overlay starting in 1990.,"[' What is an example of the application of a pattern classifier?', "" When was the method of signing one's name captured with stylus and overlay?""]","['Optical character recognition', '1990']"
999,pattern recognition,Uses,"Pattern recognition has many real-world applications in image processing. Some examples include:
",Pattern recognition has many real-world applications in image processing. Some examples include:,[' What kind of applications does pattern recognition have in image processing?'],['real-world']
1000,pattern recognition,Uses,"In psychology, pattern recognition, which is used to make sense of and identify objects, is closely related to perception. This explains how the sensory inputs humans receive are made meaningful. Pattern recognition can be thought of in two different ways. The first concerns template matching and the second concerns feature detection. A template is a pattern used to produce items of the same proportions. The template-matching hypothesis suggests that incoming stimuli are compared with templates in the long-term memory. If there is a match, the stimulus is identified. Feature detection models, such as the Pandemonium system for classifying letters (Selfridge, 1959), suggest that the stimuli are broken down into their component parts for identification. One observation is a capital E having three horizontal lines and one vertical line.","In psychology, pattern recognition, which is used to make sense of and identify objects, is closely related to perception. This explains how the sensory inputs humans receive are made meaningful.","[' In psychology, what is pattern recognition used to make sense of and identify objects?', ' Pattern recognition is closely related to what?', ' How are sensory inputs made meaningful?']","['perception', 'perception', 'pattern recognition']"
1001,pattern recognition,Algorithms,"Algorithms for pattern recognition depend on the type of label output, on whether learning is supervised or unsupervised, and on whether the algorithm is statistical or non-statistical in nature. Statistical algorithms can further be categorized as generative or discriminative.
","Algorithms for pattern recognition depend on the type of label output, on whether learning is supervised or unsupervised, and on whether the algorithm is statistical or non-statistical in nature. Statistical algorithms can further be categorized as generative or discriminative.","[' Algorithms for pattern recognition depend on what type of output?', ' What type of algorithms can be categorized as generative or discriminative?']","['label', 'Statistical']"
1002,modal logic,Summary,"Modal logic is a collection of formal systems developed to represent statements about necessity and possibility. It plays a major role in philosophy of language, epistemology, metaphysics, and natural language semantics. Modal logics extend other systems by adding unary operators 



◊


{\displaystyle \Diamond }
 and 



◻


{\displaystyle \Box }
, representing possibility and necessity respectively. For instance the modal formula 



◊
P


{\displaystyle \Diamond P}
 can be read as ""possibly 



P


{\displaystyle P}
"" while 



◻
P


{\displaystyle \Box P}
 can be read as ""necessarily 



P


{\displaystyle P}
"". Modal logics can be used to represent different phenomena depending on what kind of necessity and possibility is under consideration. When 



◻


{\displaystyle \Box }
 is used to represent epistemic necessity, 



◻
P


{\displaystyle \Box P}
 states that 



P


{\displaystyle P}
 is epistemically necessary, or in other words that it is known. When 



◻


{\displaystyle \Box }
 is used to represent deontic necessity, 



◻
P


{\displaystyle \Box P}
 states that 



P


{\displaystyle P}
 is a moral or legal obligation.","Modal logic is a collection of formal systems developed to represent statements about necessity and possibility. It plays a major role in philosophy of language, epistemology, metaphysics, and natural language semantics.","[' What is a collection of formal systems developed to represent statements about necessity and possibility?', ' Modal logic plays a major role in philosophy of what?']","['Modal logic', 'language']"
1003,modal logic,Summary,"In the standard relational semantics for modal logic, formulas are assigned truth values relative to a possible world. A formula's truth value at one possible world can depend on the truth values of other formulas at other accessible possible worlds. In particular, 



◊
P


{\displaystyle \Diamond P}
 is true at a world if 



P


{\displaystyle P}
 is true at some accessible possible world, while 



◻
P


{\displaystyle \Box P}
 is true at a world if 



P


{\displaystyle P}
 is true at every accessible possible world. A variety of proof systems exist which are sound and complete with respect to the semantics one gets by restricting the accessibility relation. For instance, the deontic modal logic D is sound and complete if one requires the accessibility relation to be serial.
","In the standard relational semantics for modal logic, formulas are assigned truth values relative to a possible world. A formula's truth value at one possible world can depend on the truth values of other formulas at other accessible possible worlds.","[' In relational semantics for modal logic, what are formulas assigned relative to?', "" A formula's truth value at one possible world can depend on the truth values of other formulas at other accessible possible worlds?""]","['a possible world', 'formulas']"
1004,modal logic,Summary,"While the intuition behind modal logic date back to antiquity, the first modal axiomatic systems were developed by C. I. Lewis in 1912. The now-standard relational semantics emerged in the mid twentieth century from work by Arthur Prior, Jaakko Hintikka, and Saul Kripke. Recent developments include alternative topological semantics such as neighborhood semantics as well as applications of the relational semantics beyond its original philosophical motivation. Such applications include game theory, moral and legal theory, web design, multiverse-based set theory, and social epistemology.","While the intuition behind modal logic date back to antiquity, the first modal axiomatic systems were developed by C. I. Lewis in 1912. The now-standard relational semantics emerged in the mid twentieth century from work by Arthur Prior, Jaakko Hintikka, and Saul Kripke.","[' Who developed the first modal axiomatic systems?', ' When did C. I. Lewis develop modal logic?', ' Who developed relational semantics?', ' In what century did relational logic emerge?']","['C. I. Lewis', '1912', 'Arthur Prior, Jaakko Hintikka, and Saul Kripke', 'mid twentieth century']"
1005,modal logic,Axiomatic systems,"The first formalizations of modal logic were axiomatic. Numerous variations with very different properties have been proposed since C. I. Lewis began working in the area in 1912. Hughes and Cresswell (1996), for example, describe 42 normal and 25 non-normal modal logics. Zeman (1973) describes some systems Hughes and Cresswell omit.
",The first formalizations of modal logic were axiomatic. Numerous variations with very different properties have been proposed since C. I. Lewis began working in the area in 1912.,"[' What were the first formalizations of modal logic?', ' When did C. I. Lewis begin working in the area?']","['axiomatic', '1912']"
1006,modal logic,Axiomatic systems,"Modern treatments of modal logic begin by augmenting the propositional calculus with two unary operations, one denoting ""necessity"" and the other ""possibility"". The notation of C. I. Lewis, much employed since, denotes ""necessarily p"" by a prefixed ""box"" (□p) whose scope is established by parentheses. Likewise, a prefixed ""diamond"" (◇p) denotes ""possibly p"". Regardless of notation, each of these operators is definable in terms of the other in classical modal logic:
","Modern treatments of modal logic begin by augmenting the propositional calculus with two unary operations, one denoting ""necessity"" and the other ""possibility"". The notation of C. I. Lewis, much employed since, denotes ""necessarily p"" by a prefixed ""box"" (□p) whose scope is established by parentheses.","[' Modern treatments of modal logic begin by augmenting the propositional calculus with how many unary operations?', ' The notation of C. I. Lewis denotes ""necessary p"" by a prefixed ""box"" (<unk>p) whose scope is established by what?']","['two', 'parentheses']"
1007,modal logic,Axiomatic systems,"Precisely what axioms and rules must be added to the propositional calculus to create a usable system of modal logic is a matter of philosophical opinion, often driven by the theorems one wishes to prove; or, in computer science, it is a matter of what sort of computational or deductive system one wishes to model.  Many modal logics, known collectively as normal modal logics, include the following rule and axiom:
","Precisely what axioms and rules must be added to the propositional calculus to create a usable system of modal logic is a matter of philosophical opinion, often driven by the theorems one wishes to prove; or, in computer science, it is a matter of what sort of computational or deductive system one wishes to model. Many modal logics, known collectively as normal modal logics, include the following rule and axiom:","[' What is a matter of philosophical opinion, often driven by the theorems one wishes to prove?', ' In computer science, what is it matter of what kind of computational or deductive logic?', ' What is a matter of what kind of system one wishes to model?', ' What are modal logics collectively known as?']","['modal logic', 'what sort of computational or deductive system one wishes to model', 'computational or deductive', 'normal modal logics']"
1008,modal logic,Axiomatic systems,"The weakest normal modal logic, named ""K"" in honor of Saul Kripke, is simply the propositional calculus augmented by □, the rule N, and the axiom K. K is weak in that it fails to determine whether a proposition can be necessary but only contingently necessary. That is, it is not a theorem of K that if □p is true then □□p is true, i.e., that necessary truths are ""necessarily necessary"". If such perplexities are deemed forced and artificial, this defect of K is not a great one. In any case, different answers to such questions yield different systems of modal logic.
","The weakest normal modal logic, named ""K"" in honor of Saul Kripke, is simply the propositional calculus augmented by □, the rule N, and the axiom K. K is weak in that it fails to determine whether a proposition can be necessary but only contingently necessary. That is, it is not a theorem of K that if □p is true then □□p is true, i.e., that necessary truths are ""necessarily necessary"".","[' What is the weakest normal modal logic named?', ' Who is K named in honor of?', ' What is K weak in?', ' What is not a theorem of K that if <unk>p is true then <unk>pe is true?', ' What are ""necessarily necessary""?']","['K', 'Saul Kripke', 'it fails to determine whether a proposition can be necessary but only contingently necessary', '□□', 'necessary truths']"
1009,modal logic,Axiomatic systems,"Adding axioms to K gives rise to other well-known modal systems. One cannot prove in K that if ""p is necessary"" then p is true. The axiom T remedies this defect:
","Adding axioms to K gives rise to other well-known modal systems. One cannot prove in K that if ""p is necessary"" then p is true.","[' Adding axioms to K gives rise to what?', ' One cannot prove in K that if ""p is necessary"" then what is true?']","['other well-known modal systems', 'p']"
1010,modal logic,Axiomatic systems,"T holds in most but not all modal logics. Zeman (1973) describes a few exceptions, such as S10.
","T holds in most but not all modal logics. Zeman (1973) describes a few exceptions, such as S10.","[' What holds in most but not all modal logics?', ' What does Zeman describe a few exceptions to?']","['T', 'S10']"
1011,modal logic,Axiomatic systems,"K through S5 form a nested hierarchy of systems, making up the core of normal modal logic. But specific rules or sets of rules may be appropriate for specific systems. For example, in deontic logic, 



◻
p
→
◊
p


{\displaystyle \Box p\to \Diamond p}
 (If it ought to be that p, then it is permitted that p) seems appropriate, but we should probably not include that 



p
→
◻
◊
p


{\displaystyle p\to \Box \Diamond p}
. In fact, to do so is to commit the appeal to nature fallacy (i.e. to state that what is natural is also good, by saying that if p is the case, p ought to be permitted).
","K through S5 form a nested hierarchy of systems, making up the core of normal modal logic. But specific rules or sets of rules may be appropriate for specific systems.",[' K through S5 form a nested hierarchy of what?'],['systems']
1012,modal logic,Axiomatic systems,"The commonly employed system S5 simply makes all modal truths necessary. For example, if p is possible, then it is ""necessary"" that p is possible. Also, if p is necessary, then it is necessary that p is necessary. Other systems of modal logic have been formulated, in part because S5 does not describe every kind of modality of interest.
","The commonly employed system S5 simply makes all modal truths necessary. For example, if p is possible, then it is ""necessary"" that p is possible.","[' What makes all modal truths necessary?', ' What is S5?']","['The commonly employed system S5', 'makes all modal truths necessary']"
1013,modal logic,Metaphysical questions,"In the most common interpretation of modal logic, one considers ""logically possible worlds"". If a statement is true in all possible worlds, then it is a necessary truth. If a statement happens to be true in our world, but is not true in all possible worlds, then it is a contingent truth. A statement that is true in some possible world (not necessarily our own) is called a possible truth.
","In the most common interpretation of modal logic, one considers ""logically possible worlds"". If a statement is true in all possible worlds, then it is a necessary truth.","[' What is the most common interpretation of modal logic?', ' What is a necessary truth if a statement is true in all possible worlds?']","['logically possible worlds', 'logically possible worlds']"
1014,modal logic,Metaphysical questions,"Under this ""possible worlds idiom,"" to maintain that Bigfoot's existence is possible but not actual, one says, ""There is some possible world in which Bigfoot exists; but in the actual world, Bigfoot does not exist"". However, it is unclear what this claim commits us to. Are we really alleging the existence of possible worlds, every bit as real as our actual world, just not actual? Saul Kripke believes that 'possible world' is something of a misnomer – that the term 'possible world' is just a useful way of visualizing the concept of possibility. For him, the sentences ""you could have rolled a 4 instead of a 6"" and ""there is a possible world where you rolled a 4, but you rolled a 6 in the actual world"" are not significantly different statements, and neither commit us to the existence of a possible world. David Lewis, on the other hand, made himself notorious by biting the bullet, asserting that all merely possible worlds are as real as our own, and that what distinguishes our world as actual is simply that it is indeed our world – this world. That position is a major tenet of ""modal realism"". Some philosophers decline to endorse any version of modal realism, considering it ontologically extravagant, and prefer to seek various ways to paraphrase away these ontological commitments. Robert Adams holds that 'possible worlds' are better thought of as 'world-stories', or consistent sets of propositions. Thus, it is possible that you rolled a 4 if such a state of affairs can be described coherently.","Under this ""possible worlds idiom,"" to maintain that Bigfoot's existence is possible but not actual, one says, ""There is some possible world in which Bigfoot exists; but in the actual world, Bigfoot does not exist"". However, it is unclear what this claim commits us to.","[' What does the ""posible worlds idiom"" say about Bigfoot\'s existence?', ' What is unclear about this claim?']","['""There is some possible world in which Bigfoot exists; but in the actual world, Bigfoot does not exist"".', 'what this claim commits us to']"
1015,modal logic,Metaphysical questions,"Computer scientists will generally pick a highly specific interpretation of the modal operators specialized to the particular sort of computation being analysed.  In place of ""all worlds"", you may have ""all possible next states of the computer"", or ""all possible future states of the computer"".
","Computer scientists will generally pick a highly specific interpretation of the modal operators specialized to the particular sort of computation being analysed. In place of ""all worlds"", you may have ""all possible next states of the computer"", or ""all possible future states of the computer"".","[' Computer scientists will generally pick a highly specific interpretation of what?', ' In place of ""all worlds"", you may have ""all possible next states of the computer"" or what else?']","['the modal operators', 'all possible future states of the computer']"
1016,modal logic,History,"The basic ideas of modal logic date back to antiquity. Aristotle developed a modal syllogistic in Book I of his Prior Analytics (chs 8–22), which Theophrastus attempted to improve. There are also passages in Aristotle's work, such as the famous sea-battle argument in De Interpretatione §9, that are now seen as anticipations of the connection of modal logic with potentiality and time. In the Hellenistic period, the logicians Diodorus Cronus, Philo the Dialectician and the Stoic Chrysippus each developed a modal system that accounted for the interdefinability of possibility and necessity, accepted axiom T (see below), and combined elements of modal logic and temporal logic in attempts to solve the notorious Master Argument. The earliest formal system of modal logic was developed by Avicenna, who ultimately developed a theory of ""temporally modal"" syllogistic. Modal logic as a self-aware subject owes much to the writings of the Scholastics, in particular William of Ockham and John Duns Scotus, who reasoned informally in a modal manner, mainly to analyze statements about essence and accident.
","The basic ideas of modal logic date back to antiquity. Aristotle developed a modal syllogistic in Book I of his Prior Analytics (chs 8–22), which Theophrastus attempted to improve.","[' When do the basic ideas of modal logic date back to?', ' Who developed a modal syllogistic in Book I of his Prior Analytics?', ' What did Theophrastus attempt to improve?']","['antiquity', 'Aristotle', 'modal syllogistic']"
1017,modal logic,History,"In the 19th century, Hugh MacColl made innovative contributions to modal logic, but did not find much acknowledgment.C. I. Lewis founded modern modal logic in a series of scholarly articles beginning in 1912 with ""Implication and the Algebra of Logic"". Lewis was led to invent modal logic, and specifically strict implication, on the grounds that classical logic grants paradoxes of material implication such as the principle that a falsehood implies any proposition. This work culminated in his 1932 book Symbolic Logic (with C. H. Langford), which introduced the five systems S1 through S5.
","In the 19th century, Hugh MacColl made innovative contributions to modal logic, but did not find much acknowledgment.C. I. Lewis founded modern modal logic in a series of scholarly articles beginning in 1912 with ""Implication and the Algebra of Logic"".","[' Who made innovative contributions to modal logic in the 19th century but did not find much recognition?', ' Who founded modern modal logical in a series of scholarly articles beginning in 1912 with ""Implication and the Algebra of Logic""?']","['Hugh MacColl', 'C. I. Lewis']"
1018,modal logic,History,"After Lewis, modal logic received little attention for several decades. Nicholas Rescher has argued that this was because Bertrand Russell rejected it. However, Jan Dejnozka has argued against this view, stating that a modal system which Dejnozka calls ""MDL"" is described in Russell's works, although Russell did believe the concept of modality to ""come from confusing propositions with propositional functions,"" as he wrote in The Analysis of Matter.","After Lewis, modal logic received little attention for several decades. Nicholas Rescher has argued that this was because Bertrand Russell rejected it.","[' After what philosopher did modal logic receive little attention?', ' Nicholas Rescher has argued that this was because Bertrand Russell rejected it?']","['Lewis', 'modal logic']"
1019,modal logic,History,"The contemporary era in modal semantics began in 1959, when Saul Kripke (then only a 18-year-old Harvard University undergraduate) introduced the now-standard Kripke semantics for modal logics. These are commonly referred to as ""possible worlds"" semantics. Kripke and A. N. Prior had previously corresponded at some length. Kripke semantics is basically simple, but proofs are eased using semantic-tableaux or analytic tableaux, as explained by E. W. Beth.
","The contemporary era in modal semantics began in 1959, when Saul Kripke (then only a 18-year-old Harvard University undergraduate) introduced the now-standard Kripke semantics for modal logics. These are commonly referred to as ""possible worlds"" semantics.","[' When did the contemporary era in modal semantics begin?', ' Who introduced the now standard Kripke semantics for modal logics?', ' What are these semantics commonly referred to as?']","['1959', 'Saul Kripke', 'possible worlds']"
1020,modal logic,History,"A. N. Prior created modern temporal logic, closely related to modal logic, in 1957 by adding modal operators [F] and [P] meaning ""eventually"" and ""previously"". Vaughan Pratt introduced dynamic logic in 1976. In 1977, Amir Pnueli proposed using temporal logic to formalise the behaviour of continually operating concurrent programs. Flavors of temporal logic include propositional dynamic logic (PDL), propositional linear temporal logic (PLTL), linear temporal logic (LTL), computation tree logic (CTL), Hennessy–Milner logic, and T.","A. N. Prior created modern temporal logic, closely related to modal logic, in 1957 by adding modal operators [F] and [P] meaning ""eventually"" and ""previously"". Vaughan Pratt introduced dynamic logic in 1976.","[' A. N. Prior created modern temporal logic in what year?', ' Prior added modal operators ""eventually"" and ""previously"" to what logic?', ' Vaughan Pratt introduced what logic in 1976?']","['1957', 'temporal logic', 'dynamic']"
1021,modal logic,History,"The mathematical structure of modal logic, namely Boolean algebras augmented with unary operations (often called modal algebras), began to emerge with J. C. C. McKinsey's 1941 proof that S2 and S4 are decidable, and reached full flower in the work of Alfred Tarski and his student Bjarni Jónsson (Jónsson and Tarski 1951–52). This work revealed that S4 and S5 are models of interior algebra, a proper extension of Boolean algebra originally designed to capture the properties of the interior and closure operators of topology. Texts on modal logic typically do little more than mention its connections with the study of Boolean algebras and topology. For a thorough survey of the history of formal modal logic and of the associated mathematics, see Robert Goldblatt (2006).","The mathematical structure of modal logic, namely Boolean algebras augmented with unary operations (often called modal algebras), began to emerge with J. C. C. McKinsey's 1941 proof that S2 and S4 are decidable, and reached full flower in the work of Alfred Tarski and his student Bjarni Jónsson (Jónsson and Tarski 1951–52). This work revealed that S4 and S5 are models of interior algebra, a proper extension of Boolean algebra originally designed to capture the properties of the interior and closure operators of topology.","[' What are Boolean algebras augmented with unary operations often called?', ' Whose 1941 proof that S2 and S4 are decidable reached full flower in the work of Alfred Tarski and Bjarni Jónsson?', ' Whose work revealed that S4 and S5 are models of interior algebra?', ' What type of algebra was originally designed to capture the properties of interior and closure operators of topology?']","['modal algebras', 'J. C. C. McKinsey', 'Alfred Tarski', 'Boolean algebra']"
1022,bayesian network,Summary,"A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor.  For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.
","A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor.","[' What is a Bayesian network also known as?', ' What is the name of the graph that represents a set of variables and their conditional dependencies?', ' Bayes networks are ideal for taking an event that occurred and predicting what?', ' What is ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor?']","['Bayes network', 'directed acyclic graph', 'the likelihood that any one of several possible known causes was the contributing factor', 'Bayesian networks']"
1023,bayesian network,Summary,"Efficient algorithms can perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.
",Efficient algorithms can perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g.,"[' Efficient algorithms can perform inference and learning in what?', ' Bayesian networks that model sequences of variables (e.g.']","['Bayesian networks', 'Bayesian networks']"
1024,bayesian network,Graphical model,"Formally, Bayesian networks are directed acyclic graphs (DAGs) whose nodes represent variables in the Bayesian sense: they may be observable quantities, latent variables, unknown parameters or hypotheses. Edges represent conditional dependencies; nodes that are not connected (no path connects one node to another) represent variables that are conditionally independent of each other. Each node is associated with a probability function that takes, as input, a particular set of values for the node's parent variables, and gives (as output) the probability (or probability distribution, if applicable) of the variable represented by the node. For example, if 



m


{\displaystyle m}
 parent nodes represent 



m


{\displaystyle m}
 Boolean variables, then the probability function could be represented by a table of 




2

m




{\displaystyle 2^{m}}
 entries, one entry for each of the 




2

m




{\displaystyle 2^{m}}
 possible parent combinations. Similar ideas may be applied to undirected, and possibly cyclic, graphs such as Markov networks.
","Formally, Bayesian networks are directed acyclic graphs (DAGs) whose nodes represent variables in the Bayesian sense: they may be observable quantities, latent variables, unknown parameters or hypotheses. Edges represent conditional dependencies; nodes that are not connected (no path connects one node to another) represent variables that are conditionally independent of each other.","[' What are directed acyclic graphs?', ' What represent variables in the Bayesian sense?', ' Edges represent what?', ' Nodes that are not connected represent variables that are conditionally independent of one another?', ' What represent variables that are conditionally independent of each other?']","['Bayesian networks', 'nodes', 'conditional dependencies', 'no path connects one node to another', 'nodes that are not connected']"
1025,bayesian network,Example,"Two events can cause grass to be wet: an active sprinkler or rain. Rain has a direct effect on the use of the sprinkler (namely that when it rains, the sprinkler usually is not active). This situation can be modeled with a Bayesian network (shown to the right). Each variable has two possible values, T (for true) and F (for false).
","Two events can cause grass to be wet: an active sprinkler or rain. Rain has a direct effect on the use of the sprinkler (namely that when it rains, the sprinkler usually is not active).","[' How many events can cause grass to be wet?', ' What has a direct effect on the use of the sprinkler?', ' When it rains, what is usually not active?']","['Two', 'Rain', 'sprinkler']"
1026,bayesian network,Example,"The model can answer questions about the presence of a cause given the presence of an effect (so-called inverse probability) like ""What is the probability that it is raining, given the grass is wet?"" by using the conditional probability formula and summing over all nuisance variables:
","The model can answer questions about the presence of a cause given the presence of an effect (so-called inverse probability) like ""What is the probability that it is raining, given the grass is wet?"" by using the conditional probability formula and summing over all nuisance variables:","[' What is inverse probability?', ' What is the model able to answer about a cause given the presence of an effect?']","['presence of a cause given the presence of an effect', 'inverse probability) like ""What is the probability that it is raining, given the grass is wet?""']"
1027,bayesian network,Example,"Using the expansion for the joint probability function 



Pr
(
G
,
S
,
R
)


{\displaystyle \Pr(G,S,R)}
 and the conditional probabilities from the conditional probability tables (CPTs) stated in the diagram, one can evaluate each term in the sums in the numerator and denominator. For example,
","Using the expansion for the joint probability function 



Pr
(
G
,
S
,
R
)


{\displaystyle \Pr(G,S,R)}
 and the conditional probabilities from the conditional probability tables (CPTs) stated in the diagram, one can evaluate each term in the sums in the numerator and denominator. For example,","[' What is the expansion for the joint probability function Pr?', ' What can one evaluate each term in the sums in the numerator and denominator?']","['Pr\n(\nG\n,\nS\n,\nR\n)\n\n\n{\\displaystyle \\Pr(G,S,R)}', 'Using the expansion for the joint probability function']"
1028,bayesian network,Example,"To answer an interventional question, such as ""What is the probability that it would rain, given that we wet the grass?"" the answer is governed by the post-intervention joint distribution function
","To answer an interventional question, such as ""What is the probability that it would rain, given that we wet the grass?"" the answer is governed by the post-intervention joint distribution function","[' What is a post-intervention joint distribution function?', ' What is the probability that it would rain?']","['probability that it would rain', 'given that we wet the grass']"
1029,bayesian network,Example,"obtained by removing the factor 



Pr
(
G
∣
S
,
R
)


{\displaystyle \Pr(G\mid S,R)}
 from the pre-intervention distribution. The do operator forces the value of G to be true. The probability of rain is unaffected by the action:
","obtained by removing the factor 



Pr
(
G
∣
S
,
R
)


{\displaystyle \Pr(G\mid S,R)}
 from the pre-intervention distribution. The do operator forces the value of G to be true.","[' What is obtained by removing the factor Pr from the pre-intervention distribution?', ' What operator forces the value of G to be true?']","['Pr\n(\nG\n∣\nS\n,\nR\n)\n\n\n{\\displaystyle \\Pr(G\\mid S,R)}\n from the pre-intervention distribution. The do operator forces the value of G to be true', 'do']"
1030,bayesian network,Example,"These predictions may not be feasible given unobserved variables, as in most policy evaluation problems. The effect of the action 




do

(
x
)


{\displaystyle {\text{do}}(x)}
 can still be predicted, however, whenever the back-door criterion is satisfied. It states that, if a set Z of nodes can be observed that d-separates (or blocks) all back-door paths from X to Y then
","These predictions may not be feasible given unobserved variables, as in most policy evaluation problems. The effect of the action 




do

(
x
)


{\displaystyle {\text{do}}(x)}
 can still be predicted, however, whenever the back-door criterion is satisfied.","[' What may not be feasible given unobserved variables?', ' What can still be predicted when the back-door criterion is satisfied?']","['predictions', 'The effect of the action \n\n\n\n\ndo\n\n(\nx\n)\n\n\n{\\displaystyle {\\text{do}}(x)}']"
1031,bayesian network,Example,"A back-door path is one that ends with an arrow into X. Sets that satisfy the back-door criterion are called ""sufficient"" or ""admissible."" For example, the set Z = R is admissible for predicting the effect of S = T on G, because R d-separates the (only) back-door path S ← R → G. However, if S is not observed, no other set d-separates this path and the effect of turning the sprinkler on (S = T) on the grass (G) cannot be predicted from passive observations. In that case P(G | do(S = T)) is not ""identified"". This reflects the fact that, lacking interventional data, the observed dependence between S and G is due to a causal connection or is spurious
(apparent dependence arising from a common cause, R). (see Simpson's paradox)
","A back-door path is one that ends with an arrow into X. Sets that satisfy the back-door criterion are called ""sufficient"" or ""admissible.""","[' What is a back-door path that ends with an arrow into X called?', ' What are sets that satisfy the back door criterion called that are ""sufficient"" or ""admissible""?']","['""sufficient"" or ""admissible.""', 'back-door path']"
1032,bayesian network,Example,"Using a Bayesian network can save considerable amounts of memory over exhaustive probability tables, if the dependencies in the joint distribution are sparse. For example, a naive way of storing the conditional probabilities of 10 two-valued variables as a table requires storage space for 




2

10


=
1024


{\displaystyle 2^{10}=1024}
 values. If no variable's local distribution depends on more than three parent variables, the Bayesian network representation stores at most 



10
⋅

2

3


=
80


{\displaystyle 10\cdot 2^{3}=80}
 values.
","Using a Bayesian network can save considerable amounts of memory over exhaustive probability tables, if the dependencies in the joint distribution are sparse. For example, a naive way of storing the conditional probabilities of 10 two-valued variables as a table requires storage space for 




2

10


=
1024


{\displaystyle 2^{10}=1024}
 values.","[' Using a Bayesian network can save considerable amounts of memory over what?', ' What is a naive way of storing conditional probabilities of 10 two-valued variables as a table required?']","['exhaustive probability tables', 'storage space for \n\n\n\n\n2\n\n10\n\n\n=\n1024\n\n\n{\\displaystyle 2^{10}=1024}\n values']"
1033,bayesian network,Statistical introduction,"Given data 



x




{\displaystyle x\,\!}
 and parameter 



θ


{\displaystyle \theta }
, a simple Bayesian analysis starts with a prior probability (prior) 



p
(
θ
)


{\displaystyle p(\theta )}
 and likelihood 



p
(
x
∣
θ
)


{\displaystyle p(x\mid \theta )}
 to compute a posterior probability 



p
(
θ
∣
x
)
∝
p
(
x
∣
θ
)
p
(
θ
)


{\displaystyle p(\theta \mid x)\propto p(x\mid \theta )p(\theta )}
.
","Given data 



x




{\displaystyle x\,\!} and parameter 



θ


{\displaystyle \theta }
, a simple Bayesian analysis starts with a prior probability (prior) 



p
(
θ
)


{\displaystyle p(\theta )}
 and likelihood 



p
(
x
∣
θ
)


{\displaystyle p(x\mid \theta )}
 to compute a posterior probability 



p
(
θ
∣
x
)
∝
p
(
x
∣
θ
)
p
(
θ
)


{\displaystyle p(\theta \mid x)\propto p(x\mid \theta )p(\theta )}
.","[' What type of analysis starts with a prior probability (prior) p ( <unk> ) <unk>displaystyle p(<unk>theta )<unk>?', ' What is a posterior probability?']","['Bayesian', 'p\n(\nθ']"
1034,bayesian network,Statistical introduction,"Often the prior on 



θ


{\displaystyle \theta }
 depends in turn on other parameters 



φ


{\displaystyle \varphi }
 that are not mentioned in the likelihood. So, the prior 



p
(
θ
)


{\displaystyle p(\theta )}
 must be replaced by a likelihood 



p
(
θ
∣
φ
)


{\displaystyle p(\theta \mid \varphi )}
, and a prior 



p
(
φ
)


{\displaystyle p(\varphi )}
 on the newly introduced parameters 



φ


{\displaystyle \varphi }
 is required, resulting in a posterior probability
","Often the prior on 



θ


{\displaystyle \theta }
 depends in turn on other parameters 



φ


{\displaystyle \varphi }
 that are not mentioned in the likelihood. So, the prior 



p
(
θ
)


{\displaystyle p(\theta )}
 must be replaced by a likelihood 



p
(
θ
∣
φ
)


{\displaystyle p(\theta \mid \varphi )}
, and a prior 



p
(
φ
)


{\displaystyle p(\varphi )}
 on the newly introduced parameters 



φ


{\displaystyle \varphi }
 is required, resulting in a posterior probability","[' What depends in turn on other parameters that are not mentioned in the likelihood?', ' What must be replaced by a likelihood p(<unk>theta <unk>mid <unk>varphi )?', ' What does a prior p ( <unk> ) <unk>displaystyle p(<unk>varphi )<unk> result in?']","['the prior', 'p', 'posterior probability']"
1035,bayesian network,Statistical introduction,"The process may be repeated; for example, the parameters 



φ


{\displaystyle \varphi }
 may depend in turn on additional parameters 



ψ




{\displaystyle \psi \,\!}
, which require their own prior. Eventually the process must terminate, with priors that do not depend on unmentioned parameters.
","The process may be repeated; for example, the parameters 



φ


{\displaystyle \varphi }
 may depend in turn on additional parameters 



ψ




{\displaystyle \psi \,\!} , which require their own prior.",[' The parameters <unk> <unk>displaystyle <unk>varphi <unk> may depend in turn on what?'],['additional parameters']
1036,bayesian network,Definitions and concepts,"Several equivalent definitions of a Bayesian network have been offered. For the following, let G = (V,E) be a directed acyclic graph (DAG) and let X = (Xv), v ∈ V be a set of random variables indexed by V.
","Several equivalent definitions of a Bayesian network have been offered. For the following, let G = (V,E) be a directed acyclic graph (DAG) and let X = (Xv), v ∈ V be a set of random variables indexed by V.","[' How many definitions of a Bayesian network have been offered?', ' Let G = (V,E) be a directed acyclic graph (DAG) and let X = (Xv) be what?']","['Several', 'a set of random variables indexed by V']"
1037,bayesian network,Inference complexity and approximation algorithms,"In 1990, while working at Stanford University on large bioinformatic applications, Cooper proved that exact inference in Bayesian networks is NP-hard. This result prompted research on approximation algorithms with the aim of developing a tractable approximation to probabilistic inference. In 1993, Paul Dagum and Michael Luby proved two surprising results on the complexity of approximation of probabilistic inference in Bayesian networks. First, they proved that no tractable deterministic algorithm can approximate probabilistic inference to within an absolute error ɛ < 1/2. Second, they proved that no tractable randomized algorithm can approximate probabilistic inference to within an absolute error ɛ < 1/2 with confidence probability greater than 1/2.
","In 1990, while working at Stanford University on large bioinformatic applications, Cooper proved that exact inference in Bayesian networks is NP-hard. This result prompted research on approximation algorithms with the aim of developing a tractable approximation to probabilistic inference.","[' In what year did Cooper prove that exact inference in Bayesian networks is NP-hard?', "" What was Cooper's goal?"", ' At what university did Cooper work on large bioinformatic applications?']","['1990', 'developing a tractable approximation to probabilistic inference', 'Stanford']"
1038,bayesian network,Inference complexity and approximation algorithms,"In practical terms, these complexity results suggested that while Bayesian networks were rich representations for AI and machine learning applications, their use in large real-world applications would need to be tempered by either topological structural constraints, such as naïve Bayes networks, or by restrictions on the conditional probabilities. The bounded variance algorithm developed by Dagum and Luby was the first provable fast approximation algorithm to efficiently approximate probabilistic inference in Bayesian networks with guarantees on the error approximation. This powerful algorithm required the minor restriction on the conditional probabilities of the Bayesian network to be bounded away from zero and one by 1/p(n) where p(n) was any polynomial on the number of nodes in the network n.
","In practical terms, these complexity results suggested that while Bayesian networks were rich representations for AI and machine learning applications, their use in large real-world applications would need to be tempered by either topological structural constraints, such as naïve Bayes networks, or by restrictions on the conditional probabilities. The bounded variance algorithm developed by Dagum and Luby was the first provable fast approximation algorithm to efficiently approximate probabilistic inference in Bayesian networks with guarantees on the error approximation.","[' What are rich representations for AI and machine learning applications?', ' What would need to be tempered by topological structural constraints, such as naive Bayes networks?', ' Who developed the bounded variance algorithm?', ' What was the first provable fast approximation algorithm to efficiently approximate probabilistic inference in Bayesian networks?']","['Bayesian networks', 'Bayesian networks', 'Dagum and Luby', 'bounded variance algorithm']"
1039,anomaly detection,Summary,"In data analysis, anomaly detection (also referred to as outlier detection) is generally understood to be the identification of rare items, events or observations which deviate significantly from the majority of the data. Such examples may arouse suspicions of being generated by a different mechanism, or appear inconsistent with the data.","In data analysis, anomaly detection (also referred to as outlier detection) is generally understood to be the identification of rare items, events or observations which deviate significantly from the majority of the data. Such examples may arouse suspicions of being generated by a different mechanism, or appear inconsistent with the data.","[' What is anomaly detection also referred to as?', ' What is the identification of rare items, events or observations that deviate significantly from the majority of the data?', ' What happens when a data is generated by a different mechanism or appears inconsistent with it?']","['outlier detection', 'anomaly detection', 'suspicions']"
1040,anomaly detection,Summary,"Typically the anomalous items will translate to some kind of problem such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are also referred to as outliers, novelties, noise, deviations and exceptions.","Typically the anomalous items will translate to some kind of problem such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are also referred to as outliers, novelties, noise, deviations and exceptions.","[' What are anomalous items referred to as?', ' What are outliers, novelties, noise, deviations and exceptions?']","['outliers', 'Anomalies']"
1041,anomaly detection,Summary,"In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts in activity. This pattern does not adhere to the common statistical definition of an outlier as a rare object, and many outlier detection methods (in particular unsupervised methods) will fail on such data, unless it has been aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro clusters formed by these patterns.","In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts in activity. This pattern does not adhere to the common statistical definition of an outlier as a rare object, and many outlier detection methods (in particular unsupervised methods) will fail on such data, unless it has been aggregated appropriately.","[' What are the interesting objects in abuse and network intrusion detection?', ' What are unexpected bursts in activity?', ' Unsupervised methods will fail on what kind of data?']","['unexpected bursts in activity', 'interesting objects', 'outlier detection']"
1042,anomaly detection,Summary,"Three broad categories of anomaly detection techniques exist. Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal by looking for instances that seem to fit least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as ""normal"" and ""abnormal"" and involves training a classifier (the key difference to many other statistical classification problems is the inherent unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set, and then test the likelihood of a test instance to be generated by the utilized model.
",Three broad categories of anomaly detection techniques exist. Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal by looking for instances that seem to fit least to the remainder of the data set.,"[' What are the three broad categories of anomaly detection techniques?', ' What do unsupervised anomaly detector techniques detect in an unlabeled test data set?', ' How do they look for instances that seem least to the rest of the data?', ' What is the least fitting to the rest of the data set?']","['Unsupervised anomaly detection techniques', 'anomalies', 'Unsupervised anomaly detection techniques', 'instances']"
1043,anomaly detection,Applications,"Anomaly detection is applicable in a very large number and variety of domains, and is an important subarea of unsupervised machine learning. As such it has applications in cyber-security intrusion detection, fraud detection, fault detection, system health monitoring, event detection in sensor networks, detecting ecosystem disturbances, defect detection in images using machine vision, medical diagnosis and law enforcement.","Anomaly detection is applicable in a very large number and variety of domains, and is an important subarea of unsupervised machine learning. As such it has applications in cyber-security intrusion detection, fraud detection, fault detection, system health monitoring, event detection in sensor networks, detecting ecosystem disturbances, defect detection in images using machine vision, medical diagnosis and law enforcement.","[' What is an important subarea of unsupervised machine learning?', ' Anomaly detection has applications in what areas?', ' What does machine vision do to detect defects in images?', ' What is the use of machine vision?']","['Anomaly detection', 'cyber-security intrusion detection, fraud detection, fault detection, system health monitoring, event detection in sensor networks, detecting ecosystem disturbances, defect detection in images using machine vision, medical diagnosis and law enforcement', 'detecting ecosystem disturbances', 'defect detection in images']"
1044,anomaly detection,Applications,"It is often used in preprocessing to remove anomalous data from the dataset. This is done for a number of reasons. Statistics of data such as the mean and standard deviation are more accurate after the removal of anomalies, and the visualisation of data can also be improved. In supervised learning, removing the anomalous data from the dataset often results in a statistically significant increase in accuracy. Anomalies are also often the most important observations in the data to be found such as in intrusion detection or detecting abnormalities in medical images.
",It is often used in preprocessing to remove anomalous data from the dataset. This is done for a number of reasons.,[' What is often used in preprocessing to remove anomalous data from the dataset?'],['It is often used in preprocessing to remove anomalous data from the dataset.']
1045,anomaly detection,Popular techniques,"Several anomaly detection techniques have been proposed in literature. Some of the popular techniques are:
",Several anomaly detection techniques have been proposed in literature. Some of the popular techniques are:,[' How many anomaly detection techniques have been proposed in the literature?'],['Several']
1046,anomaly detection,Application to data security,"Anomaly detection was proposed for intrusion detection systems (IDS) by Dorothy Denning in 1986. Anomaly detection for IDS is normally accomplished with thresholds and statistics, but can also be done with soft computing, and inductive learning. Types of statistics proposed by 1999 included profiles of users, workstations, networks, remote hosts, groups of users, and programs based on frequencies, means, variances, covariances, and standard deviations.  The counterpart of anomaly detection in intrusion detection is misuse detection.
","Anomaly detection was proposed for intrusion detection systems (IDS) by Dorothy Denning in 1986. Anomaly detection for IDS is normally accomplished with thresholds and statistics, but can also be done with soft computing, and inductive learning.","[' Who proposed anomaly detection for intrusion detection systems?', ' What is normally accomplished with thresholds and statistics?']","['Dorothy Denning', 'Anomaly detection']"
1047,anomaly detection,In data pre-processing,"In supervised learning, anomaly detection is often an important step in data pre-processing to provide the learning algorithm a proper dataset to learn on. This is also known as Data cleansing.  After detecting anomalous samples classifiers remove them, however, at times corrupted data can still provide useful samples for learning. A common method for finding appropriate samples to use is identifying Noisy data. One approach to find noisy values is to create a probabilistic model from data using models of uncorrupted data and corrupted data.","In supervised learning, anomaly detection is often an important step in data pre-processing to provide the learning algorithm a proper dataset to learn on. This is also known as Data cleansing.","[' What is an important step in data pre-processing?', ' What is Data cleansing also known as?']","['anomaly detection', 'anomaly detection']"
1048,anomaly detection,In data pre-processing,"Below is an example of the Iris flower data set with an anomaly added. With an anomaly included, classification algorithm may have difficulties properly finding patterns, or run into errors. 
","Below is an example of the Iris flower data set with an anomaly added. With an anomaly included, classification algorithm may have difficulties properly finding patterns, or run into errors.","[' What is an example of the Iris flower data set with an anomaly added?', ' What may have difficulties properly finding patterns or run into errors?']","['Below', 'classification algorithm']"
1049,anomaly detection,In data pre-processing,"In data mining, high-dimensional data will also propose high computing challenges with intensely large sets of data. By removing numerous samples that can find itself irrelevant to a classifier or detection algorithm, runtime can be significantly reduced on even the largest sets of data.
","In data mining, high-dimensional data will also propose high computing challenges with intensely large sets of data. By removing numerous samples that can find itself irrelevant to a classifier or detection algorithm, runtime can be significantly reduced on even the largest sets of data.","[' High-dimensional data will also propose high computing challenges with intense large sets of what?', ' By removing numerous samples that can find itself irrelevant to a classifier or detection algorithm, runtime can be significantly reduced on even the largest sets of data?']","['data mining', 'data mining']"
1050,information theory,Summary,"Information theory is the scientific study of the quantification, storage, and communication of digital information. The field was fundamentally established by the works of Harry Nyquist and Ralph Hartley, in the 1920s, and Claude Shannon in the 1940s.: vii  The field is at the intersection of probability theory, statistics, computer science, statistical mechanics, information engineering, and electrical engineering.
","Information theory is the scientific study of the quantification, storage, and communication of digital information. The field was fundamentally established by the works of Harry Nyquist and Ralph Hartley, in the 1920s, and Claude Shannon in the 1940s.","[' What is the scientific study of the quantification, storage, and communication of digital information?', ' What was the field of information theory founded by?', ' Who established the field?', ' When was information theory first established?']","['Information theory', 'Harry Nyquist and Ralph Hartley, in the 1920s, and Claude Shannon', 'Harry Nyquist and Ralph Hartley', '1920s']"
1051,information theory,Summary,"A key measure in information theory is entropy. Entropy quantifies the amount of uncertainty involved in the value of a random variable or the outcome of a random process. For example, identifying the outcome of a fair coin flip (with two equally likely outcomes) provides less information (lower entropy) than specifying the outcome from a roll of a dice (with six equally likely outcomes). Some other important measures in information theory are mutual information, channel capacity, error exponents, and relative entropy. Important sub-fields of information theory include source coding, algorithmic complexity theory, algorithmic information theory and information-theoretic security.
",A key measure in information theory is entropy. Entropy quantifies the amount of uncertainty involved in the value of a random variable or the outcome of a random process.,"[' What is a key measure in information theory?', ' Entropy quantifies the amount of uncertainty involved in the value of a random variable or the outcome of what process?']","['entropy', 'random process']"
1052,information theory,Summary,"Applications of fundamental topics of information theory include source coding/data compression (e.g. for ZIP files), and channel coding/error detection and correction (e.g. for DSL). Its impact has been crucial to the success of the Voyager missions to deep space, the invention of the compact disc, the feasibility of mobile phones and the development of the Internet. The theory has also found applications in other areas, including statistical inference, cryptography, neurobiology, perception, linguistics, the evolution and function of molecular codes (bioinformatics), thermal physics, molecular dynamics, quantum computing, black holes, information retrieval, intelligence gathering, plagiarism detection, pattern recognition, anomaly detection and even art creation.
","Applications of fundamental topics of information theory include source coding/data compression (e.g. for ZIP files), and channel coding/error detection and correction (e.g.","[' What is one of the applications of information theory?', ' What is another application?', ' Where is source coding/data compression used?']","['source coding/data compression (e.g. for ZIP files), and channel coding/error detection and correction', 'channel coding/error detection and correction', 'ZIP files']"
1053,information theory,Overview,"Information theory studies the transmission, processing, extraction, and utilization of information. Abstractly, information can be thought of as the resolution of uncertainty. In the case of communication of information over a noisy channel, this abstract concept was formalized in 1948 by Claude Shannon in a paper entitled A Mathematical Theory of Communication, in which information is thought of as a set of possible messages, and the goal is to send these messages over a noisy channel, and to have the receiver reconstruct the message with low probability of error, in spite of the channel noise. Shannon's main result, the noisy-channel coding theorem showed that, in the limit of many channel uses, the rate of information that is asymptotically achievable is equal to the channel capacity, a quantity dependent merely on the statistics of the channel over which the messages are sent.","Information theory studies the transmission, processing, extraction, and utilization of information. Abstractly, information can be thought of as the resolution of uncertainty.","[' Information theory studies the transmission, processing, extraction, and utilization of what?', ' What can be thought of as the resolution of uncertainty?']","['information', 'information']"
1054,information theory,Overview,"Coding theory is concerned with finding explicit methods, called codes, for increasing the efficiency and reducing the error rate of data communication over noisy channels to near the channel capacity. These codes can be roughly subdivided into data compression (source coding) and error-correction (channel coding) techniques. In the latter case, it took many years to find the methods Shannon's work proved were possible.
","Coding theory is concerned with finding explicit methods, called codes, for increasing the efficiency and reducing the error rate of data communication over noisy channels to near the channel capacity. These codes can be roughly subdivided into data compression (source coding) and error-correction (channel coding) techniques.","[' Coding theory is concerned with finding explicit methods for increasing the efficiency and reducing the error rate of data communication over what?', ' Codes can be roughly subdivided into what two types of techniques?']","['noisy channels', 'data compression (source coding) and error-correction (channel coding)']"
1055,information theory,Overview,"A third class of information theory codes are cryptographic algorithms (both codes and ciphers). Concepts, methods and results from coding theory and information theory are widely used in cryptography and cryptanalysis. See the article ban (unit) for a historical application.
","A third class of information theory codes are cryptographic algorithms (both codes and ciphers). Concepts, methods and results from coding theory and information theory are widely used in cryptography and cryptanalysis.","[' What is a third class of information theory codes?', ' What are cryptographic algorithms?', ' Concepts, methods and results from coding theory and information theory are widely used in what?']","['cryptographic algorithms', 'both codes and ciphers', 'cryptography and cryptanalysis']"
1056,information theory,Historical background,"Prior to this paper, limited information-theoretic ideas had been developed at Bell Labs, all implicitly assuming events of equal probability.  Harry Nyquist's 1924 paper, Certain Factors Affecting Telegraph Speed, contains a theoretical section quantifying ""intelligence"" and the ""line speed"" at which it can be transmitted by a communication system, giving the relation W = K log m (recalling Boltzmann's constant), where W is the speed of transmission of intelligence, m is the number of different voltage levels to choose from at each time step, and K is a constant.  Ralph Hartley's 1928 paper, Transmission of Information, uses the word information as a measurable quantity, reflecting the receiver's ability to distinguish one sequence of symbols from any other, thus quantifying information as H = log Sn = n log S, where S was the number of possible symbols, and n the number of symbols in a transmission. The unit of information was therefore the decimal digit, which since has sometimes been called the hartley in his honor as a unit or scale or measure of information. Alan Turing in 1940 used similar ideas as part of the statistical analysis of the breaking of the German second world war Enigma ciphers.
","Prior to this paper, limited information-theoretic ideas had been developed at Bell Labs, all implicitly assuming events of equal probability. Harry Nyquist's 1924 paper, Certain Factors Affecting Telegraph Speed, contains a theoretical section quantifying ""intelligence"" and the ""line speed"" at which it can be transmitted by a communication system, giving the relation W = K log m (recalling Boltzmann's constant), where W is the speed of transmission of intelligence, m is the number of different voltage levels to choose from at each time step, and K is a constant.","[' Who wrote Certain Factors Affecting Telegraph Speed?', ' What was the name of the 1924 paper by Harry Nyquist?', ' Where had limited information-theoretic ideas been developed before this paper?', ' What is the relation W = K log m?', ' W is the speed of transmission of what?', ' m is the number of different voltage levels to choose from at each time step?']","['Harry Nyquist', 'Certain Factors Affecting Telegraph Speed', 'Bell Labs', ""Boltzmann's constant"", 'intelligence', 'm']"
1057,information theory,Historical background,"Much of the mathematics behind information theory with events of different probabilities were developed for the field of thermodynamics by Ludwig Boltzmann and J. Willard Gibbs.  Connections between information-theoretic entropy and thermodynamic entropy, including the important contributions by Rolf Landauer in the 1960s, are explored in Entropy in thermodynamics and information theory.
","Much of the mathematics behind information theory with events of different probabilities were developed for the field of thermodynamics by Ludwig Boltzmann and J. Willard Gibbs. Connections between information-theoretic entropy and thermodynamic entropy, including the important contributions by Rolf Landauer in the 1960s, are explored in Entropy in thermodynamics and information theory.","[' Who developed much of the mathematics behind information theory with events of different probabilities for the field of thermodynamics?', ' What are the connections between information-theoretic entropy and thermodynamic enthropy explored in?', ' In what decade was Entropy in thermodynamics and information theory explored?']","['Ludwig Boltzmann and J. Willard Gibbs', 'Entropy in thermodynamics and information theory', '1960s']"
1058,information theory,Quantities of information,"Information theory is based on probability theory and statistics.  Information theory often concerns itself with measures of information of the distributions associated with random variables. Important quantities of information are entropy, a measure of information in a single random variable, and mutual information, a measure of information in common between two random variables.  The former quantity is a property of the probability distribution of a random variable and gives a limit on the rate at which data generated by independent samples with the given distribution can be reliably compressed. The latter is a property of the joint distribution of two random variables, and is the maximum rate of reliable communication across a noisy channel in the limit of long block lengths, when the channel statistics are determined by the joint distribution.
",Information theory is based on probability theory and statistics. Information theory often concerns itself with measures of information of the distributions associated with random variables.,"[' Information theory is based on what?', ' Information theory often concerns itself with measures of information of the distributions associated with random variables?']","['probability theory and statistics', 'Information theory']"
1059,information theory,Quantities of information,"The choice of logarithmic base in the following formulae determines the unit of information entropy that is used.  A common unit of information is the bit, based on the binary logarithm. Other units include the nat, which is based on the natural logarithm, and the decimal digit, which is based on the common logarithm.
","The choice of logarithmic base in the following formulae determines the unit of information entropy that is used. A common unit of information is the bit, based on the binary logarithm.","[' What determines the unit of information entropy that is used?', ' What is based on the binary logarithm?']","['logarithmic base', 'bit']"
1060,information theory,Quantities of information,"In what follows, an expression of the form p log p is considered by convention to be equal to zero whenever p = 0.  This is justified because 




lim

p
→
0
+


p
log
⁡
p
=
0


{\displaystyle \lim _{p\rightarrow 0+}p\log p=0}
 for any logarithmic base.
","In what follows, an expression of the form p log p is considered by convention to be equal to zero whenever p = 0. This is justified because 




lim

p
→
0
+


p
log
⁡
p
=
0


{\displaystyle \lim _{p\rightarrow 0+}p\log p=0}
 for any logarithmic base.",[' What is an expression of the form p log p considered by convention to be equal to whenever p = 0?'],['p\\log p=0}']
1061,information theory,Coding theory,"Coding theory is one of the most important and direct applications of information theory. It can be subdivided into source coding theory and channel coding theory. Using a statistical description for data, information theory quantifies the number of bits needed to describe the data, which is the information entropy of the source.
",Coding theory is one of the most important and direct applications of information theory. It can be subdivided into source coding theory and channel coding theory.,"[' What is one of the most important and direct applications of information theory?', ' Coding theory can be subdivided into source coding theory and what else?']","['Coding theory', 'channel coding theory']"
1062,information theory,Coding theory,"This division of coding theory into compression and transmission is justified by the information transmission theorems, or source–channel separation theorems that justify the use of bits as the universal currency for information in many contexts. However, these theorems only hold in the situation where one transmitting user wishes to communicate to one receiving user. In scenarios with more than one transmitter (the multiple-access channel), more than one receiver (the broadcast channel) or intermediary ""helpers"" (the relay channel), or more general networks, compression followed by transmission may no longer be optimal. Network information theory refers to these multi-agent communication models.
","This division of coding theory into compression and transmission is justified by the information transmission theorems, or source–channel separation theorems that justify the use of bits as the universal currency for information in many contexts. However, these theorems only hold in the situation where one transmitting user wishes to communicate to one receiving user.","[' What is the division of coding theory into compression and transmission justified by?', ' What do the information transmission theorems justify the use of?', ' What is the situation where one transmitting user wishes to communicate to one receiving user?']","['information transmission theorems', 'bits', 'information transmission theorems']"
1063,performance,Summary,"A performance is an act of staging or presenting a play, concert, or other form of entertainment. It is also defined as the action or process of carrying out or accomplishing an action, task, or function. ","A performance is an act of staging or presenting a play, concert, or other form of entertainment. It is also defined as the action or process of carrying out or accomplishing an action, task, or function.","[' What is an act of staging or presenting a play, concert, or other form of entertainment?', ' What is a performance also defined as?']","['performance', 'the action or process of carrying out or accomplishing an action, task, or function']"
1064,performance,Management science,"In the work place, job performance is the hypothesized conception or requirements of a role. There are two types of job performances: contextual and task. Task performance is dependent on cognitive ability, while contextual performance is dependent on personality. Task performance relates to behavioral roles that are recognized in job descriptions and remuneration systems. They are directly related to organizational performance, whereas contextual performances are value-based and add additional behavioral roles that are not recognized in job descriptions and covered by compensation; these are extra roles that are indirectly related to organizational performance. Citizenship performance, like contextual performance, relates to a set of individual activity/contribution (prosocial organizational behavior) that supports  organizational culture.","In the work place, job performance is the hypothesized conception or requirements of a role. There are two types of job performances: contextual and task.","[' What is the term for the hypothesized conception or requirements of a role in the workplace?', ' What are the two types of job performances?']","['job performance', 'contextual and task']"
1065,performance,Arts,"In performing arts, a performance generally comprises an event in which a performer, or group of performers, present one or more works of art to an audience. In instrumental music and drama, a performance is typically described as a ""play"". Typically, the performers participate in rehearsals beforehand to practice the work.
","In performing arts, a performance generally comprises an event in which a performer, or group of performers, present one or more works of art to an audience. In instrumental music and drama, a performance is typically described as a ""play"".","[' In performing arts, what is an event in which a performer or group of performers present one or more works of art to an audience?', ' In instrumental music and drama, a performance is typically described as what?']","['a performance', 'a ""play"".']"
1066,performance,Arts,"An effective performance is determined by the achieved skills and competency of the performer, also known as the level of skill and knowledge. In 1994, Spencer and McClelland defined competency as ""a combination of motives, traits, self-concepts, attitudes, cognitive behavior skills (content knowledge) that helps a performer to differentiate themselves as superior from the average performer"". A performance also describes the way in which an actor performs. In a solo capacity, it may also refer to a mime artist, comedian, conjurer, magician, or other entertainer.
","An effective performance is determined by the achieved skills and competency of the performer, also known as the level of skill and knowledge. In 1994, Spencer and McClelland defined competency as ""a combination of motives, traits, self-concepts, attitudes, cognitive behavior skills (content knowledge) that helps a performer to differentiate themselves as superior from the average performer"".","[' What determines an effective performance?', ' What is the level of skill and knowledge also known as?', ' When did Spencer and McClelland define competency?', ' What is a skill that helps a performer to differentiate themselves from the average performer?']","['achieved skills and competency of the performer', 'competency of the performer', '1994', 'competency']"
1067,performance,Aspects of Performance Art,"Another aspect of performance that grew in popularity in the early 20th century is Performance art. The origins of Performance art started with Dada and Russian constructivism groups, focusing on avant-garde poetry readings and live paintings meant to be viewed by an audience. It can be scripted or completely improvised and includes audience participation if desired.","Another aspect of performance that grew in popularity in the early 20th century is Performance art. The origins of Performance art started with Dada and Russian constructivism groups, focusing on avant-garde poetry readings and live paintings meant to be viewed by an audience.","[' What is another aspect of performance that grew in popularity in the early 20th century?', ' Who started the origins of Performance art?', ' What did Dada and Russian constructivism groups focus on?']","['Performance art', 'Dada and Russian constructivism groups', 'avant-garde poetry readings and live paintings']"
1068,performance,Aspects of Performance Art,"The emergence of Abstract expressionism in the 1950s with Jackson Pollock and Willem de Kooning gave way to Action painting, a technique that emphasized the dynamic movements of artists as they splattered paint and other media on canvas or glass. For these artists, the motion of putting paint on canvas was just as valuable as the finished painting, and so it was common for artists to document their work in film; such as the short film Jackson Pollock 51(1951), featuring Pollock dripping paint onto a massive canvas on his studio floor. Situationists in France, led by Guy Debord, married avant-garde art with revolutionary politics to incite everyday acts of anarchy. The ""Naked City Map"" (1957) fragments the 19 sections of Paris, featuring the technique of Détournement and abstraction of the traditional environment, deconstructing the geometry and order of a typical city map.","The emergence of Abstract expressionism in the 1950s with Jackson Pollock and Willem de Kooning gave way to Action painting, a technique that emphasized the dynamic movements of artists as they splattered paint and other media on canvas or glass. For these artists, the motion of putting paint on canvas was just as valuable as the finished painting, and so it was common for artists to document their work in film; such as the short film Jackson Pollock 51(1951), featuring Pollock dripping paint onto a massive canvas on his studio floor.","[' In what decade did Abstract expressionism emerge?', ' What was the name of the technique that emphasized the dynamic movements of artists?', "" What was Jackson Pollock's short film?"", ' What was the name of the short film released in 1951?', ' Pollock dripping paint on a canvas on what floor?', "" Pollock's studio floor was covered in what kind of paint?""]","['1950s', 'Action painting', 'Jackson Pollock 51', 'Jackson Pollock 51', 'studio', 'massive canvas']"
1069,performance,Aspects of Performance Art,"At the New School for Social Research in New York, John Cage and Allan Kaprow became involved in developing Happening performance art. These carefully scripted one-off events incorporated the audience into acts of chaos and spontaneity. These happenings challenged traditional art conventions and encouraged artists to carefully consider the role of an audience. In Japan, the 1954 Gutai group led by Yoshihara Jiro, Kanayma Akira, Murakami Saburo, Kazuo Shiraga, and Shimamoto Shozo made the materials of art-making come to life with body movement and blurring the line between art and theater. Kazuo Shiraga's Challenging Mud (1955) is a performance of the artist rolling and moving in mud, using their body as the art-making tool, and emphasizing the temporary nature of performance art. 
","At the New School for Social Research in New York, John Cage and Allan Kaprow became involved in developing Happening performance art. These carefully scripted one-off events incorporated the audience into acts of chaos and spontaneity.","[' John Cage and Allan Kaprow developed Happening performance art in what city?', ' What type of performance art was developed at New School for Social Research?']","['New York', 'Happening']"
1070,performance,Aspects of Performance Art,"Valie Export, an Austrian artist born Waltraud Lehner, performed ""Tap and Touch Cinema"" in 1968. She walked around the streets in Vienna during a film festival wearing a styrofoam box with a curtain over her chest. Bystanders were asked to put their hands inside the box and touch her bare chest. This commentary on women sexualization in film focused on the sense of touch rather than sight. Adrian Piper and her performance Catalysis III (1970) featured the artist walking down New York City streets with her outfit painted white and a sign across her chest that said ""wet paint."" She was interested in the invisible social and racial dynamics in America and was determined to encourage civic-mindedness and interruption of the system. Carolee Schneemann, American artist, performed Interior Scroll in 1975, where she unrolls Super-8 film ""Kitsch's Last Meal"" from her genitals. This nude performance contributes to a discourse on femininity, sexualization, and film.
","Valie Export, an Austrian artist born Waltraud Lehner, performed ""Tap and Touch Cinema"" in 1968. She walked around the streets in Vienna during a film festival wearing a styrofoam box with a curtain over her chest.","[' What Austrian artist performed ""Tap and Touch Cinema"" in 1968?', "" What was Valie Export's birth name?"", ' When did Valie export perform her song?', ' Where did she walk during a film festival wearing a styrofoam box?']","['Valie Export', 'Waltraud Lehner', '1968', 'around the streets in Vienna']"
1071,performance,Performance state,"Other related factors are: motivation to achieve success or avoid failure, task relevant attention, positive self-talk, and cognitive regulation to achieve automaticity. Performance is also dependent on adaptation of eight areas: Handling crisis, managing stress, creative problem solving, knowing necessary functional tools and skills, agile management of complex processes, interpersonal adaptability, cultural adaptability, and physical fitness. Performance is not always a result of practice, but rather about honing in a skill. Over practicing itself can result in failure due to ego depletion.","Other related factors are: motivation to achieve success or avoid failure, task relevant attention, positive self-talk, and cognitive regulation to achieve automaticity. Performance is also dependent on adaptation of eight areas: Handling crisis, managing stress, creative problem solving, knowing necessary functional tools and skills, agile management of complex processes, interpersonal adaptability, cultural adaptability, and physical fitness.","[' How many areas does performance depend on?', ' What is one of the eight areas that performance depends on adaptation?', ' How many factors are related to performance?', ' What are functional tools and skills, agile management of complex processes, interpersonal adaptability, cultural adaptability and physical fitness?']","['eight', 'Handling crisis', 'eight', 'knowing necessary']"
1072,mutual information,Summary,"In probability theory and information theory, the mutual information (MI) of two random variables is a measure of the mutual dependence between the two variables. More specifically, it quantifies the ""amount of information"" (in units such as shannons (bits), nats or hartleys) obtained about one random variable by observing the other random variable. The concept of mutual information is intimately linked to that of entropy of a random variable, a fundamental notion in information theory that quantifies the expected ""amount of information"" held in a random variable.
","In probability theory and information theory, the mutual information (MI) of two random variables is a measure of the mutual dependence between the two variables. More specifically, it quantifies the ""amount of information"" (in units such as shannons (bits), nats or hartleys) obtained about one random variable by observing the other random variable.","[' What is a measure of the mutual dependence between two variables in probability theory and information theory?', ' What is the mutual information of two random variables?', ' How does MI quantify the amount of information obtained about one random variable?', ' What is another term for a random variable obtained by observing another random variable?']","['the mutual information', 'mutual dependence between the two variables', 'by observing the other random variable', 'amount of information']"
1073,mutual information,Summary,"Not limited to real-valued random variables and linear dependence like the correlation coefficient, MI is more general and determines how different the joint distribution of the pair 



(
X
,
Y
)


{\displaystyle (X,Y)}
 is from the product of the marginal distributions of 



X


{\displaystyle X}
 and 



Y


{\displaystyle Y}
. MI is the expected value of the pointwise mutual information (PMI).
","Not limited to real-valued random variables and linear dependence like the correlation coefficient, MI is more general and determines how different the joint distribution of the pair 



(
X
,
Y
)


{\displaystyle (X,Y)}
 is from the product of the marginal distributions of 



X


{\displaystyle X}
 and 



Y


{\displaystyle Y}
. MI is the expected value of the pointwise mutual information (PMI).","[' What determines how different the joint distribution of the pair is from the product of the marginal distributions of X, Y and Y?', ' MI is the expected value of what?', ' What is the expected value of the pointwise mutual information (PMI)?']","['MI', 'pointwise mutual information', 'MI']"
1074,mutual information,Summary,"The quantity was defined and analyzed by Claude Shannon in his landmark paper ""A Mathematical Theory of Communication"", although he did not call it ""mutual information"". This term was coined later by Robert Fano. Mutual Information is also known as information gain.
","The quantity was defined and analyzed by Claude Shannon in his landmark paper ""A Mathematical Theory of Communication"", although he did not call it ""mutual information"". This term was coined later by Robert Fano.","[' Who defined and analyzed the quantity in his landmark paper?', ' What did Shannon not call the quantity?', ' Who coined the term ""mutual information""?']","['Claude Shannon', 'mutual information', 'Robert Fano']"
1075,mutual information,Definition,"Let 



(
X
,
Y
)


{\displaystyle (X,Y)}
 be a pair of random variables with values over the space 





X


×


Y




{\displaystyle {\mathcal {X}}\times {\mathcal {Y}}}
. If their joint distribution is 




P

(
X
,
Y
)




{\displaystyle P_{(X,Y)}}
 and the marginal distributions are 




P

X




{\displaystyle P_{X}}
 and 




P

Y




{\displaystyle P_{Y}}
, the mutual information is defined as
","Let 



(
X
,
Y
)


{\displaystyle (X,Y)}
 be a pair of random variables with values over the space 





X


×


Y




{\displaystyle {\mathcal {X}}\times {\mathcal {Y}}}
. If their joint distribution is 




P

(
X
,
Y
)




{\displaystyle P_{(X,Y)}}
 and the marginal distributions are 




P

X




{\displaystyle P_{X}}
 and 




P

Y




{\displaystyle P_{Y}}
, the mutual information is defined as","[' What is the joint distribution of P ( X, Y ) <unk>displaystyle P_<unk>(X,Y)?', ' What are the marginal distributions of P X and P Y?']","['P\n\n(\nX\n,\nY\n)', 'P\n\nX\n\n\n\n\n{\\displaystyle P_{X}}\n and \n\n\n\n\nP\n\nY']"
1076,mutual information,Definition,"Notice, as per property of the Kullback–Leibler divergence, that 



I
(
X
;
Y
)


{\displaystyle I(X;Y)}
 is equal to zero precisely when the joint distribution coincides with the product of the marginals, i.e. when 



X


{\displaystyle X}
 and 



Y


{\displaystyle Y}
 are independent (and hence observing 



Y


{\displaystyle Y}
 tells you nothing about 



X


{\displaystyle X}
). In general 



I
(
X
;
Y
)


{\displaystyle I(X;Y)}
 is non-negative, it is a measure of the price for encoding 



(
X
,
Y
)


{\displaystyle (X,Y)}
 as a pair of independent random variables, when in reality they are not.
","Notice, as per property of the Kullback–Leibler divergence, that 



I
(
X
;
Y
)


{\displaystyle I(X;Y)}
 is equal to zero precisely when the joint distribution coincides with the product of the marginals, i.e. when 



X


{\displaystyle X}
 and 



Y


{\displaystyle Y}
 are independent (and hence observing 



Y


{\displaystyle Y}
 tells you nothing about 



X


{\displaystyle X}
).",[' What is equal to zero precisely when the joint distribution coincides with the product of the marginals?'],['I\n(\nX\n;\nY\n)\n\n\n{\\displaystyle I(X;Y)}']
1077,mutual information,Motivation,"Intuitively, mutual information measures the information that 



X


{\displaystyle X}
 and 



Y


{\displaystyle Y}
 share: It measures how much knowing one of these variables reduces uncertainty about the other. For example, if 



X


{\displaystyle X}
 and 



Y


{\displaystyle Y}
 are independent, then knowing 



X


{\displaystyle X}
 does not give any information about 



Y


{\displaystyle Y}
 and vice versa, so their mutual information is zero.  At the other extreme, if 



X


{\displaystyle X}
 is a deterministic function of 



Y


{\displaystyle Y}
 and 



Y


{\displaystyle Y}
 is a deterministic function of 



X


{\displaystyle X}
 then all information conveyed by 



X


{\displaystyle X}
 is shared with 



Y


{\displaystyle Y}
: knowing 



X


{\displaystyle X}
 determines the value of 



Y


{\displaystyle Y}
 and vice versa. As a result, in this case the mutual information is the same as the uncertainty contained in 



Y


{\displaystyle Y}
 (or 



X


{\displaystyle X}
) alone, namely the entropy of 



Y


{\displaystyle Y}
 (or 



X


{\displaystyle X}
). Moreover, this mutual information is the same as the entropy of 



X


{\displaystyle X}
 and as the entropy of 



Y


{\displaystyle Y}
. (A very special case of this is when 



X


{\displaystyle X}
 and 



Y


{\displaystyle Y}
 are the same random variable.)
","Intuitively, mutual information measures the information that 



X


{\displaystyle X}
 and 



Y


{\displaystyle Y}
 share: It measures how much knowing one of these variables reduces uncertainty about the other. For example, if 



X


{\displaystyle X}
 and 



Y


{\displaystyle Y}
 are independent, then knowing 



X


{\displaystyle X}
 does not give any information about 



Y


{\displaystyle Y}
 and vice versa, so their mutual information is zero.","[' What does mutual information measure?', ' What does knowing X <unk>displaystyle X<unk> reduce?', ' What does X<unk> not give information about?', ' What is the mutual information between X and Y?']","['how much knowing one of these variables reduces uncertainty about the other', 'uncertainty about the other. For example, if \n\n\n\nX\n\n\n{\\displaystyle X}\n and \n\n\n\nY\n\n\n{\\displaystyle Y}\n are independent, then knowing \n\n\n\nX\n\n\n{\\displaystyle X}\n does not give any information about \n\n\n\nY\n\n\n{\\displaystyle Y}\n and vice versa, so their mutual information is zero', 'Y', 'zero']"
1078,mutual information,Motivation,"Mutual information is a measure of the inherent dependence expressed in the joint distribution of 



X


{\displaystyle X}
 and 



Y


{\displaystyle Y}
 relative to the marginal distribution of 



X


{\displaystyle X}
 and 



Y


{\displaystyle Y}
 under the assumption of independence. Mutual information therefore measures dependence in the following sense: 



I
⁡
(
X
;
Y
)
=
0


{\displaystyle \operatorname {I} (X;Y)=0}
 if and only if 



X


{\displaystyle X}
 and 



Y


{\displaystyle Y}
 are independent random variables.  This is easy to see in one direction: if 



X


{\displaystyle X}
 and 



Y


{\displaystyle Y}
 are independent, then 




p

(
X
,
Y
)


(
x
,
y
)
=

p

X


(
x
)
⋅

p

Y


(
y
)


{\displaystyle p_{(X,Y)}(x,y)=p_{X}(x)\cdot p_{Y}(y)}
, and therefore:
","Mutual information is a measure of the inherent dependence expressed in the joint distribution of 



X


{\displaystyle X}
 and 



Y


{\displaystyle Y}
 relative to the marginal distribution of 



X


{\displaystyle X}
 and 



Y


{\displaystyle Y}
 under the assumption of independence. Mutual information therefore measures dependence in the following sense: 



I
⁡
(
X
;
Y
)
=
0


{\displaystyle \operatorname {I} (X;Y)=0}
 if and only if 



X


{\displaystyle X}
 and 



Y


{\displaystyle Y}
 are independent random variables.","[' What is a measure of the inherent dependence expressed in the joint distribution of X <unk>displaystyle X<unk> and Y <unk>disposition style Y<unk> relative to the marginal distribution?', ' Mutual information measures dependence in the following sense?', ' What is dependent in the following sense?', ' What are independent random variables?']","['Mutual information', 'X', 'I\n\u2061\n(\nX\n;\nY\n)\n=\n0\n\n\n{\\displaystyle \\operatorname {I} (X;Y)=0}\n if and only if \n\n\n\nX', 'X']"
1079,mutual information,Motivation,"Moreover, mutual information is nonnegative (i.e. 



I
⁡
(
X
;
Y
)
≥
0


{\displaystyle \operatorname {I} (X;Y)\geq 0}
 see below) and symmetric (i.e. 



I
⁡
(
X
;
Y
)
=
I
⁡
(
Y
;
X
)


{\displaystyle \operatorname {I} (X;Y)=\operatorname {I} (Y;X)}
 see below).
","Moreover, mutual information is nonnegative (i.e. I
⁡
(
X
;
Y
)
≥
0


{\displaystyle \operatorname {I} (X;Y)\geq 0}
 see below) and symmetric (i.e.","[' What is nonnegative?', ' What is symmetric?', ' How is mutual information?', ' I <unk> ( X ; Y ) <unk> 0?']","['mutual information', 'mutual information', 'nonnegative (i.e. I\n\u2061\n(\nX\n;\nY\n)\n≥\n0\n\n\n{\\displaystyle \\operatorname {I} (X;Y)\\geq 0}\n see below) and symmetric', 'I\n\u2061']"
1080,mutual information,Variations,"Several variations on mutual information have been proposed to suit various needs.  Among these are normalized variants and generalizations to more than two variables.
",Several variations on mutual information have been proposed to suit various needs. Among these are normalized variants and generalizations to more than two variables.,"[' What have been proposed to suit various needs?', ' What are normalized variants and generalizations to more than two variables?']","['variations on mutual information', 'mutual information']"
1081,mutual information,Applications,"In many applications, one wants to maximize mutual information (thus increasing dependencies), which is often equivalent to minimizing conditional entropy.  Examples include:
","In many applications, one wants to maximize mutual information (thus increasing dependencies), which is often equivalent to minimizing conditional entropy. Examples include:","[' What does one want to maximize in many applications?', ' What is often equivalent to minimizing conditional entropy?']","['mutual information', 'mutual information']"
1082,constraint satisfaction problem,Summary,"Constraint satisfaction problems (CSPs) are mathematical questions defined as a set of objects whose state must satisfy a number of constraints or limitations. CSPs represent the entities in a problem as a homogeneous collection of finite constraints over variables, which is solved by constraint satisfaction methods. CSPs are the subject of research in both artificial intelligence and operations research, since the regularity in their formulation provides a common basis to analyze and solve problems of many seemingly unrelated families. CSPs often exhibit high complexity, requiring a combination of heuristics and combinatorial search methods to be solved in a reasonable time. Constraint programming (CP) is the field of research that specifically focuses on tackling these kinds of problems. Additionally, Boolean satisfiability problem (SAT), the satisfiability modulo theories (SMT), mixed integer programming (MIP) and answer set programming (ASP) are all fields of research focusing on the resolution of particular forms of the constraint satisfaction problem.
","Constraint satisfaction problems (CSPs) are mathematical questions defined as a set of objects whose state must satisfy a number of constraints or limitations. CSPs represent the entities in a problem as a homogeneous collection of finite constraints over variables, which is solved by constraint satisfaction methods.","[' Constraint satisfaction problems are defined as a set of objects whose state must satisfy a number of constraints or limitations.', ' CSPs represent the entities in a problem as what?', ' What is solved by constraint satisfaction methods?']","['CSPs', 'a homogeneous collection of finite constraints over variables', 'homogeneous collection of finite constraints over variables']"
1083,constraint satisfaction problem,Summary,"These are often provided with tutorials of CP, ASP, Boolean SAT and SMT solvers. In the general case, constraint problems can be much harder, and may not be expressible in some of these simpler systems. ""Real life"" examples include automated planning, lexical disambiguation, musicology, product configuration and resource allocation.","These are often provided with tutorials of CP, ASP, Boolean SAT and SMT solvers. In the general case, constraint problems can be much harder, and may not be expressible in some of these simpler systems.","[' What are often provided with tutorials of CP, ASP, Boolean SAT and SMT solvers?', ' In the general case, what can be much harder?']","['constraint problems can be much harder, and may not be expressible in some of these simpler systems', 'constraint problems']"
1084,constraint satisfaction problem,Summary,"The existence of a solution to a CSP can be viewed as a decision problem. This can be decided by finding a solution, or failing to find a solution after exhaustive search (stochastic algorithms typically never reach an exhaustive conclusion, while directed searches often do, on sufficiently small problems). In some cases the CSP might be known to have solutions beforehand, through some other mathematical inference process.
","The existence of a solution to a CSP can be viewed as a decision problem. This can be decided by finding a solution, or failing to find a solution after exhaustive search (stochastic algorithms typically never reach an exhaustive conclusion, while directed searches often do, on sufficiently small problems).","[' What can be viewed as a decision problem?', ' How can the existence of a solution to a CSP be decided?']","['The existence of a solution to a CSP', 'by finding a solution']"
1085,constraint satisfaction problem,Formal definition,"Each variable 




X

i




{\displaystyle X_{i}}
 can take on the values in the nonempty domain 




D

i




{\displaystyle D_{i}}
.
Every constraint 




C

j


∈
C


{\displaystyle C_{j}\in C}
 is in turn a pair 



⟨

t

j


,

R

j


⟩


{\displaystyle \langle t_{j},R_{j}\rangle }
, where 




t

j


⊂
X


{\displaystyle t_{j}\subset X}
 is a subset of 



k


{\displaystyle k}
 variables and 




R

j




{\displaystyle R_{j}}
 is a 



k


{\displaystyle k}
-ary relation on the corresponding subset of domains 




D

j




{\displaystyle D_{j}}
. An evaluation of the variables is a function from a subset of variables to a particular set of values in the corresponding subset of domains. An evaluation 



v


{\displaystyle v}
 satisfies a constraint 



⟨

t

j


,

R

j


⟩


{\displaystyle \langle t_{j},R_{j}\rangle }
 if the values assigned to the variables 




t

j




{\displaystyle t_{j}}
 satisfies the relation 




R

j




{\displaystyle R_{j}}
.
","Each variable 




X

i




{\displaystyle X_{i}}
 can take on the values in the nonempty domain 




D

i




{\displaystyle D_{i}}
. Every constraint 




C

j


∈
C


{\displaystyle C_{j}\in C}
 is in turn a pair 



⟨

t

j


,

R

j


⟩


{\displaystyle \langle t_{j},R_{j}\rangle }
, where 




t

j


⊂
X


{\displaystyle t_{j}\subset X}
 is a subset of 



k


{\displaystyle k}
 variables and 




R

j




{\displaystyle R_{j}}
 is a 



k


{\displaystyle k}
-ary relation on the corresponding subset of domains 




D

j




{\displaystyle D_{j}}
.","[' What can each variable take on the values in the nonempty domain D i <unk>displaystyle D_<unk>i<unk>?', ' Every constraint is in turn a pair <unk> what?', ' What is a k <unk>displaystyle k<unk> -ary relation on the corresponding subset of domains?']","['k\n\n\n{\\displaystyle k}\n-ary relation', 'k', 'k\n\n\n{\\displaystyle k}\n-ary relation on the corresponding subset of domains \n\n\n\n\nD']"
1086,constraint satisfaction problem,Formal definition,"An evaluation is consistent if it does not violate any of the constraints. An evaluation is complete if it includes all variables. An evaluation is a solution if it is consistent and complete; such an evaluation is said to solve the constraint satisfaction problem.
",An evaluation is consistent if it does not violate any of the constraints. An evaluation is complete if it includes all variables.,"[' What is consistent if it does not violate any constraints?', ' What is complete if an evaluation includes all variables?']","['An evaluation', 'An evaluation']"
1087,constraint satisfaction problem,Solution,"Constraint satisfaction problems on finite domains are typically solved using a form of search. The most used techniques are variants of backtracking, constraint propagation, and local search. These techniques are also often combined, as in the VLNS method, and current research involves other technologies such as linear programming.","Constraint satisfaction problems on finite domains are typically solved using a form of search. The most used techniques are variants of backtracking, constraint propagation, and local search.","[' Constraint satisfaction problems on finite domains are typically solved using what?', ' What are the most used techniques?']","['search', 'variants of backtracking, constraint propagation, and local search']"
1088,constraint satisfaction problem,Solution,"Backtracking is a recursive algorithm. It maintains a partial assignment of the variables. Initially, all variables are unassigned. At each step, a variable is chosen, and all possible values are assigned to it in turn. For each value, the consistency of the partial assignment with the constraints is checked; in case of consistency, a recursive call is performed. When all values have been tried, the algorithm backtracks. In this basic backtracking algorithm, consistency is defined as the satisfaction of all constraints whose variables are all assigned. Several variants of backtracking exist. Backmarking improves the efficiency of checking consistency. Backjumping allows saving part of the search by backtracking ""more than one variable"" in some cases. Constraint learning infers and saves new constraints that can be later used to avoid part of the search. Look-ahead is also often used in backtracking to attempt to foresee the effects of choosing a variable or a value, thus sometimes determining in advance when a subproblem is satisfiable or unsatisfiable.
",Backtracking is a recursive algorithm. It maintains a partial assignment of the variables.,"[' What is the name of the recursive algorithm?', ' What does backtracking maintain?']","['Backtracking', 'a partial assignment of the variables']"
1089,constraint satisfaction problem,Solution,"Constraint propagation techniques are methods used to modify a constraint satisfaction problem. More precisely, they are methods that enforce a form of local consistency, which are conditions related to the consistency of a group of variables and/or constraints. Constraint propagation has various uses. First, it turns a problem into one that is equivalent but is usually simpler to solve. Second, it may prove satisfiability or unsatisfiability of problems. This is not guaranteed to happen in general; however, it always happens for some forms of constraint propagation and/or for certain kinds of problems. The most known and used forms of local consistency are arc consistency, hyper-arc consistency, and path consistency. The most popular constraint propagation method is the AC-3 algorithm, which enforces arc consistency.
","Constraint propagation techniques are methods used to modify a constraint satisfaction problem. More precisely, they are methods that enforce a form of local consistency, which are conditions related to the consistency of a group of variables and/or constraints.","[' Constraint propagation techniques are methods used to modify what?', ' What are the conditions related to the consistency of a group of variables and constraints?']","['a constraint satisfaction problem', 'local consistency']"
1090,constraint satisfaction problem,Solution,"Local search methods are incomplete satisfiability algorithms. They may find a solution of a problem, but they may fail even if the problem is satisfiable. They work by iteratively improving a complete assignment over the variables. At each step, a small number of variables are changed in value, with the overall aim of increasing the number of constraints satisfied by this assignment.  The min-conflicts algorithm is a local search algorithm specific for CSPs and is based on that principle. In practice, local search appears to work well when these changes are also affected by random choices. An integration of search with local search has been developed, leading to hybrid algorithms.
","Local search methods are incomplete satisfiability algorithms. They may find a solution of a problem, but they may fail even if the problem is satisfiable.",[' What are incomplete satisfiability algorithms?'],['Local search methods']
1091,constraint satisfaction problem,Variants,"The classic model of Constraint Satisfaction Problem defines a model of static, inflexible constraints. This rigid model is a shortcoming that makes it difficult to represent problems easily. Several modifications of the basic CSP definition have been proposed to adapt the model to a wide variety of problems.
","The classic model of Constraint Satisfaction Problem defines a model of static, inflexible constraints. This rigid model is a shortcoming that makes it difficult to represent problems easily.","[' What model defines a model of static, inflexible constraints?', ' What is a shortcoming of the rigid model?']","['Constraint Satisfaction Problem', 'makes it difficult to represent problems easily']"
1092,ubiquitous computing,Summary," Ubiquitous computing (or ""ubicomp"") is a concept in software engineering, hardware engineering and computer science where computing is made to appear anytime and everywhere. In contrast to desktop computing, ubiquitous computing can occur using any device, in any location, and in any format. A user interacts with the computer, which can exist in many different forms, including laptop computers, tablets, smart phones and terminals in everyday objects such as a refrigerator or a pair of glasses. The underlying technologies to support ubiquitous computing include Internet, advanced middleware, operating system, mobile code, sensors, microprocessors, new I/O and user interfaces, computer networks, mobile protocols, location and positioning, and new materials.
"," Ubiquitous computing (or ""ubicomp"") is a concept in software engineering, hardware engineering and computer science where computing is made to appear anytime and everywhere. In contrast to desktop computing, ubiquitous computing can occur using any device, in any location, and in any format.","[' What is another name for ""ubicomp""?', ' What is a concept in software engineering, hardware engineering and computer science?', ' Where can ubiquitous computing occur?']","['Ubiquitous computing', 'Ubiquitous computing', 'using any device, in any location, and in any format']"
1093,ubiquitous computing,Summary,"This paradigm is also described as pervasive computing, ambient intelligence, or ""everyware"". Each term emphasizes slightly different aspects. When primarily concerning the objects involved, it is also known as physical computing, the Internet of Things, haptic computing, and ""things that think"".
Rather than propose a single definition for ubiquitous computing and for these related terms, a taxonomy of properties for ubiquitous computing has been proposed, from which different kinds or flavors of ubiquitous systems and applications can be described.","This paradigm is also described as pervasive computing, ambient intelligence, or ""everyware"". Each term emphasizes slightly different aspects.","[' What is a term for pervasive computing?', ' What is another term for ambient intelligence?']","['everyware', 'everyware']"
1094,ubiquitous computing,Core concepts,"Ubiquitous computing is the concept of using small internet connected and inexpensive computers to help with everyday functions in an automated fashion.
For example, a domestic ubiquitous computing environment might interconnect lighting and environmental controls with personal biometric monitors woven into clothing so that illumination and heating conditions in a room might be modulated, continuously and imperceptibly. Another common scenario posits refrigerators ""aware"" of their suitably tagged contents, able to both plan a variety of menus from the food actually on hand, and warn users of stale or spoiled food.","Ubiquitous computing is the concept of using small internet connected and inexpensive computers to help with everyday functions in an automated fashion. For example, a domestic ubiquitous computing environment might interconnect lighting and environmental controls with personal biometric monitors woven into clothing so that illumination and heating conditions in a room might be modulated, continuously and imperceptibly.","[' What is the concept of using small internet connected and inexpensive computers to help with everyday functions in an automated fashion?', ' What would a domestic ubiquitous computing environment interconnect lighting and environmental controls with?', ' What kind of clothing could be put into a room to modulate the lighting and heating conditions?', ' How would a person perceive the lighting conditions in their room?']","['Ubiquitous computing', 'personal biometric monitors woven into clothing', 'personal biometric monitors', 'continuously and imperceptibly']"
1095,ubiquitous computing,Core concepts,"Ubiquitous computing presents challenges across computer science: in systems design and engineering, in systems modelling, and in user interface design. Contemporary human-computer interaction models, whether command-line, menu-driven, or GUI-based, are inappropriate and inadequate to the ubiquitous case. This suggests that the ""natural"" interaction paradigm appropriate to a fully robust ubiquitous computing has yet to emerge – although there is also recognition in the field that in many ways we are already living in a ubicomp world (see also the main article on natural user interfaces). Contemporary devices that lend some support to this latter idea include mobile phones, digital audio players, radio-frequency identification tags, GPS, and interactive whiteboards.
","Ubiquitous computing presents challenges across computer science: in systems design and engineering, in systems modelling, and in user interface design. Contemporary human-computer interaction models, whether command-line, menu-driven, or GUI-based, are inappropriate and inadequate to the ubiquitous case.","[' What presents challenges across computer science?', ' What is inappropriate and inadequate to the ubiquitous case?']","['Ubiquitous computing', 'Contemporary human-computer interaction models']"
1096,ubiquitous computing,Core concepts,Ubiquitous computing devices proposed by Mark Weiser are all based around flat devices of different sizes with a visual display. Expanding beyond those concepts there is a large array of other ubiquitous computing devices that could exist. Some of the additional forms that have been conceptualized are:,Ubiquitous computing devices proposed by Mark Weiser are all based around flat devices of different sizes with a visual display. Expanding beyond those concepts there is a large array of other ubiquitous computing devices that could exist.,"[' Mark Weiser proposed what type of computing devices?', ' What are all of the proposed ubiquitous computing devices based around?']","['Ubiquitous', 'flat devices of different sizes with a visual display']"
1097,ubiquitous computing,Core concepts,"In Manuel Castells' book The Rise of the Network Society, Castells puts forth the concept that there is going to be a continuous evolution of computing devices. He states we will progress from stand-alone microcomputers and decentralized mainframes towards pervasive computing. Castells' model of a pervasive computing system, uses the example of the Internet as the start of a pervasive computing system. The logical progression from that paradigm is a system where that networking logic becomes applicable in every realm of daily activity, in every location and every context. Castells envisages a system where billions of miniature, ubiquitous inter-communication devices will be spread worldwide, ""like pigment in the wall paint"".
","In Manuel Castells' book The Rise of the Network Society, Castells puts forth the concept that there is going to be a continuous evolution of computing devices. He states we will progress from stand-alone microcomputers and decentralized mainframes towards pervasive computing.","[' Who wrote The Rise of the Network Society?', ' Castells states that we will progress from stand-alone microcomputers and decentralized mainframes towards what?']","['Manuel Castells', 'pervasive computing']"
1098,ubiquitous computing,History,"Mark Weiser coined the phrase ""ubiquitous computing"" around 1988, during his tenure as Chief Technologist of the Xerox Palo Alto Research Center (PARC). Both alone and with PARC Director and Chief Scientist John Seely Brown, Weiser wrote some of the earliest papers on the subject, largely defining it and sketching out its major concerns.","Mark Weiser coined the phrase ""ubiquitous computing"" around 1988, during his tenure as Chief Technologist of the Xerox Palo Alto Research Center (PARC). Both alone and with PARC Director and Chief Scientist John Seely Brown, Weiser wrote some of the earliest papers on the subject, largely defining it and sketching out its major concerns.","[' Who coined the phrase ""ubiquitous computing"" around 1988?', "" What was Mark Weiser's job title at the Xerox Palo Alto Research Center?"", ' Who was the PARC Director and Chief Scientist?', ' earliest papers on the subject, largely defining and sketching out its major concerns?']","['Mark Weiser', 'Chief Technologist', 'John Seely Brown', 'Mark Weiser coined the phrase ""ubiquitous computing']"
1099,ubiquitous computing,Recognizing the effects of extending processing power,"Recognizing that the extension of processing power into everyday scenarios would necessitate understandings of social, cultural and psychological phenomena beyond its proper ambit, Weiser was influenced by many fields outside computer science, including ""philosophy, phenomenology, anthropology, psychology, post-Modernism, sociology of science and feminist criticism"". He was explicit about ""the humanistic origins of the 'invisible ideal in post-modernist thought'"", referencing as well the ironically dystopian Philip K. Dick novel Ubik.
","Recognizing that the extension of processing power into everyday scenarios would necessitate understandings of social, cultural and psychological phenomena beyond its proper ambit, Weiser was influenced by many fields outside computer science, including ""philosophy, phenomenology, anthropology, psychology, post-Modernism, sociology of science and feminist criticism"". He was explicit about ""the humanistic origins of the 'invisible ideal in post-modernist thought'"", referencing as well the ironically dystopian Philip K. Dick novel Ubik.","[' What did Weiser believe would require understandings of beyond its proper ambit?', ' What field of study was Weiser influenced by?', ' Weiser was explicit about what?', "" What was the title of Philip K. Dick's novel Ubik?"", ' What two topics did Dick discuss in his book?']","['social, cultural and psychological phenomena', 'computer science', ""the humanistic origins of the 'invisible ideal in post-modernist thought'"", 'dystopian', ""the humanistic origins of the 'invisible ideal in post-modernist thought'""]"
1100,ubiquitous computing,Recognizing the effects of extending processing power,"Ken Sakamura of the University of Tokyo, Japan leads the Ubiquitous Networking Laboratory (UNL), Tokyo as well as the T-Engine Forum. The joint goal of Sakamura's Ubiquitous Networking specification and the T-Engine forum, is to enable any everyday device to broadcast and receive information.","Ken Sakamura of the University of Tokyo, Japan leads the Ubiquitous Networking Laboratory (UNL), Tokyo as well as the T-Engine Forum. The joint goal of Sakamura's Ubiquitous Networking specification and the T-Engine forum, is to enable any everyday device to broadcast and receive information.","[' Who is the head of the Ubiquitous Networking Laboratory at the University of Tokyo?', ' What is the goal of the T-Engine Forum?']","['Ken Sakamura', 'to enable any everyday device to broadcast and receive information']"
1101,ubiquitous computing,Recognizing the effects of extending processing power,"MIT has also contributed significant research in this field, notably Things That Think consortium (directed by Hiroshi Ishii, Joseph A. Paradiso and Rosalind Picard) at the Media Lab and the CSAIL effort known as Project Oxygen. Other major contributors include University of Washington's Ubicomp Lab (directed by Shwetak Patel), Dartmouth College's DartNets Lab, Georgia Tech's College of Computing, Cornell University's People Aware Computing Lab, NYU's Interactive Telecommunications Program, UC Irvine's Department of Informatics, Microsoft Research, Intel Research and Equator, Ajou University UCRi & CUS.","MIT has also contributed significant research in this field, notably Things That Think consortium (directed by Hiroshi Ishii, Joseph A. Paradiso and Rosalind Picard) at the Media Lab and the CSAIL effort known as Project Oxygen. Other major contributors include University of Washington's Ubicomp Lab (directed by Shwetak Patel), Dartmouth College's DartNets Lab, Georgia Tech's College of Computing, Cornell University's People Aware Computing Lab, NYU's Interactive Telecommunications Program, UC Irvine's Department of Informatics, Microsoft Research, Intel Research and Equator, Ajou University UCRi & CUS.","[' Who directed the Things That Think consortium?', ' What is the CSAIL effort known as?', "" Who directed Dartmouth's Ubicomp Lab?"", ' Who directed the Ubicomp Lab at the University of Washington?', ' What is the name of the DartNets Lab at Dartmouth College?', ' Where is the People Aware Computing Lab located?']","['Hiroshi Ishii', 'Project Oxygen', 'Shwetak Patel', 'Shwetak Patel', 'Ubicomp Lab', 'Cornell University']"
1102,ubiquitous computing,Examples,"One of the earliest ubiquitous systems was artist Natalie Jeremijenko's ""Live Wire"", also known as ""Dangling String"", installed at Xerox PARC during Mark Weiser's time there. This was a piece of string attached to a stepper motor and controlled by a LAN connection; network activity caused the string to twitch, yielding a peripherally noticeable indication of traffic. Weiser called this an example of calm technology.","One of the earliest ubiquitous systems was artist Natalie Jeremijenko's ""Live Wire"", also known as ""Dangling String"", installed at Xerox PARC during Mark Weiser's time there. This was a piece of string attached to a stepper motor and controlled by a LAN connection; network activity caused the string to twitch, yielding a peripherally noticeable indication of traffic.","[' What is another name for Natalie Jeremijenko\'s ""Live Wire""?', ' What was a piece of string attached to a stepper motor and controlled by a LAN connection?', ' What caused the string to twitch?', ' What caused a peripherally noticeable indication of traffic?']","['Dangling String', 'Dangling String', 'network activity', 'network activity']"
1103,ubiquitous computing,Examples,"A present manifestation of this trend is the widespread diffusion of mobile phones. Many mobile phones support high speed data transmission, video services, and other services with powerful computational ability. Although these mobile devices are not necessarily manifestations of ubiquitous computing, there are examples, such as Japan's Yaoyorozu (""Eight Million Gods"") Project in which mobile devices, coupled with radio frequency identification tags demonstrate that ubiquitous computing is already present in some form.","A present manifestation of this trend is the widespread diffusion of mobile phones. Many mobile phones support high speed data transmission, video services, and other services with powerful computational ability.","[' What is a present manifestation of this trend?', ' What do many mobile phones support?']","['the widespread diffusion of mobile phones', 'high speed data transmission, video services']"
1104,ubiquitous computing,Examples,"Ubiquitous computing research has focused on building an environment in which computers allow humans to focus attention on select aspects of the environment and operate in supervisory and policy-making roles. Ubiquitous computing emphasizes the creation of a human computer interface that can interpret and support a user's intentions. For example, MIT's Project Oxygen seeks to create a system in which computation is as pervasive as air:
",Ubiquitous computing research has focused on building an environment in which computers allow humans to focus attention on select aspects of the environment and operate in supervisory and policy-making roles. Ubiquitous computing emphasizes the creation of a human computer interface that can interpret and support a user's intentions.,"[' What type of computing research has focused on building an environment in which computers allow humans to focus attention on select aspects of the environment?', ' What does Ubiquitous computing emphasize the creation of?']","['Ubiquitous', 'a human computer interface']"
1105,ubiquitous computing,Examples,"In the future, computation will be human centered. It will be freely available everywhere, like batteries and power sockets, or oxygen in the air we breathe...We will not need to carry our own devices around with us. Instead, configurable generic devices, either handheld or embedded in the environment, will bring computation to us, whenever we need it and wherever we might be. As we interact with these ""anonymous"" devices, they will adopt our information personalities. They will respect our desires for privacy and security. We won't have to type, click, or learn new computer jargon. Instead, we'll communicate naturally, using speech and gestures that describe our intent...","In the future, computation will be human centered. It will be freely available everywhere, like batteries and power sockets, or oxygen in the air we breathe...We will not need to carry our own devices around with us.","[' In the future, what will be human centered?', ' What will be freely available everywhere?']","['computation', 'computation']"
1106,ubiquitous computing,Issues,"Public policy problems are often ""preceded by long shadows, long trains of activity"", emerging slowly, over decades or even the course of a century. There is a need for a long-term view to guide policy decision making, as this will assist in identifying long-term problems or opportunities related to the ubiquitous computing environment. This information can reduce uncertainty and guide the decisions of both policy makers and those directly involved in system development (Wedemeyer et al. 2001). One important consideration is the degree to which different opinions form around a single problem. Some issues may have strong consensus about their importance, even if there are great differences in opinion regarding the cause or solution. For example, few people will differ in their assessment of a highly tangible problem with physical impact such as terrorists using new weapons of mass destruction to destroy human life. The problem statements outlined above that address the future evolution of the human species or challenges to identity have clear cultural or religious implications and are likely to have greater variance in opinion about them.","Public policy problems are often ""preceded by long shadows, long trains of activity"", emerging slowly, over decades or even the course of a century. There is a need for a long-term view to guide policy decision making, as this will assist in identifying long-term problems or opportunities related to the ubiquitous computing environment.","[' Public policy problems are often ""preceded by long shadows, long trains of activity""?', ' What is a need for a long-term view to guide policy decision making?', ' What can help identify long-term problems or opportunities?']","['emerging slowly, over decades or even the course of a century', 'this will assist in identifying long-term problems or opportunities related to the ubiquitous computing environment', 'a long-term view']"
1107,global optimization,Summary,"Global optimization is a branch of applied mathematics and numerical analysis that attempts to find the global minima or maxima of a function or a set of functions on a given set. It is usually described as a minimization problem because the maximization of the real-valued function 



g
(
x
)


{\displaystyle g(x)}
 is equivalent to the minimization of the function 



f
(
x
)
:=
(
−
1
)
⋅
g
(
x
)


{\displaystyle f(x):=(-1)\cdot g(x)}
. 
","Global optimization is a branch of applied mathematics and numerical analysis that attempts to find the global minima or maxima of a function or a set of functions on a given set. It is usually described as a minimization problem because the maximization of the real-valued function 



g
(
x
)


{\displaystyle g(x)}
 is equivalent to the minimization of the function 



f
(
x
)
:=
(
−
1
)
⋅
g
(
x
)


{\displaystyle f(x):=(-1)\cdot g(x)}
.","[' What is the branch of applied mathematics and numerical analysis that attempts to find the global minima or maxima of a function or a set of functions on a given set?', ' What is usually described as a minimization problem?', ' What is the maximization of the real-valued function g ( x ) <unk>displaystyle g(x)<unk> equivalent to?', ' The minimization of what function is equivalent?']","['Global optimization', 'Global optimization', 'minimization', 'f']"
1108,global optimization,Summary,"Global optimization is distinguished from local optimization by its focus on finding the minimum or maximum over the given set, as opposed to finding local minima or maxima. Finding an arbitrary local minimum is relatively straightforward by using classical local optimization methods. Finding the global minimum of a function is far more difficult: analytical methods are frequently not applicable, and the use of numerical solution strategies often leads to very hard challenges.
","Global optimization is distinguished from local optimization by its focus on finding the minimum or maximum over the given set, as opposed to finding local minima or maxima. Finding an arbitrary local minimum is relatively straightforward by using classical local optimization methods.","[' Global optimization is distinguished from what other type of optimization?', ' Global optimization focuses on finding what over the given set?', ' What is relatively straightforward by using classical local optimization methods?']","['local optimization', 'minimum or maximum', 'Finding an arbitrary local minimum']"
1109,global optimization,General theory,"A recent approach to the global optimization problem is via minima distribution 
. In this work, a relationship between any continuous function 



f


{\displaystyle f}
 on a compact set 



Ω
⊂


R


n




{\displaystyle \Omega \subset \mathbb {R} ^{n}}
 and its global minima 




f

∗




{\displaystyle f^{*}}
 has been strictly established. As a typical case, it follows that
","A recent approach to the global optimization problem is via minima distribution 
. In this work, a relationship between any continuous function 



f


{\displaystyle f}
 on a compact set 



Ω
⊂


R


n




{\displaystyle \Omega \subset \mathbb {R} ^{n}}
 and its global minima 




f

∗




{\displaystyle f^{*}}
 has been strictly established.","[' A recent approach to the global optimization problem is via what?', ' A relationship between any continuous function on a compact set and its global minima f <unk> <unk>displaystyle f*<unk> has been strictly established in what work?']","['minima distribution', 'minima distribution \n. In this work, a relationship between any continuous function \n\n\n\nf\n\n\n{\\displaystyle f}\n on a compact set \n\n\n\nΩ\n⊂\n\n\nR\n\n\nn\n\n\n\n\n{\\displaystyle \\Omega \\subset \\mathbb {R} ^{n}}\n and its global minima \n\n\n\n\nf\n\n∗']"
1110,global optimization,General theory,"where 



μ
(

X

∗


)


{\displaystyle \mu (X^{*})}
 is the 



n


{\displaystyle n}
-dimensional Lebesgue measure of the set of minimizers 




X

∗


∈
Ω


{\displaystyle X^{*}\in \Omega }
. And if 



f


{\displaystyle f}
 is not a constant on 



Ω


{\displaystyle \Omega }
, the monotonic relationship
","where 



μ
(

X

∗


)


{\displaystyle \mu (X^{*})}
 is the 



n


{\displaystyle n}
-dimensional Lebesgue measure of the set of minimizers 




X

∗


∈
Ω


{\displaystyle X^{*}\in \Omega }
. And if 



f


{\displaystyle f}
 is not a constant on 



Ω


{\displaystyle \Omega }
, the monotonic relationship","[' What is the Lebesgue measure of the set of minimizers?', ' What is not a constant on <unk> <unk>displaystyle <unk>Omega <unk>?']","['μ\n(\n\nX\n\n∗\n\n\n)\n\n\n{\\displaystyle \\mu (X^{*})}\n is the \n\n\n\nn', 'f']"
1111,global optimization,General theory,"holds for every smooth function 



φ


{\displaystyle \varphi }
 with compact support in 



Ω


{\displaystyle \Omega }
. Here are two immediate properties of 




m

f
,
Ω




{\displaystyle m_{f,\Omega }}
:
","holds for every smooth function 



φ


{\displaystyle \varphi }
 with compact support in 



Ω


{\displaystyle \Omega }
. Here are two immediate properties of 




m

f
,
Ω




{\displaystyle m_{f,\Omega }}
:","[' What holds for every smooth function with compact support in <unk> <unk>displaystyle <unk>Omega <unk>?', ' What are two immediate properties of m f?']","['m_{f', 'φ\n\n\n{\\displaystyle \\varphi }\n with compact support in \n\n\n\nΩ\n\n\n{\\displaystyle \\Omega }\n. Here are two immediate properties of \n\n\n\n\nm\n\nf\n,\nΩ']"
1112,global optimization,General theory,"As a comparison, the well-known relationship between any differentiable convex function and its minima is strictly established by the gradient. If 



f


{\displaystyle f}
 is differentiable on a convex set 



D


{\displaystyle D}
, then 



f


{\displaystyle f}
 is convex if and only if
","As a comparison, the well-known relationship between any differentiable convex function and its minima is strictly established by the gradient. If 



f


{\displaystyle f}
 is differentiable on a convex set 



D


{\displaystyle D}
, then 



f


{\displaystyle f}
 is convex if and only if","[' What is the well-known relationship between a differentiable convex function and its minima strictly established by?', ' If f <unk>displaystyle f<unk> is differiable on a convexe set D <unk>Displaystyle D<unk>, then if and only if f is convened, then what is the difference between the two?']","['the gradient', 'f']"
1113,random oracle,Summary,"In cryptography, a random oracle is an oracle (a theoretical black box) that responds to every unique query with a (truly) random response chosen uniformly from its output domain. If a query is repeated, it responds the same way every time that query is submitted.
","In cryptography, a random oracle is an oracle (a theoretical black box) that responds to every unique query with a (truly) random response chosen uniformly from its output domain. If a query is repeated, it responds the same way every time that query is submitted.","[' What is a random oracle in cryptography?', ' What is the name of the theoretical black box that responds to every unique query with a true random response?']","['a theoretical black box', 'a random oracle']"
1114,random oracle,Summary,"Random oracles as a mathematical abstraction were first used in rigorous cryptographic proofs in the 1993 publication by Mihir Bellare and Phillip Rogaway (1993). They are typically used when the proof cannot be carried out using weaker assumptions on the cryptographic hash function. A system that is proven secure when every hash function is replaced by a random oracle is described as being secure in the random oracle model, as opposed to secure in the standard model of cryptography.
",Random oracles as a mathematical abstraction were first used in rigorous cryptographic proofs in the 1993 publication by Mihir Bellare and Phillip Rogaway (1993). They are typically used when the proof cannot be carried out using weaker assumptions on the cryptographic hash function.,"[' When were random oracles first used in rigorous cryptographic proofs?', ' Who published the 1993 publication?', ' What are typically used when the proof cannot be carried out using weaker assumptions on the cryptographic hash function?']","['1993', 'Mihir Bellare and Phillip Rogaway', 'Random oracles']"
1115,random oracle,Applications,"Random oracles are typically used as an idealised replacement for cryptographic hash functions in schemes where strong randomness assumptions are needed of the hash function's output. Such a proof often shows that a system or a protocol is secure by showing that an attacker must require impossible behavior from the oracle, or solve some mathematical problem believed hard in order to break it. However, it only proves such properties in the random oracle model, making sure no major design flaws are present. It is in general not true that such a proof implies the same properties in the standard model. Still, a proof in the random oracle model is considered better than no formal security proof at all.","Random oracles are typically used as an idealised replacement for cryptographic hash functions in schemes where strong randomness assumptions are needed of the hash function's output. Such a proof often shows that a system or a protocol is secure by showing that an attacker must require impossible behavior from the oracle, or solve some mathematical problem believed hard in order to break it.","[' Random oracles are idealised replacement for what?', "" What is needed for strong randomness assumptions of the hash function's output?"", ' A proof often shows that a system or a protocol is secure by showing that an attacker must require impossible behavior from the system or protocol?', ' What must an attacker require from the oracle to break it?', ' What must a hacker have to do to break a computer?']","['cryptographic hash functions', 'Random oracles', 'Random oracles', 'impossible behavior', 'solve some mathematical problem believed hard']"
1116,random oracle,Applications,"Not all uses of cryptographic hash functions require random oracles: schemes that require only one or more properties having a definition in the standard model (such as collision resistance, preimage resistance, second preimage resistance, etc.) can often be proven secure in the standard model (e.g., the Cramer–Shoup cryptosystem).
","Not all uses of cryptographic hash functions require random oracles: schemes that require only one or more properties having a definition in the standard model (such as collision resistance, preimage resistance, second preimage resistance, etc.) can often be proven secure in the standard model (e.g., the Cramer–Shoup cryptosystem).","[' What does not require random oracles?', ' Schemes that require only one or more properties having a definition in the standard model can be proven secure in what model?', ' What is the Cramer-Shoup cryptosystem?']","['cryptographic hash functions', 'standard model', 'secure in the standard model']"
1117,random oracle,Applications,"Random oracles have long been considered in computational complexity theory, and many schemes have been proven secure in the random oracle model, for example Optimal Asymmetric Encryption Padding, RSA-FDH and Probabilistic Signature Scheme. In 1986, Amos Fiat and Adi Shamir showed a major application of random oracles – the removal of interaction from protocols for the creation of signatures.
","Random oracles have long been considered in computational complexity theory, and many schemes have been proven secure in the random oracle model, for example Optimal Asymmetric Encryption Padding, RSA-FDH and Probabilistic Signature Scheme. In 1986, Amos Fiat and Adi Shamir showed a major application of random oracles – the removal of interaction from protocols for the creation of signatures.","[' What has long been considered in computational complexity theory?', ' What have many schemes been proven secure in the random oracle model?', ' In what year did Amos Fiat and Adi Shamir show a major application of Random Oracles?', ' What did random oracles remove from protocols for the creation of signatures?']","['Random oracles', 'Optimal Asymmetric Encryption Padding, RSA-FDH and Probabilistic Signature Scheme', '1986', 'interaction']"
1118,random oracle,Applications,"In 1993, Mihir Bellare and Phillip Rogaway were the first to advocate their use in cryptographic constructions. In their definition, the random oracle produces a bit-string of infinite length which can be truncated to the length desired.
","In 1993, Mihir Bellare and Phillip Rogaway were the first to advocate their use in cryptographic constructions. In their definition, the random oracle produces a bit-string of infinite length which can be truncated to the length desired.","[' Who were the first to advocate the use of random oracles in cryptographic constructions?', ' What did Mihir Bellare and Phillip Rogaway advocate in 1993?']","['Mihir Bellare and Phillip Rogaway', 'their use in cryptographic constructions']"
1119,random oracle,Applications,"When a random oracle is used within a security proof, it is made available to all players, including the adversary or adversaries. A single oracle may be treated as multiple oracles by pre-pending a fixed bit-string to the beginning of each query (e.g., queries formatted as ""1|x"" or ""0|x"" can be considered as calls to two separate random oracles, similarly ""00|x"", ""01|x"", ""10|x"" and ""11|x"" can be used to represent calls to four separate random oracles).
","When a random oracle is used within a security proof, it is made available to all players, including the adversary or adversaries. A single oracle may be treated as multiple oracles by pre-pending a fixed bit-string to the beginning of each query (e.g., queries formatted as ""1|x"" or ""0|x"" can be considered as calls to two separate random oracles, similarly ""00|x"", ""01|x"", ""10|x"" and ""11|x"" can be used to represent calls to four separate random oracles).","[' What is made available to all players when a random oracle is used within a security proof?', ' What may be treated as multiple oracles by pre-pending a fixed bit-string to the beginning of each query?', ' What can be used to represent calls to four separate random oracles?']","['including the adversary or adversaries', 'A single oracle', '""00|x"", ""01|x"", ""10|x"" and ""11|x""']"
1120,random oracle,Limitations,"In fact, certain artificial signature and encryption schemes are known which are proven secure in the random oracle model, but which are trivially insecure when any real function is substituted for the random oracle. Nonetheless, for any more natural protocol a proof of security in the random oracle model gives very strong evidence of the practical security of the protocol.","In fact, certain artificial signature and encryption schemes are known which are proven secure in the random oracle model, but which are trivially insecure when any real function is substituted for the random oracle. Nonetheless, for any more natural protocol a proof of security in the random oracle model gives very strong evidence of the practical security of the protocol.","[' What are known as being secure in the random oracle model but trivially insecure when a real function is substituted for it?', ' For any more natural protocol a proof of security in what model gives?', ' What model gives very strong evidence of the practical security of a protocol?']","['artificial signature and encryption schemes', 'random oracle', 'random oracle']"
1121,random oracle,Limitations,"In general, if a protocol is proven secure, attacks to that protocol must either be outside what was proven, or break one of the assumptions in the proof; for instance if the proof relies on the hardness of integer factorization, to break this assumption one must discover a fast integer factorization algorithm. Instead, to break the random oracle assumption, one must discover some unknown and undesirable property of the actual hash function; for good hash functions where such properties are believed unlikely, the considered protocol can be considered secure.
","In general, if a protocol is proven secure, attacks to that protocol must either be outside what was proven, or break one of the assumptions in the proof; for instance if the proof relies on the hardness of integer factorization, to break this assumption one must discover a fast integer factorization algorithm. Instead, to break the random oracle assumption, one must discover some unknown and undesirable property of the actual hash function; for good hash functions where such properties are believed unlikely, the considered protocol can be considered secure.","[' If a protocol is proven secure, what must be outside of what was proven?', ' If the proof relies on the hardness of integer factorization, one must discover what to break this assumption?', ' To break the random oracle assumption, one must discover what?', ' What must one discover in order to break this assumption?', ' For good hash functions where such properties are believed unlikely, what can be considered secure?']","['attacks', 'a fast integer factorization algorithm', 'some unknown and undesirable property of the actual hash function', 'some unknown and undesirable property of the actual hash function', 'the considered protocol']"
1122,random oracle,Random Oracle Hypothesis,"Although the Baker–Gill–Solovay theorem showed that there exists an oracle A such that PA = NPA, subsequent work by Bennett and Gill, showed that for a random oracle B (a function from {0,1}n to {0,1} such that each input element maps to each of 0 or 1 with probability 1/2, independently of the mapping of all other inputs), PB ⊊ NPB with probability 1. Similar separations, as well as the fact that random oracles separate classes with probability 0 or 1 (as a consequence of the Kolmogorov's zero–one law), led to the creation of the Random Oracle Hypothesis, that two ""acceptable"" complexity classes C1 and C2 are equal if and only if they are equal (with probability 1) under a random oracle (the acceptability of a complexity class is defined in BG81). This hypothesis was later shown to be false, as the two acceptable complexity classes IP and PSPACE were shown to be equal despite IPA ⊊ PSPACEA for a random oracle A with probability 1.","Although the Baker–Gill–Solovay theorem showed that there exists an oracle A such that PA = NPA, subsequent work by Bennett and Gill, showed that for a random oracle B (a function from {0,1}n to {0,1} such that each input element maps to each of 0 or 1 with probability 1/2, independently of the mapping of all other inputs), PB ⊊ NPB with probability 1. Similar separations, as well as the fact that random oracles separate classes with probability 0 or 1 (as a consequence of the Kolmogorov's zero–one law), led to the creation of the Random Oracle Hypothesis, that two ""acceptable"" complexity classes C1 and C2 are equal if and only if they are equal (with probability 1) under a random oracle (the acceptability of a complexity class is defined in BG81).","[' What theorem showed that there exists an oracle A such that PA = NPA?', ' What work by Bennett and Gill showed that for a random or acle B, each input element maps to each of 0 or 1 with probability 1/2?', "" What is a consequence of the Kolmogorov's zero-one law?"", ' What does PB <unk> NPB have in common?', ' What law led to the creation of the Random Oracle Hypothesis?', ' How many ""acceptable"" complexity classes C1 and C2 are equal if and only if they are equal under a random oracle?', ' What is the acceptability of a complexity class defined in?']","['Baker–Gill–Solovay', 'oracle B', 'random oracles separate classes with probability 0 or 1', 'separations', ""Kolmogorov's zero–one law"", 'two', 'BG81']"
1123,random oracle,Ideal Cipher,"An ideal cipher is a random permutation oracle that is used to model an idealized block cipher. A random permutation decrypts each ciphertext block into one and only one plaintext block and vice versa, so there is a one-to-one correspondence. Some cryptographic proofs make not only the ""forward"" permutation available to all players, but also the ""reverse"" permutation.
","An ideal cipher is a random permutation oracle that is used to model an idealized block cipher. A random permutation decrypts each ciphertext block into one and only one plaintext block and vice versa, so there is a one-to-one correspondence.","[' What is an ideal cipher?', ' What is a random permutation oracle used to model?']","['a random permutation oracle', 'an idealized block cipher']"
1124,random oracle,Ideal Permutation,"An ideal permutation is an idealized object sometimes used in cryptography to model the behaviour of a permutation whose outputs are indistinguishable from those of a random permutation. In the ideal permutation model, an additional oracle access is given to the ideal permutation and its inverse. The ideal permutation model can be seen as a special case of the ideal cipher model where access is given to only a single permutation, instead of a family of permutations as in the case of the ideal cipher model.
","An ideal permutation is an idealized object sometimes used in cryptography to model the behaviour of a permutation whose outputs are indistinguishable from those of a random permutation. In the ideal permutation model, an additional oracle access is given to the ideal permutation and its inverse.","[' What is an idealized object sometimes used in cryptography to model the behaviour of a permutation?', ' What outputs are indistinguishable from those of random permutations?', ' An additional oracle access is given to what?']","['An ideal permutation', 'ideal permutation', 'the ideal permutation and its inverse']"
1125,random oracle,Quantum-accessible Random Oracles,"Post-quantum cryptography studies quantum attacks on classical cryptographic schemes. As a random oracle is an abstraction of a hash function, it makes sense to assume that a quantum attacker can access the random oracle in quantum superposition. Many of the classical security proofs break down in that quantum random oracle model and need to be revised.
","Post-quantum cryptography studies quantum attacks on classical cryptographic schemes. As a random oracle is an abstraction of a hash function, it makes sense to assume that a quantum attacker can access the random oracle in quantum superposition.","[' Post-quantum cryptography studies quantum attacks on what?', ' What is an abstraction of a hash function?', ' A quantum attacker can access the random oracle in quantum superposition?']","['classical cryptographic schemes', 'random oracle', 'random oracle is an abstraction of a hash function']"
1126,social network,Summary,"A social network  is a social structure made up of a set of social actors (such as individuals or organizations), sets of dyadic ties, and other social interactions between actors. The social network perspective provides a set of methods for analyzing the structure of whole social entities as well as a variety of theories explaining the patterns observed in these structures. The study of these structures uses social network analysis to identify local and global patterns, locate influential entities, and examine network dynamics.
","A social network  is a social structure made up of a set of social actors (such as individuals or organizations), sets of dyadic ties, and other social interactions between actors. The social network perspective provides a set of methods for analyzing the structure of whole social entities as well as a variety of theories explaining the patterns observed in these structures.","[' What is a social network made up of?', ' What does the social network perspective provide?', ' What can be used for analyzing the structure of whole social entities?', ' What theories explain the patterns observed in these structures?']","['a set of social actors', 'a set of methods for analyzing the structure of whole social entities', 'The social network perspective', 'a variety of theories']"
1127,social network,Summary,"Social networks and the analysis of them is an inherently interdisciplinary academic field which emerged from social psychology, sociology, statistics, and graph theory. Georg Simmel authored early structural theories in sociology emphasizing the dynamics of triads and ""web of group affiliations"". Jacob Moreno is credited with developing the first sociograms in the 1930s to study interpersonal relationships. These approaches were mathematically formalized in the 1950s and theories and methods of social networks became pervasive in the social and behavioral sciences by the 1980s. Social network analysis is now one of the major paradigms in contemporary sociology, and is also employed in a number of other social and formal sciences. Together with other complex networks, it forms part of the nascent field of network science.","Social networks and the analysis of them is an inherently interdisciplinary academic field which emerged from social psychology, sociology, statistics, and graph theory. Georg Simmel authored early structural theories in sociology emphasizing the dynamics of triads and ""web of group affiliations"".","[' Social networks and the analysis of them are an interdisciplinary academic field that emerged from what?', ' Georg Simmel authored early structural theories in what field?', ' What did Simmel emphasize the dynamics of?']","['social psychology, sociology, statistics, and graph theory', 'sociology', 'triads']"
1128,social network,Overview,"The social network is a theoretical construct useful in the social sciences to study relationships between individuals, groups, organizations, or even entire societies (social units, see differentiation). The term is used to describe a social structure determined by such interactions. The ties through which any given social unit connects represent the convergence of the various social contacts of that unit. This theoretical approach is, necessarily, relational.  An axiom of the social network approach to understanding social interaction is that social phenomena should be primarily conceived and investigated through the properties of relations between and within units, instead of the properties of these units themselves. Thus, one common criticism of social network theory is that individual agency is often ignored although this may not be the case in practice (see agent-based modeling). Precisely because many different types of relations, singular or in combination, form these network configurations, network analytics are useful to a broad range of research enterprises. In social science, these fields of study include, but are not limited to anthropology, biology, communication studies, economics, geography, information science, organizational studies, social psychology, sociology, and sociolinguistics.
","The social network is a theoretical construct useful in the social sciences to study relationships between individuals, groups, organizations, or even entire societies (social units, see differentiation). The term is used to describe a social structure determined by such interactions.","[' What is a theoretical construct useful in the social sciences to study relationships between individuals, groups, organizations, or even entire societies?', ' What term is used to describe a social structure determined by such interactions?']","['The social network', 'The social network']"
1129,social network,History,"In the late 1890s, both Émile Durkheim and Ferdinand Tönnies foreshadowed the idea of social networks in their theories and research of social groups. Tönnies argued that social groups can exist as personal and direct social ties that either link individuals who share values and belief (Gemeinschaft, German, commonly translated as ""community"") or impersonal, formal, and instrumental social links (Gesellschaft, German, commonly translated as ""society""). Durkheim gave a non-individualistic explanation of social facts, arguing that social phenomena arise when interacting individuals constitute a reality that can no longer be accounted for in terms of the properties of individual actors. Georg Simmel, writing at the turn of the twentieth century, pointed to the nature of networks and the effect of network size on interaction and examined the likelihood of interaction in loosely knit networks rather than groups.","In the late 1890s, both Émile Durkheim and Ferdinand Tönnies foreshadowed the idea of social networks in their theories and research of social groups. Tönnies argued that social groups can exist as personal and direct social ties that either link individuals who share values and belief (Gemeinschaft, German, commonly translated as ""community"") or impersonal, formal, and instrumental social links (Gesellschaft, German, commonly translated as ""society"").","[' Who foreshadowed the idea of social networks in their theories and research of social groups?', ' Who argued that social groups can exist as personal and direct social ties that link individuals who share values and belief?', ' What is the German word for individuals who share values and belief?', ' What is another word for people who share impersonal, formal, and instrumental social links?']","['Émile Durkheim and Ferdinand Tönnies', 'Ferdinand Tönnies', 'Gemeinschaft', 'Gesellschaft']"
1130,social network,History,"Major developments in the field can be seen in the 1930s by several groups in psychology, anthropology, and mathematics working independently. In psychology, in the 1930s, Jacob L. Moreno began systematic recording and analysis of social interaction in small groups, especially classrooms and work groups (see sociometry). In anthropology, the foundation for social network theory is the theoretical and ethnographic work of Bronislaw Malinowski, Alfred Radcliffe-Brown, and Claude Lévi-Strauss. A group of social anthropologists associated with Max Gluckman and the Manchester School, including John A. Barnes, J. Clyde Mitchell and Elizabeth Bott Spillius, often are credited with performing some of the first fieldwork from which network analyses were performed, investigating community networks in southern Africa, India and the United Kingdom. Concomitantly, British anthropologist S. F. Nadel codified a theory of social structure that was influential in later network analysis. In sociology, the early (1930s) work of Talcott Parsons set the stage for taking a relational approach to understanding social structure. Later, drawing upon Parsons' theory, the work of sociologist Peter Blau provides a strong impetus for analyzing the relational ties of social units with his work on social exchange theory.","Major developments in the field can be seen in the 1930s by several groups in psychology, anthropology, and mathematics working independently. In psychology, in the 1930s, Jacob L. Moreno began systematic recording and analysis of social interaction in small groups, especially classrooms and work groups (see sociometry).",[' In what decade did Jacob L. Moreno begin systematic recording and analysis of social interaction in small groups?'],['1930s']
1131,social network,History,"By the 1970s, a growing number of scholars worked to combine the different tracks and traditions. One group consisted of sociologist Harrison White and his students at the Harvard University Department of Social Relations. Also independently active in the Harvard Social Relations department at the time were Charles Tilly, who focused on networks in political and community sociology and social movements, and Stanley Milgram, who developed the ""six degrees of separation"" thesis. Mark Granovetter and Barry Wellman are among the former students of White who elaborated and championed the analysis of social networks.","By the 1970s, a growing number of scholars worked to combine the different tracks and traditions. One group consisted of sociologist Harrison White and his students at the Harvard University Department of Social Relations.","[' When did a growing number of scholars work to combine different tracks and traditions?', ' Who was one group consisting of sociologist Harrison White and his students at Harvard University?']","['By the 1970s', 'Department of Social Relations']"
1132,social network,Levels of analysis,"In general, social networks are self-organizing, emergent, and complex, such that a globally coherent pattern appears from the local interaction of the elements that make up the system. These patterns become more apparent as network size increases. However, a global network analysis of, for example, all interpersonal relationships in the world is not feasible and is likely to contain so much information as to be uninformative. Practical limitations of computing power, ethics and participant recruitment and payment also limit the scope of a social network analysis. The nuances of a local system may be lost in a large network analysis, hence the quality of information may be more important than its scale for understanding network properties. Thus, social networks are analyzed at the scale relevant to the researcher's theoretical question. Although levels of analysis are not necessarily mutually exclusive, there are three general levels into which networks may fall: micro-level, meso-level, and macro-level.
","In general, social networks are self-organizing, emergent, and complex, such that a globally coherent pattern appears from the local interaction of the elements that make up the system. These patterns become more apparent as network size increases.","[' Social networks are self-organizing, emergent, and what else?', ' A globally coherent pattern appears from the local interaction of the elements that make up the system?', ' As network size increases, patterns become more apparent what?']","['complex', 'social networks', 'social networks']"
1133,social network,Structural holes,"In the context of networks, social capital exists where people have an advantage because of their location in a network. Contacts in a network provide information, opportunities and perspectives that can be beneficial to the central player in the network. Most social structures tend to be characterized by dense clusters of strong connections. Information within these clusters tends to be rather homogeneous and redundant. Non-redundant information is most often obtained through contacts in different clusters. When two separate clusters possess non-redundant information, there is said to be a structural hole between them. Thus, a network that bridges structural holes will provide network benefits that are in some degree additive, rather than overlapping. An ideal network structure has a vine and cluster structure, providing access to many different clusters and structural holes.","In the context of networks, social capital exists where people have an advantage because of their location in a network. Contacts in a network provide information, opportunities and perspectives that can be beneficial to the central player in the network.","[' What is social capital?', ' What provides information, opportunities and perspectives that can be beneficial to the central player in a network?']","['where people have an advantage because of their location in a network', 'Contacts in a network']"
1134,social network,Structural holes,"Networks rich in structural holes are a form of social capital in that they offer information benefits. The main player in a network that bridges structural holes is able to access information from diverse sources and clusters. For example, in business networks, this is beneficial to an individual's career because he is more likely to hear of job openings and opportunities if his network spans a wide range of contacts in different industries/sectors. This concept is similar to Mark Granovetter's theory of weak ties, which rests on the basis that having a broad range of contacts is most effective for job attainment.
",Networks rich in structural holes are a form of social capital in that they offer information benefits. The main player in a network that bridges structural holes is able to access information from diverse sources and clusters.,"[' What are networks rich in structural holes a form of?', ' What offer information benefits to a network that bridges structural holes?', ' Who can access information from diverse sources and clusters?']","['social capital', 'Networks', 'The main player in a network that bridges structural holes']"
1135,user experience,Summary,"The user experience (UX or UE) is how a user interacts with and experiences a product, system or service. It includes a person's perceptions of  utility, ease of use, and efficiency. Improving user experience is important to most companies, designers, and creators when creating and refining products because negative user experience can diminish the use of the product and, therefore, any desired positive impacts; conversely, designing toward profitability often conflicts with ethical user experience objectives and even causes harm. User experience is subjective. However, the attributes that make up the user experience are objective.
","The user experience (UX or UE) is how a user interacts with and experiences a product, system or service. It includes a person's perceptions of  utility, ease of use, and efficiency.","[' What is the term for how a user interacts with and experiences a product, system or service?', "" What is a person's perceptions of utility, ease of use, and efficiency?""]","['The user experience', 'The user experience']"
1136,user experience,Definitions,"The international standard on ergonomics of human-system interaction, ISO 9241-210, defines user experience as ""a person's perceptions and responses that result from the use or anticipated use of a product, system or service"". According to the ISO definition, user experience includes all the users' emotions, beliefs, preferences, perceptions, physical and psychological responses, behaviors and accomplishments that occur before, during, and after use. The ISO also lists three factors that influence user experience: the system, the user, and the context of use.
","The international standard on ergonomics of human-system interaction, ISO 9241-210, defines user experience as ""a person's perceptions and responses that result from the use or anticipated use of a product, system or service"". According to the ISO definition, user experience includes all the users' emotions, beliefs, preferences, perceptions, physical and psychological responses, behaviors and accomplishments that occur before, during, and after use.","[' What is the international standard on ergonomics of human-system interaction called?', "" What defines user experience as a person's perceptions and responses that result from the use or anticipated use of a product, system or service?"", "" What includes all the users' emotions, beliefs, preferences, perceptions, physical and psychological responses, behaviors and accomplishments that occur before, during and after use?""]","['ISO 9241-210', 'ISO 9241-210', 'user experience']"
1137,user experience,Definitions,"Note 3 of the standard hints that usability addresses aspects of user experience, e.g. ""usability criteria can be used to assess aspects of user experience"". The standard does not go further in clarifying the relation between user experience and usability. Clearly, the two are overlapping concepts, with usability including pragmatic aspects (getting a task done) and user experience focusing on users' feelings stemming both from pragmatic and hedonic aspects of the system. Many practitioners use the terms interchangeably. The term ""usability"" pre-dates the term ""user experience"". Part of the reason the terms are often used interchangeably is that, as a practical matter, a user will, at a minimum, require sufficient usability to accomplish a task while the feelings of the user may be less important, even to the user themselves. Since usability is about getting a task done, aspects of user experience like information architecture and user interface can help or hinder a user's experience. If a website has ""bad"" information architecture and a user has a difficult time finding what they are looking for, then a user will not have an effective, efficient, and satisfying search.
","Note 3 of the standard hints that usability addresses aspects of user experience, e.g. ""usability criteria can be used to assess aspects of user experience"".","[' What does Note 3 of the standard hints address?', ' What can be used to assess aspects of user experience?']","['aspects of user experience', 'usability criteria']"
1138,user experience,Definitions,"In addition to the ISO standard, there exist several other definitions for user experience. Some of them have been studied by Law et al.","In addition to the ISO standard, there exist several other definitions for user experience. Some of them have been studied by Law et al.","[' What are some other definitions of user experience?', ' Who has studied some of the other user experience definitions?']","['the ISO standard', 'Law et al']"
1139,user experience,History,"Early developments in user experience can be traced back to the Machine Age that includes the 19th and early 20th centuries. Inspired by the machine age intellectual framework, a quest for improving assembly processes to increase production efficiency and output led to the development of major technological advancements, such as mass production of high-volume goods on moving assembly lines, high-speed printing press, large hydroelectric power production plants, and radio technology to name a few.
","Early developments in user experience can be traced back to the Machine Age that includes the 19th and early 20th centuries. Inspired by the machine age intellectual framework, a quest for improving assembly processes to increase production efficiency and output led to the development of major technological advancements, such as mass production of high-volume goods on moving assembly lines, high-speed printing press, large hydroelectric power production plants, and radio technology to name a few.","[' The Machine Age includes the 19th and early 20th centuries.', ' What was inspired by the machine age intellectual framework?', ' What led to major technological advancements?', ' What has led to the development of major technological advancements?', ' What is one example of a technological advancement?']","['Early developments in user experience', 'a quest for improving assembly processes', 'a quest for improving assembly processes to increase production efficiency and output', 'a quest for improving assembly processes', 'mass production of high-volume goods on moving assembly lines, high-speed printing press, large hydroelectric power production plants, and radio technology']"
1140,user experience,History,Frederick Winslow Taylor and Henry Ford were in the forefront of exploring new ways to make human labor more efficient and productive. Taylor's pioneering research into the efficiency of interactions between workers and their tools is the earliest example that resembles today's user experience fundamentals.,Frederick Winslow Taylor and Henry Ford were in the forefront of exploring new ways to make human labor more efficient and productive. Taylor's pioneering research into the efficiency of interactions between workers and their tools is the earliest example that resembles today's user experience fundamentals.,"[' Who were in the forefront of exploring new ways to make human labor more efficient and productive?', "" Whose research into the efficiency of interactions between workers and tools is the earliest example that resembles today's user experience fundamentals?""]","['Frederick Winslow Taylor and Henry Ford', 'Frederick Winslow Taylor']"
1141,user experience,History,"The term user experience was brought to wider knowledge by Donald Norman in the mid-1990s. He never intended the term ""user experience"" to be applied only to the affective aspects of usage. A review of his earlier work suggests that the term ""user experience"" was used to signal a shift to include affective factors, along with the pre-requisite behavioral concerns, which had been traditionally considered in the field. Many usability practitioners continue to research and attend to affective factors associated with end-users, and have been doing so for years, long before the term ""user experience"" was introduced in the mid-1990s. In an interview in 2007, Norman discusses the widespread use of the term ""user experience"" and its imprecise meaning as a consequence thereof.","The term user experience was brought to wider knowledge by Donald Norman in the mid-1990s. He never intended the term ""user experience"" to be applied only to the affective aspects of usage.","[' Who brought the term ""user experience"" to the public?', "" What was Donald Norman's goal in bringing the term user experience to a wider audience?""]","['Donald Norman', 'He never intended the term ""user experience"" to be applied only to the affective aspects of usage']"
1142,user experience,History,"The field of user experience represents an expansion and extension of the field of usability, to include the holistic perspective of how a person feels about using a system. The focus is on pleasure and value as well as on performance. The exact definition, framework, and elements of user experience are still evolving.
","The field of user experience represents an expansion and extension of the field of usability, to include the holistic perspective of how a person feels about using a system. The focus is on pleasure and value as well as on performance.","[' What field represents an expansion and extension of the field of usability?', ' What is the focus of user experience?']","['user experience', 'pleasure and value']"
1143,user experience,History,"User experience of an interactive product or a website is usually measured by a number of methods, including questionnaires, focus groups, observed usability tests and other methods. A freely available questionnaire (available in several languages) is the User Experience Questionnaire (UEQ). The development and validation of this questionnaire is described in a computer science essay published in 2008.","User experience of an interactive product or a website is usually measured by a number of methods, including questionnaires, focus groups, observed usability tests and other methods. A freely available questionnaire (available in several languages) is the User Experience Questionnaire (UEQ).","[' What is the name of a freely available questionnaire?', ' What is UEQ?']","['User Experience Questionnaire', 'User Experience Questionnaire']"
1144,user experience,History,"Google Ngram Viewer shows wide use of the term starting in the 1930s., ""He suggested that more follow-up in the field would be welcomed by the user, and would be a means of incorporating the results of user's experience into the design of new machines."" Use of the term in relation to computer software also pre-dates Norman.","Google Ngram Viewer shows wide use of the term starting in the 1930s., ""He suggested that more follow-up in the field would be welcomed by the user, and would be a means of incorporating the results of user's experience into the design of new machines."" Use of the term in relation to computer software also pre-dates Norman.","[' When did Google Ngram Viewer show wide use of the term Ngram?', ' What did Google suggest that more follow-up in the field would be welcomed by the user?', ' What does the term ""design of new machines"" refer to?', ' What term is used in relation to computer software?']","['starting in the 1930s', ""a means of incorporating the results of user's experience into the design of new machines"", 'computer software', 'Norman']"
1145,user experience,Influences on user experience,"Many factors can influence a user's experience with a system. To address the variety, factors influencing user experience have been classified into three main categories: user's state and previous experience, system properties, and the usage context (situation). Understanding representative users, working environments, interactions and emotional reactions help in designing the system during User experience design.
","Many factors can influence a user's experience with a system. To address the variety, factors influencing user experience have been classified into three main categories: user's state and previous experience, system properties, and the usage context (situation).","["" How many main categories are there to address the variety of factors that can influence a user's experience with a system?"", ' What are the three main categories for factors influencing user experience?']","['three', ""user's state and previous experience, system properties, and the usage context (situation""]"
1146,user experience,Momentary emotion or overall user experience,"Single experiences influence the overall user experience: the experience of a key click affects the experience of typing a text message, the experience of typing a message affects the experience of text messaging, and the experience of text messaging affects the overall user experience with the phone. The overall user experience is not simply a sum of smaller interaction experiences, because some experiences are more salient than others. Overall user experience is also influenced by factors outside the actual interaction episode: brand, pricing, friends' opinions, reports in media, etc.
","Single experiences influence the overall user experience: the experience of a key click affects the experience of typing a text message, the experience of typing a message affects the experience of text messaging, and the experience of text messaging affects the overall user experience with the phone. The overall user experience is not simply a sum of smaller interaction experiences, because some experiences are more salient than others.","[' What affects the overall user experience with the phone?', ' What does the experience of typing a message affect?', ' The experience of text messaging affects what?', ' What is the overall user experience with the phone?', ' What is not simply a sum of smaller interaction experiences?']","['the experience of text messaging', 'the experience of text messaging', 'the overall user experience with the phone', 'The overall user experience is not simply a sum of smaller interaction experiences', 'The overall user experience']"
1147,user experience,Momentary emotion or overall user experience,"One branch in user experience research focuses on emotions. This includes momentary experiences during interaction: designing affective interaction and evaluating emotions. Another branch is interested in understanding the long-term relation between user experience and product appreciation. The industry sees good overall user experience with a company's products as critical for securing brand loyalty and enhancing the growth of the customer base. All temporal levels of user experience (momentary, episodic, and long-term) are important, but the methods to design and evaluate these levels can be very different.
",One branch in user experience research focuses on emotions. This includes momentary experiences during interaction: designing affective interaction and evaluating emotions.,"[' What branch of user experience research focuses on emotions?', ' What are two examples of momentary experiences during interaction?', ' Designing affective interaction and evaluating emotions are examples of what?']","['One', 'designing affective interaction and evaluating emotions', 'momentary experiences']"
1148,user experience,Developer experience,"Developer experience (DX) is a user experience from a developer's point of view. It is defined by the tools, processes, and software that a developer uses when interacting with a product or system while in the process of production of another one, such as in software development. DX has had increased attention paid to it especially in businesses who primarily offer software as a service to other businesses where ease of use is a key differentiator in the market.","Developer experience (DX) is a user experience from a developer's point of view. It is defined by the tools, processes, and software that a developer uses when interacting with a product or system while in the process of production of another one, such as in software development.","[' What does DX stand for?', ' What is a developer experience defined by?']","['Developer experience', 'the tools, processes, and software']"
1149,block cipher,Summary,"In cryptography, a block cipher is a deterministic algorithm operating on fixed-length groups of bits, called blocks. They are specified elementary components in the design of many cryptographic protocols and are widely used to  the encryption of large amounts of data, including data exchange protocols. It uses blocks as an unvarying transformation.
","In cryptography, a block cipher is a deterministic algorithm operating on fixed-length groups of bits, called blocks. They are specified elementary components in the design of many cryptographic protocols and are widely used to  the encryption of large amounts of data, including data exchange protocols.","[' What is a block cipher?', ' What is an example of a data exchange protocol?']","['a deterministic algorithm operating on fixed-length groups of bits', 'blocks']"
1150,block cipher,Summary,"Even a secure block cipher is suitable for the encryption of only a single block of data at a time, using a fixed key. A multitude of modes of operation have been designed to allow their repeated use in a secure way to achieve the security goals of confidentiality and authenticity. However, block ciphers may also feature as building blocks in other cryptographic protocols, such as universal hash functions and pseudorandom number generators.
","Even a secure block cipher is suitable for the encryption of only a single block of data at a time, using a fixed key. A multitude of modes of operation have been designed to allow their repeated use in a secure way to achieve the security goals of confidentiality and authenticity.","[' What is suitable for the encryption of only a single block of data at a time?', ' What has been designed to allow their repeated use in a secure way?', ' What are the two goals of confidentiality and authenticity?']","['a secure block cipher', 'A multitude of modes of operation', 'security']"
1151,block cipher,Definition,"A block cipher consists of two paired algorithms, one for encryption, E, and the other for decryption, D. Both algorithms accept two inputs: an input block of size n bits and a key of size k bits; and both yield an n-bit output block. The decryption algorithm D is defined to be the inverse function of encryption, i.e., D = E−1. More formally, a block cipher is specified by an encryption function
","A block cipher consists of two paired algorithms, one for encryption, E, and the other for decryption, D. Both algorithms accept two inputs: an input block of size n bits and a key of size k bits; and both yield an n-bit output block. The decryption algorithm D is defined to be the inverse function of encryption, i.e., D = E−1.","[' How many paired algorithms make up a block cipher?', ' What is the other algorithm for decryption?', ' How many inputs do both algorithms accept?', ' Which algorithm is defined?', ' What is an n-bit output block?', ' What is defined to be the inverse function of encryption?']","['two', 'D', 'two', 'D', 'an input block of size n bits and a key of size k bits', 'D']"
1152,block cipher,Definition,"which takes as input a key K, of bit length k (called the key size), and a bit string P, of length n (called the block size), and returns a string C of n bits. P is called the plaintext, and C is termed the ciphertext. For each K, the function EK(P) is required to be an invertible mapping on {0,1}n. The inverse for E is defined as a function
","which takes as input a key K, of bit length k (called the key size), and a bit string P, of length n (called the block size), and returns a string C of n bits. P is called the plaintext, and C is termed the ciphertext.","[' What is a key K of bit length k called?', ' What is another name for a bit string P of length n?', ' P is called what?']","['the key size', 'the block size', 'the plaintext']"
1153,block cipher,Definition,"For example, a block cipher encryption algorithm might take a 128-bit block of plaintext as input, and output a corresponding 128-bit block of ciphertext. The exact transformation is controlled using a second input – the secret key. Decryption is similar: the decryption algorithm takes, in this example, a 128-bit block of ciphertext together with the secret key, and yields the original 128-bit block of plain text.","For example, a block cipher encryption algorithm might take a 128-bit block of plaintext as input, and output a corresponding 128-bit block of ciphertext. The exact transformation is controlled using a second input – the secret key.","[' What might a block cipher encryption algorithm take as input?', ' What is controlled using a second input - the secret key?']","['a 128-bit block of plaintext', 'The exact transformation']"
1154,block cipher,Definition,"For each key K, EK is a permutation (a bijective mapping) over the set of input blocks. Each key selects one permutation from the set of 



(

2

n


)
!


{\displaystyle (2^{n})!}
 possible permutations.","For each key K, EK is a permutation (a bijective mapping) over the set of input blocks. Each key selects one permutation from the set of 



(

2

n


)
!","[' What is EK?', ' How many permutations does each key select from the set of input blocks?']","['a permutation (a bijective mapping) over the set of input blocks', 'one']"
1155,block cipher,History,"The modern design of block ciphers is based on the concept of an iterated product cipher. In his seminal 1949 publication, Communication Theory of Secrecy Systems, Claude Shannon analyzed product ciphers and suggested them as a means of effectively improving security by combining simple operations such as substitutions and permutations. Iterated product ciphers carry out encryption in multiple rounds, each of which uses a different subkey derived from the original key. One widespread implementation of such ciphers, named a Feistel network after Horst Feistel, is notably implemented in the DES cipher. Many other realizations of block ciphers, such as the AES, are classified as substitution–permutation networks.","The modern design of block ciphers is based on the concept of an iterated product cipher. In his seminal 1949 publication, Communication Theory of Secrecy Systems, Claude Shannon analyzed product ciphers and suggested them as a means of effectively improving security by combining simple operations such as substitutions and permutations.","[' What is the modern design of block ciphers based on?', "" What was the title of Claude Shannon's 1949 publication?""]","['an iterated product cipher', 'Communication Theory of Secrecy Systems']"
1156,block cipher,History,"The root of all cryptographic block formats used within the Payment Card Industry Data Security Standard (PCI DSS) and American National Standards Institute (ANSI) standards lies with the Atalla Key Block (AKB), which was a key innovation of the Atalla Box, the first hardware security module (HSM). It was developed in 1972 by Mohamed M. Atalla, founder of Atalla Corporation (now Utimaco Atalla), and released in 1973. The AKB was a key block, which is required to securely interchange symmetric keys or PINs with other actors of the banking industry. This secure interchange is performed using the AKB format. The Atalla Box protected over 90% of all ATM networks in operation as of 1998, and Atalla products still secure the majority of the world's ATM transactions as of 2014.","The root of all cryptographic block formats used within the Payment Card Industry Data Security Standard (PCI DSS) and American National Standards Institute (ANSI) standards lies with the Atalla Key Block (AKB), which was a key innovation of the Atalla Box, the first hardware security module (HSM). It was developed in 1972 by Mohamed M. Atalla, founder of Atalla Corporation (now Utimaco Atalla), and released in 1973.","[' What is the root of all cryptographic block formats used in PCI DSS and ANSI standards?', ' What was the first hardware security module called?', ' What is the name of the first hardware security module?', ' When was Box developed?', ' Who was the founder of Atalla Corporation?', ' What year was Box released?']","['Atalla Key Block', 'Atalla Box', 'Atalla Box', '1972', 'Mohamed M. Atalla', '1973']"
1157,block cipher,History,"The publication of the DES cipher by the United States National Bureau of Standards (subsequently the U.S. National Institute of Standards and Technology, NIST) in 1977 was fundamental in the public understanding of modern block cipher design. It also influenced the academic development of cryptanalytic attacks. Both differential and linear cryptanalysis arose out of studies on the DES design. As of 2016 there is a palette of attack techniques against which a block cipher must be secure, in addition to being robust against brute-force attacks.
","The publication of the DES cipher by the United States National Bureau of Standards (subsequently the U.S. National Institute of Standards and Technology, NIST) in 1977 was fundamental in the public understanding of modern block cipher design. It also influenced the academic development of cryptanalytic attacks.","[' When was the DES cipher published?', ' What was the name of the cryptanalytic attack that was influenced by the publication of DES?']","['1977', 'academic development']"
1158,block cipher,Modes of operation,"A block cipher by itself allows encryption only of a single data block of the cipher's block length. For a variable-length message, the data must first be partitioned into separate cipher blocks. In the simplest case, known as electronic codebook (ECB) mode, a message is first split into separate blocks of the cipher's block size (possibly extending the last block with padding bits), and then each block is encrypted and decrypted independently. However, such a naive method is generally insecure because equal plaintext blocks will always generate equal ciphertext blocks (for the same key), so patterns in the plaintext message become evident in the ciphertext output.","A block cipher by itself allows encryption only of a single data block of the cipher's block length. For a variable-length message, the data must first be partitioned into separate cipher blocks.","["" What allows encryption of only a single data block of the cipher's block length?"", ' For a variable-length message, the data must first be partitioned into what?']","['A block cipher', 'separate cipher blocks']"
1159,block cipher,Modes of operation,"To overcome this limitation, several so called block cipher modes of operation have been designed and specified in national recommendations such as NIST 800-38A and BSI TR-02102 and international standards such as ISO/IEC 10116. The general concept is to use randomization of the plaintext data based on an additional input value, frequently called an initialization vector, to create what is termed probabilistic encryption. In the popular cipher block chaining (CBC) mode, for encryption to be secure the initialization vector passed along with the plaintext message must be a random or pseudo-random value, which is added in an exclusive-or manner to the first plaintext block before it is being encrypted. The resultant ciphertext block is then used as the new initialization vector for the next plaintext block. In the cipher feedback (CFB) mode, which emulates a self-synchronizing stream cipher, the initialization vector is first encrypted and then added to the plaintext block. The output feedback (OFB) mode repeatedly encrypts the initialization vector to create a key stream for the emulation of a synchronous stream cipher. The newer counter (CTR) mode similarly creates a key stream, but has the advantage of only needing unique and not (pseudo-)random values as initialization vectors; the needed randomness is derived internally by using the initialization vector as a block counter and encrypting this counter for each block.","To overcome this limitation, several so called block cipher modes of operation have been designed and specified in national recommendations such as NIST 800-38A and BSI TR-02102 and international standards such as ISO/IEC 10116. The general concept is to use randomization of the plaintext data based on an additional input value, frequently called an initialization vector, to create what is termed probabilistic encryption.","[' What are some of the national recommendations for block cipher modes of operation?', ' What is the general concept of using randomization of plaintext data based on additional input?', ' Randomization of plaintext data based on an additional input value is called what?', ' What is the creation of what is termed probabilistic encryption?']","['NIST 800-38A and BSI TR-02102', 'probabilistic encryption', 'initialization vector', 'randomization of the plaintext data']"
1160,block cipher,Modes of operation,"From a security-theoretic point of view, modes of operation must provide what is known as semantic security. Informally, it means that given some ciphertext under an unknown key one cannot practically derive any information from the ciphertext (other than the length of the message) over what one would have known without seeing the ciphertext. It has been shown that all of the modes discussed above, with the exception of the ECB mode, provide this property under so-called chosen plaintext attacks.
","From a security-theoretic point of view, modes of operation must provide what is known as semantic security. Informally, it means that given some ciphertext under an unknown key one cannot practically derive any information from the ciphertext (other than the length of the message) over what one would have known without seeing the ciphertext.","[' What must modes of operation provide from a security-theoretic point of view?', ' What is known as semantic security?', ' Informally, it means that given some ciphertext under an unknown key one cannot practically derive any information from what?', ' What is the length of the message?', ' What would one have known without seeing the ciphertext?']","['semantic security', 'modes of operation', 'the ciphertext', 'length', 'length of the message']"
1161,block cipher,Padding,"Some modes such as the CBC mode only operate on complete plaintext blocks. Simply extending the last block of a message with zero-bits is insufficient since it does not allow a receiver to easily distinguish messages that differ only in the amount of padding bits. More importantly, such a simple solution gives rise to very efficient padding oracle attacks. A suitable padding scheme is therefore needed to extend the last plaintext block to the cipher's block size. While many popular schemes described in standards and in the literature have been shown to be vulnerable to padding oracle attacks, a solution which adds a one-bit and then extends the last block with zero-bits, standardized as ""padding method 2"" in ISO/IEC 9797-1, has been proven secure against these attacks.",Some modes such as the CBC mode only operate on complete plaintext blocks. Simply extending the last block of a message with zero-bits is insufficient since it does not allow a receiver to easily distinguish messages that differ only in the amount of padding bits.,"[' Some modes such as the CBC mode only operate on what?', ' What is insufficient since it does not allow a receiver to easily distinguish messages that differ only in the amount of padding bits?']","['complete plaintext blocks', 'Simply extending the last block of a message with zero-bits']"
1162,block cipher,Provable security,"When a block cipher is used in a given mode of operation, the resulting algorithm should ideally be about as secure as the block cipher itself. ECB (discussed above) emphatically lacks this property: regardless of how secure the underlying block cipher is, ECB mode can easily be attacked. On the other hand, CBC mode can be proven to be secure under the assumption that the underlying block cipher is likewise secure. Note, however, that making statements like this requires formal mathematical definitions for what it means for an encryption algorithm or a block cipher to ""be secure"". This section describes two common notions for what properties a block cipher should have. Each corresponds to a mathematical model that can be used to prove properties of higher level algorithms, such as CBC.
","When a block cipher is used in a given mode of operation, the resulting algorithm should ideally be about as secure as the block cipher itself. ECB (discussed above) emphatically lacks this property: regardless of how secure the underlying block cipher is, ECB mode can easily be attacked.","[' What should the resulting algorithm be about as secure as when a block cipher is used in a given mode of operation?', ' The ECB lacks what property?']","['the block cipher itself', 'regardless of how secure the underlying block cipher is, ECB mode can easily be attacked']"
1163,block cipher,Practical evaluation,Block ciphers may be evaluated according to multiple criteria in practice. Common factors include:,Block ciphers may be evaluated according to multiple criteria in practice. Common factors include:,[' What may be evaluated according to multiple criteria in practice?'],['Block ciphers']
1164,block cipher,Relation to other cryptographic primitives,"Block ciphers can be used to build other cryptographic primitives, such as those below. For these other primitives to be cryptographically secure, care has to be taken to build them the right way.
","Block ciphers can be used to build other cryptographic primitives, such as those below. For these other primitives to be cryptographically secure, care has to be taken to build them the right way.","[' What can block ciphers be used to build?', ' What must be taken to build other cryptographic primitives?']","['other cryptographic primitives', 'care has to be taken to build them the right way']"
1165,block cipher,Relation to other cryptographic primitives,"Just as block ciphers can be used to build hash functions, hash functions can be used to build block ciphers. Examples of such block ciphers are SHACAL, BEAR and LION.
","Just as block ciphers can be used to build hash functions, hash functions can be used to build block ciphers. Examples of such block ciphers are SHACAL, BEAR and LION.","[' What can block ciphers be used to build?', ' What can hash functions be used for?', ' SHACAL, BEAR and LION are examples of what?']","['hash functions', 'build block ciphers', 'block ciphers']"
1166,gaussian mixture model,Summary,"In statistics, a mixture model is a probabilistic model for representing the presence of subpopulations within an overall population, without requiring that an observed data set should identify the sub-population to which an individual observation belongs. Formally a mixture model corresponds to the mixture distribution that represents the probability distribution of observations in the overall population. However, while problems associated with ""mixture distributions"" relate to deriving the properties of the overall population from those of the sub-populations, ""mixture models"" are used to make statistical inferences about the properties of the sub-populations given only observations on the pooled population, without sub-population identity information.
","In statistics, a mixture model is a probabilistic model for representing the presence of subpopulations within an overall population, without requiring that an observed data set should identify the sub-population to which an individual observation belongs. Formally a mixture model corresponds to the mixture distribution that represents the probability distribution of observations in the overall population.","[' What is a mixture model in statistics?', ' What is not required to identify the sub-population to which an individual observation belongs?', ' Which distribution represents the probability distribution of observations in the overall population?']","['a probabilistic model for representing the presence of subpopulations within an overall population', 'an observed data set', 'mixture distribution']"
1167,gaussian mixture model,Summary,"Mixture models should not be confused with models for compositional data, i.e., data whose components are constrained to sum to a constant value (1, 100%, etc.). However, compositional models can be thought of as mixture models, where members of the population are sampled at random. Conversely, mixture models can be thought of as compositional models, where the total size reading population has been normalized to 1.
","Mixture models should not be confused with models for compositional data, i.e., data whose components are constrained to sum to a constant value (1, 100%, etc.). However, compositional models can be thought of as mixture models, where members of the population are sampled at random.","[' What model should not be confused with models for compositional data?', ' What model can be thought of as mixture models?', ' Where members of the population are sampled at random?']","['Mixture models', 'compositional models', 'Mixture models']"
1168,gaussian mixture model,Identifiability,"Identifiability refers to the existence of a unique characterization for any one of the models in the class (family) being considered. Estimation procedures may not be well-defined and asymptotic theory may not hold if a model is not identifiable.
",Identifiability refers to the existence of a unique characterization for any one of the models in the class (family) being considered. Estimation procedures may not be well-defined and asymptotic theory may not hold if a model is not identifiable.,"[' What refers to the existence of a unique characterization for any one of the models in the class being considered?', ' Estimation procedures may not be well-defined and what theory may not hold if a model is not identifiable?']","['Identifiability', 'asymptotic']"
1169,gaussian mixture model,Parameter estimation and system identification,"Parametric mixture models are often used when we know the distribution Y and we can sample from X, but we would like to determine the ai and θi values.  Such situations can arise in studies in which we sample from a population that is composed of several distinct subpopulations.
","Parametric mixture models are often used when we know the distribution Y and we can sample from X, but we would like to determine the ai and θi values. Such situations can arise in studies in which we sample from a population that is composed of several distinct subpopulations.","[' Parametric mixture models are often used when we know the distribution Y and we can sample from what?', ' When do we want to determine the ai and <unk>i values?']","['X', 'Parametric mixture models are often used when we know the distribution Y and we can sample from X, but we would like to determine the ai and θi values. Such situations can arise in studies in which we sample from a population that is composed of several distinct subpopulations']"
1170,gaussian mixture model,Parameter estimation and system identification,"It is common to think of probability mixture modeling as a missing data problem.  One way to understand this is to assume that the data points under consideration have ""membership"" in one of the distributions we are using to model the data.  When we start, this membership is unknown, or missing.  The job of estimation is to devise appropriate parameters for the model functions we choose, with the connection to the data points being represented as their membership in the individual model distributions.
","It is common to think of probability mixture modeling as a missing data problem. One way to understand this is to assume that the data points under consideration have ""membership"" in one of the distributions we are using to model the data.","[' What is common to think of probability mixture modeling as a missing data problem?', ' What is one way to understand this?']","['It is common to think of probability mixture modeling as a missing data problem. One way to understand this is to assume that the data points under consideration have ""membership"" in one of the distributions we are using to model the data', 'to assume that the data points under consideration have ""membership"" in one of the distributions we are using to model the data']"
1171,gaussian mixture model,Parameter estimation and system identification,"A variety of approaches to the problem of mixture decomposition have been proposed, many of which focus on maximum likelihood methods such as expectation maximization (EM) or maximum a posteriori estimation (MAP).  Generally these methods consider separately the questions of system identification and parameter estimation; methods to determine the number and functional form of components within a mixture are distinguished from methods to estimate the corresponding parameter values.  Some notable departures are the graphical methods as outlined in Tarter and Lock and more recently minimum message length (MML) techniques such as Figueiredo and Jain and to some extent the moment matching pattern analysis routines suggested by McWilliam and Loh (2009).","A variety of approaches to the problem of mixture decomposition have been proposed, many of which focus on maximum likelihood methods such as expectation maximization (EM) or maximum a posteriori estimation (MAP). Generally these methods consider separately the questions of system identification and parameter estimation; methods to determine the number and functional form of components within a mixture are distinguished from methods to estimate the corresponding parameter values.","[' What are some of the approaches to the problem of mixture decomposition?', ' What are expectation maximization and maximum a posteriori estimation?', ' What are two methods to determine the number and functional form of components within a mixture distinguished from methods to estimate the corresponding parameter values?']","['expectation maximization (EM) or maximum a posteriori estimation (MAP).', 'maximum likelihood methods', 'expectation maximization (EM) or maximum a posteriori estimation']"
1172,gaussian mixture model,Extensions,"In a Bayesian setting, additional levels can be added to the graphical model defining the mixture model.  For example, in the common latent Dirichlet allocation topic model, the observations are sets of words drawn from D different documents and the K mixture components represent topics that are shared across documents.  Each document has a different set of mixture weights, which specify the topics prevalent in that document.  All sets of mixture weights share common hyperparameters.
","In a Bayesian setting, additional levels can be added to the graphical model defining the mixture model. For example, in the common latent Dirichlet allocation topic model, the observations are sets of words drawn from D different documents and the K mixture components represent topics that are shared across documents.","[' What can be added to the graphical model in a Bayesian setting?', ' What are the observations drawn from in the common latent Dirichlet allocation topic model?', ' The K mixture components represent what?']","['additional levels', 'D different documents', 'topics that are shared across documents']"
1173,gaussian mixture model,Extensions,"A very common extension is to connect the latent variables defining the mixture component identities into a Markov chain, instead of assuming that they are independent identically distributed random variables.  The resulting model is termed a hidden Markov model and is one of the most common sequential hierarchical models.  Numerous extensions of hidden Markov models have been developed; see the resulting article for more information.
","A very common extension is to connect the latent variables defining the mixture component identities into a Markov chain, instead of assuming that they are independent identically distributed random variables. The resulting model is termed a hidden Markov model and is one of the most common sequential hierarchical models.","[' What is the resulting model called?', ' What is one of the most common sequential hierarchical models?']","['hidden Markov model', 'hidden Markov model']"
1174,gaussian mixture model,History,"Mixture distributions and the problem of mixture decomposition, that is the identification of its constituent components and the parameters thereof, has been cited in the literature as far back as 1846 (Quetelet in McLachlan, 2000) although common reference is made to the work of Karl Pearson (1894) as the first author to explicitly address the decomposition problem in characterising non-normal attributes of forehead to body length ratios in female shore crab populations.  The motivation for this work was provided by the zoologist Walter Frank Raphael Weldon who had speculated in 1893 (in Tarter and Lock) that asymmetry in the histogram of these ratios could signal evolutionary divergence. Pearson's approach was to fit a univariate mixture of two normals to the data by choosing the five parameters of the mixture such that the empirical moments matched that of the model.
","Mixture distributions and the problem of mixture decomposition, that is the identification of its constituent components and the parameters thereof, has been cited in the literature as far back as 1846 (Quetelet in McLachlan, 2000) although common reference is made to the work of Karl Pearson (1894) as the first author to explicitly address the decomposition problem in characterising non-normal attributes of forehead to body length ratios in female shore crab populations. The motivation for this work was provided by the zoologist Walter Frank Raphael Weldon who had speculated in 1893 (in Tarter and Lock) that asymmetry in the histogram of these ratios could signal evolutionary divergence.","[' What is the name of the problem of mixture decomposition?', ' When was mixture distributions cited in the literature?', ' Who was the first person to work on mixture distribution?', ' What year was Karl Pearson born?', ' Who was the first author to address the decomposition problem in characterising non-normal attributes of forehead to body length ratios in female shore crab populations?', ' Who provided the motivation for this work?', ' Who speculated that asymmetry in the histogram of these ratios could signal evolutionary divergence?', ' What did Walter Frank Raphael Weldon speculate in 1893?']","['the identification of its constituent components and the parameters thereof', '1846', 'Karl Pearson', '1894', 'Karl Pearson', 'Walter Frank Raphael Weldon', 'Walter Frank Raphael Weldon', 'asymmetry in the histogram of these ratios could signal evolutionary divergence']"
1175,gaussian mixture model,History,"Subsequent works focused on addressing these problems, but it was not until the advent of the modern computer and the popularisation of Maximum Likelihood (MLE) parameterisation techniques that research really took off. Since that time there has been a vast body of research on the subject spanning areas such as fisheries research, agriculture, botany, economics, medicine, genetics, psychology, palaeontology, electrophoresis, finance, geology and zoology.","Subsequent works focused on addressing these problems, but it was not until the advent of the modern computer and the popularisation of Maximum Likelihood (MLE) parameterisation techniques that research really took off. Since that time there has been a vast body of research on the subject spanning areas such as fisheries research, agriculture, botany, economics, medicine, genetics, psychology, palaeontology, electrophoresis, finance, geology and zoology.","[' When did research on MLE start taking off?', ' What is MLE?', ' Fisheries research, agriculture, botany, economics, medicine, genetics, psychology, palaeontology, electrophoresis, finance, geology and zoology are examples of what?']","['the advent of the modern computer', 'Maximum Likelihood', 'fisheries research']"
1176,artificial neural network,Summary,"
An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron receives a signal then processes it and can signal neurons connected to it. The ""signal"" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.","
An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons.","[' What is an ANN based on?', ' What do artificial neurons loosely model?']","['a collection of connected units or nodes called artificial neurons', 'the neurons in a biological brain']"
1177,artificial neural network,Training,"Neural networks learn (or are trained) by processing examples, each of which contains a known ""input"" and ""result,"" forming probability-weighted associations between the two, which are stored within the data structure of the net itself. The training of a neural network from a given example is usually conducted by determining the difference between the processed output of the network (often a prediction) and a target output. This difference is the error. The network then adjusts its weighted associations according to a learning rule and using this error value. Successive adjustments will cause the neural network to produce output which is increasingly similar to the target output. After a sufficient number of these adjustments the training can be terminated based upon certain criteria. This is known as supervised learning.
","Neural networks learn (or are trained) by processing examples, each of which contains a known ""input"" and ""result,"" forming probability-weighted associations between the two, which are stored within the data structure of the net itself. The training of a neural network from a given example is usually conducted by determining the difference between the processed output of the network (often a prediction) and a target output.","[' What do neural networks learn by processing?', ' What are the associations between input and result stored in?', ' How are neural networks trained?', ' What is usually conducted by determining the difference between the processed output of the network and a target output?']","['examples', 'the data structure of the net itself', 'by processing examples', 'The training of a neural network']"
1178,artificial neural network,Training,"Such systems ""learn"" to perform tasks by considering examples, generally without being programmed with task-specific rules. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as ""cat"" or ""no cat"" and using the results to identify cats in other images. They do this without any prior knowledge of cats, for example, that they have fur, tails, whiskers, and cat-like faces. Instead, they automatically generate identifying characteristics from the examples that they process.
","Such systems ""learn"" to perform tasks by considering examples, generally without being programmed with task-specific rules. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as ""cat"" or ""no cat"" and using the results to identify cats in other images.","[' How do image recognition systems learn to perform tasks?', ' What does an image recognition system do without being programmed with?', ' What is another name for a cat?', ' What is a way to identify cats?']","['by considering examples', 'learn to identify images that contain cats by analyzing example images that have been manually labeled as ""cat"" or ""no cat"" and using the results to identify cats in other images', 'no cat', 'image recognition']"
1179,artificial neural network,History,"Warren McCulloch and Walter Pitts (1943) opened the subject by creating a computational model for neural networks. In the late 1940s, D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Farley and Wesley A. Clark (1954) first used computational machines, then called ""calculators"", to simulate a Hebbian network. In 1958, psychologist Frank Rosenblatt invented the perceptron, the first artificial neural network, funded by the United States Office of Naval Research. The first functional networks with many layers were published by Ivakhnenko and Lapa in 1965, as the Group Method of Data Handling. The basics of continuous backpropagation were derived in the context of control theory by Kelley in 1960 and by Bryson in 1961, using principles of dynamic programming. Thereafter research stagnated following Minsky and Papert (1969), who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to process useful neural networks.
","Warren McCulloch and Walter Pitts (1943) opened the subject by creating a computational model for neural networks. In the late 1940s, D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning.","[' In what year did Warren McCulloch and Walter Pitts create a computational model for neural networks?', ' Who created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning?']","['1943', 'D. O. Hebb']"
1180,artificial neural network,History,"In 1970, Seppo Linnainmaa published the general method for automatic differentiation (AD) of discrete connected networks of nested differentiable functions. In 1973, Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients. Werbos's (1975) backpropagation algorithm enabled practical training of multi-layer networks. In 1982, he applied Linnainmaa's AD method to neural networks in the way that became widely used.","In 1970, Seppo Linnainmaa published the general method for automatic differentiation (AD) of discrete connected networks of nested differentiable functions. In 1973, Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients.","[' In what year did Seppo Linnainmaa publish the general method for automatic differentiation of discrete connected networks of nested differentiable functions?', ' Who used backpropagation to adapt parameters of controllers in proportion to error gradients?']","['1970', 'Dreyfus']"
1181,artificial neural network,History,"The development of metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI), in the form of complementary MOS (CMOS) technology, enabled increasing MOS transistor counts in digital electronics. This provided more processing power for the development of practical artificial neural networks in the 1980s.","The development of metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI), in the form of complementary MOS (CMOS) technology, enabled increasing MOS transistor counts in digital electronics. This provided more processing power for the development of practical artificial neural networks in the 1980s.","[' What is another name for metal-oxide-semiconductor (MOS) very-large-scale integration?', ' What technology enabled increasing MOS transistor counts in digital electronics?', ' In what decade did the development of practical artificial neural networks occur?']","['VLSI', 'metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI), in the form of complementary MOS (CMOS)', '1980s']"
1182,artificial neural network,History,"In 1992, max-pooling was introduced to help with least-shift invariance and tolerance to deformation to aid 3D object recognition. Schmidhuber adopted a multi-level hierarchy of networks (1992) pre-trained one level at a time by unsupervised learning and fine-tuned by backpropagation.","In 1992, max-pooling was introduced to help with least-shift invariance and tolerance to deformation to aid 3D object recognition. Schmidhuber adopted a multi-level hierarchy of networks (1992) pre-trained one level at a time by unsupervised learning and fine-tuned by backpropagation.","[' When was max-pooling introduced?', ' What was introduced to help with least-shift invariance and tolerance to deformation to aid 3D object recognition?', ' Who adopted a multi-level hierarchy of networks (1992) pre-trained one level at a time by unsupervised learning?']","['1992', 'max-pooling', 'Schmidhuber']"
1183,artificial neural network,History,"Geoffrey Hinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine to model each layer. In 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images. Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks, particularly in image and visual recognition problems, which became known as ""deep learning"".",Geoffrey Hinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine to model each layer.,[' Geoffrey Hinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine to model each layer?'],['binary or real-valued latent variables with a restricted Boltzmann machine to model each layer.']
1184,artificial neural network,History,"Ciresan and colleagues (2010) showed that despite the vanishing gradient problem, GPUs make backpropagation feasible for many-layered feedforward neural networks. Between 2009 and 2012, ANNs began winning prizes in image recognition contests, approaching human level performance on various tasks, initially in pattern recognition and handwriting recognition. For example, the bi-directional and multi-dimensional long short-term memory (LSTM) of Graves et al. won three competitions in connected handwriting recognition in 2009 without any prior knowledge about the three languages to be learned.","Ciresan and colleagues (2010) showed that despite the vanishing gradient problem, GPUs make backpropagation feasible for many-layered feedforward neural networks. Between 2009 and 2012, ANNs began winning prizes in image recognition contests, approaching human level performance on various tasks, initially in pattern recognition and handwriting recognition.","[' Who showed that GPUs make backpropagation feasible for many-layered feedforward neural networks?', ' When did ANNs begin winning prizes in image recognition contests?']","['Ciresan and colleagues', 'Between 2009 and 2012']"
1185,artificial neural network,Models,"ANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with. They soon reoriented towards improving empirical results, mostly abandoning attempts to remain true to their biological precursors. Neurons are connected to each other in various patterns, to allow the output of some neurons to become the input of others. The network forms a directed, weighted graph.","ANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with. They soon reoriented towards improving empirical results, mostly abandoning attempts to remain true to their biological precursors.","[' ANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with.', ' ANN reoriented towards what?']","['ANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with.', 'improving empirical results']"
1186,artificial neural network,Models,"An artificial neural network consists of a collection of simulated neurons. Each neuron is a node which is connected to other nodes via links that correspond to biological axon-synapse-dendrite connections. Each link has a weight, which determines the strength of one node's influence on another.",An artificial neural network consists of a collection of simulated neurons. Each neuron is a node which is connected to other nodes via links that correspond to biological axon-synapse-dendrite connections.,"[' What does an artificial neural network consist of?', ' What is each neuron a node connected to?', ' How are links connected to other nodes?']","['simulated neurons', 'other nodes', 'biological axon-synapse-dendrite connections']"
1187,artificial neural network,Types,"ANNs have evolved into a broad family of techniques that have advanced the state of the art across multiple domains.  The simplest types have one or more static components, including number of units, number of layers, unit weights and topology. Dynamic types allow one or more of these to evolve via learning. The latter are much more complicated, but can shorten learning periods and produce better results. Some types allow/require learning to be ""supervised"" by the operator, while others operate independently. Some types operate purely in hardware, while others are purely software and run on general purpose computers.
","ANNs have evolved into a broad family of techniques that have advanced the state of the art across multiple domains. The simplest types have one or more static components, including number of units, number of layers, unit weights and topology.","[' What has evolved into a broad family of techniques that have advanced the state of the art across multiple domains?', ' The simplest types have one or more static components, including number of units, number of layers, what?']","['ANNs', 'unit weights and topology']"
1188,artificial neural network,Network design,"Neural architecture search (NAS) uses machine learning to automate ANN design. Various approaches to NAS have designed networks that compare well with hand-designed systems. The basic search algorithm is to propose a candidate model, evaluate it against a dataset and use the results as feedback to teach the NAS network. Available systems include AutoML and AutoKeras.",Neural architecture search (NAS) uses machine learning to automate ANN design. Various approaches to NAS have designed networks that compare well with hand-designed systems.,"[' Neural architecture search uses machine learning to automate ANN design.', ' Various approaches to NAS have designed networks that compare well with what?']","['NAS', 'hand-designed systems']"
1189,artificial neural network,Applications,"Because of their ability to reproduce and model nonlinear processes, artificial neural networks have found applications in many disciplines. Application areas include system identification and control (vehicle control, trajectory prediction, process control, natural resource management), quantum chemistry, general game playing, pattern recognition (radar systems, face identification, signal classification, 3D reconstruction, object recognition and more), sensor data analysis, sequence recognition (gesture, speech, handwritten and printed text recognition), medical diagnosis, finance (e.g. automated trading systems), data mining, visualization, machine translation, social network filtering and e-mail spam filtering. ANNs have been used to diagnose several types of cancers and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information.","Because of their ability to reproduce and model nonlinear processes, artificial neural networks have found applications in many disciplines. Application areas include system identification and control (vehicle control, trajectory prediction, process control, natural resource management), quantum chemistry, general game playing, pattern recognition (radar systems, face identification, signal classification, 3D reconstruction, object recognition and more), sensor data analysis, sequence recognition (gesture, speech, handwritten and printed text recognition), medical diagnosis, finance (e.g.","[' Artificial neural networks are able to reproduce and model what kind of processes?', ' What are some application areas of artificial neural networks?', ' What are some examples of pattern recognition?', ' What is one example of a pattern recognition system?']","['nonlinear', 'system identification and control (vehicle control, trajectory prediction, process control, natural resource management),', 'radar systems, face identification, signal classification, 3D reconstruction, object recognition', 'radar systems']"
1190,artificial neural network,Applications,"ANNs have been used to accelerate reliability analysis of infrastructures subject to natural disasters and to predict foundation settlements. ANNs have also been used for building black-box models in geoscience: hydrology, ocean modelling and coastal engineering, and geomorphology. ANNs have been employed in cybersecurity, with the objective to discriminate between legitimate activities and malicious ones. For example, machine learning has been used for classifying Android malware, for identifying domains belonging to threat actors and for detecting URLs posing a security risk. Research is underway on ANN systems designed for penetration testing, for detecting botnets, credit cards frauds and network intrusions.
","ANNs have been used to accelerate reliability analysis of infrastructures subject to natural disasters and to predict foundation settlements. ANNs have also been used for building black-box models in geoscience: hydrology, ocean modelling and coastal engineering, and geomorphology.","[' What has been used to accelerate reliability analysis of infrastructures subject to natural disasters?', ' What have ANNs also been used for building black-box models in?']","['ANNs', 'geoscience']"
1191,artificial neural network,Applications,"ANNs have been proposed as a tool to solve partial differential equations in physics and simulate the properties of many-body open quantum systems. In brain research ANNs have studied short-term behavior of individual neurons, the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems. Studies considered long-and short-term plasticity of neural systems and their relation to learning and memory from the individual neuron to the system level.
","ANNs have been proposed as a tool to solve partial differential equations in physics and simulate the properties of many-body open quantum systems. In brain research ANNs have studied short-term behavior of individual neurons, the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems.","[' What has been proposed as a tool to solve partial differential equations in physics?', ' What have ANNs studied in brain research?', ' The dynamics of neural circuitry arise from interactions between what?', ' How can behavior arise from abstract neural modules that represent complete subsystems?']","['ANNs', 'short-term behavior of individual neurons', 'individual neurons', 'interactions between individual neurons']"
1192,fpga,Summary,"A field-programmable gate array (FPGA) is an integrated circuit designed to be configured by a customer or a designer after manufacturing – hence the term field-programmable. The FPGA configuration is generally specified using a hardware description language (HDL), similar to that used for an application-specific integrated circuit (ASIC). Circuit diagrams were previously used to specify the configuration, but this is increasingly rare due to the advent of electronic design automation tools.
","A field-programmable gate array (FPGA) is an integrated circuit designed to be configured by a customer or a designer after manufacturing – hence the term field-programmable. The FPGA configuration is generally specified using a hardware description language (HDL), similar to that used for an application-specific integrated circuit (ASIC).","[' What is another name for a field-programmable gate array?', ' What is the term for an integrated circuit designed to be configured by a customer or a designer after manufacturing?', ' How is a FPGA configuration specified?']","['FPGA', 'field-programmable gate array', 'using a hardware description language']"
1193,fpga,Summary,"FPGAs contain an array of programmable logic blocks, and a hierarchy of reconfigurable interconnects allowing blocks to be wired together. Logic blocks can be configured to perform complex combinational functions, or act as simple logic gates like AND and XOR. In most FPGAs, logic blocks also include memory elements, which may be simple flip-flops or more complete blocks of memory. Many FPGAs can be reprogrammed to implement different logic functions, allowing flexible reconfigurable computing as performed in computer software.
","FPGAs contain an array of programmable logic blocks, and a hierarchy of reconfigurable interconnects allowing blocks to be wired together. Logic blocks can be configured to perform complex combinational functions, or act as simple logic gates like AND and XOR.","[' What type of FPGAs contain an array of programmable logic blocks?', ' A hierarchy of reconfigurable interconnects allows blocks to be wired together?', ' Logic blocks can be configured to perform what?']","['FPGAs', 'FPGAs', 'complex combinational functions']"
1194,fpga,History,"The FPGA industry sprouted from programmable read-only memory (PROM) and programmable logic devices (PLDs). PROMs and PLDs both had the option of being programmed in batches in a factory or in the field (field-programmable). However, programmable logic was hard-wired between logic gates.",The FPGA industry sprouted from programmable read-only memory (PROM) and programmable logic devices (PLDs). PROMs and PLDs both had the option of being programmed in batches in a factory or in the field (field-programmable).,"[' What was PROM?', ' What were PLDs?', ' Where could PROMs be programmed?']","['programmable read-only memory', 'programmable logic devices', 'in batches in a factory or in the field']"
1195,fpga,History,"Xilinx co-founders Ross Freeman and Bernard Vonderschmitt invented the first commercially viable field-programmable gate array in 1985 – the XC2064. The XC2064 had programmable gates and programmable interconnects between gates, the beginnings of a new technology and market. The XC2064 had 64 configurable logic blocks (CLBs), with two three-input lookup tables (LUTs). More than 20 years later, Freeman was entered into the National Inventors Hall of Fame for his invention.","Xilinx co-founders Ross Freeman and Bernard Vonderschmitt invented the first commercially viable field-programmable gate array in 1985 – the XC2064. The XC2064 had programmable gates and programmable interconnects between gates, the beginnings of a new technology and market.","[' When was the first commercially viable field-programmable gate array invented?', ' Who were the co-founders of Xilinx?', ' What did the XC2064 have?']","['1985', 'Ross Freeman and Bernard Vonderschmitt', 'programmable gates and programmable interconnects between gates']"
1196,fpga,History,"In 1987, the Naval Surface Warfare Center funded an experiment proposed by Steve Casselman to develop a computer that would implement 600,000 reprogrammable gates. Casselman was successful and a patent related to the system was issued in 1992.","In 1987, the Naval Surface Warfare Center funded an experiment proposed by Steve Casselman to develop a computer that would implement 600,000 reprogrammable gates. Casselman was successful and a patent related to the system was issued in 1992.","[' In what year did the Naval Surface Warfare Center fund an experiment to develop a computer that would implement 600,000 reprogrammable gates?', ' What did Steve Casselman propose to develop in 1987?', ' When was a patent related to the system issued?']","['1987', 'a computer', '1992']"
1197,fpga,History,"Altera and Xilinx continued unchallenged and quickly grew from 1985 to the mid-1990s when competitors sprouted up, eroding a significant portion of their market share. By 1993, Actel (now Microsemi) was serving about 18 percent of the market.","Altera and Xilinx continued unchallenged and quickly grew from 1985 to the mid-1990s when competitors sprouted up, eroding a significant portion of their market share. By 1993, Actel (now Microsemi) was serving about 18 percent of the market.","[' When did Altera and Xilinx grow unchallenged?', ' When did competitors sprout up?', ' What was Actel serving in 1993?']","['1985 to the mid-1990s', 'mid-1990s', '18 percent of the market']"
1198,fpga,History,"The 1990s were a period of rapid growth for FPGAs, both in circuit sophistication and the volume of production. In the early 1990s, FPGAs were primarily used in telecommunications and networking. By the end of the decade, FPGAs found their way into consumer, automotive, and industrial applications.","The 1990s were a period of rapid growth for FPGAs, both in circuit sophistication and the volume of production. In the early 1990s, FPGAs were primarily used in telecommunications and networking.","[' What was a period of rapid growth for FPGAs in the 1990s?', ' In the early 1990s, what was primarily used in?']","['The 1990s', 'telecommunications and networking']"
1199,fpga,History,"Companies like Microsoft have started to use FPGAs to accelerate high-performance, computationally intensive systems (like the data centers that operate their Bing search engine), due to the performance per watt advantage FPGAs deliver. Microsoft began using FPGAs to accelerate Bing in 2014, and in 2018 began deploying FPGAs across other data center workloads for their Azure cloud computing platform.","Companies like Microsoft have started to use FPGAs to accelerate high-performance, computationally intensive systems (like the data centers that operate their Bing search engine), due to the performance per watt advantage FPGAs deliver. Microsoft began using FPGAs to accelerate Bing in 2014, and in 2018 began deploying FPGAs across other data center workloads for their Azure cloud computing platform.","[' Companies like Microsoft have started to use FPGAs to accelerate what?', ' Why have companies started using FPGA to accelerate high-performance, computational intensive systems?', ' What is the advantage per watt that FPGA deliver?', ' When did Microsoft begin using Bing to accelerate?', ' How did Microsoft start deploying FPGA in 2018?', ' In what year did Microsoft begin deploying FPGAs across other data center workloads for their Azure cloud computing platform?']","['high-performance, computationally intensive systems', 'performance per watt advantage FPGAs deliver', 'performance', '2014', 'across other data center workloads for their Azure cloud computing platform', '2018']"
1200,fpga,Design,"Contemporary FPGAs have large resources of logic gates and RAM blocks to implement complex digital computations. As FPGA designs employ very fast I/O rates and bidirectional data buses, it becomes a challenge to verify correct timing of valid data within setup time and hold time.
","Contemporary FPGAs have large resources of logic gates and RAM blocks to implement complex digital computations. As FPGA designs employ very fast I/O rates and bidirectional data buses, it becomes a challenge to verify correct timing of valid data within setup time and hold time.","[' How many logic gates and RAM blocks do modern FPGAs have to implement complex digital computations?', ' FPGA designs employ very fast I/O rates and what?']","['large resources', 'bidirectional data buses']"
1201,fpga,Design,"Floor planning enables resource allocation within FPGAs to meet these time constraints. FPGAs can be used to implement any logical function that an ASIC can perform. The ability to update the functionality after shipping, partial re-configuration of a portion of the design and the low non-recurring engineering costs relative to an ASIC design (notwithstanding the generally higher unit cost), offer advantages for many applications.",Floor planning enables resource allocation within FPGAs to meet these time constraints. FPGAs can be used to implement any logical function that an ASIC can perform.,"[' What allows resource allocation within FPGAs to meet time constraints?', ' What can be used to implement any logical function?']","['Floor planning', 'FPGAs']"
1202,fpga,Design,"Some FPGAs have analog features in addition to digital functions. The most common analog feature is a programmable slew rate on each output pin, allowing the engineer to set low rates on lightly loaded pins that would otherwise ring or couple unacceptably, and to set higher rates on heavily loaded pins on high-speed channels that would otherwise run too slowly. Also common are quartz-crystal oscillators, on-chip resistance-capacitance oscillators, and phase-locked loops with embedded voltage-controlled oscillators used for clock generation and management as well as for high-speed serializer-deserializer (SERDES) transmit clocks and receiver clock recovery. Fairly common are differential comparators on input pins designed to be connected to differential signaling channels. A few ""mixed signal FPGAs"" have integrated peripheral analog-to-digital converters (ADCs) and digital-to-analog converters (DACs) with analog signal conditioning blocks allowing them to operate as a system-on-a-chip (SoC). Such devices blur the line between an FPGA, which carries digital ones and zeros on its internal programmable interconnect fabric, and field-programmable analog array (FPAA), which carries analog values on its internal programmable interconnect fabric.
","Some FPGAs have analog features in addition to digital functions. The most common analog feature is a programmable slew rate on each output pin, allowing the engineer to set low rates on lightly loaded pins that would otherwise ring or couple unacceptably, and to set higher rates on heavily loaded pins on high-speed channels that would otherwise run too slowly.","[' What is the most common analog feature on an FPGA?', ' What is a programmable slew rate on each output pin?', ' Couple unacceptably, and to set higher rates on heavily loaded pins on what?']","['programmable slew rate', 'allowing the engineer to set low rates', 'high-speed channels']"
1203,fpga,Programming,"To define the behavior of the FPGA, the user provides a design in a hardware description language (HDL) or as a schematic design. The HDL form is more suited to work with large structures because it's possible to specify high-level functional behavior rather than drawing every piece by hand. However, schematic entry can allow for easier visualization of a design and its component modules.
","To define the behavior of the FPGA, the user provides a design in a hardware description language (HDL) or as a schematic design. The HDL form is more suited to work with large structures because it's possible to specify high-level functional behavior rather than drawing every piece by hand.","[' What language is used to define the behavior of the FPGA?', ' What form is more suited to work with large structures?', ' Why is the HDL form better suited?']","['hardware description language', 'HDL', ""it's possible to specify high-level functional behavior""]"
1204,fpga,Programming,"Using an electronic design automation tool, a technology-mapped netlist is generated. The netlist can then be fit to the actual FPGA architecture using a process called place-and-route, usually performed by the FPGA company's proprietary place-and-route software. The user will validate the map, place and route results via timing analysis, simulation, and other verification and validation methodologies. Once the design and validation process is complete, the binary file generated, typically using the FPGA vendor's proprietary software, is used to (re-)configure the FPGA. This file is transferred to the FPGA/CPLD via a serial interface (JTAG) or to an external memory device like an EEPROM.
","Using an electronic design automation tool, a technology-mapped netlist is generated. The netlist can then be fit to the actual FPGA architecture using a process called place-and-route, usually performed by the FPGA company's proprietary place-and-route software.","[' What is used to create a technology-mapped netlist?', ' What is the process called that allows the netlist to be fit to the actual FPGA architecture?', ' Who usually performs place-and-route?']","['electronic design automation tool', 'place-and-route', ""FPGA company's proprietary place-and-route software""]"
1205,fpga,Programming,"The most common HDLs are VHDL and Verilog as well as extensions such as SystemVerilog. However, in an attempt to reduce the complexity of designing in HDLs, which have been compared to the equivalent of assembly languages, there are moves to raise the abstraction level through the introduction of alternative languages. National Instruments' LabVIEW graphical programming language (sometimes referred to as ""G"") has an FPGA add-in module available to target and program FPGA hardware. Verilog was created to simplify the process making HDL more robust and flexible. Verilog is currently the most popular. Verilog creates a level of abstraction to hide away the details of its implementation. Verilog has a C-like syntax, unlike VHDL.","The most common HDLs are VHDL and Verilog as well as extensions such as SystemVerilog. However, in an attempt to reduce the complexity of designing in HDLs, which have been compared to the equivalent of assembly languages, there are moves to raise the abstraction level through the introduction of alternative languages.","[' What are the most common HDLs?', ' What are extensions such as SystemVerilog?', ' In an attempt to reduce the complexity of designing in HDL, what has been compared to?', ' To raise the abstraction level through the introduction of alternative languages?']","['VHDL and Verilog', 'HDLs', 'assembly languages', 'in an attempt to reduce the complexity of designing in HDLs']"
1206,fpga,Programming,"To simplify the design of complex systems in FPGAs, there exist libraries of predefined complex functions and circuits that have been tested and optimized to speed up the design process. These predefined circuits are commonly called intellectual property (IP) cores, and are available from FPGA vendors and third-party IP suppliers. They are rarely free, and typically released under proprietary licenses. Other predefined circuits are available from developer communities such as OpenCores (typically released under free and open source licenses such as the GPL, BSD or similar license), and other sources. Such designs are known as ""open-source hardware.""
","To simplify the design of complex systems in FPGAs, there exist libraries of predefined complex functions and circuits that have been tested and optimized to speed up the design process. These predefined circuits are commonly called intellectual property (IP) cores, and are available from FPGA vendors and third-party IP suppliers.","[' Why are libraries of predefined complex functions and circuits used in FPGAs used?', ' What are predefined circuits commonly called?']","['To simplify the design of complex systems in FPGAs, there exist libraries of predefined complex functions and circuits that have been tested and optimized to speed up the design process', 'intellectual property (IP) cores']"
1207,fpga,Programming,"In a typical design flow, an FPGA application developer will simulate the design at multiple stages throughout the design process. Initially the RTL description in VHDL or Verilog is simulated by creating test benches to simulate the system and observe results. Then, after the synthesis engine has mapped the design to a netlist, the netlist is translated to a gate-level description where simulation is repeated to confirm the synthesis proceeded without errors. Finally the design is laid out in the FPGA at which point propagation delays can be added and the simulation run again with these values back-annotated onto the netlist.
","In a typical design flow, an FPGA application developer will simulate the design at multiple stages throughout the design process. Initially the RTL description in VHDL or Verilog is simulated by creating test benches to simulate the system and observe results.","[' In a typical design flow, an FPGA application developer will simulate what at multiple stages throughout the design process?', ' What is created to simulate the system and observe results?']","['the design', 'test benches']"
1208,fpga,Programming,"More recently, OpenCL (Open Computing Language) is being used by programmers to take advantage of the performance and power efficiencies that FPGAs provide. OpenCL allows programmers to develop code in the C programming language and target FPGA functions as OpenCL kernels using OpenCL constructs. For further information, see high-level synthesis and C to HDL.
","More recently, OpenCL (Open Computing Language) is being used by programmers to take advantage of the performance and power efficiencies that FPGAs provide. OpenCL allows programmers to develop code in the C programming language and target FPGA functions as OpenCL kernels using OpenCL constructs.","[' What is the acronym for Open Computing Language?', ' What does OpenCL stand for?', ' How can programmers target FPGA functions?']","['OpenCL', 'Open Computing Language', 'OpenCL kernels using OpenCL constructs']"
1209,fpga,Programming,"Most FPGAs rely on an SRAM-based approach to be programmed. These FPGAs are in-system programmable and re-programmable, but require external boot devices. For example, flash memory or EEPROM devices may often load contents into internal SRAM that controls routing and logic. The SRAM approach is based on CMOS.
","Most FPGAs rely on an SRAM-based approach to be programmed. These FPGAs are in-system programmable and re-programmable, but require external boot devices.","[' What do most FPGAs rely on to be programmed?', ' What are in-system programmable and re-programmable but require external boot devices?']","['SRAM-based approach', 'FPGAs']"
1210,fpga,Major manufacturers,"In 2016, long-time industry rivals Xilinx (now part of AMD) and Altera (now an Intel subsidiary) were the FPGA market leaders. At that time, they controlled nearly 90 percent of the market.
","In 2016, long-time industry rivals Xilinx (now part of AMD) and Altera (now an Intel subsidiary) were the FPGA market leaders. At that time, they controlled nearly 90 percent of the market.","[' What industry rivals were the FPGA market leaders in 2016?', ' What percentage of the market did Xilinx and Altera control at the time?']","['Xilinx (now part of AMD) and Altera', '90 percent']"
1211,fpga,Major manufacturers,"In March 2010, Tabula announced their FPGA technology that uses time-multiplexed logic and interconnect that claims potential cost savings for high-density applications. On March 24, 2015, Tabula officially shut down.","In March 2010, Tabula announced their FPGA technology that uses time-multiplexed logic and interconnect that claims potential cost savings for high-density applications. On March 24, 2015, Tabula officially shut down.","[' In what year did Tabula announce their FPGA technology?', ' What does the FPGA use?', ' On what date did the company shut down?']","['2010', 'time-multiplexed logic and interconnect', 'March 24, 2015']"
1212,fpga,Applications,"An FPGA can be used to solve any problem which is computable. This is trivially proven by the fact that FPGAs can be used to implement a soft microprocessor, such as the Xilinx MicroBlaze or Altera Nios II. Their advantage lies in that they are significantly faster for some applications because of their parallel nature and optimality in terms of the number of gates used for certain processes.","An FPGA can be used to solve any problem which is computable. This is trivially proven by the fact that FPGAs can be used to implement a soft microprocessor, such as the Xilinx MicroBlaze or Altera Nios II.","[' What can be used to solve any problem that is computable?', ' What can FPGAs be used for to implement a soft microprocessor?']","['An FPGA', 'solve any problem which is computable']"
1213,fpga,Applications,"FPGAs originally began as competitors to CPLDs to implement glue logic for printed circuit boards. As their size, capabilities, and speed increased, FPGAs took over additional functions to the point where some are now marketed as full systems on chips (SoCs). Particularly with the introduction of dedicated multipliers into FPGA architectures in the late 1990s, applications which had traditionally been the sole reserve of digital signal processor hardware (DSPs) began to incorporate FPGAs instead.","FPGAs originally began as competitors to CPLDs to implement glue logic for printed circuit boards. As their size, capabilities, and speed increased, FPGAs took over additional functions to the point where some are now marketed as full systems on chips (SoCs).","[' What was the original purpose of FPGAs?', "" What were FPGA's competing with CPLD's to implement?"", ' As FPGA size, capabilities, and speed increased, FPGA took over what additional functions?']","['to implement glue logic for printed circuit boards', 'glue logic', 'glue logic for printed circuit boards. As their size, capabilities, and speed increased, FPGAs took over additional functions to the point where some are now marketed as full systems on chips']"
1214,fpga,Applications,"Another trend in the use of FPGAs is hardware acceleration, where one can use the FPGA to accelerate certain parts of an algorithm and share part of the computation between the FPGA and a generic processor. The search engine Bing is noted for adopting FPGA acceleration for its search algorithm in 2014. As of 2018, FPGAs are seeing increased use as AI accelerators including Microsoft's so-termed ""Project Catapult"" and for accelerating artificial neural networks for machine learning applications.
","Another trend in the use of FPGAs is hardware acceleration, where one can use the FPGA to accelerate certain parts of an algorithm and share part of the computation between the FPGA and a generic processor. The search engine Bing is noted for adopting FPGA acceleration for its search algorithm in 2014.","[' What is another trend in the use of FPGAs?', ' What can be used to accelerate certain parts of an algorithm and share part of the computation between the FPGA and a generic processor?', ' Who is noted for adopting FPGA acceleration for its search algorithm?', ' In what year did Google adopt FPGA acceleration for its search algorithm?']","['hardware acceleration', 'hardware acceleration', 'Bing', '2014']"
1215,fpga,Applications,"Traditionally, FPGAs have been reserved for specific vertical applications where the volume of production is small. For these low-volume applications, the premium that companies pay in hardware cost per unit for a programmable chip is more affordable than the development resources spent on creating an ASIC. As of 2017, new cost and performance dynamics have broadened the range of viable applications.
","Traditionally, FPGAs have been reserved for specific vertical applications where the volume of production is small. For these low-volume applications, the premium that companies pay in hardware cost per unit for a programmable chip is more affordable than the development resources spent on creating an ASIC.","[' Where have FPGAs traditionally been reserved?', ' What is more affordable for low-volume applications than the development resources spent on creating an ASIC?']","['specific vertical applications', 'the premium that companies pay in hardware cost per unit for a programmable chip']"
1216,fpga,Applications,The company Gigabyte Technology created an i-RAM card which used a Xilinx FPGA although a custom made chip would be cheaper if made in large quantities. The FPGA was chosen to bring it quickly to market and the initial run was only to be 1000 units making an FPGA the best choice. This device allows people to use computer RAM as a hard drive.,The company Gigabyte Technology created an i-RAM card which used a Xilinx FPGA although a custom made chip would be cheaper if made in large quantities. The FPGA was chosen to bring it quickly to market and the initial run was only to be 1000 units making an FPGA the best choice.,"[' What company created an i-RAM card that used a Xilinx FPGA?', ' Why was the FPGA chosen?', "" How many units were the initial run of Gigabyte Technology's card?"", ' How many units did an FPGA have to be?']","['Gigabyte Technology', 'to bring it quickly to market', '1000', '1000']"
1217,fpga,Security,"FPGAs have both advantages and disadvantages as compared to ASICs or secure microprocessors, concerning hardware security. FPGAs' flexibility makes malicious modifications during fabrication a lower risk. Previously, for many FPGAs, the design bitstream was exposed while the FPGA loads it from external memory (typically on every power-on). All major FPGA vendors now offer a spectrum of security solutions to designers such as bitstream encryption and authentication. For example, Altera and Xilinx offer AES encryption (up to 256-bit) for bitstreams stored in an external flash memory.
","FPGAs have both advantages and disadvantages as compared to ASICs or secure microprocessors, concerning hardware security. FPGAs' flexibility makes malicious modifications during fabrication a lower risk.","[' FPGAs have both advantages and disadvantages as compared to what?', ' What makes malicious modifications during fabrication a lower risk?']","['ASICs or secure microprocessors', ""FPGAs' flexibility""]"
1218,fpga,Security,"FPGAs that store their configuration internally in nonvolatile flash memory, such as Microsemi's ProAsic 3 or Lattice's XP2 programmable devices, do not expose the bitstream and do not need encryption. In addition, flash memory for a lookup table provides single event upset protection for space applications. Customers wanting a higher guarantee of tamper resistance can use write-once, antifuse FPGAs from vendors such as Microsemi.
","FPGAs that store their configuration internally in nonvolatile flash memory, such as Microsemi's ProAsic 3 or Lattice's XP2 programmable devices, do not expose the bitstream and do not need encryption. In addition, flash memory for a lookup table provides single event upset protection for space applications.","[' What do FPGAs that store their configuration internally in nonvolatile flash memory do not expose?', ' What does flash memory for a lookup table provide for space applications?']","['the bitstream', 'single event upset protection']"
1219,fpga,Security,"In 2012 researchers Sergei Skorobogatov and Christopher Woods demonstrated that FPGAs can be vulnerable to hostile intent. They discovered a critical backdoor vulnerability had been manufactured in silicon as part of the Actel/Microsemi ProAsic 3 making it vulnerable on many levels such as reprogramming crypto and access keys, accessing unencrypted bitstream, modifying low-level silicon features, and extracting configuration data.","In 2012 researchers Sergei Skorobogatov and Christopher Woods demonstrated that FPGAs can be vulnerable to hostile intent. They discovered a critical backdoor vulnerability had been manufactured in silicon as part of the Actel/Microsemi ProAsic 3 making it vulnerable on many levels such as reprogramming crypto and access keys, accessing unencrypted bitstream, modifying low-level silicon features, and extracting configuration data.","[' Who demonstrated that FPGAs can be vulnerable to hostile intent?', ' What did researchers Sergei Skorobogatov and Christopher Woods demonstrate in 2012?', ' Where was a critical backdoor vulnerability discovered?', ' Reprogramming crypto and access keys, accessing unencrypted bitstream, modifying low-level silicon features, and extracting configuration data are examples of what?']","['Sergei Skorobogatov and Christopher Woods', 'FPGAs can be vulnerable to hostile intent', 'manufactured in silicon', 'backdoor vulnerability']"
1220,fpga,Similar technologies,"Historically, FPGAs have been slower, less energy efficient and generally achieved less functionality than their fixed ASIC counterparts. A study from 2006 showed that designs implemented on FPGAs need on average 40 times as much area, draw 12 times as much dynamic power, and run at one third the speed of corresponding ASIC implementations. More recently, FPGAs such as the Xilinx Virtex-7 or the Altera Stratix 5 have come to rival corresponding ASIC and ASSP (""Application-specific standard part"", such as a standalone USB interface chip) solutions by providing significantly reduced power usage, increased speed, lower materials cost, minimal implementation real-estate, and increased possibilities for re-configuration 'on-the-fly'. A design that included 6 to 10 ASICs can now be achieved using only one FPGA. Advantages of FPGAs include the ability to re-program when already deployed (i.e. ""in the field"") to fix bugs, and often include shorter time to market and lower non-recurring engineering costs. Vendors can also take a middle road via FPGA prototyping: developing their prototype hardware on FPGAs, but manufacture their final version as an ASIC so that it can no longer be modified after the design has been committed. This is often also the case with new processor designs. Some FPGAs have the capability of partial re-configuration that lets one portion of the device be re-programmed while other portions continue running.","Historically, FPGAs have been slower, less energy efficient and generally achieved less functionality than their fixed ASIC counterparts. A study from 2006 showed that designs implemented on FPGAs need on average 40 times as much area, draw 12 times as much dynamic power, and run at one third the speed of corresponding ASIC implementations.","[' FPGAs have been slower, less energy efficient, and generally achieved less functionality than what?', ' What did a 2006 study show that FPGA designs need on average 40 times as much area?', ' How much more dynamic power does an FPGA draw than an ASIC?', ' How fast do ASIC implementations run?', ' How much dynamic power does ASIC implement?']","['their fixed ASIC counterparts', 'draw 12 times as much dynamic power, and run at one third the speed of corresponding ASIC implementations', '12 times', 'one third', '12 times']"
1221,fpga,Similar technologies,"The primary differences between complex programmable logic devices (CPLDs) and FPGAs are architectural. A CPLD has a comparatively restrictive structure consisting of one or more programmable sum-of-products logic arrays feeding a relatively small number of clocked registers. As a result, CPLDs are less flexible, but have the advantage of more predictable timing delays and a higher logic-to-interconnect ratio. FPGA architectures, on the other hand, are dominated by interconnect. This makes them far more flexible (in terms of the range of designs that are practical for implementation on them) but also far more complex to design for, or at least requiring more complex electronic design automation (EDA) software. In practice, the distinction between FPGAs and CPLDs is often one of size as FPGAs are usually much larger in terms of resources than CPLDs. Typically only FPGAs contain more complex embedded functions such as adders, multipliers, memory, and serializer/deserializers. Another common distinction is that CPLDs contain embedded flash memory to store their configuration while FPGAs usually require external non-volatile memory (but not always). When a design requires simple instant-on (logic is already configured at power-up) CPLDs are generally preferred. For most other applications FPGAs are generally preferred. Sometimes both CPLDs and FPGAs are used in a single system design. In those designs, CPLDs generally perform glue logic functions, and are responsible for ""booting"" the FPGA as well as controlling reset and boot sequence of the complete circuit board. Therefore, depending on the application it may be judicious to use both FPGAs and CPLDs in a single design.",The primary differences between complex programmable logic devices (CPLDs) and FPGAs are architectural. A CPLD has a comparatively restrictive structure consisting of one or more programmable sum-of-products logic arrays feeding a relatively small number of clocked registers.,"[' What are the primary differences between CPLDs and FPGAs?', ' What type of structure does a CPL have?', ' A CPL has a relatively restrictive structure consisting of what?']","['architectural', 'restrictive', 'one or more programmable sum-of-products logic arrays']"
1222,game theory,Summary,"Game theory is the study of mathematical models of strategic interactions among rational agents. It has applications in all fields of social science, as well as in logic, systems science and computer science. Originally, it addressed two-person zero-sum games, in which each participant's gains or losses are exactly balanced by those of other participants. In the 21st century, game theory applies to a wide range of behavioral relations; it is now an umbrella term for the science of logical decision making in humans, animals, as well as computers.
","Game theory is the study of mathematical models of strategic interactions among rational agents. It has applications in all fields of social science, as well as in logic, systems science and computer science.","[' What is the study of mathematical models of strategic interactions among rational agents?', ' Game theory has applications in all fields of what?']","['Game theory', 'social science']"
1223,game theory,Summary,"Modern game theory began with the idea of mixed-strategy equilibria in two-person zero-sum game and its proof by John von Neumann. Von Neumann's original proof used the Brouwer fixed-point theorem on continuous mappings into compact convex sets, which became a standard method in game theory and mathematical economics. His paper was followed by the 1944 book Theory of Games and Economic Behavior, co-written with Oskar Morgenstern, which considered cooperative games of several players. The second edition of this book provided an axiomatic theory of expected utility, which allowed mathematical statisticians and economists to treat decision-making under uncertainty.
","Modern game theory began with the idea of mixed-strategy equilibria in two-person zero-sum game and its proof by John von Neumann. Von Neumann's original proof used the Brouwer fixed-point theorem on continuous mappings into compact convex sets, which became a standard method in game theory and mathematical economics.","[' What did modern game theory begin with?', ' Who proofed the idea of mixed-strategy equilibria in two-person zero-sum game?', "" What did von Neumann's original proof use?"", ' Which method became a standard method in game theory and mathematical economics?']","['mixed-strategy equilibria in two-person zero-sum game', 'John von Neumann', 'Brouwer fixed-point theorem on continuous mappings into compact convex sets', 'Brouwer fixed-point theorem on continuous mappings into compact convex sets']"
1224,game theory,Summary,"Game theory was developed extensively in the 1950s by many scholars. It was explicitly applied to evolution in the 1970s, although similar developments go back at least as far as the 1930s. Game theory has been widely recognized as an important tool in many fields. As of 2020, with the Nobel Memorial Prize in Economic Sciences going to game theorists Paul Milgrom and Robert B. Wilson, fifteen game theorists have won the economics Nobel Prize. John Maynard Smith was awarded the Crafoord Prize for his application of evolutionary game theory.
","Game theory was developed extensively in the 1950s by many scholars. It was explicitly applied to evolution in the 1970s, although similar developments go back at least as far as the 1930s.","[' In what decade was game theory developed?', ' Game theory was applied to evolution in what decade?', ' In which decade did game theory apply to evolution?']","['1950s', '1970s', '1970s']"
1225,game theory,Representation of games,"The games studied in game theory are well-defined mathematical objects. To be fully defined, a game must specify the following elements: the players of the game, the information and actions available to each player at each decision point, and the payoffs for each outcome. (Eric Rasmusen refers to these four ""essential elements"" by the acronym ""PAPI"".) A game theorist typically uses these elements, along with a solution concept of their choosing, to deduce a set of equilibrium strategies for each player such that, when these strategies are employed, no player can profit by unilaterally deviating from their strategy. These equilibrium strategies determine an equilibrium to the game—a stable state in which either one outcome occurs or a set of outcomes occur with known probability.
","The games studied in game theory are well-defined mathematical objects. To be fully defined, a game must specify the following elements: the players of the game, the information and actions available to each player at each decision point, and the payoffs for each outcome.","[' What are well-defined mathematical objects?', ' To be fully defined, a game must specify what?']","['The games studied in game theory', 'the players of the game']"
1226,game theory,General and applied uses,"As a method of applied mathematics, game theory has been used to study a wide variety of human and animal behaviors. It was initially developed in economics to understand a large collection of economic behaviors, including behaviors of firms, markets, and consumers. The first use of game-theoretic analysis was by Antoine Augustin Cournot in 1838 with his solution of the Cournot duopoly. The use of game theory in the social sciences has expanded, and game theory has been applied to political, sociological, and psychological behaviors as well.","As a method of applied mathematics, game theory has been used to study a wide variety of human and animal behaviors. It was initially developed in economics to understand a large collection of economic behaviors, including behaviors of firms, markets, and consumers.","[' Game theory has been used to study a wide variety of what?', ' Game theory was originally developed to understand a large collection of economic behaviors, including behaviors of firms, markets and consumers?']","['human and animal behaviors', 'economics']"
1227,game theory,General and applied uses,"Although pre-twentieth-century naturalists such as Charles Darwin made game-theoretic kinds of statements, the use of game-theoretic analysis in biology began with Ronald Fisher's studies of animal behavior during the 1930s. This work predates the name ""game theory"", but it shares many important features with this field. The developments in economics were later applied to biology largely by John Maynard Smith in his 1982 book Evolution and the Theory of Games.","Although pre-twentieth-century naturalists such as Charles Darwin made game-theoretic kinds of statements, the use of game-theoretic analysis in biology began with Ronald Fisher's studies of animal behavior during the 1930s. This work predates the name ""game theory"", but it shares many important features with this field.","[' Who made game-theoretic statements?', ' When did Ronald Fisher study animal behavior?', "" What is the name of Ronald Fisher's work?""]","['Charles Darwin', 'during the 1930s', 'studies of animal behavior']"
1228,game theory,General and applied uses,"In addition to being used to describe, predict, and explain behavior, game theory has also been used to develop theories of ethical or normative behavior and to prescribe such behavior. In economics and philosophy, scholars have applied game theory to help in the understanding of good or proper behavior. Game-theoretic arguments of this type can be found as far back as Plato. An alternative version of game theory, called chemical game theory, represents the player's choices as metaphorical chemical reactant molecules called ""knowlecules"".  Chemical game theory then calculates the outcomes as equilibrium solutions to a system of chemical reactions.
","In addition to being used to describe, predict, and explain behavior, game theory has also been used to develop theories of ethical or normative behavior and to prescribe such behavior. In economics and philosophy, scholars have applied game theory to help in the understanding of good or proper behavior.","[' Game theory has been used to develop theories of what?', ' In economics and philosophy, scholars have applied game theory to help in the understanding of good or proper behavior?']","['ethical or normative behavior', 'game theory has also been used to develop theories of ethical or normative behavior']"
1229,evolutionary algorithm,Summary,"In computational intelligence (CI), an evolutionary algorithm (EA) is a subset of evolutionary computation, a generic population-based metaheuristic optimization algorithm. An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection. Candidate solutions to the optimization problem play the role of individuals in a population, and the fitness function determines the quality of the solutions (see also loss function). Evolution of the population then takes place after the repeated application of the above operators.
","In computational intelligence (CI), an evolutionary algorithm (EA) is a subset of evolutionary computation, a generic population-based metaheuristic optimization algorithm. An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection.","[' What is an EA subset of in computational intelligence?', ' What is EA a generic population-based metaheuristic optimization algorithm?']","['evolutionary computation', 'evolutionary algorithm']"
1230,evolutionary algorithm,Summary,"Evolutionary algorithms often perform well approximating solutions to all types of problems because they ideally do not make any assumption about the underlying fitness landscape. Techniques from evolutionary algorithms applied to the modeling of biological evolution are generally limited to explorations of microevolutionary processes and planning models based upon cellular processes. In most real applications of EAs, computational complexity is a prohibiting factor. In fact, this computational complexity is due to fitness function evaluation. Fitness approximation is one of the solutions to overcome this difficulty. However, seemingly simple EA can solve often complex problems; therefore, there may be no direct link between algorithm complexity and problem complexity.
",Evolutionary algorithms often perform well approximating solutions to all types of problems because they ideally do not make any assumption about the underlying fitness landscape. Techniques from evolutionary algorithms applied to the modeling of biological evolution are generally limited to explorations of microevolutionary processes and planning models based upon cellular processes.,"[' Why do evolutionary algorithms perform well approximating solutions to all types of problems?', ' Evolutionary algorithms do not make any assumption about what?', ' Techniques from evolutionary algorithms applied to the modeling of biological evolution are limited to explorations of what processes?', ' What are explorations of?', ' What are planning models based upon?']","['they ideally do not make any assumption about the underlying fitness landscape', 'the underlying fitness landscape', 'microevolutionary processes', 'microevolutionary processes and planning models based upon cellular processes', 'cellular processes']"
1231,evolutionary algorithm,Implementation,"Step One: Generate the initial population of individuals randomly. (First generation)
",Step One: Generate the initial population of individuals randomly. (First generation),[' Step One: Generate the initial population of individuals randomly.'],['First generation']
1232,evolutionary algorithm,Comparison to biological processes,"A possible limitation of many evolutionary algorithms is their lack of a clear genotype–phenotype distinction. In nature, the fertilized egg cell undergoes a complex process known as embryogenesis to become a mature phenotype. This indirect encoding is believed to make the genetic search more robust (i.e. reduce the probability of fatal mutations), and also may improve the evolvability of the organism. Such indirect (also known as generative or developmental) encodings also enable evolution to exploit the regularity in the environment. Recent work in the field of artificial embryogeny, or artificial developmental systems, seeks to address these concerns. And gene expression programming successfully explores a genotype–phenotype system, where the genotype consists of linear multigenic chromosomes of fixed length and the phenotype consists of multiple expression trees or computer programs of different sizes and shapes.","A possible limitation of many evolutionary algorithms is their lack of a clear genotype–phenotype distinction. In nature, the fertilized egg cell undergoes a complex process known as embryogenesis to become a mature phenotype.","[' What is a possible limitation of many evolutionary algorithms?', ' What is the complex process that the fertilized egg cell undergoes to become a mature phenotype?']","['lack of a clear genotype–phenotype distinction', 'embryogenesis']"
1233,digital library,Summary,"A digital library, also called an online library, an internet library, a digital repository, or a digital collection is an online database of digital objects that can include text, still images, audio, video, digital documents, or other digital media formats or a library accessible through the internet. Objects can consist of digitized content like print or photographs, as well as originally produced digital content like word processor files or social media posts. In addition to storing content, digital libraries provide means for organizing, searching, and retrieving the content contained in the collection. Digital libraries can vary immensely in size and scope, and can be maintained by individuals or organizations. The digital content may be stored locally, or accessed remotely via computer networks. These information retrieval systems are able to exchange information with each other through interoperability and sustainability.","A digital library, also called an online library, an internet library, a digital repository, or a digital collection is an online database of digital objects that can include text, still images, audio, video, digital documents, or other digital media formats or a library accessible through the internet. Objects can consist of digitized content like print or photographs, as well as originally produced digital content like word processor files or social media posts.","[' What is a digital library also known as?', ' What is an online database of digital objects?', ' What is a library accessible through the internet called?', ' What can be digitized content like print or photographs?']","['an online library', 'A digital library', 'digital library', 'Objects']"
1234,digital library,History,"The early history of digital libraries is not well documented, but several key thinkers are connected to the emergence of the concept. Predecessors include Paul Otlet and Henri La Fontaine's Mundaneum, an attempt begun in 1895 to gather and systematically catalogue the world's knowledge, with the hope of bringing about world peace. The visions of the digital library were largely realized a century later during the great expansion of the Internet, with access to the books and searching of the documents by millions of individuals on the World Wide Web.","The early history of digital libraries is not well documented, but several key thinkers are connected to the emergence of the concept. Predecessors include Paul Otlet and Henri La Fontaine's Mundaneum, an attempt begun in 1895 to gather and systematically catalogue the world's knowledge, with the hope of bringing about world peace.","[' What is not well documented about the early history of digital libraries?', ' Who are two of the predecessors of the digital library concept?', ' What year was the Mundaneum started?', "" What is the hope of cataloging the world's knowledge?""]","['The early history of digital libraries is not well documented, but several key thinkers are connected to the emergence of the concept', 'Paul Otlet and Henri La Fontaine', '1895', 'bringing about world peace']"
1235,digital library,History,"Vannevar Bush and J.C.R. Licklider are two contributors that advanced this idea into then current technology. Bush had supported research that led to the bomb that was dropped on Hiroshima. After seeing the disaster, he wanted to create a machine that would show how technology can lead to understanding instead of destruction. This machine would include a desk with two screens, switches and buttons, and a keyboard. He named this the ""Memex"". This way individuals would be able to access stored books and files at a rapid speed. In 1956, Ford Foundation funded Licklider to analyze how libraries could be improved with technology. Almost a decade later, his book entitled ""Libraries of the Future"" included his vision. He wanted to create a system that would use computers and networks so human knowledge would be accessible for human needs and feedback would be automatic for machine purposes. This system contained three components, the corpus of knowledge, the question, and the answer. Licklider called it a procognitive system.
",Vannevar Bush and J.C.R. Licklider are two contributors that advanced this idea into then current technology.,[' Vannevar Bush and J.C.R. Licklider are two contributors to what idea?'],['advanced this idea into then current technology']
1236,digital library,History,"Early projects centered on the creation of an electronic card catalogue known as Online Public Access Catalog (OPAC). By the 1980s, the success of these endeavors resulted in OPAC replacing the traditional card catalog in many academic, public and special libraries. This permitted libraries to undertake additional rewarding co-operative efforts to support resource sharing and expand access to library materials beyond an individual library.
","Early projects centered on the creation of an electronic card catalogue known as Online Public Access Catalog (OPAC). By the 1980s, the success of these endeavors resulted in OPAC replacing the traditional card catalog in many academic, public and special libraries.","[' What was the name of the first electronic card catalog?', ' When did OPAC replace the traditional card catalog in many academic, public and special libraries?']","['Online Public Access Catalog', '1980s']"
1237,digital library,History,"In 1994, digital libraries became widely visible in the research community due to a $24.4 million NSF managed program supported jointly by DARPA's Intelligent Integration of Information (I3) program, NASA, and NSF itself. Successful research proposals came from six U.S. universities. The universities included Carnegie Mellon University, University of California-Berkeley, University of Michigan, University of Illinois, University of California-Santa Barbara, and Stanford University. Articles from the projects summarized their progress at their halfway point in May 1996. Stanford research, by Sergey Brin and Larry Page, led to the founding of Google.
","In 1994, digital libraries became widely visible in the research community due to a $24.4 million NSF managed program supported jointly by DARPA's Intelligent Integration of Information (I3) program, NASA, and NSF itself. Successful research proposals came from six U.S. universities.","[' In what year did digital libraries become widely visible in the research community?', ' How much did the NSF managed program support?', ' What was the name of the program that supported digital libraries in 1994?', ' In 1994, how many U.S. universities submitted research proposals?']","['1994', '$24.4 million', ""NSF managed program supported jointly by DARPA's Intelligent Integration of Information (I3)"", 'six']"
1238,digital library,Terminology,"The term digital library was first popularized by the NSF/DARPA/NASA Digital Libraries Initiative in 1994. With the availability of the computer networks the information resources are expected to stay distributed and accessed as needed, whereas in Vannevar Bush's essay As We May Think (1945) they were to be collected and kept within the researcher's Memex.
","The term digital library was first popularized by the NSF/DARPA/NASA Digital Libraries Initiative in 1994. With the availability of the computer networks the information resources are expected to stay distributed and accessed as needed, whereas in Vannevar Bush's essay As We May Think (1945) they were to be collected and kept within the researcher's Memex.","[' When did the NSF/DARPA/NASA Digital Libraries Initiative first popularize the term digital library?', ' What is expected to stay distributed and accessed as needed with the availability of computer networks?', ' Who wrote As We May Think in 1945?', ' In what year did We May Think take place?']","['1994', 'information resources', 'Vannevar Bush', '1945']"
1239,digital library,Terminology,"The term virtual library was initially used interchangeably with digital library, but is now primarily used for libraries that are virtual in other senses (such as libraries which aggregate distributed content). In the early days of digital libraries, there was discussion of the similarities and differences among the terms digital, virtual, and electronic.","The term virtual library was initially used interchangeably with digital library, but is now primarily used for libraries that are virtual in other senses (such as libraries which aggregate distributed content). In the early days of digital libraries, there was discussion of the similarities and differences among the terms digital, virtual, and electronic.","[' What term was originally used interchangeably with digital library?', ' What term is now primarily used for libraries that are virtual in other ways?', ' Discussion of the similarities and differences among the terms digital, virtual, and electronic?']","['virtual library', 'virtual library', 'digital libraries']"
1240,digital library,Terminology,"A distinction is often made between content that was created in a digital format, known as born-digital, and information that has been converted from a physical medium, e.g. paper, through digitization. Not all electronic content is in digital data format. The term hybrid library is sometimes used for libraries that have both physical collections and electronic collections. For example, American Memory is a digital library within the Library of Congress.
","A distinction is often made between content that was created in a digital format, known as born-digital, and information that has been converted from a physical medium, e.g. paper, through digitization.","[' What is the term for content created in a digital format?', ' What is a term for information that has been converted from a physical medium, e.g. paper, through digitization?']","['born-digital', 'born-digital']"
1241,digital library,Terminology,"Some important digital libraries also serve as long term archives, such as arXiv and the Internet Archive. Others, such as the Digital Public Library of America, seek to make digital information from various institutions widely accessible online.","Some important digital libraries also serve as long term archives, such as arXiv and the Internet Archive. Others, such as the Digital Public Library of America, seek to make digital information from various institutions widely accessible online.","[' What serves as a long term archive?', ' What does the Digital Public Library of America seek to make digital information from various institutions?']","['arXiv and the Internet Archive', 'widely accessible online']"
1242,digital library,Features of digital libraries,"Traditional libraries are limited by storage space; digital libraries have the potential to store much more information, simply because digital information requires very little physical space to contain it. As such, the cost of maintaining a digital library can be much lower than that of a traditional library. A physical library must spend large sums of money paying for staff, book maintenance, rent, and additional books. Digital libraries may reduce or, in some instances, do away with these fees. Both types of library require cataloging input to allow users to locate and retrieve material. Digital libraries may be more willing to adopt innovations in technology providing users with improvements in electronic and audio book technology as well as presenting new forms of communication such as wikis and blogs; conventional libraries may consider that providing online access to their OP AC catalog is sufficient. An important advantage to digital conversion is increased accessibility to users. They also increase availability to individuals who may not be traditional patrons of a library, due to geographic location or organizational affiliation.
","Traditional libraries are limited by storage space; digital libraries have the potential to store much more information, simply because digital information requires very little physical space to contain it. As such, the cost of maintaining a digital library can be much lower than that of a traditional library.","[' Traditional libraries are limited by what?', ' Digital libraries have the potential to store much more information because digital information requires how little physical space to contain it?', ' The cost of maintaining a digital library can be much lower than what of a traditional library?']","['storage space', 'Traditional libraries are limited by storage space', 'storage space']"
1243,digital library,Future development,"Large scale digitization projects are underway at Google, the Million Book Project, and Internet Archive. With continued improvements in book handling and presentation technologies such as optical character recognition and development of alternative depositories and business models, digital libraries are rapidly growing in popularity. Just as libraries have ventured into audio and video collections, so have digital libraries such as the Internet Archive. Google Books project recently received a court victory on proceeding with their book-scanning project that was halted by the Authors' guild. This helped open the road for libraries to work with Google to better reach patrons who are accustomed to computerized information.
","Large scale digitization projects are underway at Google, the Million Book Project, and Internet Archive. With continued improvements in book handling and presentation technologies such as optical character recognition and development of alternative depositories and business models, digital libraries are rapidly growing in popularity.","[' Google, the Million Book Project, and Internet Archive are examples of what type of digitization projects?', ' Digital libraries are growing in popularity due to improvements in what?']","['Large scale', 'book handling and presentation technologies']"
1244,digital library,Future development,"According to Larry Lannom, Director of Information Management Technology at the nonprofit Corporation for National Research Initiatives (CNRI), ""all the problems associated with digital libraries are wrapped up in archiving."" He goes on to state, ""If in 100 years people can still read your article, we'll have solved the problem."" Daniel Akst, author of The Webster Chronicle, proposes that ""the future of libraries—and of information—is digital."" Peter Lyman and Hal Variant, information scientists at the University of California, Berkeley, estimate that ""the world's total yearly production of print, film, optical, and magnetic content would require roughly 1.5 billion gigabytes of storage."" Therefore, they believe that ""soon it will be technologically possible for an average person to access virtually all recorded information.""","According to Larry Lannom, Director of Information Management Technology at the nonprofit Corporation for National Research Initiatives (CNRI), ""all the problems associated with digital libraries are wrapped up in archiving."" He goes on to state, ""If in 100 years people can still read your article, we'll have solved the problem.""","[' Who is the Director of Information Management Technology at CNRI?', "" What is Larry Lannom's job title?""]","['Larry Lannom', 'Director of Information Management Technology']"
1245,digital library,Future development,"Digital archives are an evolving medium and they develop under various circumstances. Alongside large scale repositories, other digital archiving projects have also evolved in response to needs in research and research communication on various institutional levels. For example, during the COVID-19 pandemic, libraries and higher education institutions have launched digital archiving projects to document life during the pandemic, thus creating a digital, cultural record of collective memories from the period. Researchers have also utilized digital archiving to create specialized research databases. These databases compile digital records for use on international and interdisciplinary levels. COVID CORPUS, launched in October 2020, is an example of such a database, built in response to scientific communication needs in light of the pandemic. Beyond academia, digital collections have also recently been developed to appeal to a more general audience, as is the case with the Selected General Audience Content of the Internet-First University Press developed by Cornell University. This general-audience database contains specialized research information but is digitally organized for accessibility. The establishment of these archives has facilitated specialized forms of digital recordkeeping to fulfill various niches in online, research-based communication. 
","Digital archives are an evolving medium and they develop under various circumstances. Alongside large scale repositories, other digital archiving projects have also evolved in response to needs in research and research communication on various institutional levels.","[' What is an evolving medium and they develop under various circumstances?', ' Along with large scale repositories, other digital archiving projects have also evolved in response to needs in what?']","['Digital archives', 'research and research communication']"
1246,smart card,Summary,"A smart card, chip card, or integrated circuit card (ICC or IC card) is a physical electronic authorization device, used to control access to a resource. It is typically a plastic credit card-sized card with an embedded integrated circuit (IC) chip. Many smart cards include a pattern of metal contacts to electrically connect to the internal chip. Others are contactless, and some are both. Smart cards can provide personal identification, authentication, data storage, and application processing. Applications include identification, financial, mobile phones (SIM),  public transit, computer security, schools, and healthcare. Smart cards may provide strong security authentication for single sign-on (SSO) within organizations. Numerous nations have deployed smart cards throughout their populations.
","A smart card, chip card, or integrated circuit card (ICC or IC card) is a physical electronic authorization device, used to control access to a resource. It is typically a plastic credit card-sized card with an embedded integrated circuit (IC) chip.","[' What is an ICC or IC card?', ' What is a smart card a physical electronic authorization device used for?']","['integrated circuit card', 'to control access to a resource']"
1247,smart card,Summary,"The universal integrated circuit card, or SIM card, is also a type of smart card. As of 2015, 10.5 billion smart card IC chips are manufactured annually, including 5.44 billion SIM card IC chips.","The universal integrated circuit card, or SIM card, is also a type of smart card. As of 2015, 10.5 billion smart card IC chips are manufactured annually, including 5.44 billion SIM card IC chips.","[' What is another name for the universal integrated circuit card?', ' As of 2015, how many smart card IC chips were manufactured annually?', ' How many SIM card chips were produced annually as of 2015?']","['SIM card', '10.5\xa0billion', '5.44\xa0billion']"
1248,smart card,History,"The basis for the smart card is the silicon integrated circuit (IC) chip. It was invented by Robert Noyce at Fairchild Semiconductor in 1959, and was made possible by Mohamed M. Atalla's silicon surface passivation process (1957) and Jean Hoerni's planar process (1959). The invention of the silicon integrated circuit led to the idea of incorporating it onto a plastic card in the late 1960s. Smart cards have since used MOS integrated circuit chips, along with MOS memory technologies such as flash memory and EEPROM (electrically erasable programmable read-only memory).","The basis for the smart card is the silicon integrated circuit (IC) chip. It was invented by Robert Noyce at Fairchild Semiconductor in 1959, and was made possible by Mohamed M. Atalla's silicon surface passivation process (1957) and Jean Hoerni's planar process (1959).","[' What is the basis for the smart card?', ' Who invented the silicon integrated circuit chip?', ' What was Robert Noyce at Fairchild Semiconductor?']","['silicon integrated circuit (IC) chip', 'Robert Noyce', 'silicon integrated circuit (IC) chip']"
1249,smart card,Security,"Smart cards have been advertised as suitable for personal identification tasks, because they are engineered to be tamper resistant. The chip usually implements some cryptographic algorithm. There are, however, several methods for recovering some of the algorithm's internal state.
","Smart cards have been advertised as suitable for personal identification tasks, because they are engineered to be tamper resistant. The chip usually implements some cryptographic algorithm.","[' What is the purpose of a smart card?', ' What type of algorithm does the chip usually implement?']","['personal identification tasks', 'cryptographic']"
1250,smart card,Security,"Differential power analysis involves measuring the precise time and electric current required for certain encryption or decryption operations. This can deduce the on-chip private key used by public key algorithms such as RSA. Some implementations of symmetric ciphers can be vulnerable to timing or power attacks as well.
",Differential power analysis involves measuring the precise time and electric current required for certain encryption or decryption operations. This can deduce the on-chip private key used by public key algorithms such as RSA.,"[' What type of analysis involves measuring the precise time and electric current required for certain encryption or decryption operations?', ' What can deduce the on-chip private key used by public key algorithms like RSA?']","['Differential power', 'Differential power analysis']"
1251,smart card,Security,"Smart cards can be physically disassembled by using acid, abrasives, solvents, or some other technique to obtain unrestricted access to the on-board microprocessor. Although such techniques may involve a risk of permanent damage to the chip, they permit much more detailed information (e.g., photomicrographs of encryption hardware) to be extracted.
","Smart cards can be physically disassembled by using acid, abrasives, solvents, or some other technique to obtain unrestricted access to the on-board microprocessor. Although such techniques may involve a risk of permanent damage to the chip, they permit much more detailed information (e.g., photomicrographs of encryption hardware) to be extracted.","[' What can be physically disassembled by using acid, abrasives, solvents or some other technique to obtain unrestricted access to the on-board microprocessor?']",['Smart cards']
1252,smart card,Benefits,"The benefits of smart cards are directly related to the volume of information and applications that are programmed for use on a card. A single contact/contactless smart card can be programmed with multiple banking credentials, medical entitlement, driver's license/public transport entitlement, loyalty programs and club memberships to name just a few. Multi-factor and proximity authentication can and has been embedded into smart cards to increase the security of all services on the card. For example, a smart card can be programmed to only allow a contactless transaction if it is also within range of another device like a uniquely paired mobile phone. This can significantly increase the security of the smart card.
","The benefits of smart cards are directly related to the volume of information and applications that are programmed for use on a card. A single contact/contactless smart card can be programmed with multiple banking credentials, medical entitlement, driver's license/public transport entitlement, loyalty programs and club memberships to name just a few.","[' What are the benefits of a smart card directly related to?', "" A single contact/contactless smart card can be programmed with multiple banking credentials, medical entitlement, driver's license/public transport entitlement, loyalty programs and what else?"", ' What do entitlement, loyalty programs and club memberships all involve?']","['the volume of information and applications that are programmed for use on a card', 'club memberships', 'multiple banking credentials']"
1253,smart card,Benefits,"Governments and regional authorities save money because of improved security, better data and reduced processing costs. These savings help reduce public budgets or enhance public services. There are many examples in the UK, many using a common open LASSeO specification.
","Governments and regional authorities save money because of improved security, better data and reduced processing costs. These savings help reduce public budgets or enhance public services.","[' Why do governments and regional authorities save money?', ' What do these savings help reduce?']","['improved security, better data and reduced processing costs', 'public budgets']"
1254,smart card,Benefits,"Individuals have better security and more convenience with using smart cards that perform multiple services. For example, they only need to replace one card if their wallet is lost or stolen. The data storage on a card can reduce duplication, and even provide emergency medical information.
","Individuals have better security and more convenience with using smart cards that perform multiple services. For example, they only need to replace one card if their wallet is lost or stolen.","[' What do individuals have better security and more convenience with using?', ' How many cards do individuals need to replace if their wallet is lost or stolen?']","['smart cards', 'one']"
1255,smart card,Advantages,"The first main advantage of smart cards is their flexibility. Smart cards have multiple functions which simultaneously can be an ID, a credit card, a stored-value cash card, and a repository of personal information such as telephone numbers or medical history. The card can be easily replaced if lost, and, the requirement for a PIN (or other form of security) provides additional security from unauthorised access to information by others. At the first attempt to use it illegally, the card would be deactivated by the card reader itself.
","The first main advantage of smart cards is their flexibility. Smart cards have multiple functions which simultaneously can be an ID, a credit card, a stored-value cash card, and a repository of personal information such as telephone numbers or medical history.","[' What is the first main advantage of smart cards?', ' Smart cards can be used as an ID, credit card, a stored-value cash card, and a repository of what?']","['their flexibility', 'personal information']"
1256,smart card,Advantages,"The second main advantage is security. Smart cards can be electronic key rings, giving the bearer ability to access information and physical places without need for online connections. They are encryption devices, so that the user can encrypt and decrypt information without relying on unknown, and therefore potentially untrustworthy, appliances such as ATMs. Smart cards are very flexible in providing authentication at different level of the bearer and the counterpart. Finally, with the information about the user that smart cards can provide to the other parties, they are useful devices for customizing products and services.
","The second main advantage is security. Smart cards can be electronic key rings, giving the bearer ability to access information and physical places without need for online connections.","[' What is the second main advantage of smart cards?', ' What can be electronic key rings?', ' How can smart cards be used?']","['security', 'Smart cards', 'electronic key rings']"
1257,smart card,Smart cards and electronic commerce,"Smart cards can be used in electronic commerce, over the Internet, though the business model used in current electronic commerce applications still cannot use the full karthngnof the electronic medium. An advantage of smart cards for electronic commerce is their use customize services. For example, in order for the service supplier to deliver the customized service, the user may need to provide each supplier with their profile, a boring and time-consuming activity. A smart card can contain a non-encrypted profile of the bearer, so that the user can get customized services even without previous contacts with the supplier.
","Smart cards can be used in electronic commerce, over the Internet, though the business model used in current electronic commerce applications still cannot use the full karthngnof the electronic medium. An advantage of smart cards for electronic commerce is their use customize services.","[' What can smart cards be used in?', ' What is an advantage of using smart cards for electronic commerce?']","['electronic commerce', 'their use customize services']"
1258,smart card,Disadvantages,"The plastic or paper card in which the chip is embedded is fairly flexible. The larger the chip, the higher the probability that normal use could damage it. Cards are often carried in wallets or pockets, a harsh environment for a chip and antenna in contactless cards. PVC cards can crack or break if bent/flexed excessively. However, for large banking systems, failure-management costs can be more than offset by fraud reduction.","The plastic or paper card in which the chip is embedded is fairly flexible. The larger the chip, the higher the probability that normal use could damage it.","[' What type of card is the chip embedded in?', ' What is the probability that normal use could damage the chip?']","['plastic or paper', 'higher']"
1259,smart card,Disadvantages,"The production, use and disposal of PVC plastic is known to be more harmful to the environment than other plastics. Alternative materials including chlorine free plastics and paper are available for some smart applications.
","The production, use and disposal of PVC plastic is known to be more harmful to the environment than other plastics. Alternative materials including chlorine free plastics and paper are available for some smart applications.","[' What type of plastic is known to be more harmful to the environment than other plastics?', ' What kind of plastics are available for some smart applications?']","['PVC', 'chlorine free plastics and paper']"
1260,smart card,Disadvantages,"If the account holder's computer hosts malware, the smart card security model may be broken. Malware can override the communication (both input via keyboard and output via application screen) between the user and the application. Man-in-the-browser malware (e.g., the Trojan Silentbanker) could modify a transaction, unnoticed by the user. Banks like Fortis and Belfius in Belgium and Rabobank (""random reader"") in the Netherlands combine a smart card with an unconnected card reader to avoid this problem. The customer enters a challenge received from the bank's website, a PIN and the transaction amount into the reader. The reader returns an 8-digit signature. This signature is manually entered into the personal computer and verified by the bank, preventing point-of-sale-malware from changing the transaction amount.
","If the account holder's computer hosts malware, the smart card security model may be broken. Malware can override the communication (both input via keyboard and output via application screen) between the user and the application.","["" What happens if the account holder's computer hosts malware?"", ' What can override the communication between the user and the application?']","['the smart card security model may be broken', 'Malware']"
1261,smart card,Disadvantages,"Smart cards have also been the targets of security attacks. These attacks range from physical invasion of the card's electronics, to non-invasive attacks that exploit weaknesses in the card's software or hardware. The usual goal is to expose private encryption keys and then read and manipulate secure data such as funds. Once an attacker develops a non-invasive attack for a particular smart card model, he or she is typically able to perform the attack on other cards of that model in seconds, often using equipment that can be disguised as a normal smart card reader. While manufacturers may develop new card models with additional information security, it may be costly or inconvenient for users to upgrade vulnerable systems. Tamper-evident and audit features in a smart card system help manage the risks of compromised cards.
","Smart cards have also been the targets of security attacks. These attacks range from physical invasion of the card's electronics, to non-invasive attacks that exploit weaknesses in the card's software or hardware.","[' Smart cards have been the target of what kind of attacks?', "" Physical invasion of the card's electronics can be a cause of what type of attack?"", ' Non-invasive attacks can exploit weaknesses in what part of a card?']","['security', 'security attacks', 'software or hardware']"
1262,smart card,Disadvantages,"Another problem is the lack of standards for functionality and security. To address this problem, the Berlin Group launched the ERIDANE Project to propose ""a new functional and security framework for smart-card based Point of Interaction (POI) equipment"".","Another problem is the lack of standards for functionality and security. To address this problem, the Berlin Group launched the ERIDANE Project to propose ""a new functional and security framework for smart-card based Point of Interaction (POI) equipment"".","[' What did the Berlin Group launch to address the lack of standards for?', ' What was the ERIDANE Project to propose?']","['ERIDANE Project', 'a new functional and security framework for smart-card based Point of Interaction (POI) equipment']"
1263,recommender system,Summary,"Recommender systems are used in a variety of areas, with commonly recognised examples taking the form of playlist generators for video and music services, product recommenders for online stores, or content recommenders for social media platforms and open web content recommenders. These systems can operate using a single input, like music, or multiple inputs within and across platforms like news, books, and search queries. There are also popular recommender systems for specific topics like restaurants and online dating. Recommender systems have also been developed to explore research articles and experts, collaborators, and financial services.","Recommender systems are used in a variety of areas, with commonly recognised examples taking the form of playlist generators for video and music services, product recommenders for online stores, or content recommenders for social media platforms and open web content recommenders. These systems can operate using a single input, like music, or multiple inputs within and across platforms like news, books, and search queries.","[' Recommender systems are used in a variety of areas, such as what?', ' What is a common example of a recommendation system?', ' How can a recommender system operate?', ' What type of system can operate using a single input, like music?', ' What types of systems can operate with multiple inputs within and across platforms?']","['playlist generators for video and music services', 'playlist generators', 'using a single input, like music, or multiple inputs within and across platforms like news, books, and search queries', 'Recommender', 'Recommender systems']"
1264,recommender system,Overview,"Recommender systems usually make use of either or both collaborative filtering and content-based filtering (also known as the personality-based approach), as well as other systems such as knowledge-based systems. Collaborative filtering approaches build a model from a user's past behavior (items previously purchased or selected and/or numerical ratings given to those items) as well as similar decisions made by other users. This model is then used to predict items (or ratings for items) that the user may have an interest in. Content-based filtering approaches utilize a series of discrete, pre-tagged characteristics of an item in order to recommend additional items with similar properties.","Recommender systems usually make use of either or both collaborative filtering and content-based filtering (also known as the personality-based approach), as well as other systems such as knowledge-based systems. Collaborative filtering approaches build a model from a user's past behavior (items previously purchased or selected and/or numerical ratings given to those items) as well as similar decisions made by other users.","[' What is another name for content-based filtering?', "" What does collaborative filtering build a model from a user's past behavior?"", ' What are items previously purchased or selected and/or numerical ratings given to those items?', "" What are other users' decisions?""]","['personality-based approach', 'items previously purchased or selected and/or numerical ratings given to those items', 'past behavior', 'similar']"
1265,recommender system,Overview,"Each type of system has its strengths and weaknesses. In the above example, Last.fm requires a large amount of information about a user to make accurate recommendations. This is an example of the cold start problem, and is common in collaborative filtering systems. Whereas Pandora needs very little information to start, it is far more limited in scope (for example, it can only make recommendations that are similar to the original seed).
","Each type of system has its strengths and weaknesses. In the above example, Last.fm requires a large amount of information about a user to make accurate recommendations.","[' What type of system has its strengths and weaknesses?', ' What system requires a large amount of information about a user to make accurate recommendations?']","['Last.fm', 'Last.fm']"
1266,recommender system,Overview,"Recommender systems are a useful alternative to search algorithms since they help users discover items they might not have found otherwise. Of note, recommender systems are often implemented using search engines indexing non-traditional data.
","Recommender systems are a useful alternative to search algorithms since they help users discover items they might not have found otherwise. Of note, recommender systems are often implemented using search engines indexing non-traditional data.","[' Recommender systems are a useful alternative to what?', ' Recommendation systems help users discover what they might not have found otherwise?', ' How are recommender systems implemented?']","['search algorithms', 'search algorithms', 'using search engines indexing non-traditional data']"
1267,recommender system,Overview,"Montaner provided the first overview of recommender systems from an intelligent agent perspective. Adomavicius provided a new, alternate overview of recommender systems.  Herlocker provides an additional overview of evaluation techniques for recommender systems, and Beel et al. discussed the problems of offline evaluations. Beel et al. have also provided literature surveys on available research paper recommender systems and existing challenges.","Montaner provided the first overview of recommender systems from an intelligent agent perspective. Adomavicius provided a new, alternate overview of recommender systems.","[' Montaner provided the first overview of recommender systems from what perspective?', ' Adomavicius provided a new, alternate overview of what?']","['intelligent agent', 'recommender systems']"
1268,recommender system,The Netflix Prize,"One of the events that energized research in recommender systems was the Netflix Prize. From 2006 to 2009, Netflix sponsored a competition, offering a grand prize of $1,000,000 to the team that could take an offered dataset of over 100 million movie ratings and return recommendations that were 10% more accurate than those offered by the company's existing recommender system. This competition energized the search for new and more accurate algorithms. On 21 September 2009, the grand prize of US$1,000,000 was given to the BellKor's Pragmatic Chaos team using tiebreaking rules.","One of the events that energized research in recommender systems was the Netflix Prize. From 2006 to 2009, Netflix sponsored a competition, offering a grand prize of $1,000,000 to the team that could take an offered dataset of over 100 million movie ratings and return recommendations that were 10% more accurate than those offered by the company's existing recommender system.","[' What was one of the events that energized research in recommender systems?', ' When did Netflix sponsor a competition?', ' What was the grand prize offered to the team that took an offered dataset of over 100 million movie ratings and returned recommendations that were 10% more?', "" How many movie ratings and return recommendations were 10% more accurate than the company's existing recommender system?""]","['the Netflix Prize', 'From 2006 to 2009', '$1,000,000', 'over 100 million']"
1269,recommender system,The Netflix Prize,"The most accurate algorithm in 2007 used an ensemble method of 107 different algorithmic approaches, blended into a single prediction. As stated by the winners, Bell et al.:","The most accurate algorithm in 2007 used an ensemble method of 107 different algorithmic approaches, blended into a single prediction. As stated by the winners, Bell et al.","[' What was the most accurate algorithm in 2007?', ' How many different algorithmic approaches were used in the 2007 algorithm?']","['an ensemble method of 107 different algorithmic approaches, blended into a single prediction', '107']"
1270,recommender system,The Netflix Prize,"
Predictive accuracy is substantially improved when blending multiple predictors. Our experience is that most efforts should be concentrated in deriving substantially different approaches, rather than refining a single technique.  Consequently, our solution is an ensemble of many methods.","
Predictive accuracy is substantially improved when blending multiple predictors. Our experience is that most efforts should be concentrated in deriving substantially different approaches, rather than refining a single technique.","[' What is substantially improved when blending multiple predictors?', ' What should most efforts be concentrated on rather than refining a single technique?']","['Predictive accuracy', 'deriving substantially different approaches']"
1271,recommender system,The Netflix Prize,"Many benefits accrued to the web due to the Netflix project. Some teams have taken their technology and applied it to other markets. Some members from the team that finished second place founded Gravity R&D, a recommendation engine that's active in the RecSys community. 4-Tell, Inc. created a Netflix project–derived solution for ecommerce websites.
",Many benefits accrued to the web due to the Netflix project. Some teams have taken their technology and applied it to other markets.,"[' What project has brought many benefits to the web?', ' What has some teams taken their technology and applied it to other markets?']","['Netflix', 'Netflix']"
1272,recommender system,The Netflix Prize,"A number of privacy issues arose around the dataset offered by Netflix for the Netflix Prize competition. Although the data sets were anonymized in order to preserve customer privacy, in 2007 two researchers from the University of Texas were able to identify individual users by matching the data sets with film ratings on the Internet Movie Database. As a result, in December 2009, an anonymous Netflix user sued Netflix in Doe v. Netflix, alleging that Netflix had violated United States fair trade laws and the Video Privacy Protection Act by releasing the datasets. This, as well as concerns from the Federal Trade Commission, led to the cancellation of a second Netflix Prize competition in 2010.","A number of privacy issues arose around the dataset offered by Netflix for the Netflix Prize competition. Although the data sets were anonymized in order to preserve customer privacy, in 2007 two researchers from the University of Texas were able to identify individual users by matching the data sets with film ratings on the Internet Movie Database.","[' What did Netflix offer for the Netflix Prize competition?', ' Why were the data sets anonymized?', ' What university was able to identify individual users by matching the data with the data?', ' To identify individual users by matching the data sets with what?']","['dataset', 'to preserve customer privacy', 'University of Texas', 'film ratings']"
1273,recommender system,Performance measures,"Evaluation is important in assessing the effectiveness of recommendation algorithms. To measure the effectiveness of recommender systems, and compare different approaches, three types of evaluations are available: user studies, online evaluations (A/B tests), and offline evaluations.","Evaluation is important in assessing the effectiveness of recommendation algorithms. To measure the effectiveness of recommender systems, and compare different approaches, three types of evaluations are available: user studies, online evaluations (A/B tests), and offline evaluations.","[' What is important in assessing the effectiveness of recommendation algorithms?', ' How many types of evaluations are available?', ' What are user studies, online evaluations, and offline evaluations?']","['Evaluation', 'three', 'A/B tests']"
1274,recommender system,Performance measures,"The commonly used metrics are the mean squared error and root mean squared error, the latter having been used in the Netflix Prize. The information retrieval metrics such as precision and recall or DCG are useful to assess the quality of a recommendation method. Diversity, novelty, and coverage are also considered as important aspects in evaluation. However, many of the classic evaluation measures are highly criticized.","The commonly used metrics are the mean squared error and root mean squared error, the latter having been used in the Netflix Prize. The information retrieval metrics such as precision and recall or DCG are useful to assess the quality of a recommendation method.","[' What are two commonly used metrics?', ' What has been used in the Netflix Prize?', ' The information retrieval metrics are useful to assess what?']","['mean squared error and root mean squared error', 'mean squared error and root mean squared error', 'the quality of a recommendation method']"
1275,recommender system,Performance measures,"Evaluating the performance of a recommendation algorithm on a fixed test dataset will always be extremely challenging as it is impossible to accurately predict the reactions of real users to the recommendations. Hence any metric that computes the effectiveness of an algorithm in offline data will be imprecise.  
",Evaluating the performance of a recommendation algorithm on a fixed test dataset will always be extremely challenging as it is impossible to accurately predict the reactions of real users to the recommendations. Hence any metric that computes the effectiveness of an algorithm in offline data will be imprecise.,"[' What will always be extremely challenging?', ' What is impossible to accurately predict the reactions of real users to the recommendations?', ' Any metric that computes the effectiveness of an algorithm in offline data will be what?']","['Evaluating the performance of a recommendation algorithm on a fixed test dataset', 'recommendation algorithm', 'imprecise']"
1276,recommender system,Performance measures,"User studies are rather a small scale. A few dozens or hundreds of users are presented recommendations created by different recommendation approaches, and then the users judge which recommendations are best. 
","User studies are rather a small scale. A few dozens or hundreds of users are presented recommendations created by different recommendation approaches, and then the users judge which recommendations are best.","[' What is the scale of user studies?', ' How many users are presented with recommendations?', ' What do users judge?']","['small scale', 'A few dozens or hundreds', 'which recommendations are best']"
1277,recommender system,Performance measures,"In A/B tests, recommendations are shown to typically thousands of users of a real product, and the recommender system randomly picks at least two different recommendation approaches to generate recommendations. The effectiveness is measured with implicit measures of effectiveness such as conversion rate or click-through rate. 
","In A/B tests, recommendations are shown to typically thousands of users of a real product, and the recommender system randomly picks at least two different recommendation approaches to generate recommendations. The effectiveness is measured with implicit measures of effectiveness such as conversion rate or click-through rate.","[' In A/B tests, how many users of a real product are shown recommendations to?', ' How many different recommendations approaches does the recommender system randomly pick to generate recommendations?', ' What measures of effectiveness are implicit measures of?']","['thousands', 'two', 'conversion rate or click-through rate']"
1278,recommender system,Performance measures,"Offline evaluations are based on historic data, e.g. a dataset that contains information about how users previously rated movies.","Offline evaluations are based on historic data, e.g. a dataset that contains information about how users previously rated movies.","[' What are offline evaluations based on?', ' What is a dataset that contains information about how users previously rated movies?']","['historic data', 'historic data']"
1279,recommender system,Performance measures,"The effectiveness of recommendation approaches is then measured based on how well a recommendation approach can predict the users' ratings in the dataset. While a rating is an explicit expression of whether a user liked a movie, such information is not available in all domains. For instance, in the domain of citation recommender systems, users typically do not rate a citation or recommended article. In such cases, offline evaluations may use implicit measures of effectiveness. For instance, it may be assumed that a recommender system is effective that is able to recommend as many articles as possible that are contained in a research article's reference list. However, this kind of offline evaluations is seen critical by many researchers. For instance, it has been shown that results of offline evaluations have low correlation with results from user studies or A/B tests. A dataset popular for offline evaluation has been shown to contain duplicate data and thus to lead to wrong conclusions in the evaluation of algorithms. Often, results of so-called offline evaluations do not correlate with actually assessed user-satisfaction. This is probably because offline training is highly biased toward the highly reachable items, and offline testing data is highly influenced by the outputs of the online recommendation module. Researchers have concluded that the results of offline evaluations should be viewed critically.","The effectiveness of recommendation approaches is then measured based on how well a recommendation approach can predict the users' ratings in the dataset. While a rating is an explicit expression of whether a user liked a movie, such information is not available in all domains.","[' How is the effectiveness of recommendation approaches measured?', ' What is an explicit expression of whether a user liked a movie?']","[""based on how well a recommendation approach can predict the users' ratings in the dataset"", 'a rating']"
1280,operating system,Summary,"For hardware functions such as input and output and memory allocation, the operating system acts as an intermediary between programs and the computer hardware, although the application code is usually executed directly by the hardware and frequently makes system calls to an OS function or is interrupted by it. Operating systems are found on many devices that contain a computer –  from cellular phones and video game consoles to web servers and supercomputers.
","For hardware functions such as input and output and memory allocation, the operating system acts as an intermediary between programs and the computer hardware, although the application code is usually executed directly by the hardware and frequently makes system calls to an OS function or is interrupted by it. Operating systems are found on many devices that contain a computer –  from cellular phones and video game consoles to web servers and supercomputers.","[' What acts as an intermediary between programs and the computer hardware?', ' The application code is usually executed directly by what?', ' What frequently makes system calls to an OS function?', ' What is an example of a device that has an operating system?', ' What are examples of devices that have an OS?']","['the operating system', 'the hardware', 'the application code', 'cellular phones', 'cellular phones and video game consoles']"
1281,operating system,Summary,"The dominant general-purpose personal computer operating system is Microsoft Windows with a market share of around 76.45%. macOS by Apple Inc. is in second place (17.72%), and the varieties of Linux are collectively in third place (1.73%). In the mobile sector (including smartphones and tablets), Android's share is up to 72% in the year 2020. According to third quarter 2016 data, Android's share on smartphones is dominant with 87.5 percent with also a growth rate of 10.3 percent per year, followed by Apple's iOS with 12.1 percent with per year decrease in market share of 5.2 percent, while other operating systems amount to just 0.3 percent. Linux distributions are dominant in the server and supercomputing sectors. Other specialized classes of operating systems (special-purpose operating systems), such as embedded and real-time systems, exist for many applications. Security-focused operating systems also exist. Some operating systems have low system requirements (e.g. light-weight Linux distribution). Others may have higher system requirements.
","The dominant general-purpose personal computer operating system is Microsoft Windows with a market share of around 76.45%. macOS by Apple Inc. is in second place (17.72%), and the varieties of Linux are collectively in third place (1.73%).","[' What is the dominant general purpose personal computer operating system?', ' How much market share does Microsoft Windows have?', ' What company is in second place with macOS?', ' Which operating system is in third place?']","['Microsoft Windows', '76.45%.', 'Apple Inc.', 'Linux']"
1282,operating system,Summary,"Some operating systems require installation or may come pre-installed with purchased computers (OEM-installation), whereas others may run directly from media (i.e. live CD) or flash memory (i.e. USB stick).
","Some operating systems require installation or may come pre-installed with purchased computers (OEM-installation), whereas others may run directly from media (i.e. live CD) or flash memory (i.e.","[' Some operating systems require installation or may come pre-installed with purchased computers (OEM-installation)?', ' What may run directly from media (i.e. live CD) or flash memory?']","['Some operating systems require installation or may come pre-installed with purchased computers (OEM-installation), whereas others may run directly from media (i.e. live CD) or flash memory (i.e.', 'operating systems']"
1283,operating system,History,"Early computers were built to perform a series of single tasks, like a calculator. Basic operating system features were developed in the 1950s, such as resident monitor functions that could automatically run different programs in succession to speed up processing. Operating systems did not exist in their modern and more complex forms until the early 1960s.   Hardware features were added, that enabled use of runtime libraries, interrupts, and parallel processing. When personal computers became popular in the 1980s, operating systems were made for them similar in concept to those used on larger computers.
","Early computers were built to perform a series of single tasks, like a calculator. Basic operating system features were developed in the 1950s, such as resident monitor functions that could automatically run different programs in succession to speed up processing.","[' What were early computers built to perform?', ' When were basic operating system features developed?', ' What could automatically run different programs in succession?']","['a series of single tasks', '1950s', 'resident monitor functions']"
1284,operating system,History,"In the 1940s, the earliest electronic digital systems had no operating systems.  Electronic systems of this time were programmed on rows of mechanical switches or by jumper wires on plugboards.  These were special-purpose systems that, for example, generated ballistics tables for the military or controlled the printing of payroll checks from data on punched paper cards.  After programmable general-purpose computers were invented, machine languages(consisting of strings of the binary digits 0 and 1 on punched paper tape) were introduced that sped up the programming process (Stern, 1981).","In the 1940s, the earliest electronic digital systems had no operating systems. Electronic systems of this time were programmed on rows of mechanical switches or by jumper wires on plugboards.","[' When were the earliest electronic digital systems without operating systems?', ' What were the electronic systems of the 1940s programmed on?']","['1940s', 'rows of mechanical switches or by jumper wires on plugboards']"
1285,operating system,History,"In the early 1950s, a computer could execute only one program at a time.  Each user had sole use of the computer for a limited period and would arrive at a scheduled time with their program and data on punched paper cards or punched tape. The program would be loaded into the machine, and the machine would be set to work until the program completed or crashed. Programs could generally be debugged via a front panel using toggle switches and panel lights. It is said that Alan Turing was a master of this on the early Manchester Mark 1 machine, and he was already deriving the primitive conception of an operating system from the principles of the universal Turing machine.","In the early 1950s, a computer could execute only one program at a time. Each user had sole use of the computer for a limited period and would arrive at a scheduled time with their program and data on punched paper cards or punched tape.","[' In the early 1950s, how many programs could a computer execute at a time?', ' What did each user have sole use of the computer for?']","['one', 'a limited period']"
1286,operating system,History,"Later machines came with libraries of programs, which would be linked to a user's program to assist in operations such as input and output and compiling (generating machine code from human-readable symbolic code). This was the genesis of the modern-day operating system. However, machines still ran a single job at a time. At Cambridge University in England, the job queue was at one time a washing line (clothesline) from which tapes were hung with different colored clothes-pegs to indicate job priority.","Later machines came with libraries of programs, which would be linked to a user's program to assist in operations such as input and output and compiling (generating machine code from human-readable symbolic code). This was the genesis of the modern-day operating system.","[' What did later machines come with?', ' What was the genesis of the modern-day operating system?']","['libraries of programs', 'libraries of programs']"
1287,operating system,History,"An improvement was the Atlas Supervisor. Introduced with the Manchester Atlas in 1962, it is considered by many to be the first recognisable modern operating system. Brinch Hansen described it as ""the most significant breakthrough in the history of operating systems.""","An improvement was the Atlas Supervisor. Introduced with the Manchester Atlas in 1962, it is considered by many to be the first recognisable modern operating system.","[' What was introduced with the Manchester Atlas in 1962?', ' What was the first recognisable modern operating system?']","['the Atlas Supervisor', 'Atlas Supervisor']"
1288,operating system,Components,"The components of an operating system all exist in order to make the different parts of a computer work together. All user software needs to go through the operating system in order to use any of the hardware, whether it be as simple as a mouse or keyboard or as complex as an Internet component.
","The components of an operating system all exist in order to make the different parts of a computer work together. All user software needs to go through the operating system in order to use any of the hardware, whether it be as simple as a mouse or keyboard or as complex as an Internet component.","[' What do the components of an operating system all exist for?', ' What does all user software need to go through in order to use any of the hardware?', ' What can be as simple as a mouse or keyboard or as complex as an internet component?']","['to make the different parts of a computer work together', 'the operating system', 'hardware']"
1289,operating system,Real-time operating systems,"A real-time operating system (RTOS) is an operating system intended for applications with fixed deadlines (real-time computing). Such applications include some small embedded systems, automobile engine controllers, industrial robots, spacecraft, industrial control, and some large-scale computing systems.
","A real-time operating system (RTOS) is an operating system intended for applications with fixed deadlines (real-time computing). Such applications include some small embedded systems, automobile engine controllers, industrial robots, spacecraft, industrial control, and some large-scale computing systems.","[' What is a real-time operating system?', ' What is an RTOS intended for?']","['RTOS) is an operating system intended for applications with fixed deadlines', 'applications with fixed deadlines']"
1290,operating system,Real-time operating systems,"Embedded systems that have fixed deadlines use a real-time operating system such as VxWorks, PikeOS, eCos, QNX, MontaVista Linux and RTLinux. Windows CE is a real-time operating system that shares similar APIs to desktop Windows but shares none of desktop Windows' codebase. Symbian OS also has an RTOS kernel (EKA2) starting with version 8.0b.
","Embedded systems that have fixed deadlines use a real-time operating system such as VxWorks, PikeOS, eCos, QNX, MontaVista Linux and RTLinux. Windows CE is a real-time operating system that shares similar APIs to desktop Windows but shares none of desktop Windows' codebase.","[' What do embedded systems use?', "" What is a real-time operating system that shares similar APIs to desktop Windows but not desktop Windows' codebase?""]","['real-time operating system', 'Windows CE']"
1291,operating system,Operating system development as a hobby,"In some cases, hobby development is in support of a ""homebrew"" computing device, for example, a simple single-board computer powered by a 6502 microprocessor.  Or, development may be for an architecture already in widespread use.  Operating system development may come from entirely new concepts, or may commence by modeling an existing operating system.  In either case, the hobbyist is his/her own developer, or may interact with a small and sometimes unstructured group of individuals who have like interests.
","In some cases, hobby development is in support of a ""homebrew"" computing device, for example, a simple single-board computer powered by a 6502 microprocessor. Or, development may be for an architecture already in widespread use.","[' What is a ""homebrew"" computing device called?', ' What is the microprocessor in a single-board computer?']","['a simple single-board computer powered by a 6502 microprocessor', '6502']"
1292,operating system,Diversity of operating systems and portability,"Application software is generally written for use on a specific operating system, and sometimes even for specific hardware. When porting the application to run on another OS, the functionality required by that application may be implemented differently by that OS (the names of functions, meaning of arguments, etc.) requiring the application to be adapted, changed, or otherwise maintained.
","Application software is generally written for use on a specific operating system, and sometimes even for specific hardware. When porting the application to run on another OS, the functionality required by that application may be implemented differently by that OS (the names of functions, meaning of arguments, etc.)","[' What is generally written for use on a specific operating system and sometimes even for specific hardware?', ' When porting an application to run on another OS, what may be implemented differently by that OS?']","['Application software', 'the functionality required by that application']"
1293,operating system,Diversity of operating systems and portability,"This cost in supporting operating systems diversity can be avoided by instead writing applications against software platforms such as Java or Qt. These abstractions have already borne the cost of adaptation to specific operating systems and their system libraries.
",This cost in supporting operating systems diversity can be avoided by instead writing applications against software platforms such as Java or Qt. These abstractions have already borne the cost of adaptation to specific operating systems and their system libraries.,"[' What can be avoided by writing applications against software platforms such as Java or Qt?', ' What have already borne the cost of adaptation to specific operating systems and system libraries?']","['cost in supporting operating systems diversity', 'abstractions']"
1294,operating system,Diversity of operating systems and portability,"Another approach is for operating system vendors to adopt standards. For example, POSIX and OS abstraction layers provide commonalities that reduce porting costs.
","Another approach is for operating system vendors to adopt standards. For example, POSIX and OS abstraction layers provide commonalities that reduce porting costs.",[' POSIX and OS abstraction layers provide commonalities that reduce what?'],['porting costs']
1295,sensor node,Summary,"A sensor node, also known as a mote (chiefly in North America), is a node in a sensor network that is capable of performing some processing, gathering sensory information and communicating with other connected nodes in the network. A mote is a node but a node is not always a mote.","A sensor node, also known as a mote (chiefly in North America), is a node in a sensor network that is capable of performing some processing, gathering sensory information and communicating with other connected nodes in the network. A mote is a node but a node is not always a mote.","[' What is another name for a sensor node?', ' What is a mote in North America?', ' A mote is not always what?', ' What is a node but not always a mote?']","['mote', 'sensor node', 'node', 'A mote']"
1296,sensor node,History,"Although wireless sensor nodes have existed for decades and used for applications as diverse as earthquake measurements to warfare, the modern development of small sensor nodes dates back to the 1998 Smartdust project and the NASA Sensor Web One of the objectives of the Smartdust project was to create autonomous sensing and communication within a cubic millimeter of space. Though this project ended early on, it led to many more research projects. They include major research centres in Berkeley NEST  and CENS. The researchers involved in these projects coined the term mote to refer to a sensor node. The equivalent term in the NASA Sensor Webs Project for a physical sensor node is pod, although the sensor node in a Sensor Web can be another Sensor Web itself.  Physical sensor nodes have been able to increase their capability in conjunction with Moore's Law. The chip footprint contains more complex and lower powered microcontrollers.  Thus, for the same node footprint, more silicon capability can be packed into it. Nowadays, motes focus on providing the longest wireless range (dozens of km), the lowest energy consumption (a few uA) and the easiest development process for the user.","Although wireless sensor nodes have existed for decades and used for applications as diverse as earthquake measurements to warfare, the modern development of small sensor nodes dates back to the 1998 Smartdust project and the NASA Sensor Web One of the objectives of the Smartdust project was to create autonomous sensing and communication within a cubic millimeter of space. Though this project ended early on, it led to many more research projects.","[' How long have wireless sensor nodes been around?', ' What project was the first to develop small sensors?', ' When did the Smartdust project begin?', ' What was the goal of the Smartdust project?']","['decades', 'Smartdust', '1998', 'to create autonomous sensing and communication within a cubic millimeter of space']"
1297,model checking,Summary,"In computer science, model checking or property checking is a method for checking whether a finite-state model of a system meets a given specification (also known as correctness). This is typically associated with hardware or software systems, where the specification contains liveness requirements (such as avoidance of livelock) as well as safety requirements (such as avoidance of states representing a system crash).
","In computer science, model checking or property checking is a method for checking whether a finite-state model of a system meets a given specification (also known as correctness). This is typically associated with hardware or software systems, where the specification contains liveness requirements (such as avoidance of livelock) as well as safety requirements (such as avoidance of states representing a system crash).","[' In computer science, what is a method for checking whether a finite-state model of a system meets a given specification?', ' What is also known as correctness?', ' Hardware or software systems where specification contains liveness requirements include what?', ' What are liveness requirements?', ' What is a safety requirement?']","['model checking or property checking', 'model checking or property checking is a method for checking whether a finite-state model of a system meets a given specification', 'avoidance of livelock', 'avoidance of livelock', 'avoidance of states representing a system crash']"
1298,model checking,Summary,"In order to solve such a problem algorithmically, both the model of the system and its specification are formulated in some precise mathematical language. To this end, the problem is formulated as a task in logic, namely to check whether a structure satisfies a given logical formula. This general concept applies to many kinds of logic and many kinds of structures. A simple model-checking problem consists of verifying whether a formula in the propositional logic is satisfied by a given structure.
","In order to solve such a problem algorithmically, both the model of the system and its specification are formulated in some precise mathematical language. To this end, the problem is formulated as a task in logic, namely to check whether a structure satisfies a given logical formula.","[' In order to solve a problem algorithmically, both the model of the system and its specification are formulated in what language?', ' The problem is formulated as a task in what?']","['some precise mathematical', 'logic']"
1299,model checking,Overview,"Property checking is used for verification when two descriptions are not equivalent. During refinement, the specification is complemented with details that are unnecessary in the higher-level specification. There is no need to verify the newly introduced properties against the original specification since this is not possible. Therefore, the strict bi-directional equivalence check is relaxed to a one-way property check. The implementation or design is regarded as a model of the system, whereas the specifications are properties that the model must satisfy.","Property checking is used for verification when two descriptions are not equivalent. During refinement, the specification is complemented with details that are unnecessary in the higher-level specification.","[' What is used for verification when two descriptions are not equivalent?', ' What is complemented with details that are unnecessary in the higher-level specification?']","['Property checking', 'the specification']"
1300,model checking,Overview,"An important class of model-checking methods has been developed for checking models of hardware and software designs where the specification is given by a temporal logic formula. Pioneering work in temporal logic specification was done by Amir Pnueli, who received the 1996 Turing award for ""seminal work introducing temporal logic into computing science"".  Model checking began with the pioneering work of E. M. Clarke, E. A. Emerson, by J. P. Queille, and J. Sifakis. Clarke, Emerson, and Sifakis shared the 2007 Turing Award for their seminal work founding and developing the field of model checking.","An important class of model-checking methods has been developed for checking models of hardware and software designs where the specification is given by a temporal logic formula. Pioneering work in temporal logic specification was done by Amir Pnueli, who received the 1996 Turing award for ""seminal work introducing temporal logic into computing science"".","[' What has been developed for checking models of hardware and software designs?', ' What is given by a temporal logic formula?', ' Who did Amir Pnueli receive the 1996 Turing award for?', ' What was the 1996 Turing award for?', ' What award was given for ""seminal work introducing temporal logic into computing science""?']","['model-checking methods', 'the specification', 'seminal work introducing temporal logic into computing science', 'seminal work introducing temporal logic into computing science', '1996 Turing award']"
1301,model checking,Overview,"Model checking is most often applied to hardware designs. For software, because of undecidability (see computability theory) the approach cannot be fully algorithmic, apply to all systems, and always give an answer; in the general case, it may fail to prove or disprove a given property. In embedded-systems hardware, it is possible to validate a specification delivered, i.e., by means of UML activity diagrams or control interpreted Petri nets.","Model checking is most often applied to hardware designs. For software, because of undecidability (see computability theory) the approach cannot be fully algorithmic, apply to all systems, and always give an answer; in the general case, it may fail to prove or disprove a given property.","[' What is most often applied to hardware designs?', "" Why can't the approach be fully algorithmic?"", ' What may fail to prove or disprove a given property?']","['Model checking', 'undecidability', 'Model checking']"
1302,model checking,Overview,"The structure is usually given as a source code description in an industrial hardware description language or a special-purpose language. Such a program corresponds to a finite state machine (FSM), i.e., a directed graph consisting of nodes (or vertices) and edges. A set of atomic propositions is associated with each node, typically stating which memory elements are one. The nodes represent states of a system, the edges represent possible transitions that may alter the state, while the atomic propositions represent the basic properties that hold at a point of execution.
","The structure is usually given as a source code description in an industrial hardware description language or a special-purpose language. Such a program corresponds to a finite state machine (FSM), i.e., a directed graph consisting of nodes (or vertices) and edges.","[' What is a finite state machine?', ' What does FSM stand for?']","['FSM', 'finite state machine']"
1303,model checking,Overview,"Formally, the problem can be stated as follows: given a desired property, expressed as a temporal logic formula 



p


{\displaystyle p}
, and a structure 



M


{\displaystyle M}
 with initial state 



s


{\displaystyle s}
, decide if 



M
,
s
⊨
p


{\displaystyle M,s\models p}
.  If 



M


{\displaystyle M}
 is finite, as it is in hardware, model checking reduces to a graph search.
","Formally, the problem can be stated as follows: given a desired property, expressed as a temporal logic formula 



p


{\displaystyle p}
, and a structure 



M


{\displaystyle M}
 with initial state 



s


{\displaystyle s}
, decide if 



M
,
s
⊨
p


{\displaystyle M,s\models p}
. If 



M


{\displaystyle M}
 is finite, as it is in hardware, model checking reduces to a graph search.","[' What can be stated as a formal statement of the problem?', ' If M <unk>displaystyle M<unk> is finite, as it is in hardware, what does model checking reduce to?', ' What does model checking reduce to in hardware?']","['M', 'a graph search', 'a graph search']"
1304,model checking,Symbolic model checking,"Instead of enumerating reachable states one at a time, the state space can sometimes be traversed more efficiently by considering large numbers of states at a single step. When such state space traversal is based on representations of a set of states and transition relations as logical formulas, binary decision diagrams (BDD) or other related data structures, the model-checking method is symbolic.
","Instead of enumerating reachable states one at a time, the state space can sometimes be traversed more efficiently by considering large numbers of states at a single step. When such state space traversal is based on representations of a set of states and transition relations as logical formulas, binary decision diagrams (BDD) or other related data structures, the model-checking method is symbolic.","[' How can the state space be traversed more efficiently?', ' What is based on representations of a set of states and transition relations?', ' What is BDD?', ' What is the model-checking method?']","['by considering large numbers of states at a single step', 'state space traversal', 'binary decision diagrams', 'symbolic']"
1305,model checking,Symbolic model checking,"Historically, the first symbolic methods used BDDs. After the success of propositional satisfiability in solving the planning problem in artificial intelligence (see satplan) in 1996, the same approach was generalized to model checking for linear temporal logic (LTL): the planning problem corresponds to model checking for safety properties. This method is known as bounded model checking. The success of Boolean satisfiability solvers in bounded model checking led to the widespread use of satisfiability solvers in symbolic model checking.","Historically, the first symbolic methods used BDDs. After the success of propositional satisfiability in solving the planning problem in artificial intelligence (see satplan) in 1996, the same approach was generalized to model checking for linear temporal logic (LTL): the planning problem corresponds to model checking for safety properties.","[' When did propositional satisfiability solve the planning problem in artificial intelligence?', ' What is LTL?', ' The planning problem corresponds to what?']","['1996', 'model checking for linear temporal logic', 'model checking for safety properties']"
1306,model checking,Techniques,"Model-checking tools face a combinatorial blow up of the state-space, commonly known as the state explosion problem, that must be addressed to solve most real-world problems.  There are several approaches to combat this problem.
","Model-checking tools face a combinatorial blow up of the state-space, commonly known as the state explosion problem, that must be addressed to solve most real-world problems. There are several approaches to combat this problem.","[' What is the combinatorial blow up of the state-space commonly known as?', ' What must be addressed to solve most real-world problems?', ' There are several approaches to combat what?']","['state explosion problem', 'state explosion problem', 'state explosion problem']"
1307,model checking,First-order logic,"Model checking is also studied in the field of computational complexity theory. Specifically, a first-order logical formula is fixed without free variables and the following decision problem is considered: 
","Model checking is also studied in the field of computational complexity theory. Specifically, a first-order logical formula is fixed without free variables and the following decision problem is considered:","[' Model checking is also studied in what field?', ' A first-order logical formula is fixed without what?', ' What decision problem is considered?']","['computational complexity theory', 'free variables', 'a first-order logical formula is fixed without free variables']"
1308,model checking,First-order logic,"This problem is in the circuit class AC0. It is tractable when imposing some restrictions on the input structure: for instance, requiring that it has treewidth bounded by a constant (which more generally implies the tractability of model checking for monadic second-order logic), bounding the degree of every domain element, and more general conditions such as bounded expansion, locally bounded expansion, and nowhere-dense structures. These results have been extended to the task of enumerating all solutions to a first-order formula with free variables.","This problem is in the circuit class AC0. It is tractable when imposing some restrictions on the input structure: for instance, requiring that it has treewidth bounded by a constant (which more generally implies the tractability of model checking for monadic second-order logic), bounding the degree of every domain element, and more general conditions such as bounded expansion, locally bounded expansion, and nowhere-dense structures.","[' What is the problem with the circuit class AC0?', ' What is tractable when imposing some restrictions on the input structure?', ' Monadic second-order logic), bounding the degree of every domain element, and more general conditions such as what?']","['It is tractable when imposing some restrictions on the input structure', 'AC0', 'bounded expansion, locally bounded expansion, and nowhere-dense structures']"
1309,linear temporal logic,Summary,"In logic, linear temporal logic or linear-time temporal logic (LTL) is a modal temporal logic with modalities referring to time. In LTL, one can encode formulae about the future of paths, e.g., a condition will eventually be true, a condition will be true until another fact becomes true, etc. It is a fragment of the more complex CTL*, which additionally allows branching time and quantifiers. Subsequently, LTL is sometimes called propositional temporal logic, abbreviated PTL.
In terms of expressive power, linear temporal logic (LTL) is a fragment of first-order logic.","In logic, linear temporal logic or linear-time temporal logic (LTL) is a modal temporal logic with modalities referring to time. In LTL, one can encode formulae about the future of paths, e.g., a condition will eventually be true, a condition will be true until another fact becomes true, etc.","[' What is a modal temporal logic with modalities referring to time?', ' In LTL, one can encode formulae about the future of what?']","['linear temporal logic', 'paths']"
1310,linear temporal logic,Syntax,"LTL is built up from a finite set of propositional variables AP, the logical operators ¬ and ∨, and the temporal modal operators X (some literature uses O or N) and U. 
Formally, the set of LTL formulas over AP is inductively defined as follows:
","LTL is built up from a finite set of propositional variables AP, the logical operators ¬ and ∨, and the temporal modal operators X (some literature uses O or N) and U. Formally, the set of LTL formulas over AP is inductively defined as follows:","[' What is built up from a finite set of propositional variables AP, logical operators <unk> and <unk>, and temporal modal operators X and U?', ' The set of LTL formulas over AP is inductively defined as follows:']","['LTL', 'LTL is built up from a finite set of propositional variables AP']"
1311,linear temporal logic,Syntax,"X is read as next and U is read as until.
Other than these fundamental operators, there are additional logical and temporal operators defined in terms of the fundamental operators to write LTL formulas succinctly.
The additional logical operators are ∧, →, ↔, true, and false.
Following are the additional temporal operators.
","X is read as next and U is read as until. Other than these fundamental operators, there are additional logical and temporal operators defined in terms of the fundamental operators to write LTL formulas succinctly.","[' What is read as next and U as until?', ' What are logical and temporal operators defined in terms of?']","['X', 'the fundamental operators to write LTL formulas succinctly']"
1312,linear temporal logic,Semantics,"An LTL formula can be satisfied by an infinite sequence of truth evaluations of variables in AP.
These sequences can be viewed as a word on a path of a Kripke structure (an ω-word over alphabet 2AP).
Let w = a0,a1,a2,... be such an ω-word. Let w(i) = ai. Let wi = ai,ai+1,..., which is a suffix of w. Formally, the satisfaction relation ⊨ between a word and an LTL formula is defined as follows:
",An LTL formula can be satisfied by an infinite sequence of truth evaluations of variables in AP. These sequences can be viewed as a word on a path of a Kripke structure (an ω-word over alphabet 2AP).,"[' How can an LTL formula be satisfied?', ' What can an infinite sequence of truth evaluations of variables in AP be viewed as?']","['by an infinite sequence of truth evaluations of variables in AP', 'a word on a path of a Kripke structure']"
1313,linear temporal logic,Semantics,"We say an ω-word w satisfies an LTL formula ψ when w ⊨ ψ. 
The ω-language L(ψ) defined by ψ is {w | w ⊨ ψ}, which is the set of ω-words that satisfy ψ.
A formula ψ is satisfiable if there exist an ω-word w such that w ⊨ ψ. 
A formula ψ is valid if for each ω-word w over alphabet 2AP, w ⊨ ψ.
","We say an ω-word w satisfies an LTL formula ψ when w ⊨ ψ. The ω-language L(ψ) defined by ψ is {w | w ⊨ ψ}, which is the set of ω-words that satisfy ψ.","[' When do we say an <unk>-word w satisfies an LTL formula?', ' What is the <unk> language L(<unk>) defined by?']","['w ⊨ ψ', 'ψ']"
1314,linear temporal logic,Equivalences,"Let φ, ψ, and ρ be LTL formulas. The following tables list some of the useful equivalences that extend standard equivalences among the usual logical operators.
","Let φ, ψ, and ρ be LTL formulas. The following tables list some of the useful equivalences that extend standard equivalences among the usual logical operators.","[' Let <unk>, <unk> and <unk> be what?', ' What are some of the useful equivalences?']","['LTL formulas', 'The following tables list some of the useful equivalences that extend standard equivalences among the usual logical operators']"
1315,linear temporal logic,Negation normal form,"Using the above equivalences for negation propagation, it is possible to derive the normal form. This normal form allows R, true, false, and ∧ to appear in the formula, which are not fundamental operators of LTL. Note that the transformation to the negation normal form does not blow up the size of the formula. This normal form is useful in translation from LTL to Büchi automaton.
","Using the above equivalences for negation propagation, it is possible to derive the normal form. This normal form allows R, true, false, and ∧ to appear in the formula, which are not fundamental operators of LTL.","[' Using the above equivalences for negation propagation, it is possible to derive what?', ' The normal form allows R, true, false, and <unk> to appear in what formula?', ' What are not fundamental operators of LTL?']","['the normal form', '∧ to appear in the formula, which are not fundamental operators of LTL', 'R, true, false, and ∧']"
1316,linear temporal logic,Relations with other logics,"Computation tree logic (CTL) and linear temporal logic (LTL) are both a subset of CTL*, but are incomparable. For example,
","Computation tree logic (CTL) and linear temporal logic (LTL) are both a subset of CTL*, but are incomparable. For example,",[' Computation tree logic (CTL) and linear temporal logic (LTL) are both a subset of what?'],"['CTL*,']"
1317,linear temporal logic,Computational problems,Model checking and satisfiability against an LTL formula are PSPACE-complete problems. LTL synthesis and the problem of verification of games against an LTL winning condition is 2EXPTIME-complete.,Model checking and satisfiability against an LTL formula are PSPACE-complete problems. LTL synthesis and the problem of verification of games against an LTL winning condition is 2EXPTIME-complete.,"[' What are PSPACE-complete problems?', ' What is the problem of verification of games against an LTL winning condition?']","['Model checking and satisfiability against an LTL formula', '2EXPTIME-complete']"
1318,machine translation,Summary,"On a basic level, MT performs mechanical substitution of words in one language for words in another, but that alone rarely produces a good translation because recognition of whole phrases and their closest counterparts in the target language is needed. Not all words in one language have equivalent words in another language, and many words have more than one meaning. 
","On a basic level, MT performs mechanical substitution of words in one language for words in another, but that alone rarely produces a good translation because recognition of whole phrases and their closest counterparts in the target language is needed. Not all words in one language have equivalent words in another language, and many words have more than one meaning.","[' What does MT perform on a basic level?', ' What is needed to produce a good translation?', ' How many words have more than one meaning?']","['mechanical substitution of words in one language for words in another', 'recognition of whole phrases and their closest counterparts in the target language', 'many']"
1319,machine translation,Summary,"Current machine translation software often allows for customization by domain or profession (such as weather reports), improving output by limiting the scope of allowable substitutions. This technique is particularly effective in domains where formal or formulaic language is used. It follows that machine translation of government and legal documents more readily produces usable output than conversation or less standardised text.
","Current machine translation software often allows for customization by domain or profession (such as weather reports), improving output by limiting the scope of allowable substitutions. This technique is particularly effective in domains where formal or formulaic language is used.","[' What type of software allows for customization by domain or profession?', ' What is one example of a machine translation software that allows customization?', ' In what domains is machine translation particularly effective?']","['machine translation', 'weather reports', 'formal or formulaic language is used']"
1320,machine translation,Summary,"Improved output quality can also be achieved by human intervention: for example, some systems are able to translate more accurately if the user has unambiguously identified which words in the text are proper names. With the assistance of these techniques, MT has proven useful as a tool to assist human translators and, in a very limited number of cases, can even produce output that can be used as is (e.g., weather reports).
","Improved output quality can also be achieved by human intervention: for example, some systems are able to translate more accurately if the user has unambiguously identified which words in the text are proper names. With the assistance of these techniques, MT has proven useful as a tool to assist human translators and, in a very limited number of cases, can even produce output that can be used as is (e.g., weather reports).","[' What can also be achieved by human intervention?', ' Some systems are able to translate more accurately if the user has unambiguously identified which words in text are proper names?', ' What has proven useful as a tool to assist human?', ' MT has proven useful as a tool to assist what?', ' MT can even produce output that can be used as is (e.g., weather reports).']","['Improved output quality', 'human intervention', 'MT', 'human translators', 'MT has proven useful as a tool to assist human translators']"
1321,machine translation,Summary,"The progress and potential of machine translation have been much debated through its history. Since the 1950s, a number of scholars, first and most notably Yehoshua Bar-Hillel, have questioned the possibility of achieving fully automatic machine translation of high quality.","The progress and potential of machine translation have been much debated through its history. Since the 1950s, a number of scholars, first and most notably Yehoshua Bar-Hillel, have questioned the possibility of achieving fully automatic machine translation of high quality.",[' In what decade did Yehoshua Bar-Hillel first question the possibility of fully automatic machine translation of high quality?'],['1950s']
1322,machine translation,Translation process,"Behind this ostensibly simple procedure lies a complex cognitive operation.  To decode the meaning of the source text in its entirety, the translator must interpret and analyse all the features of the text, a process that requires in-depth knowledge of the grammar, semantics, syntax, idioms, etc., of the source language, as well as the culture of its speakers. The translator needs the same in-depth knowledge to re-encode the meaning in the target language.","Behind this ostensibly simple procedure lies a complex cognitive operation. To decode the meaning of the source text in its entirety, the translator must interpret and analyse all the features of the text, a process that requires in-depth knowledge of the grammar, semantics, syntax, idioms, etc., of the source language, as well as the culture of its speakers.","[' What is behind the simple process of decoding a source text?', ' What does the translator need to understand to understand the meaning of the source text in its entirety?', ' What does the grammar, semantics, syntax, idioms, etc., of the source language have to do with the culture of its speakers?']","['complex cognitive operation', 'all the features of the text', 'in-depth knowledge']"
1323,machine translation,Translation process,"Therein lies the challenge in machine translation: how to program a computer that will ""understand"" a text as a person does, and that will ""create"" a new text in the target language that sounds as if it has been written by a person. Unless aided by a 'knowledge base' MT provides only a general, though imperfect, approximation of the original text, getting the ""gist"" of it (a process called ""gisting""). This is sufficient for many purposes, including making best use of the finite and expensive time of a human translator, reserved for those cases in which total accuracy is indispensable.
","Therein lies the challenge in machine translation: how to program a computer that will ""understand"" a text as a person does, and that will ""create"" a new text in the target language that sounds as if it has been written by a person. Unless aided by a 'knowledge base' MT provides only a general, though imperfect, approximation of the original text, getting the ""gist"" of it (a process called ""gisting"").","[' What is the challenge in machine translation?', ' How does MT program a computer to understand a text as a person does?', "" Unless aided by a 'knowledge base', MT provides only a general, though imperfect, approximation of what?"", "" What is MT's process called?""]","['how to program a computer that will ""understand"" a text as a person does', 'MT provides only a general, though imperfect, approximation of the original text', 'the original text', 'gisting']"
1324,machine translation,Approaches,"Generally, rule-based methods parse a text, usually creating an intermediary, symbolic representation, from which the text in the target language is generated. According to the nature of the intermediary representation, an approach is described as interlingual machine translation or transfer-based machine translation. These methods require extensive lexicons with morphological, syntactic, and semantic information, and large sets of rules.
","Generally, rule-based methods parse a text, usually creating an intermediary, symbolic representation, from which the text in the target language is generated. According to the nature of the intermediary representation, an approach is described as interlingual machine translation or transfer-based machine translation.","[' Rule-based methods parse a text, creating what?', ' What is an approach described as?', ' Interlingual machine translation or transfer-based machine translation is a term for what type of representation?']","['an intermediary, symbolic representation', 'interlingual machine translation', 'intermediary representation']"
1325,machine translation,Approaches,"Given enough data, machine translation programs often work well enough for a native speaker of one language to get the approximate meaning of what is written by the other native speaker. The difficulty is getting enough data of the right kind to support the particular method. For example, the large multilingual corpus of data needed for statistical methods to work is not necessary for the grammar-based methods. But then, the grammar methods need a skilled linguist to carefully design the grammar that they use.
","Given enough data, machine translation programs often work well enough for a native speaker of one language to get the approximate meaning of what is written by the other native speaker. The difficulty is getting enough data of the right kind to support the particular method.","[' Machine translation programs often work well enough for a native speaker to get what?', ' What is the difficulty in getting enough data to support the particular method?']","['the approximate meaning of what is written by the other native speaker', 'getting enough data of the right kind']"
1326,machine translation,Translation from multiparallel sources,"Some work has been done in the utilization of multiparallel corpora, that is a body of text that has been translated into 3 or more languages.  Using these methods, a text that has been translated into 2 or more languages may be utilized in combination to provide a more accurate translation into a third language compared with if just one of those source languages were used alone.","Some work has been done in the utilization of multiparallel corpora, that is a body of text that has been translated into 3 or more languages. Using these methods, a text that has been translated into 2 or more languages may be utilized in combination to provide a more accurate translation into a third language compared with if just one of those source languages were used alone.","[' What is the term for a body of text that has been translated into 3 or more languages?', ' What is a multiparallel corpora?', ' What can be used in combination to provide a more accurate translation into a third language compared with if only one of the source languages were used?']","['multiparallel corpora', 'a body of text that has been translated into 3 or more languages', 'a text that has been translated into 2 or more languages']"
1327,machine translation,Ontologies in MT,"An ontology is a formal representation of knowledge that includes the concepts (such as objects, processes etc.) in a domain and some relations between them. If the stored information is of linguistic nature, one can speak of a lexicon.
In NLP, ontologies can be used as a source of knowledge for machine translation systems. With access to a large knowledge base, systems can be enabled to resolve many (especially lexical) ambiguities on their own.
In the following classic examples, as humans, we are able to interpret the prepositional phrase according to the context because we use our world knowledge, stored in our lexicons:
","An ontology is a formal representation of knowledge that includes the concepts (such as objects, processes etc.) in a domain and some relations between them.","[' What is an ontology?', ' What is a formal representation of knowledge?']","['a formal representation of knowledge', 'An ontology']"
1328,machine translation,Ontologies in MT,"A machine translation system initially would not be able to differentiate between the meanings because syntax does not change. With a large enough ontology as a source of knowledge however, the possible interpretations of ambiguous words in a specific context can be reduced.
Other areas of usage for ontologies within NLP include information retrieval, information extraction and text summarization.","A machine translation system initially would not be able to differentiate between the meanings because syntax does not change. With a large enough ontology as a source of knowledge however, the possible interpretations of ambiguous words in a specific context can be reduced.","[' Why would a machine translation system initially not be able to differentiate between meanings?', ' What can be reduced with a large enough ontology as a source of knowledge?']","['syntax does not change', 'the possible interpretations of ambiguous words in a specific context']"
1329,machine translation,Applications,"While no system provides the holy grail of fully automatic high-quality machine translation of unrestricted text, many fully automated systems produce reasonable output. The quality of machine translation is substantially improved if the domain is restricted and controlled.","While no system provides the holy grail of fully automatic high-quality machine translation of unrestricted text, many fully automated systems produce reasonable output. The quality of machine translation is substantially improved if the domain is restricted and controlled.","[' What is the holy grail of fully automatic high-quality machine translation of unrestricted text?', ' What is substantially improved if the domain is restricted and controlled?']","['no system', 'The quality of machine translation']"
1330,machine translation,Applications,"Despite their inherent limitations, MT programs are used around the world. Probably the largest institutional user is the European Commission. The MOLTO project, for example, coordinated by the University of Gothenburg, received more than 2.375 million euros project support from the EU to create a reliable translation tool that covers a majority of the EU languages. The further development of MT systems comes at a time when budget cuts in human translation may increase the EU's dependency on reliable MT programs. The European Commission contributed 3.072 million euros (via its ISA programme) for the creation of MT@EC, a statistical machine translation program tailored to the administrative needs of the EU, to replace a previous rule-based machine translation system.","Despite their inherent limitations, MT programs are used around the world. Probably the largest institutional user is the European Commission.",[' What is the largest institutional user of MT programs?'],['the European Commission']
1331,machine translation,Applications,"In 2005, Google claimed that promising results were obtained using a proprietary statistical machine translation engine. The statistical translation engine used in the Google language tools for Arabic <-> English and Chinese <-> English had an overall score of 0.4281 over the runner-up IBM's BLEU-4 score of 0.3954 (Summer 2006) in tests conducted by the National Institute for Standards and Technology.","In 2005, Google claimed that promising results were obtained using a proprietary statistical machine translation engine. The statistical translation engine used in the Google language tools for Arabic <-> English and Chinese <-> English had an overall score of 0.4281 over the runner-up IBM's BLEU-4 score of 0.3954 (Summer 2006) in tests conducted by the National Institute for Standards and Technology.","[' What did Google claim to have obtained promising results using in 2005?', "" What was the overall score of Google's statistical machine translation engine for Arabic <unk>-> English?"", ' Which IBM BLEU-4 score did Google use in Summer 2006?', "" What was the runner up score for IBM's BLEU-4?"", ' In what year did IBM get a BLUE-4 score?', ' Who conducted the tests?']","['a proprietary statistical machine translation engine', '0.4281', '0.3954', '0.3954', '2006', 'National Institute for Standards and Technology']"
1332,machine translation,Applications,"With the recent focus on terrorism, the military sources in the United States have been investing significant amounts of money in natural language engineering. In-Q-Tel (a venture capital fund, largely funded by the US Intelligence Community, to stimulate new technologies through private sector entrepreneurs) brought up companies like Language Weaver. Currently the military community is interested in translation and processing of languages like Arabic, Pashto, and Dari. Within these languages, the focus is on key phrases and quick communication between military members and civilians through the use of mobile phone apps. The Information Processing Technology Office in DARPA hosts programs like TIDES and Babylon translator. US Air Force has awarded a $1 million contract to develop a language translation technology.","With the recent focus on terrorism, the military sources in the United States have been investing significant amounts of money in natural language engineering. In-Q-Tel (a venture capital fund, largely funded by the US Intelligence Community, to stimulate new technologies through private sector entrepreneurs) brought up companies like Language Weaver.","[' What is the focus of the recent focus on terrorism?', ' What is a venture capital fund funded by the US Intelligence Community to stimulate new technologies?']","['natural language engineering', 'In-Q-Tel']"
1333,machine translation,Applications,"The notable rise of social networking on the web in recent years has created yet another niche for the application of machine translation software – in utilities such as Facebook, or instant messaging clients such as Skype, GoogleTalk, MSN Messenger, etc. – allowing users speaking different languages to communicate with each other. Machine translation applications have also been released for most mobile devices, including mobile telephones, pocket PCs, PDAs, etc. Due to their portability, such instruments have come to be designated as mobile translation tools enabling mobile business networking between partners speaking different languages, or facilitating both foreign language learning and unaccompanied traveling to foreign countries without the need of the intermediation of a human translator.
","The notable rise of social networking on the web in recent years has created yet another niche for the application of machine translation software – in utilities such as Facebook, or instant messaging clients such as Skype, GoogleTalk, MSN Messenger, etc. – allowing users speaking different languages to communicate with each other.","[' What has created a niche for the application of machine translation software?', ' What is a utility that allows users speaking different languages to communicate with one another?', ' What is the purpose of allowing users speaking different languages to communicate with each other?']","['social networking', 'Facebook', 'machine translation software']"
1334,machine translation,Applications,"Despite being labelled as an unworthy competitor to human translation in 1966 by the Automated Language Processing Advisory Committee put together by the United States government, the quality of machine translation has now been improved to such levels that its application in online collaboration and in the medical field are being investigated. The application of this technology in medical settings where human translators are absent is another topic of research, but difficulties arise due to the importance of accurate translations in medical diagnoses.","Despite being labelled as an unworthy competitor to human translation in 1966 by the Automated Language Processing Advisory Committee put together by the United States government, the quality of machine translation has now been improved to such levels that its application in online collaboration and in the medical field are being investigated. The application of this technology in medical settings where human translators are absent is another topic of research, but difficulties arise due to the importance of accurate translations in medical diagnoses.","[' In what year was machine translation labelled as unworthy of competition?', ' What group was put together by the United States government?', ' Machine translation has been improved to such levels that its application in online collaboration and in the medical field are what?', ' In what field are online collaboration and medical applications being investigated?', ' What is another topic of research?', ' The importance of accurate translations in medical diagnoses arises due to what?']","['1966', 'Automated Language Processing Advisory Committee', 'being investigated', 'medical', 'The application of this technology in medical settings', 'difficulties']"
1335,machine translation,Evaluation,"There are many factors that affect how machine translation systems are evaluated. These factors include the intended use of the translation, the nature of the machine translation software, and the nature of the translation process.
","There are many factors that affect how machine translation systems are evaluated. These factors include the intended use of the translation, the nature of the machine translation software, and the nature of the translation process.","[' What factors affect how machine translation systems are evaluated?', ' What are some of the factors that affect machine translation system evaluation?']","['the intended use of the translation, the nature of the machine translation software, and the nature of the translation process', 'the intended use of the translation, the nature of the machine translation software, and the nature of the translation process']"
1336,machine translation,Evaluation,"Different programs may work well for different purposes. For example, statistical machine translation (SMT) typically outperforms example-based machine translation (EBMT), but researchers found that when evaluating English to French translation, EBMT performs better. The same concept applies for technical documents, which can be more easily translated by SMT because of their formal language.
","Different programs may work well for different purposes. For example, statistical machine translation (SMT) typically outperforms example-based machine translation (EBMT), but researchers found that when evaluating English to French translation, EBMT performs better.","[' What does SMT stand for?', ' What is EBMT?']","['statistical machine translation', 'example-based machine translation']"
1337,machine translation,Evaluation,"There are various means for evaluating the output quality of machine translation systems. The oldest is the use of human judges to assess a translation's quality. Even though human evaluation is time-consuming, it is still the most reliable method to compare different systems such as rule-based and statistical systems. Automated means of evaluation include BLEU, NIST, METEOR, and LEPOR.",There are various means for evaluating the output quality of machine translation systems. The oldest is the use of human judges to assess a translation's quality.,"[' What is the oldest method for evaluating the output quality of machine translation systems?', "" What is used to assess a translation's quality?""]","['human judges', 'human judges']"
1338,machine translation,Evaluation,"Relying exclusively on unedited machine translation ignores the fact that communication in human language is context-embedded and that it takes a person to comprehend the context of the original text with a reasonable degree of probability. It is certainly true that even purely human-generated translations are prone to error. Therefore, to ensure that a machine-generated translation will be useful to a human being and that publishable-quality translation is achieved, such translations must be reviewed and edited by a human. The late Claude Piron wrote that machine translation, at its best, automates the easier part of a translator's job; the harder and more time-consuming part usually involves doing extensive research to resolve ambiguities in the source text, which the grammatical and lexical exigencies of the target language require to be resolved. Such research is a necessary prelude to the pre-editing necessary in order to provide input for machine-translation software such that the output will not be meaningless.",Relying exclusively on unedited machine translation ignores the fact that communication in human language is context-embedded and that it takes a person to comprehend the context of the original text with a reasonable degree of probability. It is certainly true that even purely human-generated translations are prone to error.,"[' What ignores the fact that communication in human language is context-embedded and that it takes a person to comprehend the context of the original text with a reasonable degree of probability?', ' Even purely human-generated translations are prone to what?']","['Relying exclusively on unedited machine translation', 'error']"
1339,machine translation,Evaluation,"In addition to disambiguation problems, decreased accuracy can occur due to varying levels of training data for machine translating programs. Both example-based and statistical machine translation rely on a vast array of real example sentences as a base for translation, and when too many or too few sentences are analyzed accuracy is jeopardized. Researchers found that when a program is trained on 203,529 sentence pairings, accuracy actually decreases. The optimal level of training data seems to be just over 100,000 sentences, possibly because as training data increases, the number of possible sentences increases, making it harder to find an exact translation match.
","In addition to disambiguation problems, decreased accuracy can occur due to varying levels of training data for machine translating programs. Both example-based and statistical machine translation rely on a vast array of real example sentences as a base for translation, and when too many or too few sentences are analyzed accuracy is jeopardized.","[' Why can disambiguation problems occur?', ' What can cause decreased accuracy?', ' How do both example-based and statistical machine translation use examples?', ' When too many or too few sentences are analyzed, accuracy is what?']","['varying levels of training data for machine translating programs', 'varying levels of training data for machine translating programs', 'a vast array of real example sentences as a base for translation', 'jeopardized']"
1340,machine translation,Using machine translation as a teaching tool,"Although there have been concerns about machine translation's accuracy, Dr. Ana Nino of the University of Manchester has researched some of the advantages in utilizing machine translation in the classroom.  One such pedagogical method is called using ""MT as a Bad Model.""  MT as a Bad Model forces the language learner to identify inconsistencies or incorrect aspects of a translation; in turn, the individual will (hopefully) possess a better grasp of the language.  Dr. Nino cites that this teaching tool was implemented in the late 1980s.  At the end of various semesters, Dr. Nino was able to obtain survey results from students who had used MT as a Bad Model (as well as other models.)  Overwhelmingly, students felt that they had observed improved comprehension, lexical retrieval, and increased confidence in their target language.","Although there have been concerns about machine translation's accuracy, Dr. Ana Nino of the University of Manchester has researched some of the advantages in utilizing machine translation in the classroom. One such pedagogical method is called using ""MT as a Bad Model.""","[' Who has researched some of the advantages of using machine translation in the classroom?', ' What is one pedagogical method called using ""MT as a Bad Model""?']","['Dr. Ana Nino', 'machine translation']"
1341,machine translation,Machine translation and signed languages,"In the early 2000s, options for machine translation between spoken and signed languages were severely limited. It was a common belief that deaf individuals could use traditional translators. However, stress, intonation, pitch, and timing are conveyed much differently in spoken languages compared to signed languages. Therefore, a deaf individual may misinterpret or become confused about the meaning of written text that is based on a spoken language.","In the early 2000s, options for machine translation between spoken and signed languages were severely limited. It was a common belief that deaf individuals could use traditional translators.","[' In the early 2000s, options for machine translation between spoken and signed languages were severely limited.', ' What was a common belief that deaf individuals could use?']","['deaf individuals could use traditional translators', 'traditional translators']"
1342,machine translation,Machine translation and signed languages,"Researchers Zhao, et al. (2000), developed a prototype called TEAM (translation from English to ASL by machine) that completed English to American Sign Language (ASL) translations. The program would first analyze the syntactic, grammatical, and morphological aspects of the English text. Following this step, the program accessed a sign synthesizer, which acted as a dictionary for ASL. This synthesizer housed the process one must follow to complete ASL signs, as well as the meanings of these signs. Once the entire text is analyzed and the signs necessary to complete the translation are located in the synthesizer, a computer generated human appeared and would use ASL to sign the English text to the user.","Researchers Zhao, et al. (2000), developed a prototype called TEAM (translation from English to ASL by machine) that completed English to American Sign Language (ASL) translations.","[' What was the name of the prototype developed by Zhao and colleagues in 2000?', ' What is TEAM?']","['TEAM', 'translation from English to ASL by machine']"
1343,machine translation,Copyright,"Only works that are original are subject to copyright protection, so some scholars claim that machine translation results are not entitled to copyright protection because MT does not involve creativity. The copyright at issue is for a derivative work; the author of the original work in the original language does not lose his rights when a work is translated: a translator must have permission to publish a translation.
","Only works that are original are subject to copyright protection, so some scholars claim that machine translation results are not entitled to copyright protection because MT does not involve creativity. The copyright at issue is for a derivative work; the author of the original work in the original language does not lose his rights when a work is translated: a translator must have permission to publish a translation.","[' What type of work is subject to copyright protection?', ' Some scholars claim that machine translation results are not entitled to what?', ' What is the copyright at issue?', ' Who does the author of the original work in the original language do?', ' Who does not lose his rights when a work is translated?', ' Who must have permission to publish a translation?']","['works that are original', 'copyright protection', 'a derivative work', 'does not lose his rights when a work is translated', 'the author of the original work in the original language', 'a translator']"
1344,multi-objective optimization,Summary,"Multi-objective optimization (also known as multi-objective programming, vector optimization, multicriteria optimization, multiattribute optimization or Pareto optimization) is an area of multiple criteria decision making that is concerned with mathematical optimization problems involving more than one objective function to be optimized simultaneously. Multi-objective optimization has been applied in many fields of science, including engineering, economics and logistics where optimal decisions need to be taken in the presence of trade-offs between two or more conflicting objectives. Minimizing cost while maximizing comfort while buying a car, and maximizing performance whilst minimizing fuel consumption and emission of pollutants of a vehicle are examples of multi-objective optimization problems involving two and three objectives, respectively. In practical problems, there can be more than three objectives.
","Multi-objective optimization (also known as multi-objective programming, vector optimization, multicriteria optimization, multiattribute optimization or Pareto optimization) is an area of multiple criteria decision making that is concerned with mathematical optimization problems involving more than one objective function to be optimized simultaneously. Multi-objective optimization has been applied in many fields of science, including engineering, economics and logistics where optimal decisions need to be taken in the presence of trade-offs between two or more conflicting objectives.","[' What is multi-objective optimization also known as?', ' What is the area of multiple criteria decision making concerned with?', ' How many objective functions are to be optimized simultaneously?', ' Multi-objective optimization has been applied in many fields of science, including engineering, economics and logistics where?']","['multi-objective programming', 'Multi-objective optimization', 'more than one', 'optimal decisions need to be taken in the presence of trade-offs between two or more conflicting objectives']"
1345,multi-objective optimization,Summary,"For a nontrivial multi-objective optimization problem, no single solution exists that simultaneously optimizes each objective. In that case, the objective functions are said to be conflicting. A solution is called nondominated, Pareto optimal, Pareto efficient or noninferior, if none of the objective functions can be improved in value without degrading some of the other objective values. Without additional subjective preference information, there may exist a (possibly infinite) number of Pareto optimal solutions, all of which are considered equally good. Researchers study multi-objective optimization problems from different viewpoints and, thus, there exist different solution philosophies and goals when setting and solving them. The goal may be to find a representative set of Pareto optimal solutions, and/or quantify the trade-offs in satisfying the different objectives, and/or finding a single solution that satisfies the subjective preferences of a human decision maker (DM).
","For a nontrivial multi-objective optimization problem, no single solution exists that simultaneously optimizes each objective. In that case, the objective functions are said to be conflicting.","[' In a nontrivial multi-objective optimization problem, what does no single solution exist that simultaneously optimizes each objective?', ' The objective functions are said to be conflicting in what case?']","['the objective functions are said to be conflicting', 'nontrivial multi-objective optimization problem']"
1346,multi-objective optimization,Introduction,"A multi-objective optimization problem is an optimization problem that involves multiple objective functions. In mathematical terms, a multi-objective optimization problem can be formulated as
","A multi-objective optimization problem is an optimization problem that involves multiple objective functions. In mathematical terms, a multi-objective optimization problem can be formulated as","[' What is an optimization problem that involves multiple objective functions?', ' What can a multi-objective optimization problem be formulated as?']","['multi-objective optimization problem', 'In mathematical terms']"
1347,multi-objective optimization,Introduction,"where the integer 



k
≥
2


{\displaystyle k\geq 2}
 is the number of objectives and the set 



X


{\displaystyle X}
 is the feasible set of decision vectors, which is typically 



X
⊆


R


n




{\displaystyle X\subseteq \mathbb {R} ^{n}}
 but it depends on the 



n


{\displaystyle n}
-dimensional application domain. The feasible set is typically defined by some constraint functions. In addition, the vector-valued objective function is often defined as
","where the integer 



k
≥
2


{\displaystyle k\geq 2}
 is the number of objectives and the set 



X


{\displaystyle X}
 is the feasible set of decision vectors, which is typically 



X
⊆


R


n




{\displaystyle X\subseteq \mathbb {R} ^{n}}
 but it depends on the 



n


{\displaystyle n}
-dimensional application domain. The feasible set is typically defined by some constraint functions.","[' Where the integer k <unk> 2 <unk>displaystyle k<unk>geq 2<unk> is the number of objectives?', ' What is the feasible set of decision vectors?']","['k\n≥\n2', 'X']"
1348,complexity,Summary,"The term is generally used to characterize something with many parts where those parts interact with each other in multiple ways, culminating in a higher order of emergence greater than the sum of its parts. The study of these complex linkages at various scales is the main goal of complex systems theory.
","The term is generally used to characterize something with many parts where those parts interact with each other in multiple ways, culminating in a higher order of emergence greater than the sum of its parts. The study of these complex linkages at various scales is the main goal of complex systems theory.","[' What term is used to characterize something with many parts where parts interact with each other in multiple ways?', ' What is the main goal of complex linkages at various scales?', ' What is the main goal of complex systems theory?']","['The term is generally used to characterize something with many parts where those parts interact with each other in multiple ways, culminating in a higher order of emergence greater than the sum of its parts', 'study', 'The study of these complex linkages at various scales']"
1349,complexity,Summary,"Science as of 2010 takes a number of approaches to characterizing complexity; Zayed et al.
reflect many of these. Neil Johnson states that ""even among scientists, there is no unique definition of complexity – and the scientific notion has traditionally been conveyed using particular examples...""  Ultimately Johnson adopts the definition of ""complexity science"" as ""the study of the phenomena which emerge from a collection of interacting objects"".",Science as of 2010 takes a number of approaches to characterizing complexity; Zayed et al. reflect many of these.,"[' What year did science begin to take a number of approaches to characterizing complexity?', ' What do Zayed and others reflect?']","['2010', 'complexity']"
1350,complexity,Overview,"Definitions of complexity often depend on the concept of a ""system"" – a set of parts or elements that have relationships among them differentiated from relationships with other elements outside the relational regime. Many definitions tend to postulate or assume that complexity expresses a condition of numerous elements in a system and numerous forms of relationships among the elements. However, what one sees as complex and what one sees as simple is relative and changes with time.
","Definitions of complexity often depend on the concept of a ""system"" – a set of parts or elements that have relationships among them differentiated from relationships with other elements outside the relational regime. Many definitions tend to postulate or assume that complexity expresses a condition of numerous elements in a system and numerous forms of relationships among the elements.","[' What is the term for a set of parts or elements that have relationships among them that are differentiated from relationships with other elements outside the relational regime?', ' Many definitions tend to postulate or assume that complexity expresses a condition of numerous elements in a system?', ' What expresses a condition of numerous elements in a system and numerous forms of relationships among the elements?']","['a ""system""', 'complexity often depend on the concept of a ""system"" – a set of parts or elements that have relationships among them differentiated from relationships with other elements outside the relational regime. Many definitions tend to postulate or assume that complexity expresses a condition of numerous elements in a system and numerous forms of relationships among the elements', 'complexity']"
1351,complexity,Overview,"Warren Weaver posited in 1948 two forms of complexity: disorganized complexity, and organized complexity.
Phenomena of 'disorganized complexity' are treated using probability theory and statistical mechanics, while 'organized complexity' deals with phenomena that escape such approaches and confront ""dealing simultaneously with a sizable number of factors which are interrelated into an organic whole"". Weaver's 1948 paper has influenced subsequent thinking about complexity.","Warren Weaver posited in 1948 two forms of complexity: disorganized complexity, and organized complexity. Phenomena of 'disorganized complexity' are treated using probability theory and statistical mechanics, while 'organized complexity' deals with phenomena that escape such approaches and confront ""dealing simultaneously with a sizable number of factors which are interrelated into an organic whole"".","[' In what year did Warren Weaver posit two forms of complexity?', ' How are phenomena of disorganized complexity treated?', ' What does organized complexity deal with?', ' What is a sizable number of factors that are interrelated into an organic whole?']","['1948', 'using probability theory and statistical mechanics', 'phenomena that escape such approaches', 'organized complexity']"
1352,complexity,Disorganized vs. organized,"In Weaver's view, disorganized complexity results from the particular system having a very large number of parts, say millions of parts, or many more. Though the interactions of the parts in a ""disorganized complexity"" situation can be seen as largely random, the properties of the system as a whole can be understood by using probability and statistical methods.
","In Weaver's view, disorganized complexity results from the particular system having a very large number of parts, say millions of parts, or many more. Though the interactions of the parts in a ""disorganized complexity"" situation can be seen as largely random, the properties of the system as a whole can be understood by using probability and statistical methods.","["" In Weaver's view, disorganized complexity results from a system having how many parts?"", ' The interactions of the parts in a ""disorganized complexity"" situation can be seen as what?', ' How can the properties of the system as a whole be understood?', ' What can be understood by using probability and statistical methods?']","['millions', 'largely random', 'by using probability and statistical methods', 'the properties of the system as a whole']"
1353,complexity,Disorganized vs. organized,"A prime example of disorganized complexity is a gas in a container, with the gas molecules as the parts. Some would suggest that a system of disorganized complexity may be compared with the (relative) simplicity of planetary orbits – the latter can be predicted by applying Newton's laws of motion. Of course, most real-world systems, including planetary orbits, eventually become theoretically unpredictable even using Newtonian dynamics; as discovered by modern chaos theory.","A prime example of disorganized complexity is a gas in a container, with the gas molecules as the parts. Some would suggest that a system of disorganized complexity may be compared with the (relative) simplicity of planetary orbits – the latter can be predicted by applying Newton's laws of motion.","[' What is a prime example of disorganized complexity?', ' What could be compared with the (relative) simplicity of planetary orbits?', ' How can the latter be predicted?']","['a gas in a container', 'a system of disorganized complexity', ""by applying Newton's laws of motion""]"
1354,complexity,Disorganized vs. organized,"Organized complexity, in Weaver's view, resides in nothing else than the non-random, or correlated, interaction between the parts. These correlated relationships create a differentiated structure that can, as a system, interact with other systems. The coordinated system manifests properties not carried or dictated by individual parts. The organized aspect of this form of complexity vis-a-vis to other systems than the subject system can be said to ""emerge,"" without any ""guiding hand"".
","Organized complexity, in Weaver's view, resides in nothing else than the non-random, or correlated, interaction between the parts. These correlated relationships create a differentiated structure that can, as a system, interact with other systems.","[' Organized complexity resides in nothing else than the non-random, or correlated, interaction between what?', "" In Weaver's view, what does organized complexity reside in?""]","['parts', 'nothing else than the non-random, or correlated, interaction between the parts']"
1355,complexity,Disorganized vs. organized,"The number of parts does not have to be very large for a particular system to have emergent properties. A system of organized complexity may be understood in its properties (behavior among the properties) through modeling and simulation, particularly modeling and simulation with computers. An example of organized complexity is a city neighborhood as a living mechanism, with the neighborhood people among the system's parts.","The number of parts does not have to be very large for a particular system to have emergent properties. A system of organized complexity may be understood in its properties (behavior among the properties) through modeling and simulation, particularly modeling and simulation with computers.","[' What does not have to be very large for a particular system to have emergent properties?', ' A system of organized complexity may be understood in its properties through what?']","['The number of parts', 'modeling and simulation']"
1356,complexity,Sources and factors,"In the case of self-organizing living systems, usefully organized complexity comes from beneficially mutated organisms being selected to survive by their environment for their differential reproductive ability or at least success over inanimate matter or less organized complex organisms. See e.g. Robert Ulanowicz's treatment of ecosystems.","In the case of self-organizing living systems, usefully organized complexity comes from beneficially mutated organisms being selected to survive by their environment for their differential reproductive ability or at least success over inanimate matter or less organized complex organisms. See e.g.","[' What type of living system has a usefully organized complexity?', ' What is selected to survive by their environment for their differential reproductive ability?']","['self-organizing', 'beneficially mutated organisms']"
1357,complexity,Sources and factors,"Complexity of an object or system is a relative property. For instance, for many functions (problems), such a computational complexity as time of computation is smaller when multitape Turing machines are used than when Turing machines with one tape are used. Random Access Machines allow one to even more decrease time complexity (Greenlaw and Hoover 1998: 226), while inductive Turing machines can decrease even the complexity class of a function, language or set (Burgin 2005). This shows that tools of activity can be an important factor of complexity.
","Complexity of an object or system is a relative property. For instance, for many functions (problems), such a computational complexity as time of computation is smaller when multitape Turing machines are used than when Turing machines with one tape are used.","[' What is a relative property of an object or system?', ' For many functions, what is smaller when multitape Turing machines are used?']","['Complexity', 'computational complexity as time of computation']"
1358,complexity,Study,"Complexity has always been a part of our environment, and therefore many scientific fields have dealt with complex systems and phenomena. From one perspective, that which is somehow complex – displaying variation without being random – is most worthy of interest given the rewards found in the depths of exploration.
","Complexity has always been a part of our environment, and therefore many scientific fields have dealt with complex systems and phenomena. From one perspective, that which is somehow complex – displaying variation without being random – is most worthy of interest given the rewards found in the depths of exploration.","[' What has always been a part of our environment?', ' Many scientific fields have dealt with what?', ' What is the most worthy of interest?']","['Complexity', 'complex systems and phenomena', 'that which is somehow complex']"
1359,complexity,Study,"The use of the term complex is often confused with the term complicated. In today's systems, this is the difference between myriad connecting ""stovepipes"" and effective ""integrated"" solutions. This means that complex is the opposite of independent, while complicated is the opposite of simple.
","The use of the term complex is often confused with the term complicated. In today's systems, this is the difference between myriad connecting ""stovepipes"" and effective ""integrated"" solutions.","[' What term is often confused with complex?', ' What is the difference between myriad connecting ""stovepipes"" and effective ""integrated"" solutions?']","['complicated', 'complex']"
1360,complexity,Study,"While this has led some fields to come up with specific definitions of complexity, there is a more recent movement to regroup observations from different fields to study complexity in itself, whether it appears in anthills, human brains, or economic systems, social systems. One such interdisciplinary group of fields is relational order theories.
","While this has led some fields to come up with specific definitions of complexity, there is a more recent movement to regroup observations from different fields to study complexity in itself, whether it appears in anthills, human brains, or economic systems, social systems. One such interdisciplinary group of fields is relational order theories.","[' What has led some fields to come up with specific definitions of complexity?', ' There is a more recent movement to regroup observations from different fields to study complexity in itself?', ' What is one interdisciplinary group of fields?', ' What is one such interdisciplinary group of fields?', ' What are relational order theories?']","['regroup observations', 'complexity', 'relational order theories', 'relational order theories', 'interdisciplinary group of fields']"
1361,complexity,Applications,"Computational complexity theory is the study of the complexity of problems – that is, the difficulty of solving them. Problems can be classified by complexity class according to the time it takes for an algorithm – usually a computer program – to solve them as a function of the problem size. Some problems are difficult to solve, while others are easy. For example, some difficult problems need algorithms that take an exponential amount of time in terms of the size of the problem to solve. Take the travelling salesman problem, for example. It can be solved in time 



O
(

n

2



2

n


)


{\displaystyle O(n^{2}2^{n})}
 (where n is the size of the network to visit – the number of cities the travelling salesman must visit exactly once). As the size of the network of cities grows, the time needed to find the route grows (more than) exponentially.
","Computational complexity theory is the study of the complexity of problems – that is, the difficulty of solving them. Problems can be classified by complexity class according to the time it takes for an algorithm – usually a computer program – to solve them as a function of the problem size.","[' What is computational complexity theory?', ' What is the study of the complexity of problems?', ' How can problems be classified by complexity class?', ' – to solve them as a function of what?']","['the study of the complexity of problems', 'Computational complexity theory', 'according to the time it takes for an algorithm – usually a computer program – to solve them as a function of the problem size', 'the problem size']"
1362,complexity,Applications,"Even though a problem may be computationally solvable in principle, in actual practice it may not be that simple. These problems might require large amounts of time or an inordinate amount of space. Computational complexity may be approached from many different aspects. Computational complexity can be investigated on the basis of time, memory or other resources used to solve the problem. Time and space are two of the most important and popular considerations when problems of complexity are analyzed.
","Even though a problem may be computationally solvable in principle, in actual practice it may not be that simple. These problems might require large amounts of time or an inordinate amount of space.","[' What may be computationally solvable in principle?', ' What might require large amounts of time or an inordinate amount of space?']","['a problem', 'problems']"
1363,complexity,Applications,"There exist a certain class of problems that although they are solvable in principle they require so much time or space that it is not practical to attempt to solve them. These problems are called intractable.
",There exist a certain class of problems that although they are solvable in principle they require so much time or space that it is not practical to attempt to solve them. These problems are called intractable.,[' What is a class of problems that require so much time or space that it is not practical to attempt to solve them?'],['intractable']
1364,complexity,Applications,"There is another form of complexity called hierarchical complexity. It is orthogonal to the forms of complexity discussed so far, which are called horizontal complexity.
","There is another form of complexity called hierarchical complexity. It is orthogonal to the forms of complexity discussed so far, which are called horizontal complexity.","[' What is another form of complexity called?', ' What are the forms of complexity discussed so far that are called horizontal complexity?']","['hierarchical complexity', 'hierarchical complexity']"
1365,inference rule,Summary,"In the philosophy of logic, a rule of inference, inference rule or transformation rule is a logical form consisting of a function which takes premises, analyzes their syntax, and returns a conclusion (or conclusions). For example, the rule of inference called modus ponens takes two premises, one in the form ""If p then q"" and another in the form ""p"", and returns the conclusion ""q"". The rule is valid with respect to the semantics of classical logic (as well as the semantics of many other non-classical logics), in the sense that if the premises are true (under an interpretation), then so is the conclusion.
","In the philosophy of logic, a rule of inference, inference rule or transformation rule is a logical form consisting of a function which takes premises, analyzes their syntax, and returns a conclusion (or conclusions). For example, the rule of inference called modus ponens takes two premises, one in the form ""If p then q"" and another in the form ""p"", and returns the conclusion ""q"".","[' What is a logical form consisting of a function that takes premises, analyzes their syntax, and returns a conclusion?', ' What is the rule of inference called?', ' How many premises does the modus ponens take?', ' How many premises does modus ponens take?', ' What is the form ""If p then q""?']","['a rule of inference', 'modus ponens', 'two', 'two', 'modus ponens']"
1366,inference rule,Summary,"Typically, a rule of inference preserves truth, a semantic property. In many-valued logic, it preserves a general designation. But a rule of inference's action is purely syntactic, and does not need to preserve any semantic property: any function from sets of formulae to formulae counts as a rule of inference. Usually only rules that are recursive are important; i.e. rules such that there is an effective procedure for determining whether any given formula is the conclusion of a given set of formulae according to the rule. An example of a rule that is not effective in this sense is the infinitary ω-rule.","Typically, a rule of inference preserves truth, a semantic property. In many-valued logic, it preserves a general designation.","[' A rule of inference typically preserves what?', ' In many-valued logic, a rule of what preserves a general designation?']","['truth', 'inference']"
1367,inference rule,Summary,"Popular rules of inference in propositional logic include modus ponens, modus tollens, and contraposition. First-order predicate logic uses rules of inference to deal with logical quantifiers.
","Popular rules of inference in propositional logic include modus ponens, modus tollens, and contraposition. First-order predicate logic uses rules of inference to deal with logical quantifiers.","[' What type of logic uses rules of inference to deal with logical quantifiers?', ' What are some of the popular rules in propositional logic?']","['First-order predicate logic', 'modus ponens, modus tollens, and contraposition']"
1368,inference rule,Standard form,"This expression states that whenever in the course of some logical derivation the given premises have been obtained, the specified conclusion can be taken for granted as well. The exact formal language that is used to describe both premises and conclusions depends on the actual context of the derivations. In a simple case, one may use logical formulae, such as in:
","This expression states that whenever in the course of some logical derivation the given premises have been obtained, the specified conclusion can be taken for granted as well. The exact formal language that is used to describe both premises and conclusions depends on the actual context of the derivations.","[' What expression states that whenever in the course of some logical derivation the given premises have been obtained, the specified conclusion can be taken for granted?', ' The exact formal language that is used to describe premises and conclusions depends on what?']","['This expression states that whenever in the course of some logical derivation the given premises have been obtained, the specified conclusion can be taken for granted as well. The exact formal language that is used to describe both premises and conclusions depends on the actual context of the derivations', 'the actual context of the derivations']"
1369,inference rule,Standard form,"This is the modus ponens rule of propositional logic. Rules of inference are often formulated as schemata employing metavariables. In the rule (schema) above, the metavariables A and B can be instantiated to any element of the universe (or sometimes, by convention, a restricted subset such as propositions) to form an infinite set of inference rules.
",This is the modus ponens rule of propositional logic. Rules of inference are often formulated as schemata employing metavariables.,"[' What is the modus ponens rule of propositional logic?', ' Rules of inference are often formulated as schemata employing what?']","['Rules of inference', 'metavariables']"
1370,inference rule,Standard form,"A proof system is formed from a set of rules chained together to form proofs, also called derivations. Any derivation has only one final conclusion, which is the statement proved or derived. If premises are left unsatisfied in the derivation, then the derivation is a proof of a hypothetical statement: ""if the premises hold, then the conclusion holds.""
","A proof system is formed from a set of rules chained together to form proofs, also called derivations. Any derivation has only one final conclusion, which is the statement proved or derived.","[' A proof system is formed from a set of rules chained together to form what?', ' A derivation has only one final conclusion, what is the statement proved or derived?']","['proofs', 'Any derivation has only one final conclusion, which is the statement proved or derived']"
1371,inference rule,Example: Hilbert systems for two propositional logics,"In a Hilbert system, the premises and conclusion of the inference rules are simply formulae of some language, usually employing metavariables. For graphical compactness of the presentation and to emphasize the distinction between axioms and rules of inference, this section uses the sequent notation (



⊢


{\displaystyle \vdash }
) instead of a vertical presentation of rules.
In this notation, 
","In a Hilbert system, the premises and conclusion of the inference rules are simply formulae of some language, usually employing metavariables. For graphical compactness of the presentation and to emphasize the distinction between axioms and rules of inference, this section uses the sequent notation (



⊢


{\displaystyle \vdash }
) instead of a vertical presentation of rules.","[' What are the premises and conclusion of the inference rules in a Hilbert system?', ' What is used to emphasize the distinction between axioms and rules of inference?', ' Why does this section use the sequent notation instead of a standard notation?', ' What is used instead of a vertical presentation of rules?']","['formulae of some language', 'sequent notation', 'For graphical compactness of the presentation and to emphasize the distinction between axioms and rules of inference', 'sequent notation']"
1372,inference rule,Example: Hilbert systems for two propositional logics,"The formal language for classical propositional logic can be expressed using just negation (¬), implication (→) and propositional symbols. A well-known axiomatization, comprising three axiom schemata and one inference rule (modus ponens), is:
","The formal language for classical propositional logic can be expressed using just negation (¬), implication (→) and propositional symbols. A well-known axiomatization, comprising three axiom schemata and one inference rule (modus ponens), is:","[' What is the formal language for classical propositional logic?', ' What can be expressed using just negation (<unk>), implication (<unk>) and propositional symbols?', ' How many axiom schemata and one inference rule are in a well-known aximatization?']","['just negation (¬), implication (→) and propositional symbols', 'The formal language for classical propositional logic', 'three']"
1373,inference rule,Example: Hilbert systems for two propositional logics,"It may seem redundant to have two notions of inference in this case, ⊢ and →. In classical propositional logic, they indeed coincide; the deduction theorem states that A ⊢ B if and only if ⊢ A → B. There is however a distinction worth emphasizing even in this case: the first notation describes a deduction, that is an activity of passing from sentences to sentences, whereas A → B is simply a formula made with a logical connective, implication in this case. Without an inference rule (like modus ponens in this case), there is no deduction or inference. This point is illustrated in Lewis Carroll's dialogue called ""What the Tortoise Said to Achilles"", as well as later attempts by Bertrand Russell and Peter Winch to resolve the paradox introduced in the dialogue.
","It may seem redundant to have two notions of inference in this case, ⊢ and →. In classical propositional logic, they indeed coincide; the deduction theorem states that A ⊢ B if and only if ⊢ A → B.","[' What may seem redundant in this case?', ' In classical propositional logic, they indeed coincide.', ' The deduction theorem states that A <unk> B if and only if what?']","['to have two notions of inference', '⊢ and →', '⊢ A → B']"
1374,inference rule,Example: Hilbert systems for two propositional logics,"For some non-classical logics, the deduction theorem does not hold. For example, the three-valued logic of Łukasiewicz can be axiomatized as:","For some non-classical logics, the deduction theorem does not hold. For example, the three-valued logic of Łukasiewicz can be axiomatized as:","[' What does the deduction theorem not hold for some non-classical logics?', ' The three-valued logic of <unk>ukasiewicz can be axiomatized as:']","['the three-valued logic', 'Łukasiewicz']"
1375,inference rule,Example: Hilbert systems for two propositional logics,"This sequence differs from classical logic by the change in axiom 2 and the addition of axiom 4. The classical deduction theorem does not hold for this logic, however a modified form does hold, namely A ⊢ B if and only if ⊢ A → (A → B).","This sequence differs from classical logic by the change in axiom 2 and the addition of axiom 4. The classical deduction theorem does not hold for this logic, however a modified form does hold, namely A ⊢ B if and only if ⊢ A → (A → B).","[' What is the difference between classical logic and classical logic?', ' What does the classical deduction theorem not hold for?', ' A <unk> B if and only if <unk> A is?']","['change in axiom 2 and the addition of axiom 4', 'this logic', '⊢ A → (A → B).']"
1376,inference rule,Admissibility and derivability,"In a set of rules, an inference rule could be redundant in the sense that it is admissible or derivable. A derivable rule is one whose conclusion can be derived from its premises using the other rules. An admissible rule is one whose conclusion holds whenever the premises hold. All derivable rules are admissible. To appreciate the difference, consider the following set of rules for defining the natural numbers (the judgment 



n




n
a
t




{\displaystyle n\,\,{\mathsf {nat}}}
 asserts the fact that 



n


{\displaystyle n}
 is a natural number):
","In a set of rules, an inference rule could be redundant in the sense that it is admissible or derivable. A derivable rule is one whose conclusion can be derived from its premises using the other rules.","[' In a set of rules, an inference rule could be redundant in the sense that it is admissible or what?', ' A derivable rule is one whose conclusion can be derived from its premises using what other rules?']","['derivable', 'inference rule']"
1377,inference rule,Admissibility and derivability,"The first rule states that 0 is a natural number, and the second states that s(n) is a natural number if n is. In this proof system, the following rule, demonstrating that the second successor of a natural number is also a natural number, is derivable:
","The first rule states that 0 is a natural number, and the second states that s(n) is a natural number if n is. In this proof system, the following rule, demonstrating that the second successor of a natural number is also a natural number, is derivable:","[' What is the first rule that states that 0 is a natural number?', ' What does the second rule state that s(n) is if n is?']","['second', 'a natural number']"
1378,inference rule,Admissibility and derivability,"Its derivation is the composition of two uses of the successor rule above. The following rule for asserting the existence of a predecessor for any nonzero number is merely admissible:
",Its derivation is the composition of two uses of the successor rule above. The following rule for asserting the existence of a predecessor for any nonzero number is merely admissible:,"[' What is the composition of two uses of the successor rule above?', ' The following rule for asserting the existence of a predecessor for any nonzero number is merely admissible?']","['Its derivation', 'successor rule']"
1379,inference rule,Admissibility and derivability,"This is a true fact of natural numbers, as can be proven by induction. (To prove that this rule is admissible, assume a derivation of the premise and induct on it to produce a derivation of 



n




n
a
t




{\displaystyle n\,\,{\mathsf {nat}}}
.) However, it is not derivable, because it depends on the structure of the derivation of the premise. Because of this, derivability is stable under additions to the proof system, whereas admissibility is not. To see the difference, suppose the following nonsense rule were added to the proof system:
","This is a true fact of natural numbers, as can be proven by induction. (To prove that this rule is admissible, assume a derivation of the premise and induct on it to produce a derivation of 



n




n
a
t




{\displaystyle n\,\,{\mathsf {nat}}}
.)","[' What is a true fact of natural numbers, as can be proven by induction?', ' To prove that this rule is admissible, assume what?']","['n\n\n\n\n\nn\na\nt', 'a derivation of the premise']"
1380,inference rule,Admissibility and derivability,"In this new system, the double-successor rule is still derivable. However, the rule for finding the predecessor is no longer admissible, because there is no way to derive 




−
3





n
a
t




{\displaystyle \mathbf {-3} \,\,{\mathsf {nat}}}
. The brittleness of admissibility comes from the way it is proved: since the proof can induct on the structure of the derivations of the premises, extensions to the system add new cases to this proof, which may no longer hold.
","In this new system, the double-successor rule is still derivable. However, the rule for finding the predecessor is no longer admissible, because there is no way to derive 




−
3





n
a
t




{\displaystyle \mathbf {-3} \,\,{\mathsf {nat}}}
.","[' What is still derivable in the new system?', ' The rule for finding the predecessor is no longer admissible because there is no way to what?']","['the double-successor rule', 'derive \n\n\n\n\n−\n3\n\n\n\n\n\nn\na\nt\n\n\n\n\n{\\displaystyle \\mathbf {-3} \\,\\,{\\mathsf {nat}}}']"
1381,inference rule,Admissibility and derivability,"Admissible rules can be thought of as theorems of a proof system. For instance, in a sequent calculus where cut elimination holds, the cut rule is admissible.
","Admissible rules can be thought of as theorems of a proof system. For instance, in a sequent calculus where cut elimination holds, the cut rule is admissible.","[' Admissible rules can be thought of as theorems of what?', ' In a sequent calculus where cut elimination holds, what is admissible?']","['a proof system', 'the cut rule']"
1382,e-learning,Summary,"Educational technology (commonly abbreviated as edutech, or edtech) is the combined use of computer hardware, software, and educational theory and practice to facilitate learning. When referred to with its abbreviation, edtech, it is often referring to the industry of companies that create educational technology.","Educational technology (commonly abbreviated as edutech, or edtech) is the combined use of computer hardware, software, and educational theory and practice to facilitate learning. When referred to with its abbreviation, edtech, it is often referring to the industry of companies that create educational technology.","[' What is the abbreviation for educational technology?', ' What does edtech refer to?']","['edutech', 'the industry of companies that create educational technology']"
1383,e-learning,Summary,"In addition to practical educational experience, educational technology is based on theoretical knowledge from various disciplines such as communication, education, psychology, sociology, artificial intelligence, and computer science. It encompasses several domains including learning theory, computer-based training, online learning, and m-learning, where mobile technologies are used.
","In addition to practical educational experience, educational technology is based on theoretical knowledge from various disciplines such as communication, education, psychology, sociology, artificial intelligence, and computer science. It encompasses several domains including learning theory, computer-based training, online learning, and m-learning, where mobile technologies are used.","[' In addition to practical educational experience, what is educational technology based on?', ' What are some of the domains of educational technology?']","['theoretical knowledge', 'learning theory, computer-based training, online learning, and m-learning']"
1384,e-learning,Definition,"The Association for Educational Communications and Technology (AECT) defined educational technology as ""the study and ethical practice of facilitating learning and improving performance by creating, using and managing appropriate technological processes and resources"". It denoted instructional technology as ""the theory and practice of design, development, utilization, management, and evaluation of processes and resources for learning"". As such, educational technology refers to all valid and reliable applied education sciences, such as equipment, as well as processes and procedures that are derived from scientific research, and in a given context may refer to theoretical, algorithmic or heuristic processes: it does not necessarily imply physical technology. Educational technology is the process of integrating technology into education in a positive manner that promotes a more diverse learning environment and a way for students to learn how to use technology as well as their common assignments.
","The Association for Educational Communications and Technology (AECT) defined educational technology as ""the study and ethical practice of facilitating learning and improving performance by creating, using and managing appropriate technological processes and resources"". It denoted instructional technology as ""the theory and practice of design, development, utilization, management, and evaluation of processes and resources for learning"".","[' What does AECT stand for?', ' What is the definition of educational technology?', ' The AECT defines educational technology as the study and ethical practice of what?', ' What is the practice of design, development, utilization, management, and evaluation of processes and resources for learning?']","['Association for Educational Communications and Technology', 'the study and ethical practice of facilitating learning and improving performance by creating, using and managing appropriate technological processes and resources"".', 'facilitating learning', 'instructional technology']"
1385,e-learning,Related terms,"Educational technology is an inclusive term for both the material tools, processes, and the theoretical foundations for supporting learning and teaching. Educational technology is not restricted to high technology but is anything that enhances classroom learning in the utilization of blended, face to face, or online learning.","Educational technology is an inclusive term for both the material tools, processes, and the theoretical foundations for supporting learning and teaching. Educational technology is not restricted to high technology but is anything that enhances classroom learning in the utilization of blended, face to face, or online learning.","[' What is an inclusive term for the material tools, processes, and theoretical foundations for supporting learning and teaching?', ' Educational technology is not restricted to what?', ' What enhances classroom learning?']","['Educational technology', 'high technology', 'Educational technology']"
1386,e-learning,Related terms,"An educational technologist is someone who is trained in the field of educational technology. Educational technologists try to analyze, design, develop, implement, and evaluate process and tools to enhance learning. While the term educational technologist is used primarily in the United States, learning technologist is synonymous and used in the UK as well as Canada.
","An educational technologist is someone who is trained in the field of educational technology. Educational technologists try to analyze, design, develop, implement, and evaluate process and tools to enhance learning.","[' What is an educational technologist trained in?', ' What do educational technologists try to do to enhance learning?']","['educational technology', 'analyze, design, develop, implement, and evaluate process and tools']"
1387,e-learning,Related terms,"Modern electronic educational technology is an important part of society today. Educational technology encompasses e-learning, instructional technology, information and communication technology (ICT) in education, edtech, learning technology, multimedia learning, technology-enhanced learning (TEL), computer-based instruction (CBI), computer managed instruction, computer-based training (CBT), computer-assisted instruction or computer-aided instruction (CAI), internet-based training (IBT), flexible learning, web-based training (WBT), online education, digital educational collaboration, distributed learning, computer-mediated communication, cyber-learning, and multi-modal instruction, virtual education, personal learning environments, networked learning, virtual learning environments (VLE) (which are also called learning platforms), m-learning, ubiquitous learning and digital education.
","Modern electronic educational technology is an important part of society today. Educational technology encompasses e-learning, instructional technology, information and communication technology (ICT) in education, edtech, learning technology, multimedia learning, technology-enhanced learning (TEL), computer-based instruction (CBI), computer managed instruction, computer-based training (CBT), computer-assisted instruction or computer-aided instruction (CAI), internet-based training (IBT), flexible learning, web-based training (WBT), online education, digital educational collaboration, distributed learning, computer-mediated communication, cyber-learning, and multi-modal instruction, virtual education, personal learning environments, networked learning, virtual learning environments (VLE) (which are also called learning platforms), m-learning, ubiquitous learning and digital education.","[' What is an important part of society today?', ' What is e-learning?', ' What is another name for computer-assisted instruction?', ' What is a term for flexible learning?', ' What are also called learning platforms?', ' What is another name for ubiquitous learning?']","['Modern electronic educational technology', 'Educational technology', 'CAI', 'internet-based training', 'virtual learning environments', 'm-learning']"
1388,e-learning,Related terms,"Each of these numerous terms has had its advocates, who point up potential distinctive features. However, many terms and concepts in educational technology have been defined nebulously; for example, Fiedler's review of the literature found a complete lack agreement of the components of a personal learning environment. Moreover, Moore saw these terminologies as emphasizing particular features such as digitization approaches, components or delivery methods rather than being fundamentally dissimilar in concept or principle. For example, m-learning emphasizes mobility, which allows for altered timing, location, accessibility and context of learning; nevertheless, its purpose and conceptual principles are those of educational technology.","Each of these numerous terms has had its advocates, who point up potential distinctive features. However, many terms and concepts in educational technology have been defined nebulously; for example, Fiedler's review of the literature found a complete lack agreement of the components of a personal learning environment.","[' What has each term had its advocates?', ' What have many terms and concepts in educational technology been defined nebulously?', ' Who found a complete lack agreement of the components of a personal learning environment?']","['numerous terms', 'complete lack agreement of the components of a personal learning environment', ""Fiedler's review of the literature""]"
1389,e-learning,Related terms,"In practice, as technology has advanced, the particular ""narrowly defined"" terminological aspect that was initially emphasized by name has blended into the general field of educational technology. Initially, ""virtual learning"" as narrowly defined in a semantic sense implied entering an environmental simulation within a virtual world, for example in treating posttraumatic stress disorder (PTSD). In practice, a ""virtual education course"" refers to any instructional course in which all, or at least a significant portion, is delivered by the Internet. ""Virtual"" is used in that broader way to describe a course that is not taught in a classroom face-to-face but through a substitute mode that can conceptually be associated ""virtually"" with classroom teaching, which means that people do not have to go to the physical classroom to learn. Accordingly, virtual education refers to a form of distance learning in which course content is delivered by various methods such as course management applications, multimedia resources, and videoconferencing. Virtual education and simulated learning opportunities, such as games or dissections, offer opportunities for students to connect classroom content to authentic situations.","In practice, as technology has advanced, the particular ""narrowly defined"" terminological aspect that was initially emphasized by name has blended into the general field of educational technology. Initially, ""virtual learning"" as narrowly defined in a semantic sense implied entering an environmental simulation within a virtual world, for example in treating posttraumatic stress disorder (PTSD).","[' What was initially emphasized by name?', ' What has blended into the general field of educational technology?', ' When was ""virtual learning"" narrowly defined?', ' What is the term for posttraumatic stress disorder?', ' What type of simulation is used to treat PTSD?']","['terminological aspect', 'the particular ""narrowly defined"" terminological aspect', 'semantic sense implied entering an environmental simulation within a virtual world', 'PTSD', 'environmental']"
1390,e-learning,Related terms,"Educational content, pervasively embedded in objects, is all around the learner, who may not even be conscious of the learning process. The combination of adaptive learning, using an individualized interface and materials, which accommodate to an individual, who thus receives personally differentiated instruction, with ubiquitous access to digital resources and learning opportunities in a range of places and at various times, has been termed smart learning. Smart learning is a component of the smart city concept.","Educational content, pervasively embedded in objects, is all around the learner, who may not even be conscious of the learning process. The combination of adaptive learning, using an individualized interface and materials, which accommodate to an individual, who thus receives personally differentiated instruction, with ubiquitous access to digital resources and learning opportunities in a range of places and at various times, has been termed smart learning.","[' What is all around the learner, who may not even be conscious of the learning process?', ' What is adaptive learning using?', ' What is the term for personally differentiated instruction?', ' What is smart learning?']","['Educational content', 'an individualized interface and materials', 'smart learning', 'adaptive learning']"
1391,e-learning,History,"Helping people and children learn in ways that are easier, faster, more accurate, or less expensive can be traced back to the emergence of very early tools, such as paintings on cave walls. Various types of abacus have been used. Writing slates and blackboards have been used for at least a millennium. From their introduction, books and pamphlets have held a prominent role in education. From the early twentieth century, duplicating machines such as the mimeograph and Gestetner stencil devices were used to produce short copy runs (typically 10–50 copies) for classroom or home use. The use of media for instructional purposes is generally traced back to the first decade of the 20th century with the introduction of educational films (1900s) and Sidney Pressey's mechanical teaching machines (1920s). The first all multiple choice, large-scale assessment was the Army Alpha, used to assess the intelligence and, more specifically, the aptitudes of World War I military recruits. Further large-scale use of technologies was employed in training soldiers during and after WWII using films and other mediated materials, such as overhead projectors. The concept of hypertext is traced to the description of memex by Vannevar Bush in 1945.
","Helping people and children learn in ways that are easier, faster, more accurate, or less expensive can be traced back to the emergence of very early tools, such as paintings on cave walls. Various types of abacus have been used.","[' What can be traced back to the emergence of very early tools?', ' What have been used to help people learn?']","['Helping people and children', 'Various types of abacus']"
1392,e-learning,History,"Slide projectors were widely used during the 1950s in educational institutional settings. Cuisenaire rods were devised in the 1920s and saw widespread use from the late 1950s.
",Slide projectors were widely used during the 1950s in educational institutional settings. Cuisenaire rods were devised in the 1920s and saw widespread use from the late 1950s.,"[' When were slide projectors widely used?', ' When were cuisenaire rods devised?']","['during the 1950s', '1920s']"
1393,e-learning,History,"In the mid-1960s, Stanford University psychology professors, Patrick Suppes and Richard C. Atkinson, experimented with using computers to teach arithmetic and spelling via Teletypes to elementary school students in the Palo Alto Unified School District in California. Stanford's Education Program for Gifted Youth is descended from those early experiments.
","In the mid-1960s, Stanford University psychology professors, Patrick Suppes and Richard C. Atkinson, experimented with using computers to teach arithmetic and spelling via Teletypes to elementary school students in the Palo Alto Unified School District in California. Stanford's Education Program for Gifted Youth is descended from those early experiments.","[' In what year did Patrick Suppes and Richard Atkinson experiment with computers to teach arithmetic and spelling?', ' In what school district was the Education Program for Gifted Youth located?']","['1960s', 'Palo Alto Unified School District']"
1394,e-learning,History,"Online education originated from the University of Illinois in 1960. Although the internet would not be created for another decade, students were able to access class information with linked computer terminals. Online learning emerged in 1982 when the Western Behavioral Sciences Institute in La Jolla, California, opened its School of Management and Strategic Studies. The school employed computer conferencing through the New Jersey Institute of Technology's Electronic Information Exchange System (EIES) to deliver a distance education program to business executives. Starting in 1985, Connected Education offered the first totally online master's degree in media studies, through The New School in New York City, also via the EIES computer conferencing system. Subsequent courses were offered in 1986 by the Electronic University Network for DOS and Commodore 64 computers. In 2002, MIT began providing online classes free of charge. As of 2009, approximately 5.5 million students were taking at least one class online. Currently, one out of three college students takes at least one online course while in college. At DeVry University, out of all students that are earning a bachelor's degree, 80% earn two-thirds of their requirements online. Also, in 2014, 2.85 million students out of 5.8 million students that took courses online, took all of their courses online. From this information, it can be concluded that the number of students taking classes online is on the steady increase.","Online education originated from the University of Illinois in 1960. Although the internet would not be created for another decade, students were able to access class information with linked computer terminals.","[' Where did online education originate from?', ' When did the University of Illinois start offering online education?', ' What did students use to access class information?']","['University of Illinois', '1960', 'linked computer terminals']"
1395,e-learning,History,"In the recent article, ""Shift happens: online education as a new paradigm in learning"", Linda Harasim covers an overview on the history of online education as well as a framework for understanding the type of need it addresses, the concept of distance learning has already been invented for many centuries. The value of online education is not found in its ability to have established a method for distance learning, but rather in its power to make this type of learning process more efficient by providing a medium in which the instructor and their students can virtually interact with one another in real time. The topic of online education started primarily in the late 1900s when institutions and businesses started to make products to assist students' learning. These groups desired a need to further develop educational services across the globe, primarily to developing countries. In 1960, the University of Illinois created a system of linked computer terminals, known as the Intranet, to give students access to recorded lectures and course materials that they could watch or use on their free time. This type of concept, called PLATO (programmed logic for automatic teaching operations), was rapidly introduced throughout the globe. Many institutions adopted this similar technique while the internet was in its developmental phase.
","In the recent article, ""Shift happens: online education as a new paradigm in learning"", Linda Harasim covers an overview on the history of online education as well as a framework for understanding the type of need it addresses, the concept of distance learning has already been invented for many centuries. The value of online education is not found in its ability to have established a method for distance learning, but rather in its power to make this type of learning process more efficient by providing a medium in which the instructor and their students can virtually interact with one another in real time.","[' What does Shift happen: online education as a new paradigm in learning cover?', ' Linda Harasim covers an overview on the history of what?', ' What has the concept of distance learning already been invented for many centuries?', ' What has already been invented for many centuries?', ' The value of online education is not found in its ability to establish a method for what?', ' How can instructors and students interact with one another in real time?']","['an overview on the history of online education as well as a framework for understanding the type of need it addresses', 'online education', 'online education as a new paradigm in learning"", Linda Harasim covers an overview on the history of online education', 'distance learning', 'distance learning', 'virtually']"
1396,e-learning,History,"In 1971, Ivan Illich published a hugely influential book, Deschooling Society, in which he envisioned ""learning webs"" as a model for people to network the learning they needed. The 1970s and 1980s saw notable contributions in computer-based learning by Murray Turoff and Starr Roxanne Hiltz at the New Jersey Institute of Technology as well as developments at the University of Guelph in Canada. In the UK, the Council for Educational Technology supported the use of educational technology, in particular administering the government's National Development Programme in Computer Aided Learning (1973–1977) and the Microelectronics Education Programme (1980–1986).
","In 1971, Ivan Illich published a hugely influential book, Deschooling Society, in which he envisioned ""learning webs"" as a model for people to network the learning they needed. The 1970s and 1980s saw notable contributions in computer-based learning by Murray Turoff and Starr Roxanne Hiltz at the New Jersey Institute of Technology as well as developments at the University of Guelph in Canada.","[' In what year did Ivan Illich publish a hugely influential book?', ' What did Illich envision as a model for people to network the learning they needed?', ' Where did Murray Turoff and Starr Roxanne Hiltz make notable contributions in computer-based learning?', ' Who are Turoff and Starr Roxanne Hiltz?', ' Where is Turoff based?', ' What is the name of the University of Guelph?']","['1971', 'learning webs', 'New Jersey Institute of Technology', 'Murray Turoff and Starr Roxanne Hiltz at the New Jersey Institute of Technology', 'New Jersey Institute of Technology', 'Canada']"
1397,e-learning,History,"By the mid-1980s, accessing course content became possible at many college libraries. In computer-based training (CBT) or computer-based learning (CBL), the learning interaction was between the student and computer drills or micro-world simulations.
","By the mid-1980s, accessing course content became possible at many college libraries. In computer-based training (CBT) or computer-based learning (CBL), the learning interaction was between the student and computer drills or micro-world simulations.","[' When did accessing course content become possible at many college libraries?', ' What was the learning interaction between the student and computer drills or micro-world simulation?']","['mid-1980s', 'computer-based training']"
1398,e-learning,History,"Digitized communication and networking in education started in the mid-1980s. Educational institutions began to take advantage of the new medium by offering distance learning courses using computer networking for information. Early e-learning systems, based on computer-based learning/training often replicated autocratic teaching styles whereby the role of the e-learning system was assumed to be for transferring knowledge, as opposed to systems developed later based on computer supported collaborative learning (CSCL), which encouraged the shared development of knowledge.
",Digitized communication and networking in education started in the mid-1980s. Educational institutions began to take advantage of the new medium by offering distance learning courses using computer networking for information.,"[' When did digital communication and networking start in education?', ' What did educational institutions begin to take advantage of the new medium by offering?']","['mid-1980s', 'distance learning courses using computer networking for information']"
1399,e-learning,History,"Videoconferencing was an important forerunner to the educational technologies known today. This work was especially popular with museum education. Even in recent years, videoconferencing has risen in popularity to reach over 20,000 students across the United States and Canada in 2008–2009. Disadvantages of this form of educational technology are readily apparent: image and sound quality is often grainy or pixelated; videoconferencing requires setting up a type of mini-television studio within the museum for broadcast, space becomes an issue, and specialised equipment is required for both the provider and the participant.",Videoconferencing was an important forerunner to the educational technologies known today. This work was especially popular with museum education.,"[' What was an important forerunner to the educational technologies known today?', ' Videoconferencing was especially popular with what type of education?']","['Videoconferencing', 'museum education']"
1400,e-learning,History,"The Open University in Britain and the University of British Columbia (where Web CT, now incorporated into Blackboard Inc., was first developed) began a revolution of using the Internet to deliver learning, making heavy use of web-based training, online distance learning and online discussion between students. Practitioners such as Harasim (1995) put heavy emphasis on the use of learning networks.
","The Open University in Britain and the University of British Columbia (where Web CT, now incorporated into Blackboard Inc., was first developed) began a revolution of using the Internet to deliver learning, making heavy use of web-based training, online distance learning and online discussion between students. Practitioners such as Harasim (1995) put heavy emphasis on the use of learning networks.","[' Where was Web CT first developed?', ' What did the Open University in Britain and the University of British Columbia begin a revolution of using the internet to deliver?', ' Harasim made heavy use of what?', ' Harasim put heavy emphasis on the use of what?']","['University of British Columbia', 'learning', 'learning networks', 'learning networks']"
1401,e-learning,History,"By 1994, the first online high school had been founded. In 1997, Graziadei described criteria for evaluating products and developing technology-based courses that include being portable, replicable, scalable, affordable, and having a high probability of long-term cost-effectiveness.","By 1994, the first online high school had been founded. In 1997, Graziadei described criteria for evaluating products and developing technology-based courses that include being portable, replicable, scalable, affordable, and having a high probability of long-term cost-effectiveness.","[' When was the first online high school founded?', ' What criteria did Graziadei describe for evaluating products and developing technology-based courses?']","['1994', 'portable, replicable, scalable, affordable, and having a high probability of long-term cost-effectiveness']"
1402,e-learning,History,"Improved Internet functionality enabled new schemes of communication with multimedia or webcams. The National Center for Education Statistics estimate the number of K-12 students enrolled in online distance learning programs increased by 65% from 2002 to 2005, with greater flexibility, ease of communication between teacher and student, and quick lecture and assignment feedback.
","Improved Internet functionality enabled new schemes of communication with multimedia or webcams. The National Center for Education Statistics estimate the number of K-12 students enrolled in online distance learning programs increased by 65% from 2002 to 2005, with greater flexibility, ease of communication between teacher and student, and quick lecture and assignment feedback.","[' What enabled new schemes of communication with multimedia or webcams?', ' What did the National Center for Education Statistics estimate the number of K-12 students enrolled in online distance learning programs increased by from 2002 to 2005?', ' What is the main benefit of having a teacher and student on the same page?']","['Improved Internet functionality', '65%', 'quick lecture and assignment feedback']"
1403,e-learning,History,"According to a 2008 study conducted by the U.S Department of Education, during the 2006–2007 academic year about 66% of postsecondary public and private schools participating in student financial aid programs offered some distance learning courses; records show 77% of enrollment in for-credit courses with an online component. In 2008, the Council of Europe passed a statement endorsing e-learning's potential to drive equality and education improvements across the EU.","According to a 2008 study conducted by the U.S Department of Education, during the 2006–2007 academic year about 66% of postsecondary public and private schools participating in student financial aid programs offered some distance learning courses; records show 77% of enrollment in for-credit courses with an online component. In 2008, the Council of Europe passed a statement endorsing e-learning's potential to drive equality and education improvements across the EU.","[' What percentage of postsecondary public and private schools offered some distance learning courses during the 2006-2007 academic year?', ' What percent of students enroll in for-credit courses with an online component in 2008?', "" In what year did the Council of Europe endorse e-learning's potential to drive equality and education improvements?""]","['66%', '77%', '2008']"
1404,e-learning,History,"Computer-mediated communication (CMC) is between learners and instructors, mediated by the computer. In contrast, CBT/CBL usually means individualized (self-study) learning, while CMC involves educator/tutor facilitation and requires scenarization of flexible learning activities. In addition, modern ICT provides education with tools for sustaining learning communities and associated knowledge management tasks.
","Computer-mediated communication (CMC) is between learners and instructors, mediated by the computer. In contrast, CBT/CBL usually means individualized (self-study) learning, while CMC involves educator/tutor facilitation and requires scenarization of flexible learning activities.","[' What is CMC?', ' What does CBT/CBL usually mean?']","['Computer-mediated communication', 'individualized (self-study) learning']"
1405,e-learning,History,Students growing up in this digital age have extensive exposure to a variety of media. Major high-tech companies have funded schools to provide them with the ability to teach their students through technology.,Students growing up in this digital age have extensive exposure to a variety of media. Major high-tech companies have funded schools to provide them with the ability to teach their students through technology.,"[' What do students in this digital age have extensive exposure to?', ' What companies have funded schools to provide them with the ability to teach their students through technology?']","['a variety of media', 'Major high-tech companies']"
1406,e-learning,History,"2015 was the first year that private nonprofit organizations enrolled more online students than for-profits, although public universities still enrolled the highest number of online students. In the fall of 2015, more than 6 million students enrolled in at least one online course.","2015 was the first year that private nonprofit organizations enrolled more online students than for-profits, although public universities still enrolled the highest number of online students. In the fall of 2015, more than 6 million students enrolled in at least one online course.","[' What was the first year that private nonprofit organizations enrolled more online students than for-profits?', ' How many students enrolled in at least one online course in the fall of 2015?']","['2015', 'more than 6 million']"
1407,e-learning,History,"In 2020, due to the COVID-19 pandemic, many schools across the world were forced to close, which left more and more grade-school students participating in remote learning, and university-level students enrolling in online courses to enforce distance learning. Organizations such as Unesco have enlisted educational technology solutions to help schools facilitate distance education. The pandemic's extended lockdowns and focus on distance learning has attracted record-breaking amounts of venture capital to the ed-tech sector. In 2020, in the United States alone, ed-tech startups raised $1.78 billion in venture capital spanning 265 deals, compared to $1.32 billion in 2019.","In 2020, due to the COVID-19 pandemic, many schools across the world were forced to close, which left more and more grade-school students participating in remote learning, and university-level students enrolling in online courses to enforce distance learning. Organizations such as Unesco have enlisted educational technology solutions to help schools facilitate distance education.","[' When was the COVID-19 pandemic?', ' Why were schools forced to close?', ' What has Unesco enlisted to help schools?', ' Unesco has enlisted what to help schools facilitate distance education?']","['2020', 'COVID-19 pandemic', 'educational technology solutions', 'educational technology solutions']"
1408,e-learning,Theory,"Various pedagogical perspectives or learning theories may be considered in designing and interacting with educational technology. E-learning theory examines these approaches. These theoretical perspectives are grouped into three main theoretical schools or philosophical frameworks: behaviorism, cognitivism and constructivism.
",Various pedagogical perspectives or learning theories may be considered in designing and interacting with educational technology. E-learning theory examines these approaches.,"[' What can be considered in designing and interacting with educational technology?', ' What examines these approaches?']","['pedagogical perspectives or learning theories', 'E-learning theory']"
1409,e-learning,Practice,"The extent to which e-learning assists or replaces other learning and teaching approaches is variable, ranging on a continuum from none to fully online distance learning. A variety of descriptive terms have been employed (somewhat inconsistently) to categorize the extent to which technology is used. For example, ""hybrid learning"" or ""blended learning"" may refer to classroom aids and laptops, or may refer to approaches in which traditional classroom time is reduced but not eliminated, and is replaced with some online learning. ""Distributed learning"" may describe either the e-learning component of a hybrid approach, or fully online distance learning environments.","The extent to which e-learning assists or replaces other learning and teaching approaches is variable, ranging on a continuum from none to fully online distance learning. A variety of descriptive terms have been employed (somewhat inconsistently) to categorize the extent to which technology is used.","[' How does e-learning help or replace other learning and teaching approaches?', ' What is a continuum from none to fully online distance learning?', ' A variety of descriptive terms have been employed to categorize what?']","['variable', 'The extent to which e-learning assists or replaces other learning and teaching approaches is variable', 'the extent to which technology is used']"
1410,e-learning,Technologies,"Numerous types of physical technology are currently used: digital cameras, video cameras, interactive whiteboard tools, document cameras, electronic media, and LCD projectors. Combinations of these techniques include blogs, collaborative software, ePortfolios, and virtual classrooms.","Numerous types of physical technology are currently used: digital cameras, video cameras, interactive whiteboard tools, document cameras, electronic media, and LCD projectors. Combinations of these techniques include blogs, collaborative software, ePortfolios, and virtual classrooms.","[' What types of technology are currently used?', ' Blogs, collaborative software, ePortfolios and virtual classrooms are examples of what?']","['digital cameras, video cameras, interactive whiteboard tools, document cameras, electronic media, and LCD projectors', 'Combinations of these techniques']"
1411,e-learning,Benefits,"Effective technology use deploys multiple evidence-based strategies concurrently (e.g. adaptive content, frequent testing, immediate feedback, etc.), as do effective teachers. Using computers or other forms of technology can give students practice on core content and skills while the teacher can work with others, conduct assessments, or perform other tasks. Through the use of educational technology, education is able to be individualized for each student allowing for better differentiation and allowing students to work for mastery at their own pace.","Effective technology use deploys multiple evidence-based strategies concurrently (e.g. adaptive content, frequent testing, immediate feedback, etc.","[' Effective technology use deploys multiple evidence-based strategies concurrently.', ' What is adaptive content, frequent testing, immediate feedback?']","['adaptive content, frequent testing, immediate feedback, etc.', 'evidence-based strategies']"
1412,e-learning,Benefits,"Modern educational technology can improve access to education, including full degree programs. It enables better integration for non-full-time students, particularly in continuing education, and improved interactions between students and instructors. Learning material can be used for long-distance learning and are accessible to a wider audience. Course materials are easy to access. In 2010, 70.3% of American family households had access to the internet. In 2013, according to Canadian Radio Television and Telecommunications Commission Canada, 79% of homes have access to the internet. Students can access and engage with numerous online resources at home. Using online resources can help students spend more time on specific aspects of what they may be learning in school, but at home. Schools like the Massachusetts Institute of Technology (MIT) have made certain course materials free online. Although some aspects of a classroom setting are missed by using these resources, they are helpful tools to add additional support to the educational system. The necessity to pay for transport to the educational facility is removed.
","Modern educational technology can improve access to education, including full degree programs. It enables better integration for non-full-time students, particularly in continuing education, and improved interactions between students and instructors.","[' What can modern educational technology improve?', ' What can it enables better integration for non-full-time students?']","['access to education', 'Modern educational technology']"
1413,e-learning,Benefits,"Students appreciate the convenience of e-learning, but report greater engagement in face-to-face learning environments. Colleges and universities are working towards combating this issue by utilizing WEB 2.0 technologies as well as incorporating more mentorships between students and faculty members.","Students appreciate the convenience of e-learning, but report greater engagement in face-to-face learning environments. Colleges and universities are working towards combating this issue by utilizing WEB 2.0 technologies as well as incorporating more mentorships between students and faculty members.","[' What are colleges and universities working towards combating?', ' What do students appreciate more than e-learning?']","['e-learning', 'convenience']"
1414,e-learning,Benefits,"According to James Kulik, who studies the effectiveness of computers used for instruction, students usually learn more in less time when receiving computer-based instruction, and they like classes more and develop more positive attitudes toward computers in computer-based classes. Students can independently solve problems. There are no intrinsic age-based restrictions on difficulty level, i.e. students can go at their own pace. Students editing their written work on word processors improve the quality of their writing. According to some studies, the students are better at critiquing and editing written work that is exchanged over a computer network with students they know. Studies completed in ""computer intensive"" settings found increases in student-centric, cooperative and higher-order learning, writing skills, problem solving, and using technology. In addition, attitudes toward technology as a learning tool by parents, students and teachers are also improved.
","According to James Kulik, who studies the effectiveness of computers used for instruction, students usually learn more in less time when receiving computer-based instruction, and they like classes more and develop more positive attitudes toward computers in computer-based classes. Students can independently solve problems.","[' Who studies the effectiveness of computers used for instruction?', ' What does James Kulik say students learn more in less time when receiving computer-based instruction?<extra_id_51> What do students develop more positive attitudes toward computers in computer-basic classes?']","['James Kulik', 'they like classes more']"
1415,e-learning,Benefits,"Employers' acceptance of online education has risen over time. More than 50% of human resource managers SHRM surveyed for an August 2010 report said that if two candidates with the same level of experience were applying for a job, it would not have any kind of effect whether the candidate's obtained degree was acquired through an online or a traditional school. Seventy-nine percent said they had employed a candidate with an online degree in the past 12 months. However, 66% said candidates who get degrees online were not seen as positively as job applicants with traditional degrees.","Employers' acceptance of online education has risen over time. More than 50% of human resource managers SHRM surveyed for an August 2010 report said that if two candidates with the same level of experience were applying for a job, it would not have any kind of effect whether the candidate's obtained degree was acquired through an online or a traditional school.","["" How much has employers' acceptance of online education risen over time?"", "" What percentage of HR managers said that if two candidates with the same level of experience were applying for a job, it would not have any kind of effect whether the candidate's experience was the same or different?"", "" What would not have any kind of effect whether the candidate's obtained degree was acquired through an online or a traditional school?""]","['risen', '50%', 'two candidates with the same level of experience']"
1416,e-learning,Benefits,"The use of educational apps generally has a positive effect on learning. Pre- and post-tests have revealed that the use of educational apps on mobile devices reduces the achievement gap between struggling and average students. Some educational apps improve group work by allowing students to receive feedback on answers and promoting collaboration in solving problems. The benefits of app-assisted learning have been exhibited in all age groups. Kindergarten students that use iPads show much higher rates of literacy than non-users. Medical students at University of California Irvine that utilized iPad academically have been reported to score 23% higher on national exams than previous classes that did not.
",The use of educational apps generally has a positive effect on learning. Pre- and post-tests have revealed that the use of educational apps on mobile devices reduces the achievement gap between struggling and average students.,"[' What has a positive effect on learning?', ' Pre- and post-tests have revealed that the use of educational apps on mobile devices reduces the achievement gap between struggling and average students?']","['The use of educational apps', 'Pre- and post-tests have revealed that the use of educational apps on mobile devices reduces the achievement gap between struggling and average students.']"
1417,e-learning,Disadvantages,"In US, state and the federal government increased funding, as well as private venture capital has been flowing into education sector. However, as of 2013, none were looking at technology return on investment (ROI) to connect expenditures on technology with improved student outcomes.","In US, state and the federal government increased funding, as well as private venture capital has been flowing into education sector. However, as of 2013, none were looking at technology return on investment (ROI) to connect expenditures on technology with improved student outcomes.","[' In what year did the US not look at technology return on investment (ROI)?', ' What did state and federal governments increase funding for?']","['2013', 'education sector']"
1418,e-learning,Disadvantages,"New technologies are frequently accompanied by unrealistic hype and promise regarding their transformative power to change education for the better or in allowing better educational opportunities to reach the masses. Examples include silent film, broadcast radio, and television, none of which have maintained much of a foothold in the daily practices of mainstream, formal education. Technology, in and of itself, does not necessarily result in fundamental improvements to educational practice. The focus needs to be on the learner's interaction with technology—not the technology itself. It needs to be recognized as ""ecological"" rather than ""additive"" or ""subtractive"". In this ecological change, one significant change will create total change.","New technologies are frequently accompanied by unrealistic hype and promise regarding their transformative power to change education for the better or in allowing better educational opportunities to reach the masses. Examples include silent film, broadcast radio, and television, none of which have maintained much of a foothold in the daily practices of mainstream, formal education.","[' What is often accompanied by unrealistic hype and promise regarding their transformative power to change education for the better or in allowing better educational opportunities to reach the masses?', ' Silent film, broadcast radio, and television have not maintained much of a foothold in what?', ' What has maintained a foothold in the daily practices of mainstream, formal education?']","['New technologies', 'daily practices of mainstream, formal education', 'silent film, broadcast radio, and television']"
1419,e-learning,Disadvantages,"According to Branford et al., ""technology does not guarantee effective learning"", and inappropriate use of technology can even hinder it. A University of Washington study of infant vocabulary shows that it is slipping due to educational baby DVDs. Published in the Journal of Pediatrics, a 2007 University of Washington study on the vocabulary of babies surveyed over 1,000 parents in Washington and Minnesota. The study found that for every one hour that babies 8–16 months of age watched DVDs and Videos, they knew 6-8 fewer of 90 common baby words than the babies that did not watch them. Andrew Meltzoff, a surveyor in this study, states that the result makes sense, that if the baby's ""alert time"" is spent in front of DVDs and TV, instead of with people speaking, the babies are not going to get the same linguistic experience. Dr. Dimitri Chistakis, another surveyor reported that the evidence is mounting that baby DVDs are of no value and may be harmful.","According to Branford et al., ""technology does not guarantee effective learning"", and inappropriate use of technology can even hinder it. A University of Washington study of infant vocabulary shows that it is slipping due to educational baby DVDs.","[' According to Branford, what does not guarantee effective learning?', ' What can inappropriate use of technology even hinder?', ' A University of Washington study shows that infant vocabulary is slipping due to what?']","['technology', 'effective learning', 'educational baby DVDs']"
1420,e-learning,Disadvantages,"Adaptive instructional materials tailor questions to each student's ability and calculate their scores, but this encourages students to work individually rather than socially or collaboratively (Kruse, 2013). Social relationships are important, but high-tech environments may compromise the balance of trust, care and respect between teacher and student.","Adaptive instructional materials tailor questions to each student's ability and calculate their scores, but this encourages students to work individually rather than socially or collaboratively (Kruse, 2013). Social relationships are important, but high-tech environments may compromise the balance of trust, care and respect between teacher and student.","["" What does adaptive instructional materials tailor to each student's ability and calculate their scores?"", ' What encourages students to work individually rather than socially or collaboratively?', ' High-tech environments may compromise what between teacher and student?']","['questions', 'Adaptive instructional materials', 'trust, care and respect']"
1421,e-learning,Disadvantages,"Massively open online courses (MOOCs), although quite popular in discussions of technology and education in developed countries (more so in the US), are not a major concern in most developing or low-income countries. One of the stated goals of MOOCs is to provide less fortunate populations (i.e., in developing countries) an opportunity to experience courses with US-style content and structure. However, research shows only 3% of the registrants are from low-income countries and although many courses have thousands of registered students only 5-10% of them complete the course. This can be attributed to lack of staff support, course difficulty and low levels of engagement with peers. MOOCs also implies that certain curriculum and teaching methods are superior, and this could eventually wash over (or possibly washing out) local educational institutions, cultural norms and educational traditions.","Massively open online courses (MOOCs), although quite popular in discussions of technology and education in developed countries (more so in the US), are not a major concern in most developing or low-income countries. One of the stated goals of MOOCs is to provide less fortunate populations (i.e., in developing countries) an opportunity to experience courses with US-style content and structure.","[' What is another name for open online courses?', ' What is the goal of MOOCs?', ' What is the purpose of providing less fortunate populations an opportunity to experience courses with US-style content?']","['MOOCs', 'to provide less fortunate populations', 'MOOCs']"
1422,e-learning,Disadvantages,"With the Internet and social media, using educational apps makes the students highly susceptible to distraction and sidetracking. Even though proper use has shown to increase student performances, being distracted would be detrimental. Another disadvantage is an increased potential for cheating. One method is done by creating multiple accounts to survey questions and gather information which can be assimilated so that the master account is able to fill in the correct answers. Smartphones can be very easy to hide and use inconspicuously, especially if their use is normalized in the classroom. These disadvantages can be managed with strict rules and regulations on mobile phone use.
","With the Internet and social media, using educational apps makes the students highly susceptible to distraction and sidetracking. Even though proper use has shown to increase student performances, being distracted would be detrimental.","[' What makes students highly susceptible to distraction and sidetracking?', ' Proper use of educational apps has shown to increase what?']","['using educational apps', 'student performances']"
1423,e-learning,Teacher training,"Since technology is not the end goal of education, but rather a means by which it can be accomplished, educators must have a good grasp of the technology and its advantages and disadvantages. Teacher training aims for effective integration of classroom technology.","Since technology is not the end goal of education, but rather a means by which it can be accomplished, educators must have a good grasp of the technology and its advantages and disadvantages. Teacher training aims for effective integration of classroom technology.","[' What is not the end goal of education?', ' What must educators have a good grasp of?']","['technology', 'the technology and its advantages and disadvantages']"
1424,e-learning,Teacher training,"The evolving nature of technology may unsettle teachers, who may experience themselves as perpetual novices. Finding quality materials to support classroom objectives is often difficult. Random professional development days are inadequate.","The evolving nature of technology may unsettle teachers, who may experience themselves as perpetual novices. Finding quality materials to support classroom objectives is often difficult.","[' What may unsettle teachers?', ' What may teachers experience themselves as?']","['The evolving nature of technology', 'perpetual novices']"
1425,e-learning,Teacher training,"According to Jenkins, ""Rather than dealing with each technology in isolation, we would do better to take an ecological approach, thinking about the interrelationship among different communication technologies, the cultural communities that grow up around them, and the activities they support."" Jenkins also suggested that the traditional school curriculum guided teachers to train students to be autonomous problem solvers. However, today's workers are increasingly asked to work in teams, drawing on different sets of expertise, and collaborating to solve problems. Learning styles and the methods of collecting information have evolved, and ""students often feel locked out of the worlds described in their textbooks through the depersonalized and abstract prose used to describe them"". These twenty-first century skills can be attained through the incorporation and engagement with technology. Changes in instruction and use of technology can also promote a higher level of learning among students with different types of intelligence.","According to Jenkins, ""Rather than dealing with each technology in isolation, we would do better to take an ecological approach, thinking about the interrelationship among different communication technologies, the cultural communities that grow up around them, and the activities they support."" Jenkins also suggested that the traditional school curriculum guided teachers to train students to be autonomous problem solvers.","[' What would Jenkins suggest instead of dealing with each technology in isolation?', ' Jenkins suggested that the traditional school curriculum guided what?', ' What did Jenkins suggest that the traditional school curriculum guided teachers to train students to be?']","['take an ecological approach', 'teachers', 'autonomous problem solvers']"
1426,e-learning,Assessment,"Educational assessment with technology may be either formative assessment or summative assessment. Instructors use both types of assessments to understand student progress and learning in the classroom. Technology has helped teachers create better assessments to help understand where students who are having trouble with the material are having issues.
",Educational assessment with technology may be either formative assessment or summative assessment. Instructors use both types of assessments to understand student progress and learning in the classroom.,[' What type of assessment is used by instructors to understand student progress and learning?'],['formative assessment or summative assessment']
1427,e-learning,Assessment,"Formative assessment is more difficult, as the perfect form is ongoing and allows the students to show their learning in different ways depending on their learning styles. Technology has helped some teachers make their formative assessments better, particularly through the use of classroom response systems (CRS). A CRS is a tool in which the students each have a handheld device that partners up with the teacher's computer. The instructor then asks multiple choice or true or false questions and the students answer on their device. Depending on the software used, the answers may then be shown on a graph so students and the teacher can see the percentage of students who gave each answer and the teacher can focus on what went wrong.","Formative assessment is more difficult, as the perfect form is ongoing and allows the students to show their learning in different ways depending on their learning styles. Technology has helped some teachers make their formative assessments better, particularly through the use of classroom response systems (CRS).","[' What is more difficult than formative assessment?', ' What technology has helped some teachers make their formative assessments better?']","['the perfect form is ongoing', 'classroom response systems (CRS).']"
1428,e-learning,Assessment,"Summative assessments are more common in classrooms and are usually set up to be more easily graded, as they take the form of tests or projects with specific grading schemes. One huge benefit of tech-based testing is the option to give students immediate feedback on their answers. When students get these responses, they are able to know how they are doing in the class which can help push them to improve or give them confidence that they are doing well. Technology also allows for different kinds of summative assessment, such as digital presentations, videos, or anything else the teacher/students may come up with, which allows different learners to show what they learned more effectively. Teachers can also use technology to post graded assessments online for students to have a better idea of what a good project is.
","Summative assessments are more common in classrooms and are usually set up to be more easily graded, as they take the form of tests or projects with specific grading schemes. One huge benefit of tech-based testing is the option to give students immediate feedback on their answers.","[' What type of assessments are more common in classrooms?', ' Summative assessments take the form of what?', ' What is one huge benefit of tech-based testing?']","['Summative', 'tests or projects with specific grading schemes', 'the option to give students immediate feedback on their answers']"
1429,e-learning,Assessment,"Electronic assessment uses information technology. It encompasses several potential applications, which may be teacher or student-oriented, including educational assessment throughout the continuum of learning, such as computerized classification testing, computerized adaptive testing, student testing, and grading an exam. E-Marking is an examiner led activity closely related to other e-assessment activities such as e-testing, or e-learning which are student-led. E-marking allows markers to mark a scanned script or online response on a computer screen rather than on paper.
","Electronic assessment uses information technology. It encompasses several potential applications, which may be teacher or student-oriented, including educational assessment throughout the continuum of learning, such as computerized classification testing, computerized adaptive testing, student testing, and grading an exam.","[' What does electronic assessment use?', ' What are some potential applications of electronic assessment?']","['information technology', 'computerized classification testing, computerized adaptive testing, student testing, and grading an exam']"
1430,e-learning,Assessment,"There are no restrictions on the types of tests that can use e-marking, with e-marking applications designed to accommodate multiple choice, written, and even video submissions for performance examinations. E-marking software is used by individual educational institutions and can also be rolled out to the participating schools of awarding exam organisations. E-marking has been used to mark many well known high stakes examinations, which in the United Kingdom include A levels and GCSE exams, and in the US includes the SAT test for college admissions. Ofqual reports that e-marking is the main type of marking used for general qualifications in the United Kingdom.
","There are no restrictions on the types of tests that can use e-marking, with e-marking applications designed to accommodate multiple choice, written, and even video submissions for performance examinations. E-marking software is used by individual educational institutions and can also be rolled out to the participating schools of awarding exam organisations.","[' What are the types of tests that can use e-marking?', ' What types of submissions can be accommodated for performance examinations?', ' Who uses the software that can be rolled out to participating schools?', ' How many schools of awarding exam organisations will be rolled out to?']","['multiple choice, written, and even video submissions for performance examinations', 'multiple choice, written, and even video', 'individual educational institutions', 'participating']"
1431,e-learning,Analytics,"The importance of self-assessment through tools made available on educational technology platforms has been growing. Self-assessment in education technology relies on students analyzing their strengths, weaknesses and areas where improvement is possible to set realistic goals in learning, improve their educational performances and track their progress. One of the unique tools for self-assessment made possible by education technology is Analytics. Analytics is data gathered on the student's activities on the learning platform, drawn into meaningful patterns that lead to a valid conclusion, usually through the medium of data visualization such as graphs. Learning analytics is the field that focuses on analyzing and reporting data about student's activities in order to facilitate learning.
","The importance of self-assessment through tools made available on educational technology platforms has been growing. Self-assessment in education technology relies on students analyzing their strengths, weaknesses and areas where improvement is possible to set realistic goals in learning, improve their educational performances and track their progress.","[' What is the importance of self-assessment through tools made available on educational technology platforms?', ' What relies on students analyzing their strengths, weaknesses and areas where improvement is possible?']","['has been growing', 'Self-assessment']"
1432,e-learning,Expenditure,"The five key sectors of the e-learning industry are consulting, content, technologies, services and support. Worldwide, e-learning was estimated in 2000 to be over $48 billion according to conservative estimates. Commercial growth has been brisk. In 2014, the worldwide commercial market activity was estimated at $6 billion venture capital over the past five years,: 38  with self-paced learning generating $35.6 billion in 2011.: 4  North American e-learning generated $23.3 billion in revenue in 2013, with a 9% growth rate in cloud-based authoring tools and learning platforms.: 19 ","The five key sectors of the e-learning industry are consulting, content, technologies, services and support. Worldwide, e-learning was estimated in 2000 to be over $48 billion according to conservative estimates.","[' What are the five key sectors of the e-learning industry?', ' What was estimated to be over $48 billion worldwide in 2000?']","['consulting, content, technologies, services and support', 'e-learning']"
1433,e-learning,Careers,"Educational technologists and psychologists apply basic educational and psychological research into an evidence-based applied science (or a technology) of learning or instruction. In research, these professions typically require a graduate degree (Master's, Doctorate, Ph.D., or D.Phil.) in a field related to educational psychology, educational media, experimental psychology, cognitive psychology or, more purely, in the fields of educational, instructional or human performance technology or instructional design. In industry, educational technology is utilized to train students and employees by a wide range of learning and communication practitioners, including instructional designers, technical trainers, technical communication and professional communication specialists, technical writers, and of course primary school and college teachers of all levels. The transformation of educational technology from a cottage industry to a profession is discussed by Shurville et al.","Educational technologists and psychologists apply basic educational and psychological research into an evidence-based applied science (or a technology) of learning or instruction. In research, these professions typically require a graduate degree (Master's, Doctorate, Ph.D., or D.Phil.)",[' Educational technologists and psychologists apply what into an evidence-based applied science of learning or instruction?'],['basic educational and psychological research']
1434,convolutional neural networks,Summary,"In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of artificial neural network, most commonly applied to analyze visual imagery. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation equivariant responses known as feature maps. Counter-intuitively, most convolutional neural networks are only equivariant, as opposed to invariant, to translation. They have applications in image and video recognition, recommender systems, image classification, image segmentation, medical image analysis, natural language processing, brain-computer interfaces, and financial time series.","In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of artificial neural network, most commonly applied to analyze visual imagery. They are also known as shift invariant or space invariant artificial neural networks (SIANN), based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation equivariant responses known as feature maps.","[' What is another name for a convolutional neural network?', ' What is the most common use of a ConvNet?', ' SIANN is based on what?', ' What is the shared-weight architecture of the convolution kernels or filters?', ' What are feature maps?']","['ConvNet', 'analyze visual imagery', 'shared-weight architecture', 'slide along input features and provide translation equivariant responses known as feature maps', 'translation equivariant responses']"
1435,convolutional neural networks,Summary,"CNNs are regularized versions of multilayer perceptrons. Multilayer perceptrons usually mean fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The ""full connectivity"" of these networks make them prone to overfitting data. Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout, etc.) CNNs take a different approach towards regularization: they take advantage of the hierarchical pattern in data and assemble patterns of increasing complexity using smaller and simpler patterns embossed in their filters. Therefore, on a scale of connectivity and complexity, CNNs are on the lower extreme.
","CNNs are regularized versions of multilayer perceptrons. Multilayer perceptrons usually mean fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer.","[' CNNs are regularized versions of what?', ' Multilayer perceptrons usually mean what kind of networks?', ' Each neuron in one layer is connected to all neurons in the next?']","['multilayer perceptrons', 'fully connected networks', 'Multilayer perceptrons usually mean fully connected networks']"
1436,convolutional neural networks,Summary,"Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.
",Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field.,"[' What was inspired by biological processes?', ' The connectivity pattern between neurons resembles the organization of what?', ' Individual cortical neurons respond to stimuli only in what region of the visual field?']","['Convolutional networks', 'the animal visual cortex', 'receptive field']"
1437,convolutional neural networks,Summary,"CNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns to optimize the filters (or kernels) through automated learning, whereas in traditional algorithms these filters are hand-engineered. This independence from prior knowledge and human intervention in feature extraction is a major advantage.
","CNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns to optimize the filters (or kernels) through automated learning, whereas in traditional algorithms these filters are hand-engineered.","[' How much pre-processing do CNNs use compared to other image classification algorithms?', ' How does the network learn to optimize the filters?', ' What are traditional algorithms hand-engineered?']","['relatively little', 'automated learning', 'filters']"
1438,convolutional neural networks,Definition,"The name ""convolutional neural network"" indicates that the network employs a mathematical operation called convolution.
Convolutional networks are a specialized type of neural networks that use convolution in place of general matrix multiplication in at least one of their layers.","The name ""convolutional neural network"" indicates that the network employs a mathematical operation called convolution. Convolutional networks are a specialized type of neural networks that use convolution in place of general matrix multiplication in at least one of their layers.","[' What does the name ""convolutional neural network"" indicate?', ' What is a specialized type of neural networks that use convolution instead of general matrix multiplication?']","['the network employs a mathematical operation called convolution', 'Convolutional networks']"
1439,convolutional neural networks,Architecture,"A convolutional neural network consists of an input layer, hidden layers and an output layer. In any feed-forward neural network, any middle layers are called hidden because their inputs and outputs are masked by the activation function and final convolution. In a convolutional neural network, the hidden layers include layers that perform convolutions. Typically this includes a layer that performs a dot product of the convolution kernel with the layer's input matrix. This product is usually the Frobenius inner product, and its activation function is commonly ReLU. As the convolution kernel slides along the input matrix for the layer, the convolution operation generates a feature map, which in turn contributes to the input of the next layer. This is followed by other layers such as pooling layers, fully connected layers, and normalization layers.
","A convolutional neural network consists of an input layer, hidden layers and an output layer. In any feed-forward neural network, any middle layers are called hidden because their inputs and outputs are masked by the activation function and final convolution.","[' What is a convolutional neural network composed of?', ' What are the middle layers of a feed-forward neural network called?']","['an input layer, hidden layers and an output layer', 'hidden']"
1440,convolutional neural networks,Distinguishing features,"In the past, traditional multilayer perceptron (MLP) models were used for image recognition. However, the full connectivity between nodes caused the curse of dimensionality, and was computationally intractable with higher resolution images. A 1000×1000-pixel image with RGB color channels has 3 million weights, which is too high to feasibly process efficiently at scale with full connectivity.
","In the past, traditional multilayer perceptron (MLP) models were used for image recognition. However, the full connectivity between nodes caused the curse of dimensionality, and was computationally intractable with higher resolution images.","[' What was used for image recognition in the past?', ' What caused the curse of dimensionality?', ' How was the full connectivity between nodes computationally intractable?']","['traditional multilayer perceptron (MLP) models', 'full connectivity between nodes', 'with higher resolution images']"
1441,convolutional neural networks,Distinguishing features,"For example, in CIFAR-10, images are only of size 32×32×3 (32 wide, 32 high, 3 color channels), so a single fully connected neuron in the first hidden layer of a regular neural network would have 32*32*3 = 3,072 weights. A 200×200 image, however, would lead to neurons that have 200*200*3 = 120,000 weights.
","For example, in CIFAR-10, images are only of size 32×32×3 (32 wide, 32 high, 3 color channels), so a single fully connected neuron in the first hidden layer of a regular neural network would have 32*32*3 = 3,072 weights. A 200×200 image, however, would lead to neurons that have 200*200*3 = 120,000 weights.","[' How many weights would a single fully connected neuron in the first hidden layer of a regular neural network have?', ' A 200<unk>200 image would lead to neurons that have what?', ' What image would lead to neurons that have 200*200*3 = 120,000 weights?']","['3,072', '200*200*3 = 120,000 weights', '200×200']"
1442,convolutional neural networks,Distinguishing features,"Also, such network architecture does not take into account the spatial structure of data, treating input pixels which are far apart in the same way as pixels that are close together. This ignores locality of reference in data with a grid-topology (such as images), both computationally and semantically. Thus, full connectivity of neurons is wasteful for purposes such as image recognition that are dominated by spatially local input patterns.
","Also, such network architecture does not take into account the spatial structure of data, treating input pixels which are far apart in the same way as pixels that are close together. This ignores locality of reference in data with a grid-topology (such as images), both computationally and semantically.","[' What does network architecture not take into account?', ' What is ignored in data with a grid-topology?']","['the spatial structure of data', 'locality of reference']"
1443,convolutional neural networks,Distinguishing features,"Convolutional neural networks are variants of multilayer perceptrons, designed to emulate the behavior of a visual cortex. These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images. As opposed to MLPs, CNNs have the following distinguishing features:
","Convolutional neural networks are variants of multilayer perceptrons, designed to emulate the behavior of a visual cortex. These models mitigate the challenges posed by the MLP architecture by exploiting the strong spatially local correlation present in natural images.","[' What are convolutional neural networks designed to emulate?', ' What are these models designed to mitigate the challenges posed by the MLP architecture?']","['the behavior of a visual cortex', 'Convolutional neural networks']"
1444,convolutional neural networks,Distinguishing features,"Together, these properties allow CNNs to achieve better generalization on vision problems. Weight sharing dramatically reduces the number of free parameters learned, thus lowering the memory requirements for running the network and allowing the training of larger, more powerful networks.
","Together, these properties allow CNNs to achieve better generalization on vision problems. Weight sharing dramatically reduces the number of free parameters learned, thus lowering the memory requirements for running the network and allowing the training of larger, more powerful networks.","[' What do weight sharing properties allow CNNs to achieve?', ' What reduces the number of free parameters learned?', ' How does weight sharing help CNNs?']","['better generalization on vision problems', 'Weight sharing', 'dramatically reduces the number of free parameters learned']"
1445,convolutional neural networks,Building blocks,"
A CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function. A few distinct types of layers are commonly used. These are further discussed below.","
A CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function.","[' What is formed by a stack of distinct layers that transform the input volume into an output volume?', ' What does a CNN architecture hold?']","['CNN architecture', 'class scores']"
1446,convolutional neural networks,Hyperparameters,"Hyperparameters are various settings that are used to control the learning process. CNNs use more hyperparameters than a standard multilayer perceptron (MLP).
",Hyperparameters are various settings that are used to control the learning process. CNNs use more hyperparameters than a standard multilayer perceptron (MLP).,"[' What are various settings that are used to control the learning process?', ' What do CNNs use more than a standard multilayer perceptron?']","['Hyperparameters', 'Hyperparameters are various settings that are used to control the learning process. CNNs use more hyperparameters']"
1447,convolutional neural networks,Translation Equivariance and Aliasing,"It is commonly assumed that CNNs are invariant to shifts of the input. Convolution or pooling layers within a CNN that do not have a stride greater than one are indeed equivariant to translations of the input. However, layers with a stride greater than one ignore the Nyquist-Shannon sampling theorem and might lead to aliasing of the input signal While, in principle, CNNs are capable of implementing anti-aliasing filters, it has been observed that this does not happens in practice  and yield models that are not equivariant to translations.
Furthermore, if a CNN makes use of fully connected layers, translation equivariance does not imply translation invariance, as the fully connected layers are not invariant to shifts of the input. One solution for complete translation invariance is avoiding any down-sampling throughout the network and applying global average pooling at the last layer. Additionally, several other partial solutions have been proposed, such as anti-aliasing before downsampling operations, spatial transformer networks, data augmentation, subsampling combined with pooling, and capsule neural networks.",It is commonly assumed that CNNs are invariant to shifts of the input. Convolution or pooling layers within a CNN that do not have a stride greater than one are indeed equivariant to translations of the input.,"[' What is commonly assumed that CNNs are invariant to shifts of the input?', ' Convolution or pooling layers within a CNN that do not have a stride greater than one are indeed what?']","['Convolution or pooling layers within a CNN that do not have a stride greater than one are indeed equivariant to translations', 'equivariant to translations of the input']"
1448,convolutional neural networks,Evaluation,"The accuracy of the final model based on a sub-part of the dataset set apart at the start, often called a test-set. Other times methods such as k-fold cross-validation are applied. Other strategies include using conformal prediction.","The accuracy of the final model based on a sub-part of the dataset set apart at the start, often called a test-set. Other times methods such as k-fold cross-validation are applied.","[' What is the term for the accuracy of the final model based on a sub-part of a dataset set apart at the start?', ' What is a test-set often called?', ' Other times methods such as k-fold cross-validation are applied?']","['a test-set', 'The accuracy of the final model based on a sub-part of the dataset set apart at the start', 'The accuracy of the final model based on a sub-part of the dataset set apart at the start']"
1449,convolutional neural networks,Regularization methods,"Regularization is a process of introducing additional information to solve an ill-posed problem or to prevent overfitting. CNNs use various types of regularization.
",Regularization is a process of introducing additional information to solve an ill-posed problem or to prevent overfitting. CNNs use various types of regularization.,"[' What is regularization a process of introducing?', ' What does regularization do?']","['additional information', 'introducing additional information to solve an ill-posed problem or to prevent overfitting']"
1450,convolutional neural networks,Hierarchical coordinate frames,"Pooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition. Overlapping the pools so that each feature occurs in multiple pools, helps retain the information. Translation alone cannot extrapolate the understanding of geometric relationships to a radically new viewpoint, such as a different orientation or scale. On the other hand, people are very good at extrapolating; after seeing a new shape once they can recognize it from a different viewpoint.",Pooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition.,"[' Pooling loses the precise spatial relationships between high-level parts, such as nose and mouth in a face image?']",['Pooling loses the precise spatial relationships between high-level parts (such as nose and mouth in a face image). These relationships are needed for identity recognition']
1451,convolutional neural networks,Hierarchical coordinate frames,"An earlier common way to deal with this problem is to train the network on transformed data in different orientations, scales, lighting, etc. so that the network can cope with these variations. This is computationally intensive for large data-sets. The alternative is to use a hierarchy of coordinate frames and use a group of neurons to represent a conjunction of the shape of the feature and its pose relative to the retina. The pose relative to the retina is the relationship between the coordinate frame of the retina and the intrinsic features' coordinate frame.","An earlier common way to deal with this problem is to train the network on transformed data in different orientations, scales, lighting, etc. so that the network can cope with these variations.","[' What was an earlier common way to deal with this problem?', ' What is a way to train the network on transformed data?']","['to train the network on transformed data', 'different orientations, scales, lighting, etc.']"
1452,convolutional neural networks,Hierarchical coordinate frames,"Thus, one way to represent something is to embed the coordinate frame within it. This allows large features to be recognized by using the consistency of the poses of their parts (e.g. nose and mouth poses make a consistent prediction of the pose of the whole face). This approach ensures that the higher-level entity (e.g. face) is present when the lower-level (e.g. nose and mouth) agree on its prediction of the pose. The vectors of neuronal activity that represent pose (""pose vectors"") allow spatial transformations modeled as linear operations that make it easier for the network to learn the hierarchy of visual entities and generalize across viewpoints. This is similar to the way the human visual system imposes coordinate frames in order to represent shapes.","Thus, one way to represent something is to embed the coordinate frame within it. This allows large features to be recognized by using the consistency of the poses of their parts (e.g.","[' What is one way to represent something?', ' What allows large features to be recognized?']","['to embed the coordinate frame within it', 'consistency of the poses of their parts']"
1453,convolutional neural networks,Fine-tuning,"For many applications, the training data is less available. Convolutional neural networks usually require a large amount of training data in order to avoid overfitting. A common technique is to train the network on a larger data set from a related domain. Once the network parameters have converged an additional training step is performed using the in-domain data to fine-tune the network weights, this is known as transfer learning. Furthermore, this technique allows convolutional network architectures to successfully be applied to problems with tiny training sets.","For many applications, the training data is less available. Convolutional neural networks usually require a large amount of training data in order to avoid overfitting.","[' For many applications, what is less available?', ' Convolutional neural networks usually require a large amount of what?']","['training data', 'training data']"
1454,convolutional neural networks,Human interpretable explanations,"End-to-end training and prediction are common practice in computer vision. However, human interpretable explanations are required for critical systems such as a self-driving cars. With recent advances in visual salience, spatial and temporal attention, the most critical spatial regions/temporal instants could be visualized to justify the CNN predictions.","End-to-end training and prediction are common practice in computer vision. However, human interpretable explanations are required for critical systems such as a self-driving cars.","[' What is common practice in computer vision?', ' What is required for critical systems such as self-driving cars?']","['End-to-end training and prediction', 'human interpretable explanations']"
1455,polynomial time,Summary,"In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to be related by a constant factor.
","In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform.","[' What is the term used to describe the amount of computer time it takes to run an algorithm?', ' How is time complexity estimated in computer science?', ' Time complexity is estimated by counting how many elementary operations?', ' How long does each elementary operation take to perform?']","['time complexity', 'by counting the number of elementary operations performed by the algorithm', 'number', 'a fixed amount of time']"
1456,polynomial time,Summary,"Since an algorithm's running time may vary among different inputs of the same size, one commonly considers the worst-case time complexity, which is the maximum amount of time required for inputs of a given size. Less common, and usually specified explicitly, is the average-case complexity, which is the average of the time taken on inputs of a given size (this makes sense because there are only a finite number of possible inputs of a given size). In both cases, the time complexity is generally expressed as a function of the size of the input.: 226  Since this function is generally difficult to compute exactly, and the running time for small inputs is usually not consequential, one commonly focuses on the behavior of the complexity when the input size increases—that is, the asymptotic behavior of the complexity. Therefore, the time complexity is commonly expressed using big O notation, typically 



O
(
n
)


{\displaystyle O(n)}
, 



O
(
n
log
⁡
n
)


{\displaystyle O(n\log n)}
, 



O
(

n

α


)


{\displaystyle O(n^{\alpha })}
, 



O
(

2

n


)


{\displaystyle O(2^{n})}
, etc., where n is the size in units of bits needed to represent the input.
","Since an algorithm's running time may vary among different inputs of the same size, one commonly considers the worst-case time complexity, which is the maximum amount of time required for inputs of a given size. Less common, and usually specified explicitly, is the average-case complexity, which is the average of the time taken on inputs of a given size (this makes sense because there are only a finite number of possible inputs of a given size).","[' What is the maximum amount of time required for inputs of a given size?', ' What is less common and usually specified explicitly?', ' The average-case complexity is what?', ' What is the average of the time taken on inputs of a given size?', ' What makes sense because there are only a finite number of possible inputs?']","['worst-case time complexity', 'the average-case complexity', 'the average of the time taken on inputs of a given size', 'average-case complexity', 'average-case complexity']"
1457,polynomial time,Summary,"Algorithmic complexities are classified according to the type of function appearing in the big O notation. For example, an algorithm with time complexity 



O
(
n
)


{\displaystyle O(n)}
 is a linear time algorithm and an algorithm with time complexity 



O
(

n

α


)


{\displaystyle O(n^{\alpha })}
 for some constant 



α
>
1


{\displaystyle \alpha >1}
 is a polynomial time algorithm.
","Algorithmic complexities are classified according to the type of function appearing in the big O notation. For example, an algorithm with time complexity 



O
(
n
)


{\displaystyle O(n)}
 is a linear time algorithm and an algorithm with time complexity 



O
(

n

α


)


{\displaystyle O(n^{\alpha })}
 for some constant 



α
>
1


{\displaystyle \alpha >1}
 is a polynomial time algorithm.","[' Algorithmic complexities are classified according to the type of function appearing in what notation?', ' What is an algorithm with time complexity O ( n ) <unk>displaystyle O(n)<unk> a linear time algorithm?']","['big O', 'O\n(\nn\n)\n\n\n{\\displaystyle O(n)}\n is a linear time algorithm and an algorithm with time complexity \n\n\n\nO\n(\n\nn\n\nα']"
1458,polynomial time,Table of common time complexities,"The following table summarizes some classes of commonly encountered time complexities. In the table, poly(x) = xO(1), i.e., polynomial in x.
","The following table summarizes some classes of commonly encountered time complexities. In the table, poly(x) = xO(1), i.e., polynomial in x.","[' What table summarizes some classes of commonly encountered time complexities?', ' What is poly(x) = xO(1)?']","['The following table', 'polynomial in\xa0x']"
1459,polynomial time,Constant time,"An algorithm is said to be constant time (also written as O(1) time) if the value of T(n) is bounded by a value that does not depend on the size of the input. For example, accessing any single element in an array takes constant time as only one operation has to be performed to locate it. In a similar manner, finding the minimal value in an array sorted in ascending order; it is the first element. However, finding the minimal value in an unordered array is not a constant time operation as scanning over each element in the array is needed in order to determine the minimal value. Hence it is a linear time operation, taking O(n) time. If the number of elements is known in advance and does not change, however, such an algorithm can still be said to run in constant time.
","An algorithm is said to be constant time (also written as O(1) time) if the value of T(n) is bounded by a value that does not depend on the size of the input. For example, accessing any single element in an array takes constant time as only one operation has to be performed to locate it.","[' What is an algorithm called if the value of T(n) is bounded by a value that does not depend on the size of the input?', ' How many operations have to be performed to access an array?', ' How long does it take to locate an array?', ' How many operations have to be performed to locate it?']","['constant time', 'one', 'one operation has to be performed', 'one']"
1460,polynomial time,Constant time,"Despite the name ""constant time"", the running time does not have to be independent of the problem size, but an upper bound for the running time has to be bounded independently of the problem size. For example, the task ""exchange the values of a and b if necessary so that a ≤ b"" is called constant time even though the time may depend on whether or not it is already true that a ≤ b. However, there is some constant t such that the time required is always at most t.
","Despite the name ""constant time"", the running time does not have to be independent of the problem size, but an upper bound for the running time has to be bounded independently of the problem size. For example, the task ""exchange the values of a and b if necessary so that a ≤ b"" is called constant time even though the time may depend on whether or not it is already true that a ≤ b.","[' What does the name ""constant time"" mean?', ' What does not have to be independent of the problem size?', ' An upper bound for the running time has to be bounded independently of what?', ' What is called constant time even though the time may depend on whether or not it is already true that a <unk> b?']","['the running time does not have to be independent of the problem size', 'the running time', 'the problem size', 'the task ""exchange the values of a and b if necessary so that a ≤ b""']"
1461,polynomial time,Logarithmic time,"An algorithm is said to take logarithmic time when T(n) = O(log n).  Since loga n and logb n are related by a constant multiplier, and such a multiplier is irrelevant to big-O classification, the standard usage for logarithmic-time algorithms is O(log n) regardless of the base of the logarithm appearing in the expression of T.
","An algorithm is said to take logarithmic time when T(n) = O(log n). Since loga n and logb n are related by a constant multiplier, and such a multiplier is irrelevant to big-O classification, the standard usage for logarithmic-time algorithms is O(log n) regardless of the base of the logarithm appearing in the expression of T.","[' What is the standard usage for logarithmic-time algorithms?', ' What is irrelevant to big-O classification?', ' What is the base of the logarithm appearing in the expression of T?']","['O(log n)', 'loga\xa0n and logb\xa0n are related by a constant multiplier, and such a multiplier', 'O(log n']"
1462,polynomial time,Logarithmic time,"An O(log n) algorithm is considered highly efficient, as the ratio of the number of operations to the size of the input decreases and tends to zero when n increases. An algorithm that must access all elements of its input cannot take logarithmic time, as the time taken for reading an input of size n is of the order of n.
","An O(log n) algorithm is considered highly efficient, as the ratio of the number of operations to the size of the input decreases and tends to zero when n increases. An algorithm that must access all elements of its input cannot take logarithmic time, as the time taken for reading an input of size n is of the order of n.","[' How efficient is an O(log n) algorithm?', ' When does the ratio of the number of operations to the size of the input decrease?', ' What does an algorithm that must access all elements of its input cannot take?', ' What is the order of the time taken for reading an input of size n?', ' What does not take logarithmic time?']","['highly efficient', 'zero when n increases', 'logarithmic time', 'n', 'An algorithm that must access all elements of its input']"
1463,polynomial time,Logarithmic time,"An example of logarithmic time is given by dictionary search. Consider a dictionary D which contains n entries, sorted by alphabetical order. We suppose that, for 1 ≤ k ≤ n, one may access the kth entry of the dictionary in a constant time. Let D(k) denote this kth entry. Under these hypotheses, the test to see if a word w is in the dictionary may be done in logarithmic time: consider 



D
(
⌊
n

/

2
⌋
)


{\displaystyle D(\lfloor n/2\rfloor )}
, where 



⌊

⌋


{\displaystyle \lfloor \;\rfloor }
 denotes the floor function. If 



w
=
D
(
⌊
n

/

2
⌋
)


{\displaystyle w=D(\lfloor n/2\rfloor )}
, then we are done. Else, if 



w
<
D
(
⌊
n

/

2
⌋
)


{\displaystyle w<D(\lfloor n/2\rfloor )}
, continue the search in the same way in the left half of the dictionary, otherwise continue similarly with the right half of the dictionary. This algorithm is similar to the method often used to find an entry in a paper dictionary.
","An example of logarithmic time is given by dictionary search. Consider a dictionary D which contains n entries, sorted by alphabetical order.","[' What is an example of logarithmic time given by a dictionary search?', ' A dictionary D contains n entries, sorted by what?']","['a dictionary D which contains n entries', 'alphabetical order']"
1464,polynomial time,Sub-linear time,"An algorithm is said to run in sub-linear time (often spelled sublinear time) if T(n) = o(n). In particular this includes algorithms with the time complexities defined above.
",An algorithm is said to run in sub-linear time (often spelled sublinear time) if T(n) = o(n). In particular this includes algorithms with the time complexities defined above.,[' How is an algorithm said to run in sub-linear time?'],['if T(n) = o(n).']
1465,polynomial time,Sub-linear time,"Typical algorithms that are exact and yet run in sub-linear time use parallel processing (as the NC1 matrix determinant calculation does), or alternatively have guaranteed assumptions on the input structure (as the logarithmic time binary search and many tree maintenance algorithms do). However, formal languages such as the set of all strings that have a 1-bit in the position indicated by the first log(n) bits of the string may depend on every bit of the input and yet be computable in sub-linear time.
","Typical algorithms that are exact and yet run in sub-linear time use parallel processing (as the NC1 matrix determinant calculation does), or alternatively have guaranteed assumptions on the input structure (as the logarithmic time binary search and many tree maintenance algorithms do). However, formal languages such as the set of all strings that have a 1-bit in the position indicated by the first log(n) bits of the string may depend on every bit of the input and yet be computable in sub-linear time.","[' What type of processing does the NC1 matrix determinant use?', ' What kind of assumptions do the logarithmic time binary search and many tree maintenance algorithms have?', ' Algorithms do what?', ' Formal languages such as the set of all strings that have a 1-bit in the position indicated by the first log(n) bits of the string may depend on every bit of the input and yet be computable in what time?']","['parallel processing', 'guaranteed', 'guaranteed assumptions on the input structure', 'sub-linear']"
1466,polynomial time,Sub-linear time,"The specific term sublinear time algorithm is usually reserved to algorithms that are unlike the above in that they are run over classical serial machine models and are not allowed prior assumptions on the input. They are however allowed to be randomized, and indeed must be randomized for all but the most trivial of tasks.
","The specific term sublinear time algorithm is usually reserved to algorithms that are unlike the above in that they are run over classical serial machine models and are not allowed prior assumptions on the input. They are however allowed to be randomized, and indeed must be randomized for all but the most trivial of tasks.","[' What term is reserved to algorithms that are run over classical serial machine models and are not allowed prior assumptions on the input?', ' Sublinear time algorithms are allowed to be what?', ' What must be randomized for all but the most trivial tasks?']","['sublinear time algorithm', 'randomized', 'sublinear time algorithm']"
1467,polynomial time,Sub-linear time,"As such an algorithm must provide an answer without reading the entire input, its particulars heavily depend on the access allowed to the input. Usually for an input that is represented as a binary string b1,…,bk it is assumed that the algorithm can in time O(1) request and obtain the value of bi for any i.
","As such an algorithm must provide an answer without reading the entire input, its particulars heavily depend on the access allowed to the input. Usually for an input that is represented as a binary string b1,…,bk it is assumed that the algorithm can in time O(1) request and obtain the value of bi for any i.","[' What must an algorithm provide without reading the entire input?', ' What heavily depends on the access allowed to the input?<extra_id_51> What is assumed that the algorithm can in time O(1) request and obtain?', ' The algorithm can obtain the value of bi for what?']","['an answer', 'the value of bi for any i', 'any i']"
1468,polynomial time,Sub-linear time,"Sub-linear time algorithms are typically randomized, and provide only approximate solutions. In fact, the property of a binary string having only zeros (and no ones) can be easily proved not to be decidable by a (non-approximate) sub-linear time algorithm. Sub-linear time algorithms arise naturally in the investigation of property testing.
","Sub-linear time algorithms are typically randomized, and provide only approximate solutions. In fact, the property of a binary string having only zeros (and no ones) can be easily proved not to be decidable by a (non-approximate) sub-linear time algorithm.","[' Sub-linear time algorithms are typically randomized and provide only what?', ' What can be easily proved not to be decidable by a (non-approximate) sub linear time algorithm?']","['approximate solutions', 'the property of a binary string having only zeros']"
1469,polynomial time,Linear time,"An algorithm is said to take linear time, or O(n) time, if its time complexity is O(n). Informally, this means that the running time increases at most linearly with the size of the input. More precisely, this means that there is a constant c such that the running time is at most cn for every input of size n. For example, a procedure that adds up all elements of a list requires time proportional to the length of the list, if the adding time is constant, or, at least, bounded by a constant.
","An algorithm is said to take linear time, or O(n) time, if its time complexity is O(n). Informally, this means that the running time increases at most linearly with the size of the input.","[' What is the term for an algorithm that takes linear time?', ' What does O(n) time mean informally?', ' How does the running time increase linearly?']","['O(n) time', 'that the running time increases at most linearly with the size of the input', 'with the size of the input']"
1470,polynomial time,Linear time,"Linear time is the best possible time complexity in situations where the algorithm has to sequentially read its entire input. Therefore, much research has been invested into discovering algorithms exhibiting linear time or, at least, nearly linear time. This research includes both software and hardware methods. There are several hardware technologies which exploit parallelism to provide this. An example is content-addressable memory. This concept of linear time is used in string matching algorithms such as the Boyer–Moore algorithm and Ukkonen's algorithm.
","Linear time is the best possible time complexity in situations where the algorithm has to sequentially read its entire input. Therefore, much research has been invested into discovering algorithms exhibiting linear time or, at least, nearly linear time.","[' What is the best possible time complexity in situations where the algorithm has to sequentially read its entire input?', ' What has been invested into discovering algorithms exhibiting linear time or nearly linear time?']","['Linear time', 'research']"
1471,polynomial time,Quasilinear time,"An algorithm is said to run in quasilinear time (also referred to as log-linear time) if T(n) = O(n logk n) for some positive constant k; linearithmic time is the case k = 1. Using soft O notation these algorithms are Õ(n). Quasilinear time algorithms are also O(n1+ε) for every constant ε > 0, and thus run faster than any polynomial time algorithm whose time bound includes a term nc for any c > 1.
",An algorithm is said to run in quasilinear time (also referred to as log-linear time) if T(n) = O(n logk n) for some positive constant k; linearithmic time is the case k = 1. Using soft O notation these algorithms are Õ(n).,"[' What is another name for quasilinear time?', ' What is the case if T(n) = O(n logk n) for some positive constant k?']","['log-linear time', 'linearithmic time']"
1472,polynomial time,Quasilinear time,"In many cases, the n log n running time is simply the result of performing a Θ(log n) operation n times (for the notation, see Big O notation § Family of Bachmann–Landau notations). For example, binary tree sort creates a binary tree by inserting each element of the n-sized array one by one. Since the insert operation on a self-balancing binary search tree takes O(log n) time, the entire algorithm takes O(n log n) time.
","In many cases, the n log n running time is simply the result of performing a Θ(log n) operation n times (for the notation, see Big O notation § Family of Bachmann–Landau notations). For example, binary tree sort creates a binary tree by inserting each element of the n-sized array one by one.","[' What is the result of performing a <unk>(log n) operation n times?', ' What creates a binary tree by inserting each element of the n-sized array one element at a time?', ' How is each element of the n-sized array inserted?']","['the n log n running time', 'binary tree sort', 'one by one']"
1473,polynomial time,Quasilinear time,"Comparison sorts require at least Ω(n log n) comparisons in the worst case because log(n!) = Θ(n log n), by Stirling's approximation. They also frequently arise from the recurrence relation T(n) = 2T(n/2) + O(n).
","Comparison sorts require at least Ω(n log n) comparisons in the worst case because log(n!) = Θ(n log n), by Stirling's approximation.","[' What type of comparisons require at least <unk>(n log n) comparisons in the worst case?', "" What does log(n!) = by Stirling's approximation?""]","['Comparison sorts', 'Θ(n log n),']"
1474,polynomial time,Sub-quadratic time,"For example, simple, comparison-based sorting algorithms are quadratic (e.g. insertion sort), but more advanced algorithms can be found that are subquadratic (e.g. shell sort). No general-purpose sorts run in linear time, but the change from quadratic to sub-quadratic is of great practical importance.
","For example, simple, comparison-based sorting algorithms are quadratic (e.g. insertion sort), but more advanced algorithms can be found that are subquadratic (e.g.","[' What are simple, comparison-based sorting algorithms?', ' What are more advanced algorithms that are subquadratic?']","['quadratic', 'comparison-based sorting algorithms']"
1475,polynomial time,Polynomial time,"An algorithm is said to be of polynomial time if its running time is upper bounded by a polynomial expression in the size of the input for the algorithm, that is, T(n) = O(nk) for some positive constant k. Problems for which a deterministic polynomial time algorithm exists belong to the complexity class P, which is central in the field of computational complexity theory. Cobham's thesis states that polynomial time is a synonym for ""tractable"", ""feasible"", ""efficient"", or ""fast"".","An algorithm is said to be of polynomial time if its running time is upper bounded by a polynomial expression in the size of the input for the algorithm, that is, T(n) = O(nk) for some positive constant k. Problems for which a deterministic polynomial time algorithm exists belong to the complexity class P, which is central in the field of computational complexity theory. Cobham's thesis states that polynomial time is a synonym for ""tractable"", ""feasible"", ""efficient"", or ""fast"".","[' What is said to be of polynomial time if its running time is upper bounded by a polynomal expression in the size of the input for the algorithm?', ' Problems for which a deterministic polynomic time algorithm exists belong to what?', ' What class is central in the field of computational complexity theory?', ' What is a synonym for ""tractable"", ""feasible"", ""efficient"" or ""fast?""']","['An algorithm', 'complexity class P', 'P', 'polynomial time']"
1476,polynomial time,Superpolynomial time,"An algorithm is said to take superpolynomial time if T(n) is not bounded above by any polynomial. Using little omega notation, it is ω(nc) time for all constants c, where n is the input parameter, typically the number of bits in the input.
","An algorithm is said to take superpolynomial time if T(n) is not bounded above by any polynomial. Using little omega notation, it is ω(nc) time for all constants c, where n is the input parameter, typically the number of bits in the input.","[' What is an algorithm said to take if T(n) is not bounded above by any polynomial?', ' What is the input parameter, typically the number of bits in the input?']","['superpolynomial time', 'n']"
1477,polynomial time,Superpolynomial time,"An algorithm that uses exponential resources is clearly superpolynomial, but some algorithms are only very weakly superpolynomial. For example, the Adleman–Pomerance–Rumely primality test runs for nO(log log n) time on n-bit inputs; this grows faster than any polynomial for large enough n, but the input size must become impractically large before it cannot be dominated by a polynomial with small degree.
","An algorithm that uses exponential resources is clearly superpolynomial, but some algorithms are only very weakly superpolynomial. For example, the Adleman–Pomerance–Rumely primality test runs for nO(log log n) time on n-bit inputs; this grows faster than any polynomial for large enough n, but the input size must become impractically large before it cannot be dominated by a polynomial with small degree.","[' What is an algorithm that uses exponential resources clearly superpolynomial?', ' Some algorithms are only very weak what?', ' The Adleman–Pomerance–Rumely primality test runs for nO(log log n) time?', ' What must the input size become before it cannot be dominated by a polynomial with small degree?']","['Adleman–Pomerance–Rumely primality test runs for nO(log log n) time on n-bit inputs', 'superpolynomial', 'n-bit inputs', 'impractically large']"
1478,polynomial time,Superpolynomial time,"An algorithm that requires superpolynomial time lies outside the complexity class P. Cobham's thesis posits that these algorithms are impractical, and in many cases they are. Since the P versus NP problem is unresolved, it is unknown whether NP-complete problems require superpolynomial time.
","An algorithm that requires superpolynomial time lies outside the complexity class P. Cobham's thesis posits that these algorithms are impractical, and in many cases they are. Since the P versus NP problem is unresolved, it is unknown whether NP-complete problems require superpolynomial time.","[' What class does an algorithm that requires superpolynomial time lie outside of?', "" Cobham's thesis posits that these algorithms are impractical, and in many cases they are."", ' What is unresolved?']","['complexity class P', 'An algorithm that requires superpolynomial time', 'P versus NP problem']"
1479,polynomial time,Quasi-polynomial time,"Quasi-polynomial time algorithms are algorithms that run longer than polynomial time, yet not so long as to be exponential time. The worst case running time of a quasi-polynomial time algorithm is 




2

O
(

log

c


⁡
n
)




{\displaystyle 2^{O(\log ^{c}n)}}
 for some fixed 



c
>
0


{\displaystyle c>0}
. For 



c
=
1


{\displaystyle c=1}
 we get a polynomial time algorithm, for 



c
<
1


{\displaystyle c<1}
 we get a sub-linear time algorithm.
","Quasi-polynomial time algorithms are algorithms that run longer than polynomial time, yet not so long as to be exponential time. The worst case running time of a quasi-polynomial time algorithm is 




2

O
(

log

c


⁡
n
)




{\displaystyle 2^{O(\log ^{c}n)}}
 for some fixed 



c
>
0


{\displaystyle c>0}
.",[' What is the worst case running time of a quasi-polynomial time algorithm?'],['2\n\nO\n(\n\nlog\n\nc\n\n\n\u2061\nn\n)\n\n\n\n\n{\\displaystyle 2^{O(\\log ^{c}n)}}\n for some fixed \n\n\n\nc\n>\n0\n\n\n{\\displaystyle c>0}']
1480,polynomial time,Quasi-polynomial time,"Quasi-polynomial time algorithms typically arise in reductions from an NP-hard problem to another problem. For example, one can take an instance of an NP hard problem, say 3SAT, and convert it to an instance of another problem B, but the size of the instance becomes 




2

O
(

log

c


⁡
n
)




{\displaystyle 2^{O(\log ^{c}n)}}
. In that case, this reduction does not prove that problem B is NP-hard; this reduction only shows that there is no polynomial time algorithm for B unless there is a quasi-polynomial time algorithm for 3SAT (and thus all of NP). Similarly, there are some problems for which we know quasi-polynomial time algorithms, but no polynomial time algorithm is known. Such problems arise in approximation algorithms; a famous example is the directed Steiner tree problem, for which there is a quasi-polynomial time approximation algorithm achieving an approximation factor of 



O
(

log

3


⁡
n
)


{\displaystyle O(\log ^{3}n)}
 (n being the number of vertices), but showing the existence of such a polynomial time algorithm is an open problem.
","Quasi-polynomial time algorithms typically arise in reductions from an NP-hard problem to another problem. For example, one can take an instance of an NP hard problem, say 3SAT, and convert it to an instance of another problem B, but the size of the instance becomes 




2

O
(

log

c


⁡
n
)




{\displaystyle 2^{O(\log ^{c}n)}}
.","[' Quasi-polynomial time algorithms arise in reductions from an NP-hard problem to what other problem?', ' What can one take an instance of NP hard problem, say 3SAT, and convert it to another problem B?']","['B', '2\n\nO\n(\n\nlog\n\nc\n\n\n\u2061\nn\n)\n\n\n\n\n{\\displaystyle 2^{O(\\log ^{c}n)}}']"
1481,polynomial time,Quasi-polynomial time,"Other computational problems with quasi-polynomial time solutions but no known polynomial time solution include the planted clique problem in which the goal is to find a large clique in the union of a clique and a random graph. Although quasi-polynomially solvable, it has been conjectured that the planted clique problem has no polynomial time solution; this planted clique conjecture has been used as a computational hardness assumption to prove the difficulty of several other problems in computational game theory, property testing, and machine learning.","Other computational problems with quasi-polynomial time solutions but no known polynomial time solution include the planted clique problem in which the goal is to find a large clique in the union of a clique and a random graph. Although quasi-polynomially solvable, it has been conjectured that the planted clique problem has no polynomial time solution; this planted clique conjecture has been used as a computational hardness assumption to prove the difficulty of several other problems in computational game theory, property testing, and machine learning.","[' What problem has no known polynomial time solution?', ' What is the goal of the planted clique problem?', ' What has been conjectured that the planted clique problem has no polynomial time solution?']","['planted clique problem', 'to find a large clique in the union of a clique and a random graph', 'planted clique conjecture']"
1482,polynomial time,Quasi-polynomial time,The complexity class QP consists of all problems that have quasi-polynomial time algorithms. It can be defined in terms of DTIME as follows.,The complexity class QP consists of all problems that have quasi-polynomial time algorithms. It can be defined in terms of DTIME as follows.,"[' The complexity class QP consists of all problems that have what?', ' What can be defined in terms of DTIME as follows?']","['quasi-polynomial time algorithms', 'The complexity class QP']"
1483,polynomial time,Sub-exponential time,"The term sub-exponential time is used to express that the running time of some algorithm may grow faster than any polynomial but is still significantly smaller than an exponential. In this sense, problems that have sub-exponential time algorithms are somewhat more tractable than those that only have exponential algorithms. The precise definition of ""sub-exponential"" is not generally agreed upon, and we list the two most widely used ones below.
","The term sub-exponential time is used to express that the running time of some algorithm may grow faster than any polynomial but is still significantly smaller than an exponential. In this sense, problems that have sub-exponential time algorithms are somewhat more tractable than those that only have exponential algorithms.","[' What term is used to express that the running time of some algorithm may grow faster than any polynomial but is still significantly smaller than an exponential?', ' Problems that have sub-exponential time algorithms are somewhat more tractable than those that only have what?']","['sub-exponential time', 'exponential algorithms']"
1484,polynomial time,Factorial time,"An algorithm is said to be factorial time if T(n) is upper bounded by the factorial function n!. Factorial time is a subset of exponential time (EXP) because 



n
!
=
O

(

2


n

1
+
ϵ




)



{\displaystyle n!=O\left(2^{n^{1+\epsilon }}\right)}
 for all 



ϵ
>
0


{\displaystyle \epsilon >0}
. However, it is not a subset of E.
","An algorithm is said to be factorial time if T(n) is upper bounded by the factorial function n!. Factorial time is a subset of exponential time (EXP) because 



n
!","[' What is said to be factorial time if T(n) is upper bounded by the factorial function n?', ' Factorial time is a subset of what?']","['An algorithm', 'exponential time']"
1485,polynomial time,Factorial time,"An example of an algorithm that runs in factorial time is bogosort, a notoriously inefficient sorting algorithm based on trial and error.  Bogosort sorts a list of n items by repeatedly shuffling the list until it is found to be sorted. In the average case, each pass through the bogosort algorithm will examine one of the n! orderings of the n items.  If the items are distinct, only one such ordering is sorted. Bogosort shares patrimony with the infinite monkey theorem.
","An example of an algorithm that runs in factorial time is bogosort, a notoriously inefficient sorting algorithm based on trial and error. Bogosort sorts a list of n items by repeatedly shuffling the list until it is found to be sorted.","[' What is an example of an algorithm that runs in factorial time?', ' What is bogosort based on?', ' Bogosort sorts a list of n items by repeatedly shuffling what?']","['bogosort', 'trial and error', 'the list until it is found to be sorted']"
1486,modeling,Summary,"A model is an informative representation of an object, person or system. The term originally denoted the plans of a building in late 16th-century English, and derived via French and Italian ultimately from Latin modulus, a measure. 
","A model is an informative representation of an object, person or system. The term originally denoted the plans of a building in late 16th-century English, and derived via French and Italian ultimately from Latin modulus, a measure.","[' What is an informative representation of an object, person or system?', ' What did the term model originally denote in late 16th-century English?', ' From what Latin word is modulus derived?']","['A model', 'the plans of a building', 'a measure']"
1487,modeling,Summary,"Models can be divided into physical models (e.g. a successful pupil as a role model for others in the school) and abstract models (e.g. mathematical expressions describing behavioural patterns). Abstract or conceptual models are central to philosophy of science, as almost every scientific theory effectively embeds some kind of model of the physical or human sphere. 
",Models can be divided into physical models (e.g. a successful pupil as a role model for others in the school) and abstract models (e.g.,"[' What can models be divided into?', ' What is a successful pupil as a role model for others in the school?']","['physical models (e.g. a successful pupil as a role model for others in the school) and abstract models', 'physical models']"
1488,modeling,Summary,"In commerce, ""model"" can refer to a specific design of a product as displayed in a catalogue or show room (e.g. Ford Model T), and by extension to the sold product itself.
","In commerce, ""model"" can refer to a specific design of a product as displayed in a catalogue or show room (e.g. Ford Model T), and by extension to the sold product itself.","[' In commerce, what can refer to a specific design of a product as displayed in a catalogue or show room?', ' What is a Ford Model T?']","['model', 'a specific design of a product as displayed in a catalogue or show room']"
1489,modeling,Conceptual model,"A conceptual model is a theoretical representation of a system, e.g. a set of equations attempting to describe the workings of the atmosphere for the purpose of weather forecasting.
","A conceptual model is a theoretical representation of a system, e.g. a set of equations attempting to describe the workings of the atmosphere for the purpose of weather forecasting.","[' What is a conceptual model a theoretical representation of?', ' A set of equations trying to describe the workings of the atmosphere for the purpose of weather forecasting?']","['a system', 'conceptual model']"
1490,data structure,Summary,"In computer science, a data structure is a data organization, management, and storage format that enables efficient access and modification. More precisely, a data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data, i.e., it is an algebraic structure about data.
","In computer science, a data structure is a data organization, management, and storage format that enables efficient access and modification. More precisely, a data structure is a collection of data values, the relationships among them, and the functions or operations that can be applied to the data, i.e., it is an algebraic structure about data.","[' In computer science, what is a data organization, management, and storage format called?', ' What is the collection of data values, the relationships among them and the functions or operations that can be applied to the data?', ' What can be applied to the data?', ' What is the structure about data called?']","['a data structure', 'a data structure', 'functions or operations', 'algebraic']"
1491,data structure,Usage,Data structures serve as the basis for abstract data types (ADT). The ADT defines the logical form of the data type. The data structure implements the physical form of the data type.,Data structures serve as the basis for abstract data types (ADT). The ADT defines the logical form of the data type.,"[' Data structures serve as the basis for what?', ' What defines the logical form of the data type?']","['abstract data types', 'The ADT']"
1492,data structure,Usage,"Different types of data structures are suited to different kinds of applications, and some are highly specialized to specific tasks. For example, relational databases commonly use B-tree indexes for data retrieval, while compiler implementations usually use hash tables to look up identifiers.","Different types of data structures are suited to different kinds of applications, and some are highly specialized to specific tasks. For example, relational databases commonly use B-tree indexes for data retrieval, while compiler implementations usually use hash tables to look up identifiers.","[' What type of databases use B-tree indexes for data retrieval?', ' Compiler implementations use what to look up identifiers?']","['relational databases', 'hash tables']"
1493,data structure,Usage,"Data structures provide a means to manage large amounts of data efficiently for uses such as large databases and internet indexing services. Usually, efficient data structures are key to designing efficient algorithms. Some formal design methods and programming languages emphasize data structures, rather than algorithms, as the key organizing factor in software design. Data structures can be used to organize the storage and retrieval of information stored in both main memory and secondary memory.","Data structures provide a means to manage large amounts of data efficiently for uses such as large databases and internet indexing services. Usually, efficient data structures are key to designing efficient algorithms.","[' Data structures provide a means to manage large amounts of data efficiently for uses such as large databases and what else?', ' Data structures are usually key to designing what?']","['internet indexing services', 'efficient algorithms']"
1494,data structure,Implementation,"Data structures are generally based on the ability of a computer to fetch and store data at any place in its memory, specified by a pointer—a bit string, representing a memory address, that can be itself stored in memory and manipulated by the program. Thus, the array and record data structures are based on computing the addresses of data items with arithmetic operations, while the linked data structures are based on storing addresses of data items within the structure itself. 
","Data structures are generally based on the ability of a computer to fetch and store data at any place in its memory, specified by a pointer—a bit string, representing a memory address, that can be itself stored in memory and manipulated by the program. Thus, the array and record data structures are based on computing the addresses of data items with arithmetic operations, while the linked data structures are based on storing addresses of data items within the structure itself.","[' Data structures are based on the ability of a computer to fetch and store data at any place in its memory, specified by what?', ' A bit string, representing a memory address, can be itself stored in memory and manipulated by the program?', ' What are array and record data structures based on computing?', ' What are linked data structures?']","['a pointer', 'Data structures', 'the addresses of data items with arithmetic operations', 'storing addresses of data items within the structure itself']"
1495,data structure,Implementation,"The implementation of a data structure usually requires writing a set of procedures that create and manipulate instances of that structure. The efficiency of a data structure cannot be analyzed separately from those operations. This observation motivates the theoretical concept of an abstract data type, a data structure that is defined indirectly by the operations that may be performed on it, and the mathematical properties of those operations (including their space and time cost).",The implementation of a data structure usually requires writing a set of procedures that create and manipulate instances of that structure. The efficiency of a data structure cannot be analyzed separately from those operations.,"[' The implementation of a data structure usually requires writing a set of procedures that create and manipulate instances of what structure?', ' What cannot be analyzed separately from those operations?']","['that structure', 'The efficiency of a data structure']"
1496,data structure,Examples,"There are numerous types of data structures, generally built upon simpler primitive data types. Well known examples are:","There are numerous types of data structures, generally built upon simpler primitive data types. Well known examples are:","[' What types of data structures are generally built upon simpler primitive data types?', ' What are some well known examples of data structure?']","['numerous types of data structures', 'simpler primitive data types']"
1497,data structure,Language support,"Most assembly languages and some low-level languages, such as BCPL (Basic Combined Programming Language), lack built-in support for data structures. On the other hand, many high-level programming languages and some higher-level assembly languages, such as MASM, have special syntax or other built-in support for certain data structures, such as records and arrays. For example, the C (a direct descendant of BCPL) and Pascal languages support structs and records, respectively, in addition to vectors (one-dimensional arrays) and multi-dimensional arrays.","Most assembly languages and some low-level languages, such as BCPL (Basic Combined Programming Language), lack built-in support for data structures. On the other hand, many high-level programming languages and some higher-level assembly languages, such as MASM, have special syntax or other built-in support for certain data structures, such as records and arrays.","[' What assembly language lacks built-in support for data structures?', ' What high-level assembly languages have special syntax?', ' Other built-in support for certain data structures, such as records and arrays?']","['BCPL', 'MASM', 'special syntax']"
1498,data structure,Language support,"Most programming languages feature some sort of library mechanism that allows data structure implementations to be reused by different programs. Modern languages usually come with standard libraries that implement the most common data structures. Examples are the C++ Standard Template Library, the Java Collections Framework, and the Microsoft .NET Framework.
",Most programming languages feature some sort of library mechanism that allows data structure implementations to be reused by different programs. Modern languages usually come with standard libraries that implement the most common data structures.,"[' Most programming languages feature some sort of what?', ' What allows data structure implementations to be reused by different programs?', ' Modern languages usually come with what kind of libraries?']","['library mechanism', 'library mechanism', 'standard']"
1499,data structure,Language support,"Modern languages also generally support modular programming, the separation between the interface of a library module and its implementation. Some provide opaque data types that allow clients to hide implementation details. Object-oriented programming languages, such as C++, Java, and Smalltalk, typically use classes for this purpose.
","Modern languages also generally support modular programming, the separation between the interface of a library module and its implementation. Some provide opaque data types that allow clients to hide implementation details.","[' Modern languages generally support what type of programming?', ' What is the separation between the interface of a library module and its implementation?', ' Some provide opaque data types that allow clients to hide what?']","['modular', 'modular programming', 'implementation details']"
1500,text mining,Summary,"Text mining, also referred to as text data mining, similar to text analytics, is the process of deriving high-quality information from text. It involves ""the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources."" Written resources may include websites, books, emails, reviews, and articles.
High-quality information is typically obtained by devising patterns and trends by means such as statistical pattern learning. According to Hotho et al. (2005) we can differ three different perspectives of text mining: information extraction, data mining, and a KDD (Knowledge Discovery in Databases) process. Text mining usually involves the process of structuring the input text (usually parsing, along with the addition of some derived linguistic features and the removal of others, and subsequent insertion into a database), deriving patterns within the structured data, and finally evaluation and interpretation of the output. 'High quality' in text mining usually refers to some combination of relevance, novelty, and interest. Typical text mining tasks include text categorization, text clustering, concept/entity extraction, production of granular taxonomies, sentiment analysis, document summarization, and entity relation modeling (i.e., learning relations between named entities).
","Text mining, also referred to as text data mining, similar to text analytics, is the process of deriving high-quality information from text. It involves ""the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.""","[' What is text mining also referred to as?', ' What is the process of deriving high-quality information from text?']","['text data mining', 'Text mining']"
1501,text mining,Summary,"Text analysis involves information retrieval, lexical analysis to study word frequency distributions, pattern recognition, tagging/annotation, information extraction, data mining techniques including link and association analysis, visualization, and predictive analytics. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP), different types of algorithms and analytical methods. An important phase of this process is the interpretation of the gathered information.
","Text analysis involves information retrieval, lexical analysis to study word frequency distributions, pattern recognition, tagging/annotation, information extraction, data mining techniques including link and association analysis, visualization, and predictive analytics. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP), different types of algorithms and analytical methods.","[' What type of analysis is used to study word frequency distributions?', ' What is the overarching goal of text analysis?', ' How does NLP work?', ' What does NLP stand for?', ' What is NLP?']","['lexical', 'to turn text into data for analysis', 'natural language processing', 'natural language processing', 'natural language processing']"
1502,text mining,Summary,"A typical application is to scan a set of documents written in a natural language and either model the document set for predictive classification purposes or populate a database or search index with the information extracted.
The document is the basic element while starting with text mining. Here, we define a document as a unit of textual data, which normally exists in many types of collections.",A typical application is to scan a set of documents written in a natural language and either model the document set for predictive classification purposes or populate a database or search index with the information extracted. The document is the basic element while starting with text mining.,"[' What is a typical application to scan a set of documents written in a natural language?', ' What is the basic element while starting with text mining?']","['either model the document set for predictive classification purposes', 'The document']"
1503,text mining,Text analytics,"The term text analytics describes a set of linguistic, statistical, and machine learning techniques that model and structure the information content of textual sources for business intelligence, exploratory data analysis, research, or investigation. The term is roughly synonymous with text mining; indeed, Ronen Feldman modified a 2000 description of ""text mining"" in 2004 to describe ""text analytics"". The latter term is now used more frequently in business settings while ""text mining"" is used in some of the earliest application areas, dating to the 1980s, notably life-sciences research and government intelligence.
","The term text analytics describes a set of linguistic, statistical, and machine learning techniques that model and structure the information content of textual sources for business intelligence, exploratory data analysis, research, or investigation. The term is roughly synonymous with text mining; indeed, Ronen Feldman modified a 2000 description of ""text mining"" in 2004 to describe ""text analytics"".","[' What term describes a set of linguistic, statistical, and machine learning techniques that model and structure the information content of textual sources for business intelligence?', ' What is the term text analytics roughly synonymous with?', ' Who modified a 2000 description of ""text mining""?', ' Who modified a 2000 description of ""text mining"" to describe ""text analytics""?']","['text analytics', 'text mining', 'Ronen Feldman', 'Ronen Feldman']"
1504,text mining,Text analytics,"The term text analytics also describes that application of text analytics to respond to business problems, whether independently or in conjunction with query and analysis of fielded, numerical data. It is a truism that 80 percent of business-relevant information originates in unstructured form, primarily text. These techniques and processes discover and present knowledge – facts, business rules, and relationships – that is otherwise locked in textual form, impenetrable to automated processing.
","The term text analytics also describes that application of text analytics to respond to business problems, whether independently or in conjunction with query and analysis of fielded, numerical data. It is a truism that 80 percent of business-relevant information originates in unstructured form, primarily text.","[' What term describes the application of text analytics to respond to business problems?', ' What percentage of business-relevant information originates in unstructured form?']","['text analytics', '80']"
1505,text mining,Applications,"Text mining technology is now broadly applied to a wide variety of government, research, and business needs. All these groups may use text mining for records management and searching documents relevant to their daily activities. Legal professionals may use text mining for e-discovery, for example. Governments and military groups use text mining for national security and intelligence purposes. Scientific researchers incorporate text mining approaches into efforts to organize large sets of text data (i.e., addressing the problem of unstructured data), to determine ideas communicated through text (e.g., sentiment analysis in social media) and to support scientific discovery in fields such as the life sciences and bioinformatics. In business, applications are used to support competitive intelligence and automated ad placement, among numerous other activities.
","Text mining technology is now broadly applied to a wide variety of government, research, and business needs. All these groups may use text mining for records management and searching documents relevant to their daily activities.","[' What type of technology is now widely applied to a wide variety of government, research, and business needs?', ' All these groups may use text mining for what?']","['Text mining', 'records management and searching documents relevant to their daily activities']"
1506,text mining,Software,"Text mining computer programs are available from many commercial and open source companies and sources. See List of text mining software.
",Text mining computer programs are available from many commercial and open source companies and sources. See List of text mining software.,[' What type of computer programs are available from many commercial and open source companies?'],['Text mining']
1507,text mining,Implications,"Until recently, websites most often used text-based searches, which only found documents containing specific user-defined words or phrases. Now, through use of a semantic web, text mining can find content based on meaning and context (rather than just by a specific word). Additionally, text mining software can be used to build large dossiers of information about specific people and events.  For example, large datasets based on data extracted from news reports can be built to facilitate social networks analysis or counter-intelligence.  In effect, the text mining software may act in a capacity similar to an intelligence analyst or research librarian, albeit with a more limited scope of analysis. Text mining is also used in some email spam filters as a way of determining the characteristics of messages that are likely to be advertisements or other unwanted material. Text mining plays an important role in determining financial market sentiment.
","Until recently, websites most often used text-based searches, which only found documents containing specific user-defined words or phrases. Now, through use of a semantic web, text mining can find content based on meaning and context (rather than just by a specific word).","[' Until recently, websites used what type of searches?', ' What type of search found documents containing specific user defined words or phrases?']","['text-based searches', 'text-based searches']"
1508,text mining,Future,"The challenge of exploiting the large proportion of enterprise information that originates in ""unstructured"" form has been recognized for decades. It is recognized in the earliest definition of business intelligence (BI), in an October 1958 IBM Journal article by H.P. Luhn, A Business Intelligence System, which describes a system that will:
","The challenge of exploiting the large proportion of enterprise information that originates in ""unstructured"" form has been recognized for decades. It is recognized in the earliest definition of business intelligence (BI), in an October 1958 IBM Journal article by H.P.","[' How long has the challenge of exploiting the large proportion of enterprise information that originates in ""unstructured"" form been recognized?', ' What is the earliest definition of business intelligence?', ' Who wrote an October 1958 IBM Journal article?']","['decades', 'BI', 'H.P']"
1509,text mining,Future,"""...utilize data-processing machines for auto-abstracting and auto-encoding of documents and for creating interest profiles for each of the 'action points' in an organization. Both incoming and internally generated documents are automatically abstracted, characterized by a word pattern, and sent automatically to appropriate action points.""
","""...utilize data-processing machines for auto-abstracting and auto-encoding of documents and for creating interest profiles for each of the 'action points' in an organization. Both incoming and internally generated documents are automatically abstracted, characterized by a word pattern, and sent automatically to appropriate action points.""","["" What do data-processing machines create for each of the 'action points' in an organization?"", ' Both incoming and internally generated documents are automatically abstracted, characterized by what?']","['interest profiles', 'a word pattern']"
1510,text mining,Future,"Yet as management information systems developed starting in the 1960s, and as BI emerged in the '80s and '90s as a software category and field of practice, the emphasis was on numerical data stored in relational databases. This is not surprising: text in ""unstructured"" documents is hard to process. The emergence of text analytics in its current form stems from a refocusing of research in the late 1990s from algorithm development to application, as described by Prof. Marti A. Hearst in the paper Untangling Text Data Mining:","Yet as management information systems developed starting in the 1960s, and as BI emerged in the '80s and '90s as a software category and field of practice, the emphasis was on numerical data stored in relational databases. This is not surprising: text in ""unstructured"" documents is hard to process.","[' When did management information systems develop?', ' When did BI emerge as a software category and field of practice?', ' Where was the emphasis on numerical data stored?', ' What is hard to process in unstructured documents?']","['starting in the 1960s', ""'80s and '90s"", 'relational databases', 'text']"
1511,text mining,Future,"For almost a decade the computational linguistics community has viewed large text collections as a resource to be tapped in order to produce better text analysis algorithms. In this paper, I have attempted to suggest a new emphasis: the use of large online text collections to discover new facts and trends about the world itself. I suggest that to make progress we do not need fully artificial intelligent text analysis; rather, a mixture of computationally-driven and user-guided analysis may open the door to exciting new results.
","For almost a decade the computational linguistics community has viewed large text collections as a resource to be tapped in order to produce better text analysis algorithms. In this paper, I have attempted to suggest a new emphasis: the use of large online text collections to discover new facts and trends about the world itself.","[' For how long has the computational linguistics community viewed large text collections as a resource to be tapped in order to produce better text analysis algorithms?', ' I have attempted to suggest a new emphasis on the use of large online text collections to discover what?', ' What is the purpose of large online text collections to discover new facts and trends?']","['almost a decade', 'new facts and trends about the world itself', 'the world itself']"
1512,performance evaluation,Summary,"A performance appraisal, also referred to as a performance review, performance evaluation,  (career) development discussion, or  employee appraisal is a method by which the job performance of an employee is documented and evaluated. Performance appraisals are a part of career development and consist of regular reviews of employee performance within organizations.
","A performance appraisal, also referred to as a performance review, performance evaluation,  (career) development discussion, or  employee appraisal is a method by which the job performance of an employee is documented and evaluated. Performance appraisals are a part of career development and consist of regular reviews of employee performance within organizations.","[' What is a performance appraisal a part of?', ' What is an employee appraisal?', ' How is the job performance of an employee documented?', ' What type of reviews are conducted on a regular basis?']","['career development', 'a method by which the job performance of an employee is documented and evaluated', 'performance appraisal', 'Performance appraisals']"
1513,performance evaluation,Summary,"Performance appraisals are most often conducted by an immediate manager, such as line managers or front-line managers (Tyskbo, 2020])  Annual performance reviews have been criticized (Evans & Tourish, 2017) as providing feedback too infrequently to be useful, and argue performance reviews in general do more harm than good. It is an element of the principal-agent framework, that describes the relationship of information between the employer and employee, and in this case the direct effect and response received when a performance review is conducted.","Performance appraisals are most often conducted by an immediate manager, such as line managers or front-line managers (Tyskbo, 2020])  Annual performance reviews have been criticized (Evans & Tourish, 2017) as providing feedback too infrequently to be useful, and argue performance reviews in general do more harm than good. It is an element of the principal-agent framework, that describes the relationship of information between the employer and employee, and in this case the direct effect and response received when a performance review is conducted.","[' Who conducts performance appraisals most often?', ' What has been criticized as providing feedback too infrequently?', ' What do performance reviews in general do more harm than good?', ' What describes the relationship of information between the employer and employee?']","['an immediate manager', 'Annual performance reviews', 'feedback too infrequently to be useful', 'principal-agent framework']"
1514,performance evaluation,Main features,"A performance appraisal is a systematic, general and periodic process that assesses an individual employee's job performance and productivity in relation to certain pre-established criteria and organizational objectives.  Other aspects of individual employees are considered as well, such as organizational citizenship behavior, accomplishments, potential for future improvement, strengths and weaknesses, etc.","A performance appraisal is a systematic, general and periodic process that assesses an individual employee's job performance and productivity in relation to certain pre-established criteria and organizational objectives. Other aspects of individual employees are considered as well, such as organizational citizenship behavior, accomplishments, potential for future improvement, strengths and weaknesses, etc.","["" What is a systematic, general and periodic process that assesses an individual employee's job performance and productivity in relation to certain pre-established criteria and organizational objectives?"", ' What are other aspects of individual employees considered?', ' What does citizenship behavior consist of?', ' What do citizens have in common?']","['A performance appraisal', 'organizational citizenship behavior, accomplishments, potential for future improvement, strengths and weaknesses', 'organizational', 'organizational citizenship behavior']"
1515,performance evaluation,Main features,"To collect PA data, there are three main methods: objective production, personnel, and judgmental evaluation.  Judgmental evaluations are the most commonly used with a large variety of evaluation methods.  Historically, PA has been conducted annually (long-cycle appraisals); however, many companies are moving towards shorter cycles (every six months, every quarter), and some have been moving into short-cycle (weekly, bi-weekly) PA.  The interview could function as ""providing feedback to employees, counseling and developing employees, and conveying and discussing compensation, job status, or disciplinary decisions"".  PA is often included in performance management systems.  PA helps the subordinate answer two key questions: first, ""What are your expectations of me?"" second, ""How am I doing to meet your expectations?""","To collect PA data, there are three main methods: objective production, personnel, and judgmental evaluation. Judgmental evaluations are the most commonly used with a large variety of evaluation methods.","[' How many main methods are used to collect PA data?', ' What is the most commonly used evaluation method?']","['three', 'Judgmental evaluations']"
1516,performance evaluation,Main features,"Performance management systems are employed ""to manage and align"" all of an organization's resources in order to achieve highest possible performance and to eliminate distractions procured from individual agents that neglect the companies goals.  ""How performance is managed in an organization determines to a large extent the success or failure of the organization. Therefore, improving PA for everyone should be among the highest priorities of contemporary organizations"".","Performance management systems are employed ""to manage and align"" all of an organization's resources in order to achieve highest possible performance and to eliminate distractions procured from individual agents that neglect the companies goals. ""How performance is managed in an organization determines to a large extent the success or failure of the organization.","[' What are performance management systems used for?', ' What is the goal of performance management?', ' How is performance managed in an organization?', ' What determines to a large extent the success or failure of an organization?']","['to manage and align"" all of an organization\'s resources', 'achieve highest possible performance', 'How performance is managed in an organization determines to a large extent the success or failure of the organization', 'How performance is managed in an organization']"
1517,performance evaluation,Main features,"Some applications of PA are compensation, performance improvement, promotions, termination, test validation, and more.  While there are many potential benefits of PA, there are also some potential drawbacks.  For example, PA can help facilitate management-employee communication; however, PA may result in legal issues if not executed appropriately, as many employees tend to be unsatisfied with the PA process, as well as, the misuse of PA's can incur apathy towards organizational goals and values.  PAs created in and determined as useful in the United States are not necessarily able to be transferable cross-culturally.","Some applications of PA are compensation, performance improvement, promotions, termination, test validation, and more. While there are many potential benefits of PA, there are also some potential drawbacks.","[' What are some applications of PA?', ' There are many potential benefits of PA, but what are there drawbacks?']","['compensation, performance improvement, promotions, termination, test validation, and more', 'drawbacks']"
1518,performance evaluation,Applications of results,"A central reason for the utilization of performance appraisals (PAs) is performance improvement (""initially at the level of the individual employee, and ultimately at the level of the organization"").  Other fundamental reasons include ""as a basis for employment decisions (e.g. promotions, terminations, transfers), as criteria in research (e.g. test validation), to aid with communication (e.g. allowing employees to know how they are doing and organizational expectations), to establish personal objectives for training"" programs, for transmission of objective feedback for personal development, ""as a means of documentation to aid in keeping track of decisions and legal requirements"" and in wage and salary administration.  Additionally, PAs can aid in the formulation of job criteria and selection of individuals ""who are best suited to perform the required organizational tasks"".  A PA can be part of guiding and monitoring employee career development.  PAs can also be used to aid in work motivation through the use of reward systems.","A central reason for the utilization of performance appraisals (PAs) is performance improvement (""initially at the level of the individual employee, and ultimately at the level of the organization""). Other fundamental reasons include ""as a basis for employment decisions (e.g.","[' What is a central reason for the utilization of performance appraisals?', ' What is another fundamental reason?']","['performance improvement', 'as a basis for employment decisions']"
1519,performance evaluation,Conducting,"Human resource management (HRM) conducts performance management.  Performance management systems consist of the activities and/or processes embraced by an organization in anticipation of improving employee performance, and therefore, organizational performance.  Consequently, performance management is conducted at the organizational level and the individual level.  At the organizational level, performance management oversees organizational performance and compares present performance with organizational performance goals.  The achievement of these organizational performance goals depends on the performance of the individual organizational members.  Therefore, measuring individual employee performance can prove to be a valuable performance management process for the purposes of HRM and for the organization.  Many researchers would argue that ""performance appraisal is one of the most important processes in Human Resource Management"".","Human resource management (HRM) conducts performance management. Performance management systems consist of the activities and/or processes embraced by an organization in anticipation of improving employee performance, and therefore, organizational performance.","[' What does HRM stand for?', ' What is the purpose of HRM?']","['Human resource management', 'conducts performance management']"
1520,performance evaluation,Conducting,"The performance management process begins with leadership within the organization creating a performance management policy.  Primarily, management governs performance by influencing employee performance input (e.g. training programs) and by providing feedback via output (i.e. performance assessment and appraisal).  ""The ultimate objective of a performance management process is to align individual performance with organizational performance"".  A very common and central process of performance management systems is performance appraisal (PA).  The PA process should be able to inform employees about the ""organization's goals, priorities, and expectations and how well they are contributing to them"".","The performance management process begins with leadership within the organization creating a performance management policy. Primarily, management governs performance by influencing employee performance input (e.g.","[' What is the first step in the performance management process?', ' What does leadership within the organization create?', ' How does management govern performance?']","['creating a performance management policy', 'performance management policy', 'by influencing employee performance input']"
1521,performance evaluation,When they are conducted,"Performance appraisals (PAs) are conducted at least annually, and annual employee performance reviews appear to be the standard in most American organizations.  However, ""it has been acknowledged that appraisals conducted more frequently (more than once a year) may have positive implications for both the organization and employee.""  It is suggested that regular performance feedback provided to employees may quell any unexpected and/or surprising feedback to year-end discussions.  In a recent research study concerning the timeliness of PAs, ""one of the respondents even suggested that the performance review should be done formally and more frequently, perhaps once a month, and recorded twice a year.""","Performance appraisals (PAs) are conducted at least annually, and annual employee performance reviews appear to be the standard in most American organizations. However, ""it has been acknowledged that appraisals conducted more frequently (more than once a year) may have positive implications for both the organization and employee.""","[' At least how often are performance appraisals conducted?', ' What is the standard in most American organizations?']","['annually', 'annual employee performance reviews']"
1522,performance evaluation,When they are conducted,"Other researchers propose that the purpose of PAs and the frequency of their feedback are contingent upon the nature of the job and characteristics of the employee.  For example, employees of routine jobs where performance maintenance is the goal would benefit sufficiently from annual PA feedback.  On the other hand, employees of more discretionary and non-routine jobs, where goal-setting is appropriate and there is room for development, would benefit from more frequent PA feedback. Non formal performance appraisals may be done more often, to prevent the element of surprise from the formal appraisal.","Other researchers propose that the purpose of PAs and the frequency of their feedback are contingent upon the nature of the job and characteristics of the employee. For example, employees of routine jobs where performance maintenance is the goal would benefit sufficiently from annual PA feedback.","[' What do some researchers believe is contingent upon the nature of the job and characteristics of the employee?', ' What type of job would routine employees benefit from annual PA feedback?']","['the purpose of PAs and the frequency of their feedback', 'performance maintenance is the goal']"
1523,performance evaluation,Methods of collecting data,"There are three main methods used to collect performance appraisal (PA) data: objective production, personnel, and judgmental evaluation.  Judgmental evaluations are the most commonly used with a large variety of evaluation methods.","There are three main methods used to collect performance appraisal (PA) data: objective production, personnel, and judgmental evaluation. Judgmental evaluations are the most commonly used with a large variety of evaluation methods.","[' How many main methods are used to collect performance appraisal data?', ' What is the most commonly used evaluation method?']","['three', 'Judgmental evaluations']"
1524,performance evaluation,Principal - Agent Framework,"The Principal-agent framework is a model describing the relationship of information held between an employer and an employee. It is used to forecast responses from employees and strategies at finding resolutions against misaligned incentives that interfere with the goals of the employer. The model makes two assumptions: the principals wants agents to work for the principal's best interest, but agents possess different goals than the principals; and, the agents have more information than the principals resulting in the asymmetry of information between the two parties. This paradigm creates adverse selections and moral hazards for the hiring company in deciding how to effectively minimize the potential threat of shirking; disruption to daily operations; and loss in output margins due to actions of the employee.",The Principal-agent framework is a model describing the relationship of information held between an employer and an employee. It is used to forecast responses from employees and strategies at finding resolutions against misaligned incentives that interfere with the goals of the employer.,"[' What is the Principal-agent framework?', ' What is it used to forecast responses from employees?']","['a model describing the relationship of information held between an employer and an employee', 'Principal-agent framework']"
1525,performance evaluation,Organizational citizenship behavior,"Also referred to as contextual behavior, prosocial behavior, and extra-role behavior, organizational citizenship behavior (OCB) consists of employee behavior that contributes to the welfare of the organization but is beyond the scope of the employee's job duties.  These extra-role behaviors may help or hinder the attainment of organizational goals.  Research supports five dimensions of OCB: altruism, conscientiousness, courtesy, sportsmanship, and civic virtue.  Researchers have found that the OCB dimensions of altruism and civic virtue can have just as much of an impact on manager's subjective evaluations of employees’ performances as employees’ objective productivity levels.  The degree to which OCB can influence judgments of job performance is relatively high.  Controversy exists as to whether OCB should be formally considered as a part of performance appraisal (PA).
","Also referred to as contextual behavior, prosocial behavior, and extra-role behavior, organizational citizenship behavior (OCB) consists of employee behavior that contributes to the welfare of the organization but is beyond the scope of the employee's job duties. These extra-role behaviors may help or hinder the attainment of organizational goals.","[' What is Occupation citizenship behavior?', ' What is contextual behavior, prosocial behavior, and extra-role behavior called?']","[""employee behavior that contributes to the welfare of the organization but is beyond the scope of the employee's job duties"", 'organizational citizenship behavior']"
1526,performance evaluation,Interviews,"The performance appraisal (PA) interview is typically the final step of the appraisal process.  The interview is held between the subordinate and supervisor.  The PA interview can be considered of great significance to an organization's PA system.  It is most advantageous when both the superior and subordinate participate in the interview discussion and establish goals together.  Three factors consistently contribute to effective PA interviews: the supervisor's knowledge of the subordinate's job and performance in it, the supervisor's support of the subordinate, and a welcoming of the subordinate's participation.
The objective of performance appraisal is to assess the training development needs of employees.
",The performance appraisal (PA) interview is typically the final step of the appraisal process. The interview is held between the subordinate and supervisor.,"[' What is the final step of the appraisal process?', ' Who holds the performance appraisal interview?']","['The performance appraisal (PA) interview', 'the subordinate and supervisor']"
1527,performance evaluation,Employee reactions,"Numerous researchers have reported that many employees are not satisfied with their performance appraisal (PA) systems.  Studies have shown that subjectivity as well as appraiser bias is often a problem perceived by as many as half of employees. Subjectivity has been associated with supervisor-subordinate conflict, psychological empowerment and subordinate performance. Appraiser bias, however, appears to be perceived as more of a problem in government and public sector organizations.  Also, according to some studies, employees wished to see changes in the PA system by making ""the system more objective, improving the feedback process, and increasing the frequency of review.""  In light of traditional PA operation defects, ""organizations are now increasingly incorporating practices that may improve the system.  These changes are particularly concerned with areas such as elimination of subjectivity and bias, training of appraisers, improvement of the feedback process and the performance review discussion.""",Numerous researchers have reported that many employees are not satisfied with their performance appraisal (PA) systems. Studies have shown that subjectivity as well as appraiser bias is often a problem perceived by as many as half of employees.,"[' How many employees are not satisfied with performance appraisal systems?', ' According to studies, what is a problem perceived by as many as half of employees?']","['many', 'subjectivity as well as appraiser bias']"
1528,performance evaluation,Employee reactions,"According to a meta-analysis of 27 field studies, general employee participation in his/her own appraisal process was positively correlated with employee reactions to the PA system.  More specifically, employee participation in the appraisal process was most strongly related to employee satisfaction with the PA system.  Concerning the reliability of employee reaction measures, researchers have found employee reaction scales to be sound with few concerns through using a confirmatory factor analysis that is representative of employee reaction scales.","According to a meta-analysis of 27 field studies, general employee participation in his/her own appraisal process was positively correlated with employee reactions to the PA system. More specifically, employee participation in the appraisal process was most strongly related to employee satisfaction with the PA system.","[' According to a meta-analysis of 27 field studies, what was positively correlated with employee reactions to the PA system?', ' Employee participation in the appraisal process was most strongly related to what?']","['general employee participation in his/her own appraisal process', 'employee satisfaction with the PA system']"
1529,performance evaluation,Employee reactions,"Researchers suggest that the study of employees' reactions to PA is important because of two main reasons: employee reactions symbolizes a criterion of interest to practitioners of PAs and employee reactions have been associated through theory to determinants of appraisal acceptance and success.  Researchers translate these reasons into the context of the scientist-practitioner gap or the ""lack of alignment between research and practice.""","Researchers suggest that the study of employees' reactions to PA is important because of two main reasons: employee reactions symbolizes a criterion of interest to practitioners of PAs and employee reactions have been associated through theory to determinants of appraisal acceptance and success. Researchers translate these reasons into the context of the scientist-practitioner gap or the ""lack of alignment between research and practice.""","["" Why do researchers believe that the study of employees' reactions to PA is important?"", ' Employee reactions symbolizes a criterion of interest to practitioners of what?', ' What have employee reactions been associated through theory to determinants of?', ' What are the two main reasons for acceptance and success?', ' What is the ""lack of alignment between research and practice""?']","['because of two main reasons', 'PAs', 'appraisal acceptance and success', 'employee reactions', 'scientist-practitioner gap']"
1530,performance evaluation,Employee reactions,"Schultz & Schultz notes that opposition to performance appraisals generally don't receive positive ratings from anyone involved. ""So employees that will be directly affected by the Performance Appraisals are less than enthusiastic about participating in them"". When an employee knows that their work performance has been less than perfect it's nerve-racking to be evaluated. Employees tend to be hostile knowing they could be given bad news on their performance.","Schultz & Schultz notes that opposition to performance appraisals generally don't receive positive ratings from anyone involved. ""So employees that will be directly affected by the Performance Appraisals are less than enthusiastic about participating in them"".","["" Who notes that opposition to performance appraisals generally don't receive positive ratings from?"", ' Employees that will be directly affected by Performance Appraisals are less than enthusiastic about participating in them?']","['Schultz & Schultz', 'employees']"
1531,performance evaluation,Employee reactions,"Most managers prefer to begin with positive information and then add bad news or suggestions for improvement at the end.  However, employees are most satisfied when bad news is addressed early in the interview and positive information is saved until the end, so that the meeting ends with a positive feeling.","Most managers prefer to begin with positive information and then add bad news or suggestions for improvement at the end. However, employees are most satisfied when bad news is addressed early in the interview and positive information is saved until the end, so that the meeting ends with a positive feeling.","[' What do most managers prefer to begin with positive information?', ' What do managers usually add at the end of a meeting?', ' Employees are most satisfied when bad news is addressed early in the interview and what is saved?', ' What does the end of a meeting end with?']","['bad news or suggestions for improvement at the end', 'bad news or suggestions for improvement', 'positive information', 'a positive feeling']"
1532,performance evaluation,Legal implications,"There are federal laws addressing fair employment practices, and this also concerns performance appraisal (PA).  Discrimination can occur within predictions of performance and evaluations of job behaviors.  The revision of many court cases has revealed the involvement of alleged discrimination which was often linked to the assessment of the employee's job performance.  Some of the laws which protect individuals against discrimination are ""the Title VII of the Civil Rights Act of 1964, the Civil Rights Act of 1991, the Age Discrimination in Employment Act (ADEA), and the Americans with Disabilities Act (ADA).""  Lawsuits may also results from charges of an employer's negligence, defamation, and/or misrepresentation.  A few appraisal criteria to keep in mind for a legally sound PA is to keep the content of the appraisal objective, job-related, behavior-based, within the control of the ratee, and related to specific functions rather than a global assessment.  Some appraisal procedure suggestions for a legally sound PA is to standardize operations, communicate formally with employees, provide information of performance deficits and give opportunities to employees to correct those deficits, give employees access to appraisal results, provide written instructions for the training of raters, and use multiple, diverse and unbiased raters.  These are valuable but not exhaustive lists of recommendations for PAs.
The Employment Opportunity Commission (EEOC) guidelines apply to any selection procedure that is used for making employment decisions, not only for hiring, but also for promotion, demotion, transfer, layoff, discharge, or early retirement. Therefore, employment appraisal procedures must be validated like tests or any other selection device. Employers who base their personnel decisions on the results of a well-designed performance review program that includes formal appraisal interviews are much more likely to be successful in defending themselves against claims of discrimination.","There are federal laws addressing fair employment practices, and this also concerns performance appraisal (PA). Discrimination can occur within predictions of performance and evaluations of job behaviors.","[' What does PA stand for?', ' Where can discrimination occur?']","['performance appraisal', 'within predictions of performance and evaluations of job behaviors']"
1533,performance evaluation,Cross-cultural implications,"Performance appraisal (PA) systems, and the premises of which they were based, that have been formed and regarded as effective in the United States may not have the transferability for effectual utilization in other countries or cultures, and vice versa.  Performance ""appraisal is thought to be deeply rooted in the norms, values, and beliefs of a society"".  ""Appraisal reflects attitudes towards motivation and performance (self) and relationships (e.g. peers, subordinates, supervisors, organization), all of which vary from one country to the next"".  Therefore, appraisal should be in conjunction with cultural norms, values, and beliefs in order to be operative.  The deep-seated norms, values and beliefs in different cultures affect employee motivation and perception of organizational equity and justice.  In effect, a PA system created and considered effectual in one country may not be an appropriate assessment in another cultural region.","Performance appraisal (PA) systems, and the premises of which they were based, that have been formed and regarded as effective in the United States may not have the transferability for effectual utilization in other countries or cultures, and vice versa. Performance ""appraisal is thought to be deeply rooted in the norms, values, and beliefs of a society"".","[' What is the acronym for performance appraisal systems?', ' What may not be transferable to other countries or cultures?', ' What is considered to be deeply rooted in the norms, values, and beliefs of a society?']","['PA', 'Performance appraisal (PA) systems', 'Performance ""appraisal']"
1534,performance evaluation,Cross-cultural implications,"For example, some countries and cultures value the trait of assertiveness and personal accomplishment while others instead place more merit on cooperation and interpersonal connection.  Countries scoring high on assertiveness consider PA to be a way of assuring equity among employees so that higher performing employees receive greater rewards or higher salaries.  Countries scoring low on assertiveness but higher in interpersonal relations may not like the social separation and pay inequity of higher/lower performing employees; employees from this more cooperative rather than individualistic culture place more concern on interpersonal relationships with other employees rather than on individual interests.  High assertive countries value performance feedback for self-management and effectiveness purposes while countries low in assertiveness view performance feedback as ""threatening and obtrusive"".  In this case, the PA of the high assertive countries would likely not be beneficial for countries scoring lower in assertiveness to employ.   However, countries scoring lower in assertiveness could employ PA for purposes of improving long-term communication development within the organization such as clarifying job objectives, guide training and development plans, and lessen the gap between job performance and organizational expectations.","For example, some countries and cultures value the trait of assertiveness and personal accomplishment while others instead place more merit on cooperation and interpersonal connection. Countries scoring high on assertiveness consider PA to be a way of assuring equity among employees so that higher performing employees receive greater rewards or higher salaries.","[' What trait do some countries and cultures value?', ' What do countries scoring high on assertiveness consider PA to be a way of assuring?', ' How do higher performing employees receive rewards or higher salaries?']","['assertiveness', 'equity among employees', 'assuring equity']"
1535,performance evaluation,Developments in information technology,"Computers have been playing an increasing role in PA for some time (Sulsky & Keown, 1998). There are two main aspects to this. The first is in relation to the electronic monitoring of performance, which affords the ability to record a huge amount of data on multiple dimensions of work performance (Stanton, 2000). Not only does it facilitate a more continuous and detailed collection of performance data in some jobs, e.g. call centres, but it has the capacity to do so in a non-obvious, covert manner. The second aspect is in mediating the feedback process, by recording and aggregating performance ratings and written observations and making the information available on-line; many software packages are available for this. The use of IT in these ways undoubtedly helps in making the appraisal process more manageable, especially where multiple rating sources are involved, but it also raises many questions about appraisees' reactions and possible effects on PA outcomes. Mostly, the evidence so far is positive.","Computers have been playing an increasing role in PA for some time (Sulsky & Keown, 1998). There are two main aspects to this.","[' What has been playing an increasing role in PA for some time?', ' There are two main aspects to what?']","['Computers', 'Computers']"
1536,performance evaluation,Rater errors,"Mistakes made by raters is a major source of problems in performance appraisal. There is no simple way to completely eliminate these errors, but making raters aware of them through training is helpful. Rater errors are based on the feelings and it has consequences at the time of appraisal.","Mistakes made by raters is a major source of problems in performance appraisal. There is no simple way to completely eliminate these errors, but making raters aware of them through training is helpful.","[' What is a major source of problems in performance appraisal?', ' What can be done to eliminate errors by raters?']","['Mistakes made by raters', 'making raters aware of them through training']"
1537,performance evaluation,Rater errors,"We have been looking one by one at the possible solutions to each of the situations, which are also complicated to put into practice, thus here we have a general solution that could be applied to all the possible rating errors. It is difficult to minimized rater errors, since we are humans and we are not objective. Moreover, sometimes, we are not aware of our behavior of having preferences towards people but there are some tools in order to have a more objective information as using available technology to track performances and record it which enables the manager to have some objective information about the process.
","We have been looking one by one at the possible solutions to each of the situations, which are also complicated to put into practice, thus here we have a general solution that could be applied to all the possible rating errors. It is difficult to minimized rater errors, since we are humans and we are not objective.","[' How have we been looking at the possible solutions to each situation?', ' What is difficult to minimized rater errors?']","['one by one', 'since we are humans and we are not objective']"
1538,performance evaluation,Rater errors,"Consultant Marcus Buckingham and executive Ashley Goodall, reporting on a large-scale Deloitte performance management survey on Harvard Business Review, went as far as to say that, contrary to the assumptions underlying performance rating, the rating mainly measured the unique rating tendencies of the rater and thus reveals more about the rater than about the person who is rated. They referred to this as the idiosyncratic rater effect. In view of this effect, they advocate a radically different approach to performance management. In their scenario, 360-degree feedback and similar time-intensive exercises are replaced by team leaders' ""performance snapshots"" that focus on what they would do with each team member rather than what they think of that individual, and yearly appraisals of past performance are replaced by weekly check-ins among team leader and team member, preferably initiated by the team member, that focus on current and upcoming work.","Consultant Marcus Buckingham and executive Ashley Goodall, reporting on a large-scale Deloitte performance management survey on Harvard Business Review, went as far as to say that, contrary to the assumptions underlying performance rating, the rating mainly measured the unique rating tendencies of the rater and thus reveals more about the rater than about the person who is rated. They referred to this as the idiosyncratic rater effect.","[' Marcus Buckingham and Ashley Goodall reported on a large-scale Deloitte performance management survey on what publication?', ' What do tendencies of the rater reveal more about?', ' What is the idiosyncratic rater effect?']","['Harvard Business Review', 'the rater than about the person who is rated', 'the rating mainly measured the unique rating tendencies of the rater and thus reveals more about the rater than about the person who is rated']"
1539,knowledge representation,Summary,"Knowledge representation and reasoning (KRR, KR&R, KR²) is the field of artificial intelligence (AI) dedicated to representing information about the world in a form that a computer system can use to solve complex tasks such as diagnosing a medical condition or having a dialog in a natural language. Knowledge representation incorporates findings from psychology about how humans solve problems and represent knowledge in order to design formalisms that will make complex systems easier to design and build.  Knowledge representation and reasoning also incorporates findings from logic to automate various kinds of reasoning, such as the application of rules or the relations of sets and subsets.
","Knowledge representation and reasoning (KRR, KR&R, KR²) is the field of artificial intelligence (AI) dedicated to representing information about the world in a form that a computer system can use to solve complex tasks such as diagnosing a medical condition or having a dialog in a natural language. Knowledge representation incorporates findings from psychology about how humans solve problems and represent knowledge in order to design formalisms that will make complex systems easier to design and build.","[' What is the field of AI dedicated to representing information about the world in a form that a computer system can use to solve complex tasks such as diagnosing a medical condition?', ' What does knowledge representation incorporate findings from?', ' What is the purpose of knowledge representation?']","['Knowledge representation and reasoning', 'psychology', 'representing information about the world in a form that a computer system can use to solve complex tasks such as diagnosing a medical condition or having a dialog in a natural language. Knowledge representation incorporates findings from psychology about how humans solve problems and represent knowledge in order to design formalisms that will make complex systems easier to design and build']"
1540,knowledge representation,Summary,"Examples of knowledge representation formalisms include semantic nets, systems architecture, frames, rules, and ontologies. Examples of automated reasoning engines include inference engines, theorem provers, and classifiers.
","Examples of knowledge representation formalisms include semantic nets, systems architecture, frames, rules, and ontologies. Examples of automated reasoning engines include inference engines, theorem provers, and classifiers.","[' What are some examples of knowledge representation formalisms?', ' Inference engines, theorm provers, and classifiers are examples of what?']","['semantic nets, systems architecture, frames, rules, and ontologies', 'automated reasoning engines']"
1541,knowledge representation,History,"The earliest work in computerized knowledge representation was focused on general problem-solvers such as the General Problem Solver (GPS) system developed by Allen Newell and Herbert A. Simon in 1959. These systems featured data structures for planning and decomposition. The system would begin with a goal. It would then decompose that goal into sub-goals and then set out to construct strategies that could accomplish each subgoal.
",The earliest work in computerized knowledge representation was focused on general problem-solvers such as the General Problem Solver (GPS) system developed by Allen Newell and Herbert A. Simon in 1959. These systems featured data structures for planning and decomposition.,"[' When was the General Problem Solver system developed?', ' Who developed the GPS system?']","['1959', 'Allen Newell and Herbert A. Simon']"
1542,knowledge representation,History,"In these early days of AI, general search algorithms such as A* were also developed. However, the amorphous problem definitions for systems such as GPS meant that they worked only for very constrained toy domains (e.g. the ""blocks world""). In order to tackle non-toy problems, AI researchers such as Ed Feigenbaum and Frederick Hayes-Roth realized that it was necessary to focus systems on more constrained problems.
","In these early days of AI, general search algorithms such as A* were also developed. However, the amorphous problem definitions for systems such as GPS meant that they worked only for very constrained toy domains (e.g.","[' What search algorithms were developed in the early days of AI?', ' What type of problem definitions were used in GPS systems?']","['A*', 'amorphous']"
1543,knowledge representation,History,"These efforts led to the cognitive revolution in psychology and to the phase of AI focused on knowledge representation that resulted in expert systems in the 1970s and 80s, production systems, frame languages, etc. Rather than general problem solvers, AI changed its focus to expert systems that could match human competence on a specific task, such as medical diagnosis.
","These efforts led to the cognitive revolution in psychology and to the phase of AI focused on knowledge representation that resulted in expert systems in the 1970s and 80s, production systems, frame languages, etc. Rather than general problem solvers, AI changed its focus to expert systems that could match human competence on a specific task, such as medical diagnosis.","["" What did AI focus on in the 1970's and 80's?"", ' Rather than general problem solvers, what did AI change its focus to?', ' What did the company change its focus to?', ' What type of systems could match human competence?']","['knowledge representation', 'expert systems that could match human competence on a specific task, such as medical diagnosis', 'expert systems that could match human competence on a specific task, such as medical diagnosis', 'expert systems']"
1544,knowledge representation,History,"Expert systems gave us the terminology still in use today where AI systems are divided into a knowledge base, with facts about the world and rules, and an inference engine, which applies the rules to the knowledge base in order to answer questions and solve problems. In these early systems the knowledge base tended to be a fairly flat structure, essentially assertions about the values of variables used by the rules.","Expert systems gave us the terminology still in use today where AI systems are divided into a knowledge base, with facts about the world and rules, and an inference engine, which applies the rules to the knowledge base in order to answer questions and solve problems. In these early systems the knowledge base tended to be a fairly flat structure, essentially assertions about the values of variables used by the rules.","[' Expert systems gave us the terminology still in use today where AI systems are divided into a knowledge base, with facts about the world and rules, and what?', ' An inference engine applies rules to the knowledge base in order to answer questions and solve problems?', ' In early systems the knowledge base tended to be a flat structure, essentially assertions about the values of what?']","['an inference engine', 'AI systems are divided into a knowledge base, with facts about the world and rules, and an inference engine', 'variables used by the rules']"
1545,knowledge representation,History,"In addition to expert systems, other researchers developed the concept of frame-based languages in the mid-1980s. A frame is similar to an object class: It is an abstract description of a category describing things in the world, problems, and potential solutions. Frames were originally used on systems geared toward human interaction, e.g. understanding natural language and the social settings in which various default expectations such as ordering food in a restaurant narrow the search space and allow the system to choose appropriate responses to dynamic situations.
","In addition to expert systems, other researchers developed the concept of frame-based languages in the mid-1980s. A frame is similar to an object class: It is an abstract description of a category describing things in the world, problems, and potential solutions.","[' When did other researchers develop the concept of frame-based languages?', ' What is a frame similar to?']","['mid-1980s', 'an object class']"
1546,knowledge representation,History,"It was not long before the frame communities and the rule-based researchers realized that there was a synergy between their approaches. Frames were good for representing the real world, described as classes, subclasses, slots (data values) with various constraints on possible values. Rules were good for representing and utilizing complex logic such as the process to make a medical diagnosis. Integrated systems were developed that combined frames and rules. One of the most powerful and well known was the 1983 Knowledge Engineering Environment (KEE) from Intellicorp. KEE had a complete rule engine with forward and backward chaining. It also had a complete frame-based knowledge base with triggers, slots (data values), inheritance, and message passing. Although message passing originated in the object-oriented community rather than AI it was quickly embraced by AI researchers as well in environments such as KEE and in the operating systems for Lisp machines from Symbolics, Xerox, and Texas Instruments.","It was not long before the frame communities and the rule-based researchers realized that there was a synergy between their approaches. Frames were good for representing the real world, described as classes, subclasses, slots (data values) with various constraints on possible values.","[' When did the frame communities and rule-based researchers realize that there was a synergy between their approaches?', ' What were frames good for representing?']","['It was not long before', 'the real world']"
1547,knowledge representation,History,"The integration of frames, rules, and object-oriented programming was significantly driven by commercial ventures such as KEE and Symbolics spun off from various research projects. At the same time as this was occurring, there was another strain of research that was less commercially focused and was driven by mathematical logic and automated theorem proving.  One of the most influential languages in this research was the KL-ONE language of the mid-'80s. KL-ONE was a frame language that had a rigorous semantics, formal definitions for concepts such as an Is-A relation. KL-ONE and languages that were influenced by it such as Loom had an automated reasoning engine that was based on formal logic rather than on IF-THEN rules. This reasoner is called the classifier. A classifier can analyze a set of declarations and infer new assertions, for example, redefine a class to be a subclass or superclass of some other class that wasn't formally specified. In this way the classifier can function as an inference engine, deducing new facts from an existing knowledge base. The classifier can also provide consistency checking on a knowledge base (which in the case of KL-ONE languages is also referred to as an Ontology).","The integration of frames, rules, and object-oriented programming was significantly driven by commercial ventures such as KEE and Symbolics spun off from various research projects. At the same time as this was occurring, there was another strain of research that was less commercially focused and was driven by mathematical logic and automated theorem proving.","[' What commercial ventures drove the integration of frames, rules, and object-oriented programming?', ' What was a strain of research that was less commercially focused and driven by mathematical logic?', ' What was less commercially focused?', ' What was driven by mathematical logic and automated theorem proving?']","['KEE and Symbolics', 'another strain of research that was less commercially focused and was driven by mathematical logic and automated theorem proving', 'research', 'research']"
1548,knowledge representation,History,"Another area of knowledge representation research was the problem of common sense reasoning. One of the first realizations learned from trying to make software that can function with human natural language was that humans regularly draw on an extensive foundation of knowledge about the real world that we simply take for granted but that is not at all obvious to an artificial agent. Basic principles of common sense physics, causality, intentions, etc. An example is the frame problem, that in an event driven logic there need to be axioms that state things maintain position from one moment to the next unless they are moved by some external force. In order to make a true artificial intelligence agent that can converse with humans using natural language and can process basic statements and questions about the world, it is essential to represent this kind of knowledge. One of the most ambitious programs to tackle this problem was Doug Lenat's Cyc project. Cyc established its own Frame language and had large numbers of analysts document various areas of common sense reasoning in that language. The knowledge recorded in Cyc included common sense models of time, causality, physics, intentions, and many others.",Another area of knowledge representation research was the problem of common sense reasoning. One of the first realizations learned from trying to make software that can function with human natural language was that humans regularly draw on an extensive foundation of knowledge about the real world that we simply take for granted but that is not at all obvious to an artificial agent.,"[' What was the problem of common sense reasoning?', ' What was one of the first realizations learned from trying to make software that can function with human natural language?', ' What do we take for granted?', ' What is not obvious to an artificial agent?']","['knowledge representation research', 'common sense reasoning', 'knowledge about the real world', 'an extensive foundation of knowledge about the real world']"
1549,knowledge representation,History,"Currently, one of the most active areas of knowledge representation research are projects associated with the Semantic Web. The Semantic Web seeks to add a layer of semantics (meaning) on top of the current Internet. Rather than indexing web sites and pages via keywords, the Semantic Web creates large ontologies of concepts. Searching for a concept will be more effective than traditional text only searches. Frame languages and automatic classification play a big part in the vision for the future Semantic Web. The automatic classification gives developers technology to provide order on a constantly evolving network of knowledge. Defining ontologies that are static and incapable of evolving on the fly would be very limiting for Internet-based systems. The classifier technology provides the ability to deal with the dynamic environment of the Internet.
","Currently, one of the most active areas of knowledge representation research are projects associated with the Semantic Web. The Semantic Web seeks to add a layer of semantics (meaning) on top of the current Internet.","[' What is one of the most active areas of knowledge representation research?', ' What seeks to add a layer of semantics on top of the current Internet?']","['projects associated with the Semantic Web', 'The Semantic Web']"
1550,knowledge representation,History,"Recent projects funded primarily by the Defense Advanced Research Projects Agency (DARPA) have integrated frame languages and classifiers with markup languages based on XML. The Resource Description Framework (RDF) provides the basic capability to define classes, subclasses, and properties of objects. The Web Ontology Language (OWL) provides additional levels of semantics and enables integration with classification engines.","Recent projects funded primarily by the Defense Advanced Research Projects Agency (DARPA) have integrated frame languages and classifiers with markup languages based on XML. The Resource Description Framework (RDF) provides the basic capability to define classes, subclasses, and properties of objects.","[' What does RDF stand for?', ' What is the DARPA?']","['Resource Description Framework', 'Defense Advanced Research Projects Agency']"
1551,knowledge representation,Overview,"The justification for knowledge representation is that conventional procedural code is not the best formalism to use to solve complex problems. Knowledge representation makes complex software easier to define and maintain than procedural code and can be used in expert systems.
",The justification for knowledge representation is that conventional procedural code is not the best formalism to use to solve complex problems. Knowledge representation makes complex software easier to define and maintain than procedural code and can be used in expert systems.,"[' What is not the best formalism to use to solve complex problems?', ' Knowledge representation makes complex software easier to define and maintain than what?']","['conventional procedural code', 'procedural code']"
1552,knowledge representation,Overview,"Knowledge representation goes hand in hand with automated reasoning because one of the main purposes of explicitly representing knowledge is to be able to reason about that knowledge, to make inferences, assert new knowledge, etc. Virtually all knowledge representation languages have a reasoning or inference engine as part of the system.","Knowledge representation goes hand in hand with automated reasoning because one of the main purposes of explicitly representing knowledge is to be able to reason about that knowledge, to make inferences, assert new knowledge, etc. Virtually all knowledge representation languages have a reasoning or inference engine as part of the system.","[' What does knowledge representation go hand in hand with?', ' What is one of the main purposes of explicitly representing knowledge?', ' Most knowledge representation languages have what?', ' What kind of engine is a part of a system?']","['automated reasoning', 'to be able to reason about that knowledge', 'a reasoning or inference engine', 'inference']"
1553,knowledge representation,Overview,"A key trade-off in the design of a knowledge representation formalism is that between expressivity and practicality. The ultimate knowledge representation formalism in terms of expressive power and compactness is First Order Logic (FOL). There is no more powerful formalism than that used by mathematicians to define general propositions about the world. However, FOL has two drawbacks as a knowledge representation formalism: ease of use and practicality of implementation. First order logic can be intimidating even for many software developers. Languages that do not have the complete formal power of FOL can still provide close to the same expressive power with a user interface that is more practical for the average developer to understand. The issue of practicality of implementation is that FOL in some ways is too expressive. With FOL it is possible to create statements (e.g. quantification over infinite sets) that would cause a system to never terminate if it attempted to verify them.
",A key trade-off in the design of a knowledge representation formalism is that between expressivity and practicality. The ultimate knowledge representation formalism in terms of expressive power and compactness is First Order Logic (FOL).,"[' What is the ultimate knowledge representation formalism in terms of expressive power and compactness?', ' What is First Order Logic?']","['First Order Logic', 'expressive power and compactness']"
1554,knowledge representation,Overview,"Thus, a subset of FOL can be both easier to use and more practical to implement. This was a driving motivation behind rule-based expert systems. IF-THEN rules provide a subset of FOL but a very useful one that is also very intuitive. The history of most of the early AI knowledge representation formalisms; from databases to semantic nets to theorem provers and production systems can be viewed as various design decisions on whether to emphasize expressive power or computability and efficiency.","Thus, a subset of FOL can be both easier to use and more practical to implement. This was a driving motivation behind rule-based expert systems.","[' What was the driving motivation behind rule-based expert systems?', ' What can be easier to use and more practical to implement?']","['FOL can be both easier to use and more practical to implement', 'a subset of FOL']"
1555,knowledge representation,Overview,"Knowledge representation and reasoning are a key enabling technology for the Semantic Web. Languages based on the Frame model with automatic classification provide a layer of semantics on top of the existing Internet. Rather than searching via text strings as is typical today, it will be possible to define logical queries and find pages that map to those queries. The automated reasoning component in these systems is an engine known as the classifier. Classifiers focus on the subsumption relations in a knowledge base rather than rules. A classifier can infer new classes and dynamically change the ontology as new information becomes available. This capability is ideal for the ever-changing and evolving information space of the Internet.",Knowledge representation and reasoning are a key enabling technology for the Semantic Web. Languages based on the Frame model with automatic classification provide a layer of semantics on top of the existing Internet.,"[' What is a key enabling technology for the Semantic Web?', ' Languages based on what model provide a layer of semantics on top of the existing Internet?']","['Knowledge representation and reasoning', 'Frame model']"
1556,knowledge representation,Overview,The Semantic Web integrates concepts from knowledge representation and reasoning with markup languages based on XML.  The Resource Description Framework (RDF) provides the basic capabilities to define knowledge-based objects on the Internet with basic features such as Is-A relations and object properties. The Web Ontology Language (OWL) adds additional semantics and integrates with automatic classification reasoners.,The Semantic Web integrates concepts from knowledge representation and reasoning with markup languages based on XML. The Resource Description Framework (RDF) provides the basic capabilities to define knowledge-based objects on the Internet with basic features such as Is-A relations and object properties.,"[' What integrates concepts from knowledge representation and reasoning with markup languages based on XML?', ' What provides the basic capabilities to define knowledge-based objects on the Internet?']","['The Semantic Web', 'The Resource Description Framework']"
1557,knowledge representation,Ontology engineering,"In the early years of knowledge-based systems the knowledge-bases were fairly small. The knowledge-bases that were meant to actually solve real problems rather than do proof of concept demonstrations needed to focus on well defined problems. So for example, not just medical diagnosis as a whole topic, but medical diagnosis of certain kinds of diseases.
",In the early years of knowledge-based systems the knowledge-bases were fairly small. The knowledge-bases that were meant to actually solve real problems rather than do proof of concept demonstrations needed to focus on well defined problems.,"[' In the early years of knowledge-based systems, what were the knowledge-bases?', ' What were knowledge-basics meant to do?']","['fairly small', 'actually solve real problems']"
1558,knowledge representation,Ontology engineering,"As knowledge-based technology scaled up, the need for larger knowledge bases and for modular knowledge bases that could communicate and integrate with each other became apparent. This gave rise to the discipline of ontology engineering, designing and building large knowledge bases that could be used by multiple projects. One of the leading research projects in this area was the Cyc project. Cyc was an attempt to build a huge encyclopedic knowledge base that would contain not just expert knowledge but common sense knowledge. In designing an artificial intelligence agent, it was soon realized that representing common sense knowledge, knowledge that humans simply take for granted, was essential to make an AI that could interact with humans using natural language. Cyc was meant to address this problem. The language they defined was known as CycL.
","As knowledge-based technology scaled up, the need for larger knowledge bases and for modular knowledge bases that could communicate and integrate with each other became apparent. This gave rise to the discipline of ontology engineering, designing and building large knowledge bases that could be used by multiple projects.","[' As knowledge-based technology scaled up, the need for larger knowledge bases and modular knowledge bases became apparent what?', ' Ontology engineering was a discipline that designed and built large knowledge bases that could be used by multiple projects?']","['communicate and integrate with each other', 'knowledge-based technology scaled up, the need for larger knowledge bases']"
1559,knowledge representation,Ontology engineering,"After CycL, a number of ontology languages have been developed. Most are declarative languages, and are either frame languages, or are based on first-order logic. Modularity—the ability to define boundaries around specific domains and problem spaces—is essential for these languages because as stated by Tom Gruber, ""Every ontology is a treaty- a social agreement among people with common motive in sharing."" There are always many competing and differing views that make any general purpose ontology impossible. A general purpose ontology would have to be applicable in any domain and different areas of knowledge need to be unified.","After CycL, a number of ontology languages have been developed. Most are declarative languages, and are either frame languages, or are based on first-order logic.","[' After CycL, what have a number of ontology languages been developed?', ' What are the most declarative languages?']","['declarative languages', 'ontology languages']"
1560,knowledge representation,Ontology engineering,"There is a long history of work attempting to build ontologies for a variety of task domains, e.g., an ontology for liquids, the lumped element model widely used in representing electronic circuits (e.g.,), as well as ontologies for time, belief, and even programming itself. Each of these offers a way to see some part of the world.
","There is a long history of work attempting to build ontologies for a variety of task domains, e.g., an ontology for liquids, the lumped element model widely used in representing electronic circuits (e.g.,), as well as ontologies for time, belief, and even programming itself. Each of these offers a way to see some part of the world.","[' What is an ontology for liquids?', ' What is a lumped element model widely used in representing?', ' Ontologies for time, belief, and even programming itself offer what?', ' What offers a way to see some part of the world?']","['the lumped element model', 'electronic circuits', 'a way to see some part of the world', 'ontologies']"
1561,knowledge representation,Ontology engineering,"The lumped element model, for instance, suggests that we think of circuits in terms of components with connections between them, with signals flowing instantaneously along the connections. This is a useful view, but not the only possible one. A different ontology arises if we need to attend to the electrodynamics in the device: Here signals propagate at finite speed and an object (like a resistor) that was previously viewed as a single component with an I/O behavior may now have to be thought of as an extended medium through which an electromagnetic wave flows.
","The lumped element model, for instance, suggests that we think of circuits in terms of components with connections between them, with signals flowing instantaneously along the connections. This is a useful view, but not the only possible one.","[' What model suggests that we think of circuits in terms of components with connections between them?', ' What does the lumped element model suggest?']","['lumped element model', 'we think of circuits in terms of components with connections between them']"
1562,knowledge representation,Ontology engineering,"Ontologies can of course be written down in a wide variety of languages and notations (e.g., logic, LISP, etc.); the essential information is not the form of that language but the content, i.e., the set of concepts offered as a way of thinking about the world. Simply put, the important part is notions like connections and components, not the choice between writing them as predicates or LISP constructs.
","Ontologies can of course be written down in a wide variety of languages and notations (e.g., logic, LISP, etc. ); the essential information is not the form of that language but the content, i.e., the set of concepts offered as a way of thinking about the world.","[' What can be written down in a wide variety of languages and notations?', ' The essential information is not the form of what language but the content?']","['Ontologies', 'Ontologies']"
1563,knowledge representation,Ontology engineering,"The commitment made selecting one or another ontology can produce a sharply different view of the task at hand. Consider the difference that arises in selecting the lumped element view of a circuit rather than the electrodynamic view of the same device. As a second example, medical diagnosis viewed in terms of rules (e.g., MYCIN) looks substantially different from the same task viewed in terms of frames (e.g., INTERNIST). Where MYCIN sees the medical world as made up of empirical associations connecting symptom to disease, INTERNIST sees a set of prototypes, in particular prototypical diseases, to be matched against the case at hand.
",The commitment made selecting one or another ontology can produce a sharply different view of the task at hand. Consider the difference that arises in selecting the lumped element view of a circuit rather than the electrodynamic view of the same device.,"[' What can produce a sharply different view of the task at hand?', ' What can arise when selecting the lumped element view of a circuit rather than the electrodynamic view?']","['The commitment made selecting one or another ontology', 'difference']"
1564,planar graph,Summary,"In graph theory, a planar graph is a graph that can be embedded in the plane, i.e., it can be drawn on the plane in such a way that its edges intersect only at their endpoints.  In other words, it can be drawn in such a way that no edges cross each other.   Such a drawing is called a plane graph or planar embedding of the graph. A plane graph can be defined as a planar graph with a mapping from every node to a point on a plane, and from every edge to a plane curve on that plane, such that the extreme points of each curve are the points mapped from its end nodes, and all curves are disjoint except on their extreme points.
","In graph theory, a planar graph is a graph that can be embedded in the plane, i.e., it can be drawn on the plane in such a way that its edges intersect only at their endpoints. In other words, it can be drawn in such a way that no edges cross each other.","[' In graph theory, a planar graph is a graph that can be embedded in what?', ' What can a plane graph be drawn in such a way that its edges intersect only at?', ' What can be drawn in such a way that no edges cross each other?']","['the plane', 'endpoints', 'a planar graph']"
1565,planar graph,Summary,"An equivalence class of topologically equivalent drawings on the sphere, usually with additional assumptions such as the absence of isthmuses, is called a planar map. Although a plane graph has an external or unbounded face, none of the faces of a planar map has a particular status.
","An equivalence class of topologically equivalent drawings on the sphere, usually with additional assumptions such as the absence of isthmuses, is called a planar map. Although a plane graph has an external or unbounded face, none of the faces of a planar map has a particular status.","[' What is an equivalence class of topologically equivalent drawings on the sphere called?', ' What is a planar map?', ' A plane graph has an external or unbounded face but none of the faces of a what?']","['a planar map', 'An equivalence class of topologically equivalent drawings on the sphere', 'planar map']"
1566,planar graph,Summary,"Planar graphs generalize to graphs drawable on a surface of a given genus. In this terminology, planar graphs have genus 0, since the plane (and the sphere) are surfaces of genus 0. See ""graph embedding"" for other related topics.
","Planar graphs generalize to graphs drawable on a surface of a given genus. In this terminology, planar graphs have genus 0, since the plane (and the sphere) are surfaces of genus 0.","[' What do planar graphs generalize to?', ' What are graphs drawable on a surface of a given genus?']","['graphs drawable on a surface of a given genus', 'Planar graphs']"
1567,planar graph,Kuratowski's and Wagner's theorems,"Klaus Wagner asked more generally whether any minor-closed class of graphs is determined by a finite set of ""forbidden minors"". This is now the Robertson–Seymour theorem, proved in a long series of papers. In the language of this theorem, 




K

5




{\displaystyle K_{5}}
 and 




K

3
,
3




{\displaystyle K_{3,3}}
 are the forbidden minors for the class of finite planar graphs.
","Klaus Wagner asked more generally whether any minor-closed class of graphs is determined by a finite set of ""forbidden minors"". This is now the Robertson–Seymour theorem, proved in a long series of papers.","[' Who asked whether any minor-closed class of graphs is determined by a finite set of ""forbidden minors""?', ' What is the Robertson-Seymour theorem?']","['Klaus Wagner', 'proved in a long series of papers']"
1568,planar graph,Other planarity criteria,"In practice, it is difficult to use Kuratowski's criterion to quickly decide whether a given graph is planar. However, there exist fast algorithms for this problem: for a graph with n vertices, it is possible to determine in time O(n) (linear time) whether the graph may be planar or not (see planarity testing).
","In practice, it is difficult to use Kuratowski's criterion to quickly decide whether a given graph is planar. However, there exist fast algorithms for this problem: for a graph with n vertices, it is possible to determine in time O(n) (linear time) whether the graph may be planar or not (see planarity testing).","[' What criterion is difficult to use in practice to quickly decide whether a given graph is planar?', ' There are fast algorithms for what problem?', ' For a graph with n vertices, it is possible to determine in time O(n) whether the graph may be planar or not?', ' (linear time) whether the graph may be planar or not (see planarity testing)']","[""Kuratowski's criterion"", 'planar', 'fast algorithms', 'O(n)']"
1569,planar graph,Other planarity criteria,"In this sense, planar graphs are sparse graphs, in that they have only O(v) edges, asymptotically smaller than the maximum O(v2). The graph K3,3, for example, has 6 vertices, 9 edges, and no cycles of length 3.  Therefore, by Theorem 2, it cannot be planar. These theorems provide necessary conditions for planarity that are not sufficient conditions, and therefore can only be used to prove a graph is not planar, not that it is planar. If both theorem 1 and 2 fail, other methods may be used.
","In this sense, planar graphs are sparse graphs, in that they have only O(v) edges, asymptotically smaller than the maximum O(v2). The graph K3,3, for example, has 6 vertices, 9 edges, and no cycles of length 3.","[' What are sparse graphs?', ' What are planar graphs asymptotically smaller than?', ' How many vertices does K3,3 have?']","['planar graphs', 'the maximum O(v2).', '6']"
1570,planar graph,Enumeration of planar graphs,"The asymptotic for the number of (labeled) planar graphs on 



n


{\displaystyle n}
 vertices is 



g
⋅

n

−
7

/

2


⋅

γ

n


⋅
n
!


{\displaystyle g\cdot n^{-7/2}\cdot \gamma ^{n}\cdot n!}
, where 



γ
≈
27.22687


{\displaystyle \gamma \approx 27.22687}
 and 



g
≈
0.43
×

10

−
5




{\displaystyle g\approx 0.43\times 10^{-5}}
.","The asymptotic for the number of (labeled) planar graphs on 



n


{\displaystyle n}
 vertices is 



g
⋅

n

−
7

/

2


⋅

γ

n


⋅
n
! {\displaystyle g\cdot n^{-7/2}\cdot \gamma ^{n}\cdot n!}",[' What is the asymptotic for the number of (labeled) planar graphs on n <unk>displaystyle n<unk> vertices?'],['n\n\n−\n7\n\n/\n\n2\n\n\n⋅\n\nγ\n\nn\n\n\n⋅\nn\n!']
1571,planar graph,Other facts and definitions,"Fáry's theorem states that every simple planar graph admits an embedding in the plane such that all edges are straight line segments which don't intersect. A universal point set is a set of points such that every planar graph with n vertices has such an embedding with all vertices in the point set; there exist universal point sets of quadratic size, formed by taking a rectangular subset of the integer lattice. Every simple outerplanar graph admits an embedding in the plane such that all vertices lie on a fixed circle and all edges are straight line segments that lie inside the disk and don't intersect, so n-vertex regular polygons are universal for outerplanar graphs.
","Fáry's theorem states that every simple planar graph admits an embedding in the plane such that all edges are straight line segments which don't intersect. A universal point set is a set of points such that every planar graph with n vertices has such an embedding with all vertices in the point set; there exist universal point sets of quadratic size, formed by taking a rectangular subset of the integer lattice.","["" What states that every simple planar graph admits an embedding in the plane such that all edges are straight line segments which don't intersect?"", ' A universal point set is a set of points such that every planar Graph with n vertices has such an embedded with what?', "" What does Fáry's theorem state?"", ' What is a universal point set of quadratic size formed by taking a rectangular subset of?']","[""Fáry's theorem"", 'all vertices in the point set', 'every simple planar graph admits an embedding in the plane', 'the integer lattice']"
1572,planar graph,Other facts and definitions,"Given an embedding G of a (not necessarily simple) connected graph in the plane without edge intersections, we construct the dual graph G* as follows: we choose one vertex in each face of G (including the outer face) and for each edge e in G we introduce a new edge in G* connecting the two vertices in G* corresponding to the two faces in G that meet at e. Furthermore, this edge is drawn so that it crosses e exactly once and that no other edge of G or G* is intersected. Then G* is again the embedding of a (not necessarily simple) planar graph; it has as many edges as G, as many vertices as G has faces and as many faces as G has vertices. The term ""dual"" is justified by the fact that G** = G; here the equality is the equivalence of embeddings on the sphere. If G is the planar graph corresponding to a convex polyhedron, then G* is the planar graph corresponding to the dual polyhedron.
","Given an embedding G of a (not necessarily simple) connected graph in the plane without edge intersections, we construct the dual graph G* as follows: we choose one vertex in each face of G (including the outer face) and for each edge e in G we introduce a new edge in G* connecting the two vertices in G* corresponding to the two faces in G that meet at e. Furthermore, this edge is drawn so that it crosses e exactly once and that no other edge of G or G* is intersected. Then G* is again the embedding of a (not necessarily simple) planar graph; it has as many edges as G, as many vertices as G has faces and as many faces as G has vertices.","[' What do we construct in G*?', ' How many vertex in each face of G?', ' For each edge e in G, we introduce what?', ' Each edge e in G introduces a new edge in G* connecting the two vertices in what?', ' What is drawn so that it crosses e exactly once?', ' What is the embedding of a (not necessarily simple) planar graph?', ' How many edges does G* have?']","['dual graph', 'one', 'a new edge in G*', 'G*', 'edge', 'G*', 'as many edges as G']"
1573,planar graph,Other facts and definitions,"While the dual constructed for a particular embedding is unique (up to isomorphism), graphs may have different (i.e. non-isomorphic) duals, obtained from different (i.e. non-homeomorphic) embeddings.
","While the dual constructed for a particular embedding is unique (up to isomorphism), graphs may have different (i.e. non-isomorphic) duals, obtained from different (i.e.","[' What is unique about the dual constructed for a particular embedding?', ' What type of graphs may have different duals?']","['up to isomorphism', 'non-isomorphic']"
1574,planar graph,Other facts and definitions,"A plane graph is said to be convex if all of its faces (including the outer face) are convex polygons. Not all planar graphs have a convex embedding (e.g. the complete bipartite graph 




K

2
,
4




{\displaystyle K_{2,4}}
). A sufficient condition that a graph can be drawn convexly is that it is a subdivision of a 3-vertex-connected planar graph. Tutte's spring theorem even states that for simple 3-vertex-connected planar graphs the position of the inner vertices can be chosen to be the average of its neighbors.
",A plane graph is said to be convex if all of its faces (including the outer face) are convex polygons. Not all planar graphs have a convex embedding (e.g.,"[' What is a plane graph said to be if all of its faces are convex polygons?', ' Not all planar graphs have what embedding?']","['convex', 'convex']"
1575,planar graph,Other facts and definitions,"The planar separator theorem states that every n-vertex planar graph can be partitioned into two subgraphs of size at most 2n/3 by the removal of O(√n) vertices. As a consequence, planar graphs also have treewidth and branch-width O(√n).
","The planar separator theorem states that every n-vertex planar graph can be partitioned into two subgraphs of size at most 2n/3 by the removal of O(√n) vertices. As a consequence, planar graphs also have treewidth and branch-width O(√n).","[' The planar separator theorem states that every n-vertex planar graph can be partitioned into how many subgraphs of size at most 2n/3?', ' The removal of what vertices causes the partitioning of planar Graphs into two?', ' Planar graphs also have treewidth and what else?']","['two', 'O(√n)', 'branch-width']"
1576,planar graph,Other facts and definitions,"The planar product structure theorem states that every planar graph is a subgraph of the strong graph product of a graph of treewidth at most 8 and a path.
This result has been used to show that planar graphs have bounded queue number, bounded non-repetitive chromatic number, and universal graphs of near-linear size.  It also has applications to vertex ranking
and p-centered colouring
of planar graphs.
","The planar product structure theorem states that every planar graph is a subgraph of the strong graph product of a graph of treewidth at most 8 and a path. This result has been used to show that planar graphs have bounded queue number, bounded non-repetitive chromatic number, and universal graphs of near-linear size.","[' What states that every planar graph is a subgraph of the strong graph product of a graph of treewidth at most 8 and a path?', ' What has the result of the planar product structure theorem been used to show?', ' What is bounded queue number?', ' What are universal graphs of near linear size?']","['The planar product structure theorem', 'planar graphs have bounded queue number, bounded non-repetitive chromatic number, and universal graphs of near-linear size', 'planar graphs have bounded queue number, bounded non-repetitive chromatic number', 'bounded queue number, bounded non-repetitive chromatic number']"
1577,planar graph,Other facts and definitions,"The meshedness coefficient of a planar graph normalizes its number of bounded faces (the same as the circuit rank of the graph, by Mac Lane's planarity criterion) by dividing it by 2n − 5, the maximum possible number of bounded faces in a planar graph with n vertices. Thus, it ranges from 0 for trees to 1 for maximal planar graphs.","The meshedness coefficient of a planar graph normalizes its number of bounded faces (the same as the circuit rank of the graph, by Mac Lane's planarity criterion) by dividing it by 2n − 5, the maximum possible number of bounded faces in a planar graph with n vertices. Thus, it ranges from 0 for trees to 1 for maximal planar graphs.","["" What normalizes a planar graph's number of bounded faces?"", ' What is the meshedness coefficient of a plane graph the same as?', "" How does Mac Lane's planarity criterion determine a normalize?"", ' A planar graph with n vertices ranges from 0 for trees to what?']","['The meshedness coefficient', 'the circuit rank', 'by dividing it by 2n\xa0−\xa05', '1']"
1578,dynamic programming,Summary,"Dynamic programming is both a mathematical optimization method and a computer programming method. The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering to economics.
","Dynamic programming is both a mathematical optimization method and a computer programming method. The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering to economics.","[' What is dynamic programming?', ' Who developed dynamic programming in the 1950s?', ' In what fields has dynamic programming found applications?']","['mathematical optimization method and a computer programming method', 'Richard Bellman', 'aerospace engineering to economics']"
1579,dynamic programming,Summary,"In both contexts it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner. While some decision problems cannot be taken apart this way, decisions that span several points in time do often break apart recursively. Likewise, in computer science, if a problem can be solved optimally by breaking it into sub-problems and then recursively finding the optimal solutions to the sub-problems, then it is said to have optimal substructure.
","In both contexts it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner. While some decision problems cannot be taken apart this way, decisions that span several points in time do often break apart recursively.","[' What does it mean to simplify a complicated problem by breaking it down into simpler sub-problems in a recursive manner?', ' Decisions that span several points in time do often break apart recurrencely?']","['In both contexts', 'decision problems']"
1580,dynamic programming,Summary,"If sub-problems can be nested recursively inside larger problems, so that dynamic programming methods are applicable, then there is a relation between the value of the larger problem and the values of the sub-problems. In the optimization literature this relationship is called the Bellman equation.
","If sub-problems can be nested recursively inside larger problems, so that dynamic programming methods are applicable, then there is a relation between the value of the larger problem and the values of the sub-problems. In the optimization literature this relationship is called the Bellman equation.","[' What is the relationship between the value of the larger problem and the values of the sub-problems called in the optimization literature?', ' What can be nested recursively inside larger problems?']","['Bellman equation', 'sub-problems']"
1581,dynamic programming,History,"The term dynamic programming was originally used in the 1940s by Richard Bellman to describe the process of solving problems where one needs to find the best decisions one after another.  By 1953, he refined this to the modern meaning, referring specifically to nesting smaller decision problems inside larger decisions,  and the field was thereafter recognized by the IEEE as a systems analysis and engineering topic.  Bellman's contribution is remembered in the name of the Bellman equation, a central result of dynamic programming which restates an optimization problem in recursive form.
","The term dynamic programming was originally used in the 1940s by Richard Bellman to describe the process of solving problems where one needs to find the best decisions one after another. By 1953, he refined this to the modern meaning, referring specifically to nesting smaller decision problems inside larger decisions,  and the field was thereafter recognized by the IEEE as a systems analysis and engineering topic.","[' When was the term dynamic programming first used?', ' Who first used dynamic programming?', ' What did Richard Bellman use the term to describe?', ' When did Bellman refine dynamic programming to the modern meaning?', ' What was the field of nesting smaller decision problems into larger decisions recognized by?', ' What organization recognized the field as a systems analysis and engineering topic?']","['1940s', 'Richard Bellman', 'dynamic programming', '1953', 'the IEEE', 'the IEEE']"
1582,dynamic programming,History,"I spent the Fall quarter (of 1950) at RAND. My first task was to find a name for multistage decision processes. An interesting question is, ""Where did the name, dynamic programming, come from?"" The 1950s were not good years for mathematical research. We had a very interesting gentleman in Washington named Wilson. He was Secretary of Defense, and he actually had a pathological fear and hatred of the word ""research"". I’m not using the term lightly; I’m using it precisely. His face would suffuse, he would turn red, and he would get violent if people used the term research in his presence. You can imagine how he felt, then, about the term mathematical. The RAND Corporation was employed by the Air Force, and the Air Force had Wilson as its boss, essentially. Hence, I felt I had to do something to shield Wilson and the Air Force from the fact that I was really doing mathematics inside the RAND Corporation. What title, what name, could I choose? In the first place I was interested in planning, in decision making, in thinking. But planning, is not a good word for various reasons. I decided therefore to use the word ""programming"". I wanted to get across the idea that this was dynamic, this was multistage, this was time-varying. I thought, let's kill two birds with one stone. Let's take a word that has an absolutely precise meaning, namely dynamic, in the classical physical sense. It also has a very interesting property as an adjective, and that is it's impossible to use the word dynamic in a pejorative sense. Try thinking of some combination that will possibly give it a pejorative meaning. It's impossible. Thus, I thought dynamic programming was a good name. It was something not even a Congressman could object to. So I used it as an umbrella for my activities.",I spent the Fall quarter (of 1950) at RAND. My first task was to find a name for multistage decision processes.,"[' When did I spend the Fall quarter at RAND?', ' What was my first task?']","['1950', 'to find a name for multistage decision processes']"
1583,dynamic programming,History,"The word dynamic was chosen by Bellman to capture the time-varying aspect of the problems, and because it sounded impressive. The word programming referred to the use of the method to find an optimal program, in the sense of a military schedule for training or logistics.  This usage is the same as that in the phrases linear programming and mathematical programming, a synonym for mathematical optimization.","The word dynamic was chosen by Bellman to capture the time-varying aspect of the problems, and because it sounded impressive. The word programming referred to the use of the method to find an optimal program, in the sense of a military schedule for training or logistics.","[' What word was chosen by Bellman to capture the time-varying aspect of the problems?', ' What word referred to the use of the method to find an optimal program?']","['dynamic', 'programming']"
1584,dynamic programming,History,"The above explanation of the origin of the term is lacking. As Russell and Norvig in their book have written, referring to the above story: ""This cannot be strictly true, because his first paper using the term (Bellman, 1952) appeared before Wilson became Secretary of Defense in 1953."" Also, there is a comment in a speech by Harold J. Kushner, where he remembers Bellman. Quoting Kushner as he speaks of Bellman: ""On the other hand, when I asked him the same question, he replied that he was trying to upstage Dantzig's linear programming by adding dynamic. Perhaps both motivations were true.""
","The above explanation of the origin of the term is lacking. As Russell and Norvig in their book have written, referring to the above story: ""This cannot be strictly true, because his first paper using the term (Bellman, 1952) appeared before Wilson became Secretary of Defense in 1953.""","[' What is lacking in the above explanation of the origin of the term?', "" What is the name of Wilson's first paper using the term Bellman?"", ' When did Wilson become Secretary of Defense?']","['This cannot be strictly true', '1952', '1953']"
1585,learning,Summary,"Learning is the process of acquiring new understanding, knowledge, behaviors, skills, values, attitudes, and preferences. The ability to learn is possessed by humans, animals, and some machines; there is also evidence for some kind of learning in certain plants. Some learning is immediate, induced by a single event (e.g. being burned by a hot stove), but much skill and knowledge accumulate from repeated experiences. The changes induced by learning often last a lifetime, and it is hard to distinguish learned material that seems to be ""lost"" from that which cannot be retrieved.","Learning is the process of acquiring new understanding, knowledge, behaviors, skills, values, attitudes, and preferences. The ability to learn is possessed by humans, animals, and some machines; there is also evidence for some kind of learning in certain plants.","[' What is the process of acquiring new understanding, knowledge, behaviors, skills, values, attitudes, and preferences?', ' What is possessed by humans, animals, and some machines?', ' There is evidence for learning in what?']","['Learning', 'The ability to learn', 'plants']"
1586,learning,Summary,"Human learning starts at birth (it might even start before in terms of an embryo's need for both interaction with, and freedom within its environment within the womb.) and continues until death as a consequence of ongoing interactions between people and their environment. The nature and processes involved in learning are studied in many established fields (including educational psychology, neuropsychology, experimental psychology, cognitive sciences, and pedagogy), as well as emerging fields of knowledge (e.g. with a shared interest in the topic of learning from safety events such as incidents/accidents, or in collaborative learning health systems). Research in such fields has led to the identification of various sorts of learning. For example, learning may occur as a result of habituation, or classical conditioning, operant conditioning or as a result of more complex activities such as play, seen only in relatively intelligent animals. Learning may occur consciously or without conscious awareness. Learning that an aversive event can't be avoided or escaped may result in a condition called learned helplessness. There is evidence for human behavioral learning prenatally, in which habituation has been observed as early as 32 weeks into gestation, indicating that the central nervous system is sufficiently developed and primed for learning and memory to occur very early on in development.","Human learning starts at birth (it might even start before in terms of an embryo's need for both interaction with, and freedom within its environment within the womb.) and continues until death as a consequence of ongoing interactions between people and their environment.","[' Human learning starts at what age?', ' Human learning continues until what age as a result of interactions between people and their environment?']","['birth', 'death']"
1587,learning,Summary,"Play has been approached by several theorists as a form of learning. Children experiment with the world, learn the rules, and learn to interact through play. Lev Vygotsky agrees that play is pivotal for children's development, since they make meaning of their environment through playing educational games. For Vygotsky, however, play is the first form of learning language and communication, and the stage where a child begins to understand rules and symbols. This has led to a view that learning in organisms is always related to semiosis, and often associated with representational systems/activity.
","Play has been approached by several theorists as a form of learning. Children experiment with the world, learn the rules, and learn to interact through play.","[' What has been approached by several theorists as a form of learning?', ' Children experiment with the world, learn the rules, and learn to interact through play?']","['Play', 'Children']"
1588,learning,Domains,"These domains are not mutually exclusive. For example, in learning to play chess, the person must learn the rules (cognitive domain)—but must also learn how to set up the chess pieces and how to properly hold and move a chess piece (psychomotor). Furthermore, later in the game the person may even learn to love the game itself, value its applications in life, and appreciate its history (affective domain).","These domains are not mutually exclusive. For example, in learning to play chess, the person must learn the rules (cognitive domain)—but must also learn how to set up the chess pieces and how to properly hold and move a chess piece (psychomotor).","[' What are not mutually exclusive?', ' What must a person learn in learning to play chess?', ' The person must also learn how to properly hold and move what?']","['domains', 'the rules', 'a chess piece']"
1589,learning,Transfer,"Transfer of learning is the application of skill, knowledge or understanding to resolve a novel problem or situation that happens when certain conditions are fulfilled. Research indicates that learning transfer is infrequent; most common when ""... cued, primed, and guided..."" and has sought to clarify what it is, and how it might be promoted through instruction.
","Transfer of learning is the application of skill, knowledge or understanding to resolve a novel problem or situation that happens when certain conditions are fulfilled. Research indicates that learning transfer is infrequent; most common when ""... cued, primed, and guided..."" and has sought to clarify what it is, and how it might be promoted through instruction.","[' What is the term for the application of skill, knowledge or understanding to resolve a novel problem or situation?', ' Research indicates that learning transfer is infrequent; most common when what is cued, primed, and guided?', ' What has sought to clarify?', ' How might it be promoted?']","['Transfer of learning', '""...', 'learning transfer', 'through instruction']"
1590,learning,Transfer,"Over the history of its discourse, various hypotheses and definitions have been advanced. First, it is speculated that different types of transfer exist, including: near transfer, the application of skill to solve a novel problem in a similar context; and far transfer, the application of skill to solve a novel problem presented in a different context. Furthermore, Perkins and Salomon (1992) suggest that positive transfer in cases when learning supports novel problem solving, and negative transfer occurs when prior learning inhibits performance on highly correlated tasks, such as second or third-language learning. Concepts of positive and negative transfer have a long history; researchers in the early 20th century described the possibility that ""...habits or mental acts developed by a particular kind of training may inhibit rather than facilitate other mental activities"". Finally, Schwarz, Bransford and Sears (2005) have proposed that transferring knowledge into a situation may differ from transferring knowledge out to a situation as a means to reconcile findings that transfer may both be frequent and challenging to promote.","Over the history of its discourse, various hypotheses and definitions have been advanced. First, it is speculated that different types of transfer exist, including: near transfer, the application of skill to solve a novel problem in a similar context; and far transfer, the application of skill to solve a novel problem presented in a different context.","[' What is the term for the application of skill to solve a novel problem in a similar context?', ' What is far transfer?', ' Far transfer, the application of skill to solve a novel problem presented in a different context?']","['near transfer', 'the application of skill to solve a novel problem presented in a different context', 'far transfer']"
1591,learning,Transfer,"A significant and long research history has also attempted to explicate the conditions under which transfer of learning might occur. Early research by Ruger, for example, found that the ""level of attention"", ""attitudes"", ""method of attack"" (or method for tackling a problem), a ""search for new points of view"", a ""careful testing of hypothesis"" and ""generalization"" were all valuable approaches for promoting transfer. To encourage transfer through teaching, Perkins and Salomon recommend aligning (""hugging"") instruction with practice and assessment, and ""bridging"", or encouraging learners to reflect on past experiences or make connections between prior knowledge and current content.","A significant and long research history has also attempted to explicate the conditions under which transfer of learning might occur. Early research by Ruger, for example, found that the ""level of attention"", ""attitudes"", ""method of attack"" (or method for tackling a problem), a ""search for new points of view"", a ""careful testing of hypothesis"" and ""generalization"" were all valuable approaches for promoting transfer.","[' What has a significant and long research history attempted to explicate the conditions under which transfer of learning might occur?', ' Early research by Ruger found that the ""level of attention"", ""attitudes"", ""method of attack"" and ""search for new points of view"" were what?', ' What were all valuable approaches for promoting transfer?']","['Ruger', 'valuable approaches for promoting transfer', 'the ""level of attention"", ""attitudes"", ""method of attack"" (or method for tackling a problem), a ""search for new points of view"", a ""careful testing of hypothesis"" and ""generalization""']"
1592,learning,In animal evolution,"Animals gain knowledge in two ways. First is learning—in which an animal gathers information about its environment and uses this information. For example, if an animal eats something that hurts its stomach, it learns not to eat that again. The second is innate knowledge that is genetically inherited. An example of this is when a horse is born and can immediately walk. The horse has not learned this behavior; it simply knows how to do it. In some scenarios, innate knowledge is more beneficial than learned knowledge. However, in other scenarios the opposite is true—animals must learn certain behaviors when it is disadvantageous to have a specific innate behavior. In these situations, learning evolves in the species.
",Animals gain knowledge in two ways. First is learning—in which an animal gathers information about its environment and uses this information.,"[' How many ways do animals gain knowledge?', ' What is the first way an animal learns about its environment?']","['two', 'learning']"
1593,learning,In plants,"In recent years, plant physiologists have examined the physiology of plant behavior and cognition. The concepts of learning and memory are relevant in identifying how plants respond to external cues, a behavior necessary for survival. Monica Gagliano, an Australian professor of evolutionary ecology, makes an argument for associative learning in the garden pea, Pisum sativum. The garden pea is not specific to a region, but rather grows in cooler, higher altitude climates. Gagliano and colleagues' 2016 paper aims to differentiate between innate phototropism behavior and learned behaviors. Plants use light cues in various ways, such as to sustain their metabolic needs and to maintain their internal circadian rhythms. Circadian rhythms in plants are modulated by endogenous bioactive substances that encourage leaf-opening and leaf-closing and are the basis of nyctinastic behaviors.","In recent years, plant physiologists have examined the physiology of plant behavior and cognition. The concepts of learning and memory are relevant in identifying how plants respond to external cues, a behavior necessary for survival.","[' What have plant physiologists examined in recent years?', ' What are the concepts of learning and memory relevant in identifying how plants respond to external cues?']","['the physiology of plant behavior and cognition', 'behavior necessary for survival']"
1594,learning,In plants,"Gagliano and colleagues constructed a classical conditioning test in which pea seedlings were divided into two experimental categories and placed in Y-shaped tubes. In a series of training sessions, the plants were exposed to light coming down different arms of the tube. In each case, there was a fan blowing lightly down the tube in either the same or opposite arm as the light. The unconditioned stimulus (US) was the predicted occurrence of light and the conditioned stimulus (CS) was the wind blowing by the fan. Previous experimentation shows that plants respond to light by bending and growing towards it through differential cell growth and division on one side of the plant stem mediated by auxin signaling pathways.","Gagliano and colleagues constructed a classical conditioning test in which pea seedlings were divided into two experimental categories and placed in Y-shaped tubes. In a series of training sessions, the plants were exposed to light coming down different arms of the tube.","[' Who constructed a classical conditioning test?', ' How were pea seedlings divided into two experimental categories?', ' What were the plants exposed to in training sessions?']","['Gagliano and colleagues', 'placed in Y-shaped tubes', 'light']"
1595,learning,In plants,"During the testing phase of Gagliano's experiment, the pea seedlings were placed in different Y-pipes and exposed to the fan alone. Their direction of growth was subsequently recorded. The 'correct' response by the seedlings was deemed to be growing into the arm where the light was ""predicted"" from the previous day.  The majority of plants in both experimental conditions grew in a direction consistent with the predicted location of light based on the position of the fan the previous day. For example, if the seedling was trained with the fan and light coming down the same arm of the Y-pipe, the following day the seedling grew towards the fan in the absence of light cues despite the fan being placed in the opposite side of the Y-arm. Plants in the control group showed no preference to a particular arm of the Y-pipe. The percentage difference in population behavior observed between the control and experimental groups is meant to distinguish innate phototropism behavior from active associative learning.","During the testing phase of Gagliano's experiment, the pea seedlings were placed in different Y-pipes and exposed to the fan alone. Their direction of growth was subsequently recorded.","["" Where were the pea seedlings placed during the testing phase of Gagliano's experiment?"", ' What was the direction of growth recorded?']","['different Y-pipes', 'pea seedlings']"
1596,learning,In plants,"While the physiological mechanism of associative learning in plants is not known, Telewski et al. describes a hypothesis that describes photoreception as the basis of mechano-perception in plants. One mechanism for mechano-perception in plants relies on MS ion channels and calcium channels. Mechanosensory proteins in cell lipid bilayers, known as MS ion channels, are activated once they are physically deformed in response to pressure or tension. Ca2+ permeable ion channels are ""stretch-gated"" and allow for the influx of osmolytes and calcium, a well-known second messenger, into the cell. This ion influx triggers a passive flow of water into the cell down its osmotic gradient, effectively increasing turgor pressure and causing the cell to depolarize. Gagliano hypothesizes that the basis of associative learning in Pisum sativum is the coupling of mechanosensory and photosensory pathways and is mediated by auxin signaling pathways. The result is directional growth to maximize a plant's capture of sunlight.","While the physiological mechanism of associative learning in plants is not known, Telewski et al. describes a hypothesis that describes photoreception as the basis of mechano-perception in plants.","[' What is the physiological mechanism of associative learning in plants not known?', ' What does Telewski et al. describe as the basis of mechano-perception in plants?']","['photoreception', 'photoreception']"
1597,learning,In plants,"Gagliano et al. published another paper on habituation behaviors in the mimosa pudica plant whereby the innate behavior of the plant was diminished by repeated exposure to a stimulus. There has been controversy around this paper and more generally around the topic of plant cognition. Charles Abrahmson, a psychologist and behavioral biologist, says that part of the issue of why scientists disagree about whether plants have the ability to learn is that researchers do not use a consistent definition of ""learning"" and ""cognition"". Similarly, Michael Pollan, an author, and journalist, says in his piece The Intelligent Plant that researchers do not doubt Gagliano's data but rather her language, specifically her use of the term ""learning"" and ""cognition"" with respect to plants. A direction for future research is testing whether circadian rhythms in plants modulate learning and behavior and surveying researchers' definitions of ""cognition"" and ""learning.""
",Gagliano et al. published another paper on habituation behaviors in the mimosa pudica plant whereby the innate behavior of the plant was diminished by repeated exposure to a stimulus.,"[' Who published another paper on habituation behaviors in the mimosa pudica plant?', ' What did Gagliano et al. publish?']","['Gagliano et al.', 'another paper on habituation behaviors']"
1598,learning,Machine learning,"Machine learning, a branch of artificial intelligence, concerns the construction and study of systems that can learn from data. For example, a machine learning system could be trained on email messages to learn to distinguish between spam and non-spam messages. Most of the Machine Learning models are based on probabilistic theories where each input (e.g. an image ) is associated with a probability to become the desired output.
","Machine learning, a branch of artificial intelligence, concerns the construction and study of systems that can learn from data. For example, a machine learning system could be trained on email messages to learn to distinguish between spam and non-spam messages.","[' Machine learning is a branch of what?', ' What is machine learning?', ' How could a machine learning system be trained?']","['artificial intelligence', 'a branch of artificial intelligence', 'on email messages']"
1599,speech recognition,Summary,"
Speech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers with the main benefit of searchability. It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT). It incorporates knowledge and research in the computer science, linguistics and computer engineering fields. The reverse process is speech synthesis.
","
Speech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers with the main benefit of searchability. It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT).","[' What subfield of computer science and computational linguistics develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers?', ' What is automatic speech recognition also known as?', ' What is ASR?', ' What is STT?']","['Speech recognition', 'ASR', 'automatic speech recognition', 'speech to text']"
1600,speech recognition,Summary,"Some speech recognition systems require ""training"" (also called ""enrollment"") where an individual speaker reads text or isolated vocabulary into the system. The system analyzes the person's specific voice and uses it to fine-tune the recognition of that person's speech, resulting in increased accuracy. Systems that do not use training are called ""speaker-independent"" systems. Systems that use training are called ""speaker dependent"".
","Some speech recognition systems require ""training"" (also called ""enrollment"") where an individual speaker reads text or isolated vocabulary into the system. The system analyzes the person's specific voice and uses it to fine-tune the recognition of that person's speech, resulting in increased accuracy.","[' What is another term for ""training""?', ' What does training allow an individual speaker to read into the system?']","['enrollment', 'text or isolated vocabulary']"
1601,speech recognition,Summary,"Speech recognition applications include voice user interfaces such as voice dialing (e.g. ""call home""), call routing (e.g. ""I would like to make a collect call""), domotic appliance control, search key words (e.g. find a podcast where particular words were spoken), simple data entry (e.g., entering a credit card number), preparation of structured documents (e.g. a radiology report), determining speaker characteristics, speech-to-text processing (e.g., word processors or emails), and aircraft (usually termed direct voice input).
","Speech recognition applications include voice user interfaces such as voice dialing (e.g. ""call home""), call routing (e.g.","[' What are some examples of speech recognition applications?', ' What is one example of a voice user interface?']","['voice dialing (e.g. ""call home""), call routing', 'voice dialing']"
1602,speech recognition,Summary,"The term voice recognition or speaker identification refers to identifying the speaker, rather than what they are saying. Recognizing the speaker can simplify the task of translating speech in systems that have been trained on a specific person's voice or it can be used to authenticate or verify the identity of a speaker as part of a security process.
","The term voice recognition or speaker identification refers to identifying the speaker, rather than what they are saying. Recognizing the speaker can simplify the task of translating speech in systems that have been trained on a specific person's voice or it can be used to authenticate or verify the identity of a speaker as part of a security process.","[' What term refers to identifying the speaker rather than what they are saying?', "" Recognizing the speaker can simplify the task of translating speech in systems that have been trained on a specific person's voice?"", ' What can be used to authenticate or verify the identity of a speaker?']","['voice recognition or speaker identification', 'voice recognition', 'Recognizing the speaker']"
1603,speech recognition,Summary,"From the technology perspective, speech recognition has a long history with several waves of major innovations. Most recently, the field has benefited from advances in deep learning and big data. The advances are evidenced not only by the surge of academic papers published in the field, but more importantly by the worldwide industry adoption of a variety of deep learning methods in designing and deploying speech recognition systems.
","From the technology perspective, speech recognition has a long history with several waves of major innovations. Most recently, the field has benefited from advances in deep learning and big data.","[' From a technology perspective, speech recognition has a long history with several waves of what?', ' Speech recognition has recently benefited from advances in deep learning and what else?']","['major innovations', 'big data']"
1604,speech recognition,"Models, methods, and algorithms","Both acoustic modeling and language modeling are important parts of modern statistically based speech recognition algorithms. Hidden Markov models (HMMs) are widely used in many systems. Language modeling is also used in many other natural language processing applications such as document classification or statistical machine translation.
",Both acoustic modeling and language modeling are important parts of modern statistically based speech recognition algorithms. Hidden Markov models (HMMs) are widely used in many systems.,"[' What are two important parts of modern statistical based speech recognition algorithms?', ' What are HMMs?']","['acoustic modeling and language modeling', 'Hidden Markov models']"
1605,speech recognition,Performance,"The performance of speech recognition systems is usually evaluated in terms of accuracy and speed. Accuracy is usually rated with word error rate (WER), whereas speed is measured with the real time factor. Other measures of accuracy include Single Word Error Rate (SWER) and Command Success Rate (CSR).
","The performance of speech recognition systems is usually evaluated in terms of accuracy and speed. Accuracy is usually rated with word error rate (WER), whereas speed is measured with the real time factor.","[' How is the performance of speech recognition systems evaluated?', ' What is the word error rate?', ' How is speed measured?']","['accuracy and speed', 'WER', 'real time factor']"
1606,speech recognition,Performance,"Speech recognition by machine is a very complex problem, however. Vocalizations vary in terms of accent, pronunciation, articulation, roughness, nasality, pitch, volume, and speed. Speech is distorted by a background noise and echoes, electrical characteristics. Accuracy of speech recognition may vary with the following:","Speech recognition by machine is a very complex problem, however. Vocalizations vary in terms of accent, pronunciation, articulation, roughness, nasality, pitch, volume, and speed.","[' What is a very complex problem with speech recognition by machine?', ' Vocalizations vary in terms of what?']","['Vocalizations', 'accent, pronunciation, articulation, roughness, nasality, pitch, volume, and speed']"
1607,fault tolerance,Summary,"Fault tolerance is the property that enables a system to continue operating properly in the event of the failure of one or more faults within some of its components. If its operating quality decreases at all, the decrease is proportional to the severity of the failure, as compared to a naively designed system, in which even a small failure can cause total breakdown. Fault tolerance is particularly sought after in high-availability, mission-critical, or even life-critical systems. The ability of maintaining functionality when portions of a system break down is referred to as graceful degradation.","Fault tolerance is the property that enables a system to continue operating properly in the event of the failure of one or more faults within some of its components. If its operating quality decreases at all, the decrease is proportional to the severity of the failure, as compared to a naively designed system, in which even a small failure can cause total breakdown.","[' What is the property that enables a system to continue operating properly in the event of one or more faults within some of its components?', ' If its operating quality decreases at all, what is proportional to the severity of the failure?', ' What is the difference between a naively designed system and a small failure?']","['Fault tolerance', 'the decrease', 'can cause total breakdown']"
1608,fault tolerance,Summary,"A fault-tolerant design enables a system to continue its intended operation, possibly at a reduced level, rather than failing completely, when some part of the system fails. The term is most commonly used to describe computer systems designed to continue more or less fully operational with, perhaps, a reduction in throughput or an increase in response time in the event of some partial failure. That is, the system as a whole is not stopped due to problems either in the hardware or the software. An example in another field is a motor vehicle designed so it will continue to be drivable if one of the tires is punctured, or a structure that is able to retain its integrity in the presence of damage due to causes such as fatigue, corrosion, manufacturing flaws, or impact.
","A fault-tolerant design enables a system to continue its intended operation, possibly at a reduced level, rather than failing completely, when some part of the system fails. The term is most commonly used to describe computer systems designed to continue more or less fully operational with, perhaps, a reduction in throughput or an increase in response time in the event of some partial failure.","[' What type of design enables a system to continue its intended operation rather than failing completely?', ' What is the most commonly used term for computer systems designed to continue more or less fully operational with a reduction in the number of failures?', ' What would happen in the event of a partial failure?', ' What would occur if the system was partially operational?']","['fault-tolerant', 'fault-tolerant', 'reduction in throughput or an increase in response time', 'reduction in throughput or an increase in response time']"
1609,fault tolerance,Summary,"Within the scope of an individual system, fault tolerance can be achieved by anticipating exceptional conditions and building the system to cope with them, and, in general, aiming for self-stabilization so that the system converges towards an error-free state. However, if the consequences of a system failure are catastrophic, or the cost of making it sufficiently reliable is very high, a better solution may be to use some form of duplication. In any case, if the consequence of a system failure is so catastrophic, the system must be able to use reversion to fall back to a safe mode. This is similar to roll-back recovery but can be a human action if humans are present in the loop.
","Within the scope of an individual system, fault tolerance can be achieved by anticipating exceptional conditions and building the system to cope with them, and, in general, aiming for self-stabilization so that the system converges towards an error-free state. However, if the consequences of a system failure are catastrophic, or the cost of making it sufficiently reliable is very high, a better solution may be to use some form of duplication.","[' How can fault tolerance be achieved within the scope of an individual system?', ' What can be achieved by anticipating exceptional conditions and building the system to cope with them?', ' How can the consequences of a system failure be catastrophic?', ' What might be a better solution if the consequences of a system failure are catastrophic or the cost of making it sufficiently reliable is very high?']","['by anticipating exceptional conditions and building the system to cope with them', 'fault tolerance', 'the cost of making it sufficiently reliable is very high, a better solution may be to use some form of duplication', 'some form of duplication']"
1610,fault tolerance,History,"The first known fault-tolerant computer was SAPO, built in 1951 in Czechoslovakia by Antonín Svoboda.: 155  Its basic design was magnetic drums connected via relays, with a voting method of memory error detection (triple modular redundancy). Several other machines were developed along this line, mostly for military use. Eventually, they separated into three distinct categories: machines that would last a long time without any maintenance, such as the ones used on NASA space probes and satellites; computers that were very dependable but required constant monitoring, such as those used to monitor and control nuclear power plants or supercollider experiments; and finally, computers with a high amount of runtime which would be under heavy use, such as many of the supercomputers used by insurance companies for their probability monitoring.
","The first known fault-tolerant computer was SAPO, built in 1951 in Czechoslovakia by Antonín Svoboda. : 155  Its basic design was magnetic drums connected via relays, with a voting method of memory error detection (triple modular redundancy).","[' What was the name of the first fault-tolerant computer?', ' When was SAPO built?', ' Who was the architect of SAPO?', ' What type of drums did SAPO use?']","['SAPO', '1951', 'Antonín Svoboda', 'magnetic']"
1611,fault tolerance,History,"Most of the development in the so-called LLNM (Long Life, No Maintenance) computing was done by NASA during the 1960s, in preparation for Project Apollo and other research aspects. NASA's first machine went into a space observatory, and their second attempt, the JSTAR computer, was used in Voyager. This computer had a backup of memory arrays to use memory recovery methods and thus it was called the JPL Self-Testing-And-Repairing computer. It could detect its own errors and fix them or bring up redundant modules as needed. The computer is still working today.
","Most of the development in the so-called LLNM (Long Life, No Maintenance) computing was done by NASA during the 1960s, in preparation for Project Apollo and other research aspects. NASA's first machine went into a space observatory, and their second attempt, the JSTAR computer, was used in Voyager.","[' What is the acronym for ""Long Life, No Maintenance"" computing?', ' When was LLNM developed?', "" What was NASA's first machine used for?"", ' Where was the JSTAR computer used?']","['LLNM', 'during the 1960s', 'space observatory', 'Voyager']"
1612,fault tolerance,History,"Hyper-dependable computers were pioneered mostly by aircraft manufacturers,: 210  nuclear power companies, and the railroad industry in the USA. These needed computers with massive amounts of uptime that would fail gracefully enough with a fault to allow continued operation while relying on the fact that the computer output would be constantly monitored by humans to detect faults. Again, IBM developed the first computer of this kind for NASA for guidance of Saturn V rockets, but later on BNSF, Unisys, and General Electric built their own.: 223 ","Hyper-dependable computers were pioneered mostly by aircraft manufacturers,: 210  nuclear power companies, and the railroad industry in the USA. These needed computers with massive amounts of uptime that would fail gracefully enough with a fault to allow continued operation while relying on the fact that the computer output would be constantly monitored by humans to detect faults.","[' What type of computers were pioneered by aircraft manufacturers?', ' How many nuclear power companies were there in the US?', ' What industry was the railroad industry in?', ' On the fact that the computer output would be constantly monitored by humans to detect faults?']","['Hyper-dependable', '210', 'nuclear power companies, and the railroad industry in the USA', 'Hyper-dependable computers']"
1613,fault tolerance,History,"In the 1970s, much work has happened in the field
. For instance, F14 CADC had built-in self-test and redundancy.","In the 1970s, much work has happened in the field
. For instance, F14 CADC had built-in self-test and redundancy.","[' In what decade was much work done in the field?', ' What type of test did the F14 CADC have?']","['1970s', 'self']"
1614,fault tolerance,History,"In general, the early efforts at fault-tolerant designs were focused mainly on internal diagnosis, where a fault would indicate something was failing and a worker could replace it. SAPO, for instance, had a method by which faulty memory drums would emit a noise before failure. Later efforts showed that to be fully effective, the system had to be self-repairing and diagnosing – isolating a fault and then implementing a redundant backup while alerting a need for repair. This is known as N-model redundancy, where faults cause automatic fail-safes and a warning to the operator, and it is still the most common form of level one fault-tolerant design in use today.
","In general, the early efforts at fault-tolerant designs were focused mainly on internal diagnosis, where a fault would indicate something was failing and a worker could replace it. SAPO, for instance, had a method by which faulty memory drums would emit a noise before failure.","[' What was the focus of early fault-tolerant designs?', ' What would a fault indicate and a worker could replace?']","['internal diagnosis', 'something was failing']"
1615,fault tolerance,History,"Voting was another initial method, as discussed above, with multiple redundant backups operating constantly and checking each other's results, with the outcome that if, for example, four components reported an answer of 5 and one component reported an answer of 6, the other four would ""vote"" that the fifth component was faulty and have it taken out of service. This is called M out of N majority voting.
","Voting was another initial method, as discussed above, with multiple redundant backups operating constantly and checking each other's results, with the outcome that if, for example, four components reported an answer of 5 and one component reported an answer of 6, the other four would ""vote"" that the fifth component was faulty and have it taken out of service. This is called M out of N majority voting.","[' What was the initial method of voting?', ' How many components reported an answer of 5?', ' What did the other four components vote?', ' How many components would vote that the fifth component was faulty and have it taken out of service?', ' What is the process called?']","[""multiple redundant backups operating constantly and checking each other's results"", 'four', 'that the fifth component was faulty and have it taken out of service', 'four', 'M out of N majority voting']"
1616,fault tolerance,Examples,"Hardware fault tolerance sometimes requires that broken parts be taken out and replaced with new parts while the system is still operational (in computing known as hot swapping). Such a system implemented with a single backup is known as single point tolerant and represents the vast majority of fault-tolerant systems. In such systems the mean time between failures should be long enough for the operators to have sufficient time to fix the broken devices (mean time to repair) before the backup also fails. It is helpful if the time between failures is as long as possible, but this is not specifically required in a fault-tolerant system.
",Hardware fault tolerance sometimes requires that broken parts be taken out and replaced with new parts while the system is still operational (in computing known as hot swapping). Such a system implemented with a single backup is known as single point tolerant and represents the vast majority of fault-tolerant systems.,"[' What is a system implemented with a single backup known as?', ' What is the vast majority of fault-tolerant systems?']","['single point tolerant', 'single point tolerant']"
1617,fault tolerance,Examples,"Fault tolerance is notably successful in computer applications. Tandem Computers built their entire business on such machines, which used single-point tolerance to create their NonStop systems with uptimes measured in years.
","Fault tolerance is notably successful in computer applications. Tandem Computers built their entire business on such machines, which used single-point tolerance to create their NonStop systems with uptimes measured in years.","[' What is notably successful in computer applications?', ' What company built their entire business on fault tolerance machines?', ' Tandem Computers used what to create their NonStop systems?']","['Fault tolerance', 'Tandem Computers', 'single-point tolerance']"
1618,fault tolerance,Examples,"Data formats may also be designed to degrade gracefully. HTML for example, is designed to be forward compatible, allowing Web browsers to ignore new and unsupported HTML entities without causing the document to be unusable. Additionally, some sites, including popular platforms such as Twitter (until December 2020), provide an optional lightweight front end that does not rely on JavaScript and has a minimal layout, to ensure wide accessibility and outreach, such as on game consoles with limited web browsing capabilities.","Data formats may also be designed to degrade gracefully. HTML for example, is designed to be forward compatible, allowing Web browsers to ignore new and unsupported HTML entities without causing the document to be unusable.","[' Data formats can be designed to degrade gracefully, what is it?', ' What is designed to be forward compatible?']","['HTML', 'HTML']"
1619,fault tolerance,Terminology,"A highly fault-tolerant system might continue at the same level of performance even though one or more components have failed.  For example, a building with a backup electrical generator will provide the same voltage to wall outlets even if the grid power fails.
","A highly fault-tolerant system might continue at the same level of performance even though one or more components have failed. For example, a building with a backup electrical generator will provide the same voltage to wall outlets even if the grid power fails.","[' What kind of system might continue at the same level of performance even though one or more components have failed?', ' A building with a backup electrical generator will provide the same voltage to what?']","['highly fault-tolerant', 'wall outlets']"
1620,fault tolerance,Terminology,"A system that is designed to fail safe, or fail-secure, or fail gracefully, whether it functions at a reduced level or fails completely, does so in a way that protects people, property, or data from injury, damage, intrusion, or disclosure.  In computers, a program might fail-safe by executing a graceful exit (as opposed to an uncontrolled crash) in order to prevent data corruption after experiencing an error.  A similar distinction is made between ""failing well"" and ""failing badly"".
","A system that is designed to fail safe, or fail-secure, or fail gracefully, whether it functions at a reduced level or fails completely, does so in a way that protects people, property, or data from injury, damage, intrusion, or disclosure. In computers, a program might fail-safe by executing a graceful exit (as opposed to an uncontrolled crash) in order to prevent data corruption after experiencing an error.","[' What does a system that is designed to fail safe do?', ' What is one way a program might fail-safe?', ' In computers, a program might fail-safe by executing a graceful exit instead of what?']","['protects people, property, or data from injury, damage, intrusion, or disclosure', 'executing a graceful exit', 'an uncontrolled crash']"
1621,fault tolerance,Terminology,"A system that is designed to experience graceful degradation, or to fail soft (used in computing, similar to ""fail safe"") operates at a reduced level of performance after some component failures.  For example, a building may operate lighting at reduced levels and elevators at reduced speeds if grid power fails, rather than either trapping people in the dark completely or continuing to operate at full power.  In computing an example of graceful degradation is that if insufficient network bandwidth is available to stream an online video, a lower-resolution version might be streamed in place of the high-resolution version.  Progressive enhancement is an example in computing, where web pages are available in a basic functional format for older, small-screen, or limited-capability web browsers, but in an enhanced version for browsers capable of handling additional technologies or that have a larger display available.
","A system that is designed to experience graceful degradation, or to fail soft (used in computing, similar to ""fail safe"") operates at a reduced level of performance after some component failures. For example, a building may operate lighting at reduced levels and elevators at reduced speeds if grid power fails, rather than either trapping people in the dark completely or continuing to operate at full power.","[' What is a system designed to experience graceful degradation?', ' What is used in computing, similar to ""fail safe""?', ' What happens to levels and elevators if grid power fails?']","['fail soft (used in computing, similar to ""fail safe"") operates at a reduced level of performance after some component failures', 'fail soft', 'at reduced speeds']"
1622,fault tolerance,Terminology,"In fault-tolerant computer systems, programs that are considered robust are designed to continue operation despite an error, exception, or invalid input, instead of crashing completely.  Software brittleness is the opposite of robustness.  Resilient networks continue to transmit data despite the failure of some links or nodes; resilient buildings and infrastructure are likewise expected to prevent complete failure in situations like earthquakes, floods, or collisions.
","In fault-tolerant computer systems, programs that are considered robust are designed to continue operation despite an error, exception, or invalid input, instead of crashing completely. Software brittleness is the opposite of robustness.",[' What is the opposite of robustness in fault-tolerant computer systems?'],['Software brittleness']
1623,fault tolerance,Terminology,"A system with high failure transparency will alert users that a component failure has occurred, even if it continues to operate with full performance, so that failure can be repaired or imminent complete failure anticipated.  Likewise, a fail-fast component is designed to report at the first point of failure, rather than allow downstream components to fail and generate reports then.  This allows easier diagnosis of the underlying problem, and may prevent improper operation in a broken state.
","A system with high failure transparency will alert users that a component failure has occurred, even if it continues to operate with full performance, so that failure can be repaired or imminent complete failure anticipated. Likewise, a fail-fast component is designed to report at the first point of failure, rather than allow downstream components to fail and generate reports then.","[' A system with high failure transparency will alert users that a component failure has occurred, even if it continues to operate with what?', ' A fail-fast component is designed to report at what point of failure?', ' What is designed to report at the first point of failure rather than allow downstream components to fail?']","['full performance', 'first', 'fail-fast component']"
1624,fault tolerance,Criteria,"Providing fault-tolerant design for every component is normally not an option. Associated redundancy brings a number of penalties: increase in weight, size, power consumption, cost, as well as time to design, verify, and test. Therefore, a number of choices have to be examined to determine which components should be fault tolerant:","Providing fault-tolerant design for every component is normally not an option. Associated redundancy brings a number of penalties: increase in weight, size, power consumption, cost, as well as time to design, verify, and test.","[' What type of design is normally not an option?', ' Redundancy brings a number of what?']","['fault-tolerant', 'penalties']"
1625,fault tolerance,Criteria,"An example of a component that passes all the tests is a car's occupant restraint system. While we do not normally think of the primary occupant restraint system, it is gravity. If the vehicle rolls over or undergoes severe g-forces, then this primary method of occupant restraint may fail. Restraining the occupants during such an accident is absolutely critical to safety, so we pass the first test. Accidents causing occupant ejection were quite common before seat belts, so we pass the second test. The cost of a redundant restraint method like seat belts is quite low, both economically and in terms of weight and space, so we pass the third test. Therefore, adding seat belts to all vehicles is an excellent idea. Other ""supplemental restraint systems"", such as airbags, are more expensive and so pass that test by a smaller margin.
","An example of a component that passes all the tests is a car's occupant restraint system. While we do not normally think of the primary occupant restraint system, it is gravity.","[' What is an example of a component that passes all tests?', ' What is the primary occupant restraint system?']","[""a car's occupant restraint system"", 'gravity']"
1626,fault tolerance,Criteria,"Another excellent and long-term example of this principle being put into practice is the braking system: whilst the actual brake mechanisms are critical, they are not particularly prone to sudden (rather than progressive) failure, and are in any case necessarily duplicated to allow even and balanced application of brake force to all wheels. It would also be prohibitively costly to further double-up the main components and they would add considerable weight. However, the similarly critical systems for actuating the brakes under driver control are inherently less robust, generally using a cable (can rust, stretch, jam, snap) or hydraulic fluid (can leak, boil and develop bubbles, absorb water and thus lose effectiveness). Thus in most modern cars the footbrake hydraulic brake circuit is diagonally divided to give two smaller points of failure, the loss of either only reducing brake power by 50% and not causing as much dangerous brakeforce imbalance as a straight front-back or left-right split, and should the hydraulic circuit fail completely (a relatively very rare occurrence), there is a failsafe in the form of the cable-actuated parking brake that operates the otherwise relatively weak rear brakes, but can still bring the vehicle to a safe halt in conjunction with transmission/engine braking so long as the demands on it are in line with normal traffic flow. The cumulatively unlikely combination of total foot brake failure with the need for harsh braking in an emergency will likely result in a collision, but still one at lower speed than would otherwise have been the case.
","Another excellent and long-term example of this principle being put into practice is the braking system: whilst the actual brake mechanisms are critical, they are not particularly prone to sudden (rather than progressive) failure, and are in any case necessarily duplicated to allow even and balanced application of brake force to all wheels. It would also be prohibitively costly to further double-up the main components and they would add considerable weight.","[' What is another example of a braking system being put into practice?', ' The actual brake mechanisms are critical, but are not particularly prone to what?', ' What would it be prohibitively costly to double-up the main components?', ' What would add considerable weight?']","['brake mechanisms are critical, they are not particularly prone to sudden (rather than progressive) failure, and are in any case necessarily duplicated to allow even and balanced application of brake force to all wheels', 'sudden (rather than progressive) failure', 'they would add considerable weight', 'braking system: whilst the actual brake mechanisms are critical, they are not particularly prone to sudden (rather than progressive) failure, and are in any case necessarily duplicated to allow even and balanced application of brake force to all wheels. It would also be prohibitively costly to further double-up the main components']"
1627,fault tolerance,Criteria,"In comparison with the foot pedal activated service brake, the parking brake itself is a less critical item, and unless it is being used as a one-time backup for the footbrake, will not cause immediate danger if it is found to be nonfunctional at the moment of application. Therefore, no redundancy is built into it per se (and it typically uses a cheaper, lighter, but less hardwearing cable actuation system), and it can suffice, if this happens on a hill, to use the footbrake to momentarily hold the vehicle still, before driving off to find a flat piece of road on which to stop. Alternatively, on shallow gradients, the transmission can be shifted into Park, Reverse or First gear, and the transmission lock / engine compression used to hold it stationary, as there is no need for them to include the sophistication to first bring it to a halt.
","In comparison with the foot pedal activated service brake, the parking brake itself is a less critical item, and unless it is being used as a one-time backup for the footbrake, will not cause immediate danger if it is found to be nonfunctional at the moment of application. Therefore, no redundancy is built into it per se (and it typically uses a cheaper, lighter, but less hardwearing cable actuation system), and it can suffice, if this happens on a hill, to use the footbrake to momentarily hold the vehicle still, before driving off to find a flat piece of road on which to stop.","[' What is a less critical item in comparison to the foot pedal activated service brake?', ' The parking brake itself is what?', ' What is not built into the vehicle at the moment of application?', ' What type of actuation system is used to hold the vehicle still?', ' What is used to hold the vehicle still before driving off to find a flat piece of road?']","['the parking brake', 'less critical item', 'redundancy', 'cable', 'footbrake']"
1628,fault tolerance,Criteria,"On motorcycles, a similar level of fail-safety is provided by simpler methods; firstly the front and rear brake systems being entirely separate, regardless of their method of activation (that can be cable, rod or hydraulic), allowing one to fail entirely whilst leaving the other unaffected. Secondly, the rear brake is relatively strong compared to its automotive cousin, even being a powerful disc on sports models, even though the usual intent is for the front system to provide the vast majority of braking force; as the overall vehicle weight is more central, the rear tyre is generally larger and grippier, and the rider can lean back to put more weight on it, therefore allowing more brake force to be applied before the wheel locks up. On cheaper, slower utility-class machines, even if the front wheel should use a hydraulic disc for extra brake force and easier packaging, the rear will usually be a primitive, somewhat inefficient, but exceptionally robust rod-actuated drum, thanks to the ease of connecting the footpedal to the wheel in this way and, more importantly, the near impossibility of catastrophic failure even if the rest of the machine, like a lot of low-priced bikes after their first few years of use, is on the point of collapse from neglected maintenance.
","On motorcycles, a similar level of fail-safety is provided by simpler methods; firstly the front and rear brake systems being entirely separate, regardless of their method of activation (that can be cable, rod or hydraulic), allowing one to fail entirely whilst leaving the other unaffected. Secondly, the rear brake is relatively strong compared to its automotive cousin, even being a powerful disc on sports models, even though the usual intent is for the front system to provide the vast majority of braking force; as the overall vehicle weight is more central, the rear tyre is generally larger and grippier, and the rider can lean back to put more weight on it, therefore allowing more brake force to be applied before the wheel locks up.","[' On what type of motorcycles is fail-safety provided by simpler methods?', ' The front and rear brake systems are completely separate, regardless of what method of activation?', ' What is the rear brake system completely separate from?', ' Why is the rear brake relatively strong compared to its automotive cousin?', ' What is the usual intent for the front system to provide the vast majority of braking force?', ' The overall vehicle weight is more what?', ' What is more central in a vehicle?', ' What is the rear tyre generally larger and grippier?', ' The rider can lean back to put more weight on what?']","['motorcycles', 'cable, rod or hydraulic', 'front and rear brake systems being entirely separate, regardless of their method of activation', 'being a powerful disc on sports models', 'as the overall vehicle weight is more central', 'central', 'the overall vehicle weight', 'the overall vehicle weight is more central', 'the rear tyre']"
1629,fault tolerance,Requirements,"In addition, fault-tolerant systems are characterized in terms of both planned service outages and unplanned service outages. These are usually measured at the application level and not just at a hardware level. The figure of merit is called availability and is expressed as a percentage. For example, a five nines system would statistically provide 99.999% availability.
","In addition, fault-tolerant systems are characterized in terms of both planned service outages and unplanned service outages. These are usually measured at the application level and not just at a hardware level.","[' What are fault-tolerant systems characterized in terms of?', ' What are unplanned service outages measured at?']","['planned service outages and unplanned service outages', 'application level']"
1630,fault tolerance,Fault tolerance techniques,"Research into the kinds of tolerances needed for critical systems involves a large amount of interdisciplinary work. The more complex the system, the more carefully all possible interactions have to be considered and prepared for. Considering the importance of high-value systems in transport, public utilities and the military, the field of topics that touch on research is very wide: it can include such obvious subjects as software modeling and reliability, or hardware design, to arcane elements such as stochastic models, graph theory, formal or exclusionary logic, parallel processing, remote data transmission, and more.","Research into the kinds of tolerances needed for critical systems involves a large amount of interdisciplinary work. The more complex the system, the more carefully all possible interactions have to be considered and prepared for.","[' Research into the kinds of tolerances needed for critical systems involves a large amount of what?', ' The more complex the system, the more carefully all possible interactions have to be considered and prepared for?']","['interdisciplinary work', 'Research into the kinds of tolerances needed for critical systems']"
1631,fault tolerance,Redundancy,"Redundancy is the provision of functional capabilities that would be unnecessary in a fault-free environment.
This can consist of backup components that automatically ""kick in"" if one component fails. For example, large cargo trucks can lose a tire without any major consequences. They have many tires, and no one tire is critical (with the exception of the front tires, which are used to steer, but generally carry less load, each and in total, than the other four to 16, so are less likely to fail).
The idea of incorporating redundancy in order to improve the reliability of a system was pioneered by John von Neumann in the 1950s.","Redundancy is the provision of functional capabilities that would be unnecessary in a fault-free environment. This can consist of backup components that automatically ""kick in"" if one component fails.","[' What is the term for the provision of functional capabilities that would be unnecessary in a fault-free environment?', ' Backup components that automatically ""kick in"" if one component fails?']","['Redundancy', 'Redundancy']"
1632,fault tolerance,Redundancy,"Two kinds of redundancy are possible: space redundancy and time redundancy. Space redundancy provides additional components, functions, or data items that are unnecessary for fault-free operation.  Space redundancy is further classified into hardware, software and information redundancy, depending on the type of redundant resources added to the system.  In time redundancy the computation or data transmission is repeated and the result is compared to a stored copy of the previous result. The current terminology for this kind of testing is referred to as 'In Service Fault Tolerance Testing or ISFTT for short.
","Two kinds of redundancy are possible: space redundancy and time redundancy. Space redundancy provides additional components, functions, or data items that are unnecessary for fault-free operation.","[' How many kinds of redundancy are possible?', ' What provides additional components, functions, or data items that are unnecessary for fault-free operation?']","['Two', 'Space redundancy']"
1633,fault tolerance,Related terms,"There is a difference between fault tolerance and systems that rarely have problems. For instance, the Western Electric crossbar systems had failure rates of two hours per forty years, and therefore were highly fault resistant. But when a fault did occur they still stopped operating completely, and therefore were not fault tolerant.
","There is a difference between fault tolerance and systems that rarely have problems. For instance, the Western Electric crossbar systems had failure rates of two hours per forty years, and therefore were highly fault resistant.","[' What is the difference between systems that rarely have problems?', "" What was the failure rate of Western Electric's crossbar systems?""]","['fault tolerance', 'two hours per forty years']"
1634,communication network,Summary,"A telecommunications network is a group of nodes interconnected by telecommunications links that are used to exchange messages between the nodes. The links may use a variety of technologies based on the methodologies of circuit switching, message switching, or packet switching, to pass messages and signals. 
","A telecommunications network is a group of nodes interconnected by telecommunications links that are used to exchange messages between the nodes. The links may use a variety of technologies based on the methodologies of circuit switching, message switching, or packet switching, to pass messages and signals.","[' What is a group of nodes interconnected by telecommunications links that are used to exchange messages between the nodes?', ' What may use a variety of technologies based on the methodologies of circuit switching?']","['A telecommunications network', 'telecommunications network']"
1635,communication network,Summary,"Multiple nodes may cooperate to pass the message from an originating node to the destination node, via multiple network hops. For this routing function, each node in the network is assigned a network address for identification and locating it on the network. The collection of addresses in the network is called the address space of the network.
","Multiple nodes may cooperate to pass the message from an originating node to the destination node, via multiple network hops. For this routing function, each node in the network is assigned a network address for identification and locating it on the network.","[' Multiple nodes may cooperate to pass the message from an originating node to the destination node via what?', ' Each node in the network is assigned a network address for identification and what else?']","['multiple network hops', 'locating it on the network']"
1636,communication network,Data networks,"Data networks are used extensively throughout the world for communication between individuals and organizations. Data networks can be connected to allow users seamless access to resources that are hosted outside of the particular provider they are connected to. The Internet is the best example of the internetworking of many data networks from different organizations.
",Data networks are used extensively throughout the world for communication between individuals and organizations. Data networks can be connected to allow users seamless access to resources that are hosted outside of the particular provider they are connected to.,"[' Data networks are used extensively throughout the world for communication between individuals and what?', ' Data networks can be connected to allow users seamless access to resources that are hosted outside of what provider?']","['organizations', 'the particular provider they are connected to']"
1637,communication network,Data networks,"Terminals attached to IP networks like the Internet are addressed using IP addresses. Protocols of the Internet protocol suite (TCP/IP) provide the control and routing of messages across the and IP data network. There are many different network structures that IP can be used across to efficiently route messages, for example:
",Terminals attached to IP networks like the Internet are addressed using IP addresses. Protocols of the Internet protocol suite (TCP/IP) provide the control and routing of messages across the and IP data network.,"[' What are terminals attached to IP networks like the Internet addressed using?', ' What provide the control and routing of messages across the and IP data network?']","['IP addresses', 'Protocols of the Internet protocol suite (TCP/IP)']"
1638,communication network,Data networks,"Data center networks also rely highly on TCP/IP for communication across machines. They connect thousands of servers, are designed to be highly robust, provide low latency and high bandwidth. Data center network topology plays a significant role in determining the level of failure resiliency, ease of incremental expansion, communication bandwidth and latency.","Data center networks also rely highly on TCP/IP for communication across machines. They connect thousands of servers, are designed to be highly robust, provide low latency and high bandwidth.","[' What do data center networks rely on for communication across machines?', ' How many servers do TCP/IP networks connect?', ' What are data centers designed to be?']","['TCP/IP', 'thousands', 'highly robust']"
1639,communication network,Capacity and speed,"In analogy to the improvements in the speed and capacity of digital computers, provided by advances in semiconductor technology and expressed in the bi-yearly doubling of transistor density, which is described empirically by Moore's law, the capacity and speed of telecommunications networks have followed similar advances, for similar reasons. In telecommunication, this is expressed in Edholm's law, proposed by and named after Phil Edholm in 2004. This empirical law holds that the bandwidth of telecommunication networks doubles every 18 months, which has proven to be true since the 1970s. The trend is evident in the Internet, cellular (mobile), wireless and wired local area networks (LANs), and personal area networks. This development is the consequence of rapid advances in the development of metal-oxide-semiconductor technology.","In analogy to the improvements in the speed and capacity of digital computers, provided by advances in semiconductor technology and expressed in the bi-yearly doubling of transistor density, which is described empirically by Moore's law, the capacity and speed of telecommunications networks have followed similar advances, for similar reasons. In telecommunication, this is expressed in Edholm's law, proposed by and named after Phil Edholm in 2004.","[' What law describes the bi-yearly doubling of transistor density?', ' The speed and capacity of digital computers have followed similar advances for what reason?', ' What has followed similar advances in telecommunications networks?', ' What was the name of the law proposed by and named after Phil Edholm?']","[""Moore's law"", 'bi-yearly doubling of transistor density', 'the capacity and speed', ""Edholm's law""]"
1640,simulated annealing,Summary,"Simulated annealing (SA) is a probabilistic technique for approximating the global optimum of a given function. Specifically, it is a metaheuristic to approximate global optimization in a large search space for an optimization problem. It is often used when the search space is discrete (for example the traveling salesman problem, the boolean satisfiability problem, protein structure prediction, and job-shop scheduling). For problems where finding an approximate global optimum is more important than finding a precise local optimum in a fixed amount of time, simulated annealing may be preferable to exact algorithms such as gradient descent or branch and bound. 
","Simulated annealing (SA) is a probabilistic technique for approximating the global optimum of a given function. Specifically, it is a metaheuristic to approximate global optimization in a large search space for an optimization problem.","[' What is a probabilistic technique for approximating the global optimum of a given function?', ' What is SA?']","['Simulated annealing', 'Simulated annealing']"
1641,simulated annealing,Summary,"The name of the algorithm comes from annealing in metallurgy, a technique involving heating and controlled cooling of a material to alter its physical properties. Both are attributes of the material that depend on their thermodynamic free energy. Heating and cooling the material affects both the temperature and the thermodynamic free energy or Gibbs energy.
Simulated annealing can be used for very hard computational optimization problems where exact algorithms fail; even though it usually achieves an approximate solution to the global minimum, it could be enough for many practical problems.
","The name of the algorithm comes from annealing in metallurgy, a technique involving heating and controlled cooling of a material to alter its physical properties. Both are attributes of the material that depend on their thermodynamic free energy.","[' What is the name of the algorithm?', ' What is annealing in metallurgy a technique that involves heating and controlled cooling of?']","['annealing in metallurgy', 'a material']"
1642,simulated annealing,Summary,"The problems solved by SA are currently formulated by an objective function of many variables, subject to several constraints. In practice, the constraint can be penalized as part of the objective function.
","The problems solved by SA are currently formulated by an objective function of many variables, subject to several constraints. In practice, the constraint can be penalized as part of the objective function.","[' What is the objective function of many variables subject to?', ' What can the constraint be penalized as part of?']","['several constraints', 'the objective function']"
1643,simulated annealing,Summary,"Similar techniques have been independently introduced on several occasions, including Pincus (1970), Khachaturyan et al (1979, 1981), Kirkpatrick, Gelatt and Vecchi (1983), and Cerny (1985). In 1983, this approach was used by Kirkpatrick, Gelatt Jr., Vecchi, for a solution of the traveling salesman problem. They also proposed its current name, simulated annealing.
","Similar techniques have been independently introduced on several occasions, including Pincus (1970), Khachaturyan et al (1979, 1981), Kirkpatrick, Gelatt and Vecchi (1983), and Cerny (1985). In 1983, this approach was used by Kirkpatrick, Gelatt Jr., Vecchi, for a solution of the traveling salesman problem.","[' When was Pincus born?', ' When was Khachaturyan et al born?<extra_id_51> When was the technique used by Kirkpatrick, Gelatt Jr., Vecchi?']","['1970', '1979']"
1644,simulated annealing,Summary,"This notion of slow cooling implemented in the simulated annealing algorithm is interpreted as a slow decrease in the probability of accepting worse solutions as the solution space is explored. Accepting worse solutions allows for a more extensive search for the global optimal solution. In general, simulated annealing algorithms work as follows. The temperature progressively decreases from an initial positive value to zero. At each time step, the algorithm randomly selects a solution close to the current one, measures its quality, and moves to it according to the temperature-dependent probabilities of selecting better or worse solutions, which during the search respectively remain at 1 (or positive) and decrease towards zero.
",This notion of slow cooling implemented in the simulated annealing algorithm is interpreted as a slow decrease in the probability of accepting worse solutions as the solution space is explored. Accepting worse solutions allows for a more extensive search for the global optimal solution.,"[' What is a slow decrease in the probability of accepting worse solutions?', ' What allows for a more extensive search for the global optimal solution?']","['slow cooling', 'Accepting worse solutions']"
1645,simulated annealing,Summary,"The simulation can be performed either by a solution of kinetic equations for density functions or by using the stochastic sampling method. The method is an adaptation of the Metropolis–Hastings algorithm, a Monte Carlo method to generate sample states of a thermodynamic system, published by N. Metropolis et al. in 1953.","The simulation can be performed either by a solution of kinetic equations for density functions or by using the stochastic sampling method. The method is an adaptation of the Metropolis–Hastings algorithm, a Monte Carlo method to generate sample states of a thermodynamic system, published by N. Metropolis et al.","[' What method can be used to perform the simulation?', ' What is the adaptation of the Metropolis-Hastings algorithm?', ' Which method is used to generate sample states of a thermodynamic system?']","['stochastic sampling method', 'stochastic sampling method', 'Metropolis–Hastings algorithm']"
1646,simulated annealing,Overview,"The state of some physical systems, and the function E(s) to be minimized, is analogous to the internal energy of the system in that state. The goal is to bring the system, from an arbitrary initial state, to a state with the minimum possible energy.
","The state of some physical systems, and the function E(s) to be minimized, is analogous to the internal energy of the system in that state. The goal is to bring the system, from an arbitrary initial state, to a state with the minimum possible energy.","[' The state of some physical systems is analogous to what?', ' The goal is to bring the system from an arbitrary initial state to a state with the minimum possible energy?']","['the internal energy', 'The state of some physical systems']"
1647,simulated annealing,Pseudocode,"The following pseudocode presents the simulated annealing heuristic as described above. It starts from a state s0 and continues until a maximum of kmax steps have been taken. In the process, the call neighbour(s) should generate a randomly chosen neighbour of a given state s; the call random(0, 1) should pick and return a value in the range [0, 1], uniformly at random. The annealing schedule is defined by the call temperature(r), which should yield the temperature to use, given the fraction r of the time budget that has been expended so far.
",The following pseudocode presents the simulated annealing heuristic as described above. It starts from a state s0 and continues until a maximum of kmax steps have been taken.,[' What pseudocode presents the simulated annealing heuristic as described above?'],['The following']
1648,simulated annealing,Selecting the parameters,"In order to apply the simulated annealing method to a specific problem, one must specify the following parameters: the state space, the energy (goal) function E(), the candidate generator procedure neighbour(), the acceptance probability function P(), and the annealing schedule temperature() AND initial temperature <init temp>. These choices can have a significant impact on the method's effectiveness.  Unfortunately, there are no choices of these parameters that will be good for all problems, and there is no general way to find the best choices for a given problem.  The following sections give some general guidelines.
","In order to apply the simulated annealing method to a specific problem, one must specify the following parameters: the state space, the energy (goal) function E(), the candidate generator procedure neighbour(), the acceptance probability function P(), and the annealing schedule temperature() AND initial temperature <init temp>. These choices can have a significant impact on the method's effectiveness.","[' What must one specify in order to apply the simulated annealing method to a specific problem?', ' What is the energy (goal) function of the candidate generator procedure neighbour()?', ' The acceptance probability function P() and the initial temperature can have what effect?', "" What can have a significant impact on the method's effectiveness?""]","['the state space, the energy (goal) function E(), the candidate generator procedure neighbour(), the acceptance probability function P(), and the annealing schedule temperature() AND initial temperature <init temp>.', 'E(),', ""significant impact on the method's effectiveness"", 'annealing schedule temperature() AND initial temperature <init temp>.']"
1649,simulated annealing,Restarts,"Sometimes it is better to move back to a solution that was significantly better rather than always moving from the current state.  This process is called restarting of simulated annealing.  To do this we set s and e to sbest and ebest and perhaps restart the annealing schedule.  The decision to restart could be based on several criteria. Notable among these include restarting based on a fixed number of steps, based on whether the current energy is too high compared to the best energy obtained so far, restarting randomly, etc.
",Sometimes it is better to move back to a solution that was significantly better rather than always moving from the current state. This process is called restarting of simulated annealing.,"[' When is it better to move back to a solution that was significantly better than the current state?', ' What is the process called that is called restarting of simulated annealing?']","['Sometimes', 'Sometimes it is better to move back to a solution']"
1650,collaboration,Summary,"Collaboration (from Latin com- ""with"" + laborare ""to labor"", ""to work"") is the process of two or more people, entities or organizations working together to complete a task or achieve a goal. Collaboration is similar to cooperation. Most collaboration requires leadership, although the form of leadership can be social within a decentralized and egalitarian group. Teams that work collaboratively often access greater resources, recognition and rewards when facing competition for finite resources.","Collaboration (from Latin com- ""with"" + laborare ""to labor"", ""to work"") is the process of two or more people, entities or organizations working together to complete a task or achieve a goal. Collaboration is similar to cooperation.","[' What is the process of two or more people, entities or organizations working together to complete a task or achieve a goal?']",['Collaboration']
1651,collaboration,Summary,"Structured methods of collaboration encourage introspection of behavior and communication. Such methods aim to increase the success of teams as they engage in collaborative problem-solving. Collaboration is present in opposing goals exhibiting the notion of adversarial collaboration, though this is not a common use of the term. In its applied sense, ""(a) collaboration is a purposeful relationship in which all parties strategically choose to cooperate in order to accomplish a shared outcome.""",Structured methods of collaboration encourage introspection of behavior and communication. Such methods aim to increase the success of teams as they engage in collaborative problem-solving.,"[' What encourages introspection of behavior and communication?', ' What do structured methods of collaboration aim to increase?']","['Structured methods of collaboration', 'the success of teams']"
1652,design,Summary,"A design is a plan or specification for the construction of an object or system or for the implementation of an activity or process, or the result of that plan or specification in the form of a prototype, product or process. The verb to design expresses the process of developing a design. In some cases, the direct construction of an object without an explicit prior plan (such as in craftwork, some engineering, coding, and graphic design) may also be considered to be a design activity. The design usually has to satisfy certain goals and constraints, may take into account aesthetic, functional, economic, or socio-political considerations, and is expected to interact with a certain environment. Typical examples of designs include  architectural blueprints, engineering drawings, business processes, circuit diagrams, and  sewing patterns.","A design is a plan or specification for the construction of an object or system or for the implementation of an activity or process, or the result of that plan or specification in the form of a prototype, product or process. The verb to design expresses the process of developing a design.","[' What is a plan or specification for the construction of an object or system or for the implementation of an activity or process?', ' What expresses the process of developing a prototype, product or process in the form of a design?', ' What expresses the process of developing a design?', ' What does the verb to design express?']","['A design', 'The verb to design', 'The verb to design', 'the process of developing a design']"
1653,design,Summary,"People who produce designs are called designers. The term ""designer"" generally refers to someone who works professionally in one of the various design areas. The word is generally qualified by the area involved (so one can speak of a fashion designer, a product designer, a web designer or an interior designer), but can also designate others such as architects and engineers. A designer's sequence of activities is called a design process, possibly using design methods. The process of creating a design can be brief (a quick sketch) or lengthy and complicated, involving considerable research, negotiation, reflection,  modeling, interactive adjustment and re-design.
","People who produce designs are called designers. The term ""designer"" generally refers to someone who works professionally in one of the various design areas.","[' What are people who create designs called?', ' What does the term ""designer"" generally refer to?']","['designers', 'someone who works professionally in one of the various design areas']"
1654,design,"Process<span id=""Process""></span>","Substantial disagreement exists concerning how designers in many fields, whether amateur or professional, alone or in teams, produce designs. Kees Dorst and Judith Dijkhuis, both designers themselves, argued that ""there are many ways of describing design processes"" and discussed ""two basic and fundamentally different ways"", both of which have several names. The prevailing view has been called ""the rational model"", ""technical problem solving"" and ""the reason-centric perspective"". The alternative view has been called ""reflection-in-action"", ""co-evolution"", and ""the action-centric perspective"".","Substantial disagreement exists concerning how designers in many fields, whether amateur or professional, alone or in teams, produce designs. Kees Dorst and Judith Dijkhuis, both designers themselves, argued that ""there are many ways of describing design processes"" and discussed ""two basic and fundamentally different ways"", both of which have several names.","[' Kees Dorst and Judith Dijkhuis argued that ""there are many ways of describing design processes"" and discussed ""two basic and fundamentally different ways""?', ' What are the names of the two types of ways?']","['designers', 'basic and fundamentally different']"
1655,design,Philosophies,"Philosophy of design is the study of definitions of design, and the assumptions, foundations, and implications of design. There are also countless informal or personal philosophies for guiding design as design values and its accompanying aspects within modern design vary, both between different schools of thought and among practicing designers. Design philosophies are usually for determining design goals. In this sense, design philosophies are fundamental guiding principles that dictate how a designer approaches his/her practice. For example, reflections on material culture and environmental concerns (sustainable design) can guide a design philosophy.
","Philosophy of design is the study of definitions of design, and the assumptions, foundations, and implications of design. There are also countless informal or personal philosophies for guiding design as design values and its accompanying aspects within modern design vary, both between different schools of thought and among practicing designers.","[' What is the study of definitions of design and the assumptions, foundations, and implications of design?', ' There are countless informal or personal philosophies for guiding design as design values and its accompanying aspects within modern design vary between what schools of thought?']","['Philosophy of design', 'different']"
1656,software architecture,Summary,"Software architecture refers to the fundamental structures of a software system and the discipline of creating such structures and systems. Each structure comprises software elements, relations among them, and properties of both elements and relations. The architecture of a software system is a metaphor, analogous to the architecture of a building. It functions as a blueprint for the system and the developing project, laying out the tasks necessary to be executed by the design teams.","Software architecture refers to the fundamental structures of a software system and the discipline of creating such structures and systems. Each structure comprises software elements, relations among them, and properties of both elements and relations.","[' What refers to the fundamental structures of a software system?', ' What is the discipline of creating such structures and systems?']","['Software architecture', 'Software architecture']"
1657,software architecture,Summary,"Software architecture is about making fundamental structural choices that are costly to change once implemented. Software architecture choices include specific structural options from possibilities in the design of the software. For example, the systems that controlled the Space Shuttle launch vehicle had the requirement of being very fast and very reliable. Therefore, an appropriate real-time computing language would need to be chosen. Additionally, to satisfy the need for reliability the choice could be made to have multiple redundant and independently produced copies of the program, and to run these copies on independent hardware while cross-checking results.
",Software architecture is about making fundamental structural choices that are costly to change once implemented. Software architecture choices include specific structural options from possibilities in the design of the software.,"[' What is software architecture about making fundamental structural choices that are costly to change once implemented?', ' What are software architecture choices?']","['Software architecture choices include specific structural options from possibilities in the design of the software', 'specific structural options from possibilities in the design of the software']"
1658,software architecture,Summary,"Documenting software architecture facilitates communication between stakeholders, captures early decisions about the high-level design, and allows reuse of design components between projects.: 29–35 ","Documenting software architecture facilitates communication between stakeholders, captures early decisions about the high-level design, and allows reuse of design components between projects. : 29–35","[' Documenting software architecture facilitates communication between stakeholders, captures early decisions about the high-level design and allows reuse of what?']",['design components between projects']
1659,software architecture,Scope,"There is no sharp distinction between software architecture versus design and requirements engineering (see Related fields below). They are all part of a ""chain of intentionality"" from high-level intentions to low-level details.: 18 ","There is no sharp distinction between software architecture versus design and requirements engineering (see Related fields below). They are all part of a ""chain of intentionality"" from high-level intentions to low-level details.","[' What is the difference between software architecture and design and requirements engineering?', ' What is a chain of intentionality?']","['no sharp distinction', 'high-level intentions to low-level details']"
1660,software architecture,Characteristics,"Multitude of stakeholders: software systems have to cater to a variety of stakeholders such as business managers, owners, users, and operators. These stakeholders all have their own concerns with respect to the system. Balancing these concerns and demonstrating that they are addressed is part of designing the system.: 29–31  This implies that architecture involves dealing with a broad variety of concerns and stakeholders, and has a multidisciplinary nature.
","Multitude of stakeholders: software systems have to cater to a variety of stakeholders such as business managers, owners, users, and operators. These stakeholders all have their own concerns with respect to the system.","[' What do software systems have to cater to?', ' What do business managers, owners, users, and operators have?']","['Multitude of stakeholders', 'Multitude of stakeholders']"
1661,software architecture,Characteristics,"Separation of concerns: the established way for architects to reduce complexity is to separate the concerns that drive the design. Architecture documentation shows that all stakeholder concerns are addressed by modeling and describing the architecture from separate points of view associated with the various stakeholder concerns. These separate descriptions are called architectural views (see for example the 4+1 architectural view model).
",Separation of concerns: the established way for architects to reduce complexity is to separate the concerns that drive the design. Architecture documentation shows that all stakeholder concerns are addressed by modeling and describing the architecture from separate points of view associated with the various stakeholder concerns.,"[' What is the established way for architects to reduce complexity?', ' What shows that all stakeholder concerns are addressed by modeling and describing the architecture from separate points of view?']","['to separate the concerns that drive the design', 'Architecture documentation']"
1662,software architecture,Characteristics,"Quality-driven: classic software design approaches (e.g. Jackson Structured Programming) were driven by required functionality and the flow of data through the system, but the current insight: 26–28  is that the architecture of a software system is more closely related to its quality attributes such as fault-tolerance, backward compatibility, extensibility, reliability, maintainability, availability, security, usability, and other such –ilities. Stakeholder concerns often translate into requirements on these quality attributes, which are variously called non-functional requirements, extra-functional requirements, behavioral requirements, or quality attribute requirements.
","Quality-driven: classic software design approaches (e.g. Jackson Structured Programming) were driven by required functionality and the flow of data through the system, but the current insight: 26–28  is that the architecture of a software system is more closely related to its quality attributes such as fault-tolerance, backward compatibility, extensibility, reliability, maintainability, availability, security, usability, and other such –ilities.","[' What type of software design approach was driven by required functionality and the flow of data through a system?', ' What is the current insight?', ' What are some quality attributes such as fault-tolerance, backward compatibility and extensibility?']","['Quality-driven', 'the architecture of a software system is more closely related to its quality attributes', 'reliability, maintainability, availability, security, usability']"
1663,software architecture,Characteristics,"Recurring styles: like building architecture, the software architecture discipline has developed standard ways to address recurring concerns. These ""standard ways"" are called by various names at various levels of abstraction. Common terms for recurring solutions are architectural style,: 273–277  tactic,: 70–72  reference architecture and architectural pattern.: 203–205 ","Recurring styles: like building architecture, the software architecture discipline has developed standard ways to address recurring concerns. These ""standard ways"" are called by various names at various levels of abstraction.","[' What has the software architecture discipline developed to address recurring concerns?', ' What are the ""standard ways"" called?']","['standard ways', 'various names at various levels of abstraction']"
1664,software architecture,Characteristics,"Conceptual integrity: a term introduced by Fred Brooks in his 1975 book The Mythical Man-Month to denote the idea that the architecture of a software system represents an overall vision of what it should do and how it should do it. This vision should be separated from its implementation. The architect assumes the role of ""keeper of the vision"", making sure that additions to the system are in line with the architecture, hence preserving conceptual integrity.: 41–50 ",Conceptual integrity: a term introduced by Fred Brooks in his 1975 book The Mythical Man-Month to denote the idea that the architecture of a software system represents an overall vision of what it should do and how it should do it. This vision should be separated from its implementation.,"[' Who introduced the term conceptual integrity?', "" What was the name of Fred Brooks' 1975 book?"", ' Who wrote The Mythical Man-Month?', ' How should the vision of a software system be separated?']","['Fred Brooks', 'The Mythical Man-Month', 'Fred Brooks', 'from its implementation']"
1665,software architecture,Characteristics,"Cognitive constraints: an observation first made in a 1967 paper by computer programmer Melvin Conway that organizations which design systems are constrained to produce designs which are copies of the communication structures of these organizations. As with conceptual integrity, it was Fred Brooks who introduced it to a wider audience when he cited the paper and the idea in his elegant classic The Mythical Man-Month, calling it ""Conway's Law.""
","Cognitive constraints: an observation first made in a 1967 paper by computer programmer Melvin Conway that organizations which design systems are constrained to produce designs which are copies of the communication structures of these organizations. As with conceptual integrity, it was Fred Brooks who introduced it to a wider audience when he cited the paper and the idea in his elegant classic The Mythical Man-Month, calling it ""Conway's Law.""","[' What was the name of the computer programmer who first observed cognitive constraints?', ' What did Melvin Conway say organizations that design systems are constrained to produce?', ' Who introduced cognitive constraints to a wider audience?', ' Who was the author of The Mythical Man-Month?', "" What did Fred Brooks call Conway's Law?""]","['Melvin Conway', 'designs which are copies of the communication structures of these organizations', 'Fred Brooks', 'Fred Brooks', 'Cognitive constraints']"
1666,software architecture,Motivation,"Software architecture is an ""intellectually graspable"" abstraction of a complex system.: 5–6  This abstraction provides a number of benefits:
","Software architecture is an ""intellectually graspable"" abstraction of a complex system. : 5–6  This abstraction provides a number of benefits:","[' What is software architecture?', ' What is an abstract of a complex system?']","['an ""intellectually graspable"" abstraction of a complex system', 'Software architecture']"
1667,software architecture,History,"The comparison between software design and (civil) architecture was first drawn in the late 1960s, but the term ""software architecture"" did not see widespread usage until the 1990s. The field of computer science had encountered problems associated with complexity since its formation. Earlier problems of complexity were solved by developers by choosing the right data structures, developing algorithms, and by applying the concept of separation of concerns. Although the term ""software architecture"" is relatively new to the industry, the fundamental principles of the field have been applied sporadically by software engineering pioneers since the mid-1980s. Early attempts to capture and explain software architecture of a system were imprecise and disorganized, often characterized by a set of box-and-line diagrams.","The comparison between software design and (civil) architecture was first drawn in the late 1960s, but the term ""software architecture"" did not see widespread usage until the 1990s. The field of computer science had encountered problems associated with complexity since its formation.","[' When was the comparison between software design and (civil) architecture first drawn?', ' When did the term ""software architecture"" see widespread use?', ' The field of computer science had encountered problems associated with what?']","['late 1960s', '1990s', 'complexity']"
1668,software architecture,History,"Software architecture as a concept has its origins in the research of Edsger Dijkstra in 1968 and David Parnas in the early 1970s. These scientists emphasized that the structure of a software system matters and getting the structure right is critical. During the 1990s there was a concerted effort to define and codify fundamental aspects of the discipline, with research work concentrating on architectural styles (patterns), architecture description languages, architecture documentation, and formal methods.",Software architecture as a concept has its origins in the research of Edsger Dijkstra in 1968 and David Parnas in the early 1970s. These scientists emphasized that the structure of a software system matters and getting the structure right is critical.,"[' In what year did Edsger Dijkstra and David Parnas first study software architecture?', ' What was the name of the scientist who first studied software architecture in 1968?', ' In what decade did David Parnes first work on software architecture as a concept?', ' How important was the importance of the structure of a software system?']","['1968', 'Edsger Dijkstra', '1970s', 'getting the structure right is critical']"
1669,software architecture,History,"Research institutions have played a prominent role in furthering software architecture as a discipline. Mary Shaw and David Garlan of Carnegie Mellon wrote a book titled Software Architecture: Perspectives on an Emerging Discipline in 1996, which promoted software architecture concepts such as components, connectors, and styles. The University of California, Irvine's Institute for Software Research's efforts in software architecture research is directed primarily in architectural styles, architecture description languages, and dynamic architectures.
","Research institutions have played a prominent role in furthering software architecture as a discipline. Mary Shaw and David Garlan of Carnegie Mellon wrote a book titled Software Architecture: Perspectives on an Emerging Discipline in 1996, which promoted software architecture concepts such as components, connectors, and styles.",[' In what year did Mary Shaw and David Garlan publish Software Architecture: Perspectives on an Emerging Discipline?'],['1996']
1670,software architecture,History,"IEEE 1471-2000, ""Recommended Practice for Architecture Description of Software-Intensive Systems"", was the first formal standard in the area of software architecture. It was adopted in 2007 by ISO as ISO/IEC 42010:2007. In November 2011, IEEE 1471–2000 was superseded by ISO/IEC/IEEE 42010:2011, ""Systems and software engineering – Architecture description"" (jointly published by IEEE and ISO).","IEEE 1471-2000, ""Recommended Practice for Architecture Description of Software-Intensive Systems"", was the first formal standard in the area of software architecture. It was adopted in 2007 by ISO as ISO/IEC 42010:2007.","[' What was the first formal standard in the area of software architecture?', ' When was ISO/IEC 42010:2007 adopted?', ' What is IEEE 1471-2000?']","['IEEE 1471-2000, ""Recommended Practice for Architecture Description of Software-Intensive Systems"",', '2007', 'Recommended Practice for Architecture Description of Software-Intensive Systems']"
1671,software architecture,History,"While in IEEE 1471, software architecture was about the architecture of ""software-intensive systems"", defined as ""any system where software contributes essential influences to the design, construction, deployment, and evolution of the system as a whole"", the 2011 edition goes a step further by including the ISO/IEC 15288 and ISO/IEC 12207 definitions of a system, which embrace not only hardware and software, but also ""humans, processes, procedures, facilities, materials and naturally occurring entities"". This reflects the relationship between software architecture, enterprise architecture and solution architecture.
","While in IEEE 1471, software architecture was about the architecture of ""software-intensive systems"", defined as ""any system where software contributes essential influences to the design, construction, deployment, and evolution of the system as a whole"", the 2011 edition goes a step further by including the ISO/IEC 15288 and ISO/IEC 12207 definitions of a system, which embrace not only hardware and software, but also ""humans, processes, procedures, facilities, materials and naturally occurring entities"". This reflects the relationship between software architecture, enterprise architecture and solution architecture.","[' What was the definition of ""software-intensive systems"" in IEEE 1471?', ' What is a software-intensive system defined as?', ' In what year was the ISO/IEC 15288 included?', ' What definitions of a system include not only hardware and software, but also ""humans, processes, procedures, facilities, materials and naturally occurring entities""?', ' What is the relationship between software architecture, enterprise architecture and solution architecture?']","['any system where software contributes essential influences to the design, construction, deployment, and evolution of the system as a whole"",', 'any system where software contributes essential influences to the design, construction, deployment, and evolution of the system as a whole', '2011', 'ISO/IEC 15288 and ISO/IEC 12207', 'ISO/IEC 15288 and ISO/IEC 12207 definitions of a system, which embrace not only hardware and software, but also ""humans, processes, procedures, facilities, materials and naturally occurring entities"".']"
1672,software architecture,Architecture activities,"There are many activities that a software architect performs. A software architect typically works with project managers, discusses architecturally significant requirements with stakeholders, designs a software architecture, evaluates a design, communicates with designers and stakeholders, documents the architectural design and more. There are four core activities in software architecture design. These core architecture activities are performed iteratively and at different stages of the initial software development life-cycle, as well as over the evolution of a system.
","There are many activities that a software architect performs. A software architect typically works with project managers, discusses architecturally significant requirements with stakeholders, designs a software architecture, evaluates a design, communicates with designers and stakeholders, documents the architectural design and more.","[' What do software architects typically work with?', ' What does a software architect typically discuss with stakeholders?', ' A software architect evaluates what?']","['project managers', 'architecturally significant requirements', 'a design']"
1673,software architecture,Architecture activities,"Architectural analysis is the process of understanding the environment in which a proposed system will operate and determining the requirements for the system. The input or requirements to the analysis activity can come from any number of stakeholders and include items such as:
",Architectural analysis is the process of understanding the environment in which a proposed system will operate and determining the requirements for the system. The input or requirements to the analysis activity can come from any number of stakeholders and include items such as:,"[' What is the process of understanding the environment in which a proposed system will operate?', ' What can the input or requirements to the analysis activity come from?']","['Architectural analysis', 'any number of stakeholders']"
1674,software architecture,Architecture activities,"Architectural synthesis or design is the process of creating an architecture. Given the architecturally significant requirements determined by the analysis, the current state of the design and the results of any evaluation activities, the design is created and improved.: 311–326 ","Architectural synthesis or design is the process of creating an architecture. Given the architecturally significant requirements determined by the analysis, the current state of the design and the results of any evaluation activities, the design is created and improved.","[' What is the process of creating an architecture called?', ' What is an architectural synthesis?', ' How is a design created?']","['Architectural synthesis or design', 'design is the process of creating an architecture', 'Given the architecturally significant requirements determined by the analysis, the current state of the design and the results of any evaluation activities']"
1675,software architecture,Architecture activities,"Architecture evaluation is the process of determining how well the current design or a portion of it satisfies the requirements derived during analysis. An evaluation can occur whenever an architect is considering a design decision, it can occur after some portion of the design has been completed, it can occur after the final design has been completed or it can occur after the system has been constructed. Some of the available software architecture evaluation techniques include Architecture Tradeoff Analysis Method (ATAM) and TARA. Frameworks for comparing the techniques are discussed in frameworks such as SARA Report and Architecture Reviews: Practice and Experience.","Architecture evaluation is the process of determining how well the current design or a portion of it satisfies the requirements derived during analysis. An evaluation can occur whenever an architect is considering a design decision, it can occur after some portion of the design has been completed, it can occur after the final design has been completed or it can occur after the system has been constructed.","[' What is the process of determining how well the current design satisfies the requirements derived during analysis?', ' When can an evaluation occur?', ' What can happen after some portion of the design has been completed?', ' What can occur after the final design has been completed?', ' What can happen after the system has been constructed?']","['Architecture evaluation', 'whenever an architect is considering a design decision', 'An evaluation', 'An evaluation', 'evaluation']"
1676,software architecture,Architecture activities,"Architecture evolution is the process of maintaining and adapting an existing software architecture to meet changes in requirements and environment. As software architecture provides a fundamental structure of a software system, its evolution and maintenance would necessarily impact its fundamental structure. As such, architecture evolution is concerned with adding new functionality as well as maintaining existing functionality and system behavior.
","Architecture evolution is the process of maintaining and adapting an existing software architecture to meet changes in requirements and environment. As software architecture provides a fundamental structure of a software system, its evolution and maintenance would necessarily impact its fundamental structure.","[' What is the process of maintaining and adapting an existing software architecture to meet changes in requirements and environment?', ' What provides a fundamental structure of a software system?']","['Architecture evolution', 'software architecture']"
1677,software architecture,Architecture activities,"Architecture requires critical supporting activities. These supporting activities take place throughout the core software architecture process. They include knowledge management and communication, design reasoning and decision making, and documentation.
",Architecture requires critical supporting activities. These supporting activities take place throughout the core software architecture process.,"[' What does architecture require?', ' Where do these supporting activities take place?']","['critical supporting activities', 'throughout the core software architecture process']"
1678,software engineering,Summary,"A software engineer is a person who applies the principles of software engineering to design, develop, maintain, test, and evaluate computer software. The term programmer is sometimes used as a synonym, but may also lack connotations of engineering education or skills.
","A software engineer is a person who applies the principles of software engineering to design, develop, maintain, test, and evaluate computer software. The term programmer is sometimes used as a synonym, but may also lack connotations of engineering education or skills.","[' What is a software engineer?', ' What is the term programmer sometimes used as?']","['a person who applies the principles of software engineering to design, develop, maintain, test, and evaluate computer software', 'a synonym']"
1679,software engineering,Summary,"Engineering techniques are used to inform the software development process which involves the definition, implementation, assessment, measurement, management, change, and improvement of the software life cycle process itself. It heavily uses software configuration management which is about systematically controlling changes to the configuration, and maintaining the integrity and traceability of the configuration and code throughout the system life cycle. Modern processes use software versioning.
","Engineering techniques are used to inform the software development process which involves the definition, implementation, assessment, measurement, management, change, and improvement of the software life cycle process itself. It heavily uses software configuration management which is about systematically controlling changes to the configuration, and maintaining the integrity and traceability of the configuration and code throughout the system life cycle.","[' What techniques are used to inform the software development process?', ' What is software configuration management about?', ' What is the main goal of maintaining the integrity of the configuration and code throughout the system life cycle?']","['Engineering techniques', 'systematically controlling changes to the configuration, and maintaining the integrity and traceability of the configuration and code throughout the system life cycle', 'systematically controlling changes to the configuration']"
1680,software engineering,History,"Beginning in the 1960s, software engineering was seen as its own type of engineering. Additionally, the development of software engineering was seen as a struggle. It was difficult to keep up with the hardware which caused many problems for software engineers. Problems included software that was over budget, exceeded deadlines, required extensive de-bugging and maintenance, and unsuccessfully met the needs of consumers or was never even completed. In 1968 NATO held the first Software Engineering conference where issues related to software were addressed: guidelines and best practices for the development of software were established. ","Beginning in the 1960s, software engineering was seen as its own type of engineering. Additionally, the development of software engineering was seen as a struggle.","[' When did software engineering begin to be seen as its own type of engineering?', ' What was also seen as a struggle?']","['1960s', 'the development of software engineering']"
1681,software engineering,History,"The origins of the term ""software engineering"" have been attributed to various sources. The term ""software engineering"" appeared in a list of services offered by companies in the June 1965 issue of COMPUTERS and AUTOMATION and was used more formally in the August 1966 issue of Communications of the ACM (Volume 9, number 8) “letter to the ACM membership” by the ACM President Anthony A. Oettinger,  it is also associated with the title of a NATO conference in 1968 by Professor Friedrich L. Bauer, the first conference on software engineering. Margaret Hamilton described the discipline ""software engineering"" during the Apollo missions to give what they were doing legitimacy.  At the time there was perceived to be a ""software crisis"". The 40th International Conference on Software Engineering (ICSE 2018) celebrates 50 years of ""Software Engineering"" with the Plenary Sessions' keynotes of Frederick Brooks and Margaret Hamilton.","The origins of the term ""software engineering"" have been attributed to various sources. The term ""software engineering"" appeared in a list of services offered by companies in the June 1965 issue of COMPUTERS and AUTOMATION and was used more formally in the August 1966 issue of Communications of the ACM (Volume 9, number 8) “letter to the ACM membership” by the ACM President Anthony A. Oettinger,  it is also associated with the title of a NATO conference in 1968 by Professor Friedrich L. Bauer, the first conference on software engineering.","[' What term has been attributed to various sources?', ' When did the term ""software engineering"" appear in a list of services offered by companies?', ' What was the term used more formally in the August 1966 issue of Communications of the ACM?', ' What was the name of the first NATO conference held in 1968?', ' Who was the ACM President in 1966?', ' What was the name of the first conference on software engineering?']","['software engineering', 'June 1965', 'software engineering', 'Professor Friedrich L. Bauer', 'Anthony A. Oettinger', 'NATO conference in 1968 by Professor Friedrich L. Bauer']"
1682,software engineering,History,"In 1984, the Software Engineering Institute (SEI) was established as a federally funded research and development center headquartered on the campus of Carnegie Mellon University in Pittsburgh, Pennsylvania, United States. Watts Humphrey founded the SEI Software Process Program, aimed at understanding and managing the software engineering process.  The Process Maturity Levels introduced would become the Capability Maturity Model Integration for Development(CMMI-DEV), which has defined how the US Government evaluates the abilities of a software development team.
","In 1984, the Software Engineering Institute (SEI) was established as a federally funded research and development center headquartered on the campus of Carnegie Mellon University in Pittsburgh, Pennsylvania, United States. Watts Humphrey founded the SEI Software Process Program, aimed at understanding and managing the software engineering process.","[' In what year was the Software Engineering Institute established?', ' Where is the SEI headquartered?', ' Who founded the Software Process Program?']","['1984', 'Carnegie Mellon University', 'Watts Humphrey']"
1683,software engineering,History,"Modern, generally accepted best-practices for software engineering have been collected by the ISO/IEC JTC 1/SC 7 subcommittee and published as the Software Engineering Body of Knowledge (SWEBOK). Software engineering is considered one of major computing disciplines.","Modern, generally accepted best-practices for software engineering have been collected by the ISO/IEC JTC 1/SC 7 subcommittee and published as the Software Engineering Body of Knowledge (SWEBOK). Software engineering is considered one of major computing disciplines.","[' What is the name of the ISO/IEC JTC 1/SC 7 subcommittee that has collected the best practices for software engineering?', ' What does SWEBOK stand for?', ' Software engineering is considered one of what major computing disciplines?']","['Software Engineering Body of Knowledge', 'Software Engineering Body of Knowledge', 'Software Engineering Body of Knowledge']"
1684,software engineering,Education,"Knowledge of computer programming is a prerequisite for becoming a software engineer. In 2004 the IEEE Computer Society produced the SWEBOK, which has been published as ISO/IEC Technical Report 1979:2005, describing the body of knowledge that they recommend to be mastered by a graduate software engineer with four years of experience.
Many software engineers enter the profession by obtaining a university degree or training at a vocational school. One standard international curriculum for undergraduate software engineering degrees was defined by the Joint Task Force on Computing Curricula of the IEEE Computer Society and the Association for Computing Machinery, and updated in 2014. A number of universities have Software Engineering degree programs; as of 2010, there were 244 Campus Bachelor of Software Engineering programs, 70 Online programs, 230 Masters-level programs, 41 Doctorate-level programs, and 69 Certificate-level programs in the United States.
","Knowledge of computer programming is a prerequisite for becoming a software engineer. In 2004 the IEEE Computer Society produced the SWEBOK, which has been published as ISO/IEC Technical Report 1979:2005, describing the body of knowledge that they recommend to be mastered by a graduate software engineer with four years of experience.","[' What is a prerequisite for becoming a software engineer?', ' When was the SWEBOK produced?', ' What is the name of the document published by the IEEE Computer Society?', ' How many years of experience does a graduate software engineer need to master?', ' How many years of experience did a graduate software engineer have?']","['Knowledge of computer programming', '2004', 'SWEBOK', 'four', 'four']"
1685,software engineering,Education,"In addition to university education, many companies sponsor internships for students wishing to pursue careers in information technology. These internships can introduce the student to interesting real-world tasks that typical software engineers encounter every day. Similar experience can be gained through military service in software engineering.
","In addition to university education, many companies sponsor internships for students wishing to pursue careers in information technology. These internships can introduce the student to interesting real-world tasks that typical software engineers encounter every day.","[' What do many companies sponsor for students wishing to pursue careers in information technology?', ' What can internships introduce the student to?']","['internships', 'interesting real-world tasks']"
1686,software engineering,Profession,"Legal requirements for the licensing or certification of professional software engineers vary around the world. In the UK, there is no licensing or legal requirement to assume or use the job title Software Engineer.  In some areas of Canada, such as Alberta, British Columbia, Ontario, and Quebec, software engineers can hold the Professional Engineer (P.Eng) designation and/or the Information Systems Professional (I.S.P.) designation. In Europe, Software Engineers can obtain the European Engineer (EUR ING) professional title.
","Legal requirements for the licensing or certification of professional software engineers vary around the world. In the UK, there is no licensing or legal requirement to assume or use the job title Software Engineer.","[' What are the legal requirements for the licensing or certification of professional software engineers?', ' Where is there no licensing or legal requirement to assume or use the job title Software Engineer?']","['vary around the world', 'the UK']"
1687,software engineering,Profession,"The United States, since 2013, has offered an NCEES Professional Engineer exam for Software Engineering, thereby allowing Software Engineers to be licensed and recognized. NCEES will end the exam after April 2019 due to lack of participation. Mandatory licensing is currently still largely debated, and perceived as controversial. In some parts of the US such as Texas, the use of the term Engineer is regulated by law and reserved only for use by individuals who have a Professional Engineer license.","The United States, since 2013, has offered an NCEES Professional Engineer exam for Software Engineering, thereby allowing Software Engineers to be licensed and recognized. NCEES will end the exam after April 2019 due to lack of participation.","[' Since what year has the United States offered an NCEES Professional Engineer exam for Software Engineering?', ' What is the name of the exam offered by the US government for Software Engineers?', ' After what date will the exam be discontinued?']","['2013', 'NCEES Professional Engineer', 'April 2019']"
1688,software engineering,Profession,"The IEEE Computer Society and the ACM, the two main US-based professional organizations of software engineering, publish guides to the profession of software engineering. The IEEE's Guide to the Software Engineering Body of Knowledge – 2004 Version, or SWEBOK, defines the field and describes the knowledge the IEEE expects a practicing software engineer to have. The most current SWEBOK v3 is an updated version and was released in 2014. The IEEE also promulgates a ""Software Engineering Code of Ethics"".","The IEEE Computer Society and the ACM, the two main US-based professional organizations of software engineering, publish guides to the profession of software engineering. The IEEE's Guide to the Software Engineering Body of Knowledge – 2004 Version, or SWEBOK, defines the field and describes the knowledge the IEEE expects a practicing software engineer to have.","[' What are the two main US-based professional organizations of software engineering?', ' What defines the field and describes the knowledge the IEEE expects a person to have?', ' What does the IEEE expect a practicing software engineer to have?']","['The IEEE Computer Society and the ACM', ""The IEEE's Guide to the Software Engineering Body of Knowledge"", 'knowledge']"
1689,software engineering,Criticism,"Software engineering sees its practitioners as individuals who follow well-defined engineering approaches to problem-solving. These approaches are specified in various software engineering books and research papers, always with the connotations of predictability, precision, mitigated risk and professionalism. This perspective has led to calls for licensing, certification and codified bodies of knowledge as mechanisms for spreading the engineering knowledge and maturing the field.
","Software engineering sees its practitioners as individuals who follow well-defined engineering approaches to problem-solving. These approaches are specified in various software engineering books and research papers, always with the connotations of predictability, precision, mitigated risk and professionalism.","[' What does software engineering see its practitioners as?', ' What are well-defined engineering approaches to problem-solving described in?']","['individuals who follow well-defined engineering approaches to problem-solving', 'software engineering books and research papers']"
1690,software engineering,Criticism,"Software engineering extends engineering and draws on the engineering model, i.e. engineering process, engineering project management, engineering requirements, engineering design, engineering construction, and engineering validation. The concept is so new that it is rarely understood, and it is widely misinterpreted, including in software engineering textbooks, papers, and among the communities of programmers and crafters.
","Software engineering extends engineering and draws on the engineering model, i.e. engineering process, engineering project management, engineering requirements, engineering design, engineering construction, and engineering validation.","[' Software engineering extends engineering and draws on what?', ' Software engineering draws on engineering model, i.e. engineering process, engineering project management, and what else?']","['the engineering model', 'engineering validation']"
1691,software engineering,Criticism,"
A number of these phenomena have been bundled under the name ""Software Engineering"". As economics is known as ""The Miserable Science"", software engineering should be known as ""The Doomed Discipline"", doomed because it cannot even approach its goal since its goal is self-contradictory. Software engineering, of course, presents itself as another worthy cause, but that is eyewash: if you carefully read its literature and analyse what its devotees actually do, you will discover that software engineering has accepted as its charter ""How to program if you cannot.""","
A number of these phenomena have been bundled under the name ""Software Engineering"". As economics is known as ""The Miserable Science"", software engineering should be known as ""The Doomed Discipline"", doomed because it cannot even approach its goal since its goal is self-contradictory.","[' What is the economics of software engineering known as?', ' What is software engineering called because it cannot even approach its goal?']","['The Miserable Science', 'The Doomed Discipline']"
1692,signature scheme,Summary,"A digital signature is a mathematical scheme for verifying the authenticity of digital messages or documents. A valid digital signature, where the prerequisites are satisfied, gives a recipient very strong reason to believe that the message was created by a known sender (authenticity), and that the message was not altered in transit (integrity).","A digital signature is a mathematical scheme for verifying the authenticity of digital messages or documents. A valid digital signature, where the prerequisites are satisfied, gives a recipient very strong reason to believe that the message was created by a known sender (authenticity), and that the message was not altered in transit (integrity).","[' What is a mathematical scheme for verifying the authenticity of digital messages or documents?', ' A valid digital signature gives a recipient very strong reason to believe that the message was created by who?', ' What is known sender (authenticity) and that the message was not altered in transit (integrity)?']","['A digital signature', 'a known sender', 'A valid digital signature']"
1693,signature scheme,Summary,"Digital signatures are often used to implement electronic signatures, which includes any electronic data that carries the intent of a signature, but not all electronic signatures use digital signatures. Electronic signatures have legal significance in some countries, including Canada, South Africa, the United States, Algeria, Turkey, India, Brazil, Indonesia, Mexico, Saudi Arabia, Uruguay, Switzerland, Chile and the countries of the European Union.","Digital signatures are often used to implement electronic signatures, which includes any electronic data that carries the intent of a signature, but not all electronic signatures use digital signatures. Electronic signatures have legal significance in some countries, including Canada, South Africa, the United States, Algeria, Turkey, India, Brazil, Indonesia, Mexico, Saudi Arabia, Uruguay, Switzerland, Chile and the countries of the European Union.","[' What are often used to implement electronic signatures?', ' What includes any electronic data that carries the intent of a signature?', ' What countries are in the European Union?', ' What country is in the United States?']","['Digital signatures', 'electronic signatures', 'Canada, South Africa, the United States, Algeria, Turkey, India, Brazil, Indonesia, Mexico, Saudi Arabia, Uruguay, Switzerland, Chile', 'Algeria']"
1694,signature scheme,Summary,"Digital signatures employ asymmetric cryptography. In many instances, they provide a layer of validation and security to messages sent through a non-secure channel: Properly implemented, a digital signature gives the receiver reason to believe the message was sent by the claimed sender. Digital signatures are equivalent to traditional handwritten signatures in many respects, but properly implemented digital signatures are more difficult to forge than the handwritten type. Digital signature schemes, in the sense used here, are cryptographically based, and must be implemented properly to be effective. They can also provide non-repudiation, meaning that the signer cannot successfully claim they did not sign a message, while also claiming their private key remains secret. Further, some non-repudiation schemes offer a timestamp for the digital signature, so that even if the private key is exposed, the signature is valid. Digitally signed messages may be anything representable as a bitstring: examples include electronic mail, contracts, or a message sent via some other cryptographic protocol.
","Digital signatures employ asymmetric cryptography. In many instances, they provide a layer of validation and security to messages sent through a non-secure channel: Properly implemented, a digital signature gives the receiver reason to believe the message was sent by the claimed sender.","[' What type of cryptography do digital signatures employ?', ' What does asymmetric cryptography provide to messages sent through a non-secure channel?', ' Properly implemented, what gives the receiver reason to believe the message was sent by the claimed sender?']","['asymmetric', 'a layer of validation and security', 'Digital signatures employ asymmetric cryptography. In many instances, they provide a layer of validation and security to messages sent through a non-secure channel: Properly implemented, a digital signature']"
1695,signature scheme,Definition,"Two main properties are required. First, the authenticity of a signature generated from a fixed message and fixed private key can be verified by using the corresponding public key. Secondly, it should be computationally infeasible to generate a valid signature for a party without knowing that party's private key.
A digital signature is an authentication mechanism that enables the creator of the message to attach a code that acts as a signature.
The Digital Signature Algorithm (DSA), developed by the National Institute of Standards and Technology, is one of many examples of a signing algorithm.
","Two main properties are required. First, the authenticity of a signature generated from a fixed message and fixed private key can be verified by using the corresponding public key.","[' How many main properties are required?', ' How can the authenticity of a signature generated from a fixed message and fixed private key be verified by using the corresponding public key?']","['Two', 'First']"
1696,signature scheme,History,"In 1976, Whitfield Diffie and Martin Hellman first described the notion of a digital signature scheme, although they only conjectured that such schemes existed based on functions that are trapdoor one-way permutations.  Soon afterwards, Ronald Rivest, Adi Shamir, and Len Adleman invented the RSA algorithm, which could be used to produce primitive digital signatures (although only as a proof-of-concept – ""plain"" RSA signatures are not secure). The first widely marketed software package to offer digital signature was Lotus Notes 1.0, released in 1989, which used the RSA algorithm.","In 1976, Whitfield Diffie and Martin Hellman first described the notion of a digital signature scheme, although they only conjectured that such schemes existed based on functions that are trapdoor one-way permutations. Soon afterwards, Ronald Rivest, Adi Shamir, and Len Adleman invented the RSA algorithm, which could be used to produce primitive digital signatures (although only as a proof-of-concept – ""plain"" RSA signatures are not secure).","[' When did Whitfield Diffie and Martin Hellman first describe the concept of a digital signature scheme?', ' Who invented the RSA algorithm?', ' What did Ronald Rivest, Adi Shamir and Len Adleman invent?', ' What could be used to produce primitive digital signatures?']","['1976', 'Ronald Rivest, Adi Shamir, and Len Adleman', 'the RSA algorithm', 'RSA algorithm']"
1697,signature scheme,History,"In 1988, Shafi Goldwasser, Silvio Micali, and Ronald Rivest became the first to rigorously define the security requirements of digital signature schemes. They described a hierarchy of attack models for signature schemes, and also presented the GMR signature scheme, the first that could be proved to prevent even an existential forgery against a chosen message attack, which is the currently accepted security definition for signature schemes.  The first such scheme which is not built on trapdoor functions but rather on a family of function with a much weaker required property of one-way permutation was presented by Moni Naor and Moti Yung.","In 1988, Shafi Goldwasser, Silvio Micali, and Ronald Rivest became the first to rigorously define the security requirements of digital signature schemes. They described a hierarchy of attack models for signature schemes, and also presented the GMR signature scheme, the first that could be proved to prevent even an existential forgery against a chosen message attack, which is the currently accepted security definition for signature schemes.","[' Who were the first to rigorously define the security requirements of digital signature schemes?', ' What did Shafi Goldwasser, Silvio Micali, and Ronald Rivest describe?', ' The GMR signature scheme was the first that could prove to prevent what?', ' What is the current accepted security definition for signature schemes?']","['Shafi Goldwasser, Silvio Micali, and Ronald Rivest', 'a hierarchy of attack models for signature schemes', 'an existential forgery', 'GMR signature scheme']"
1698,signature scheme,Method,"One digital signature scheme (of many) is based on RSA. To create signature keys, generate an RSA key pair containing a modulus, N, that is the product of two random secret distinct large primes, along with integers, e and d, such that e d ≡ 1 (mod φ(N)), where φ is the Euler's totient function. The signer's public key consists of N and e, and the signer's secret key contains d.
","One digital signature scheme (of many) is based on RSA. To create signature keys, generate an RSA key pair containing a modulus, N, that is the product of two random secret distinct large primes, along with integers, e and d, such that e d ≡ 1 (mod φ(N)), where φ is the Euler's totient function.","[' How many digital signature schemes are based on RSA?', ' What is the modulus of an RSA key pair?', ' How many random secret distinct large primes are present in e and d?']","['many', 'N', 'two']"
1699,signature scheme,Method,"To sign a message, m, the signer computes a signature, σ, such that σ ≡  md (mod N). To verify, the receiver checks that σe ≡ m (mod N).
","To sign a message, m, the signer computes a signature, σ, such that σ ≡  md (mod N). To verify, the receiver checks that σe ≡ m (mod N).","[' What does the signer compute to sign a message?', ' To verify, what does the receiver check?']","['σ', 'σe\xa0≡\xa0m\xa0(mod\xa0N).']"
1700,signature scheme,Method,"Several early signature schemes were of a similar type: they involve the use of a trapdoor permutation, such as the RSA function, or in the case of the Rabin signature scheme, computing square modulo composite, N. A trapdoor permutation family is a family of permutations, specified by a parameter, that is easy to compute in the forward direction, but is difficult to compute in the reverse direction without already knowing the private key (""trapdoor"").  Trapdoor permutations can be used for digital signature schemes, where computing the reverse direction with the secret key is required for signing, and computing the forward direction is used to verify signatures.
","Several early signature schemes were of a similar type: they involve the use of a trapdoor permutation, such as the RSA function, or in the case of the Rabin signature scheme, computing square modulo composite, N. A trapdoor permutation family is a family of permutations, specified by a parameter, that is easy to compute in the forward direction, but is difficult to compute in the reverse direction without already knowing the private key (""trapdoor""). Trapdoor permutations can be used for digital signature schemes, where computing the reverse direction with the secret key is required for signing, and computing the forward direction is used to verify signatures.","[' What is a trapdoor permutation?', ' What is the Rabin signature scheme?', ' How are permutations specified?', ' What is a family of permutations specified by a parameter?', ' What is difficult to compute in the forward direction without already knowing the private key?', ' What is required for signing?', ' What is used to verify signatures?']","['a family of permutations', 'computing square modulo composite,\xa0N', 'by a parameter', 'A trapdoor permutation family', 'trapdoor permutation family', 'computing the reverse direction with the secret key', 'computing the forward direction']"
1701,signature scheme,Method,"Used directly, this type of signature scheme is vulnerable to key-only existential forgery attack. To create a forgery, the attacker picks a random signature σ and uses the verification procedure to determine the message, m, corresponding to that signature. In practice, however, this type of signature is not used directly, but rather, the message to be signed is first hashed to produce a short digest, that is then padded to larger width comparable to N, then signed with the reverse trapdoor function. This forgery attack, then, only produces the padded hash function output that corresponds to σ, but not a message that leads to that value, which does not lead to an attack. In the random oracle model, hash-then-sign (an idealized version of that practice where hash and padding combined have close to N possible outputs), this form of signature is existentially unforgeable, even against a chosen-plaintext attack.","Used directly, this type of signature scheme is vulnerable to key-only existential forgery attack. To create a forgery, the attacker picks a random signature σ and uses the verification procedure to determine the message, m, corresponding to that signature.","[' What type of attack is this type of signature scheme vulnerable to?', ' To create a forgery, the attacker picks a random signature and uses what procedure to determine the message corresponding to it?']","['key-only existential forgery attack', 'verification']"
1702,signature scheme,Applications,"As organizations move away from paper documents with ink signatures or authenticity stamps, digital signatures can provide added assurances of the evidence to provenance, identity, and status of an electronic document as well as acknowledging informed consent and approval by a signatory.  The United States Government Printing Office (GPO) publishes electronic versions of the budget, public and private laws, and congressional bills with digital signatures.  Universities including Penn State, University of Chicago, and Stanford are publishing electronic student transcripts with digital signatures.
","As organizations move away from paper documents with ink signatures or authenticity stamps, digital signatures can provide added assurances of the evidence to provenance, identity, and status of an electronic document as well as acknowledging informed consent and approval by a signatory. The United States Government Printing Office (GPO) publishes electronic versions of the budget, public and private laws, and congressional bills with digital signatures.","[' What can digital signatures provide as organizations move away from paper documents with ink signatures or authenticity stamps?', ' What is the US Government Printing Office known as?', ' Who publishes electronic versions of the budget, public and private laws and congressional bills?']","['added assurances of the evidence to provenance, identity, and status of an electronic document', 'GPO', 'The United States Government Printing Office (GPO)']"
1703,signature scheme,The current state of use – legal and practical,"Only if all of these conditions are met will a digital signature actually be any evidence of who sent the message, and therefore of their assent to its contents. Legal enactment cannot change this reality of the existing engineering possibilities, though some such have not reflected this actuality.
","Only if all of these conditions are met will a digital signature actually be any evidence of who sent the message, and therefore of their assent to its contents. Legal enactment cannot change this reality of the existing engineering possibilities, though some such have not reflected this actuality.","[' What is the only way a digital signature can be a proof of who sent a message?', ' Legal enactment cannot change what reality of engineering possibilities?']","['if all of these conditions are met', 'this reality']"
1704,signature scheme,The current state of use – legal and practical,"Legislatures, being importuned by businesses expecting to profit from operating a PKI, or by the technological avant-garde advocating new solutions to old problems, have enacted statutes and/or regulations in many jurisdictions authorizing, endorsing, encouraging, or permitting digital signatures and providing for (or limiting) their legal effect. The first appears to have been in Utah in the United States, followed closely by the states Massachusetts and California. Other countries have also passed statutes or issued regulations in this area as well and the UN has had an active model law project for some time. These enactments (or proposed enactments) vary from place to place, have typically embodied expectations at variance (optimistically or pessimistically) with the state of the underlying cryptographic engineering, and have had the net effect of confusing potential users and specifiers, nearly all of whom are not cryptographically knowledgeable.
","Legislatures, being importuned by businesses expecting to profit from operating a PKI, or by the technological avant-garde advocating new solutions to old problems, have enacted statutes and/or regulations in many jurisdictions authorizing, endorsing, encouraging, or permitting digital signatures and providing for (or limiting) their legal effect. The first appears to have been in Utah in the United States, followed closely by the states Massachusetts and California.","[' What has enacted statutes and/or regulations in many jurisdictions authorizing, endorsing, encouraging, or permitting digital signatures?', ' Businesses expecting to profit from operating a PKI or by the technological avant-garde advocating new solutions to old problems have what?', ' Where did the first states appear to be located?', ' What states followed suit?']","['Legislatures', 'Legislatures', 'Utah', 'Massachusetts and California']"
1705,signature scheme,Industry standards,"Some industries have established common interoperability standards for the use of digital signatures between members of the industry and with regulators.  These include the Automotive Network Exchange for the automobile industry and the SAFE-BioPharma Association for the healthcare industry.
",Some industries have established common interoperability standards for the use of digital signatures between members of the industry and with regulators. These include the Automotive Network Exchange for the automobile industry and the SAFE-BioPharma Association for the healthcare industry.,"[' What industry has established common interoperability standards for the use of digital signatures?', ' What is the Automotive Network Exchange for?', ' The SAFE-BioPharma Association is for what industry?']","['Automotive Network Exchange for the automobile industry', 'the automobile industry', 'healthcare']"
1706,prediction,Summary,"A prediction (Latin præ-, ""before,"" and dicere, ""to say""), or forecast, is a statement about a future event or data. They are often, but not always, based upon experience or knowledge. There is no universal agreement about the exact difference from ""estimation""; different authors and disciplines ascribe different connotations.
","A prediction (Latin præ-, ""before,"" and dicere, ""to say""), or forecast, is a statement about a future event or data. They are often, but not always, based upon experience or knowledge.","[' What is a prediction a statement about?', ' What does dicere mean?']","['a future event or data', 'to say']"
1707,prediction,Summary,"Future events are necessarily uncertain, so guaranteed accurate information about the future is impossible. Prediction can be useful to assist in making plans about possible developments; Howard H. Stevenson writes that prediction in business ""is at least two things: Important and hard.""","Future events are necessarily uncertain, so guaranteed accurate information about the future is impossible. Prediction can be useful to assist in making plans about possible developments; Howard H. Stevenson writes that prediction in business ""is at least two things: Important and hard.""","[' What is impossible?', ' What can be useful to assist in making plans about possible developments?', ' Who writes that prediction in business is at least two things?']","['guaranteed accurate information about the future', 'Prediction', 'Howard H. Stevenson']"
1708,prediction,Opinion,"The Delphi method is a technique for eliciting such expert-judgement-based predictions in a controlled way.  This type of prediction might be perceived as consistent with statistical techniques in the sense that, at minimum, the ""data"" being used is the predicting expert's cognitive experiences forming an intuitive ""probability curve.""
","The Delphi method is a technique for eliciting such expert-judgement-based predictions in a controlled way. This type of prediction might be perceived as consistent with statistical techniques in the sense that, at minimum, the ""data"" being used is the predicting expert's cognitive experiences forming an intuitive ""probability curve.""","[' What is a technique for eliciting expert-judgement-based predictions in a controlled way?', ' What might be perceived as consistent with statistical techniques in the sense that the ""data"" being used is the predicting expert\'s cognitive experiences forming what?']","['The Delphi method', 'an intuitive ""probability curve.""']"
1709,prediction,Statistics,"In statistics, prediction is a part of statistical inference. One particular approach to such inference is known as predictive inference, but the prediction can be undertaken within any of the several approaches to statistical inference. Indeed, one possible description of statistics is that it provides a means of transferring knowledge about a sample of a population to the whole population, and to other related populations, which is not necessarily the same as prediction over time. When information is transferred across time, often to specific points in time, the process is known as forecasting. Forecasting usually requires time series methods, while prediction is often performed on cross-sectional data.
","In statistics, prediction is a part of statistical inference. One particular approach to such inference is known as predictive inference, but the prediction can be undertaken within any of the several approaches to statistical inference.","[' In statistics, what is a part of statistical inference?', ' What is one particular approach to inference known as?', ' Predictive inference can be undertaken within any approach to what?']","['prediction', 'predictive inference', 'statistical inference']"
1710,prediction,Statistics,"Statistical techniques used for prediction include regression analysis and its various sub-categories such as linear regression, generalized linear models (logistic regression, Poisson regression, Probit regression), etc. In case of forecasting, autoregressive moving average models and vector autoregression models can be utilized. When these and/or related, generalized set of regression or machine learning methods are deployed in commercial usage, the field is known as predictive analytics.","Statistical techniques used for prediction include regression analysis and its various sub-categories such as linear regression, generalized linear models (logistic regression, Poisson regression, Probit regression), etc. In case of forecasting, autoregressive moving average models and vector autoregression models can be utilized.","[' What is one of the statistical techniques used for prediction?', ' What are some of the sub-categories of regression analysis?']","['regression analysis', 'linear regression, generalized linear models (logistic regression, Poisson regression, Probit regression']"
1711,prediction,Statistics,"In many applications, such as time series analysis, it is possible to estimate the models that generate the observations. If models can be expressed as transfer functions or in terms of state-space parameters then smoothed, filtered and predicted data estimates can be calculated.  If the underlying generating models are linear then a minimum-variance Kalman filter and a minimum-variance smoother may be used to recover data of interest from noisy measurements. These techniques rely on one-step-ahead predictors (which minimise the variance of the prediction error). When the generating models are nonlinear then stepwise linearizations may be applied within Extended Kalman Filter and smoother recursions. However, in nonlinear cases, optimum minimum-variance performance guarantees no longer apply.","In many applications, such as time series analysis, it is possible to estimate the models that generate the observations. If models can be expressed as transfer functions or in terms of state-space parameters then smoothed, filtered and predicted data estimates can be calculated.","[' In time series analysis, what is it possible to estimate the models that generate the observations?', ' If models can be expressed as transfer functions or in terms of state-space parameters then smoothed, filtered and predicted data estimates can be calculated?']","['transfer functions or in terms of state-space parameters', 'time series analysis']"
1712,prediction,Statistics,"To use regression analysis for prediction, data are collected on the variable that is to be predicted, called the dependent variable or response variable, and on one or more variables whose values are hypothesized to influence it, called independent variables or explanatory variables. A functional form, often linear, is hypothesized for the postulated causal relationship, and the parameters of the function are estimated from the data—that is, are chosen so as to optimize is some way the fit of the function, thus parameterized, to the data. That is the estimation step. For the prediction step, explanatory variable values that are deemed relevant to future (or current but not yet observed) values of the dependent variable are input to the parameterized function to generate predictions for the dependent variable.","To use regression analysis for prediction, data are collected on the variable that is to be predicted, called the dependent variable or response variable, and on one or more variables whose values are hypothesized to influence it, called independent variables or explanatory variables. A functional form, often linear, is hypothesized for the postulated causal relationship, and the parameters of the function are estimated from the data—that is, are chosen so as to optimize is some way the fit of the function, thus parameterized, to the data.","[' What is the term for a variable that is to be predicted?', ' What are the terms for variables whose values are hypothesized to influence it?', ' A functional form is often what?', ' What is a functional form often hypothesized for the postulated causal relationship?', ' What are the parameters of a function estimated from?']","['dependent variable or response variable', 'independent variables or explanatory variables', 'linear', 'linear', 'the data']"
1713,prediction,Science,"In science, a prediction is a rigorous, often quantitative, statement, forecasting what would be observed under specific conditions; for example, according to theories of gravity, if an apple fell from a tree it would be seen to move towards the center of the earth with a specified and constant acceleration. The scientific method is built on testing statements that are logical consequences of scientific theories. This is done through repeatable experiments or observational studies.
","In science, a prediction is a rigorous, often quantitative, statement, forecasting what would be observed under specific conditions; for example, according to theories of gravity, if an apple fell from a tree it would be seen to move towards the center of the earth with a specified and constant acceleration. The scientific method is built on testing statements that are logical consequences of scientific theories.","[' In science, a prediction is a rigorous, often what?', ' According to theories of gravity, if an apple fell from a tree it would be seen to move towards the center of the earth with a specified and constant acceleration?', ' What is the scientific method built on testing statements that are logical consequences of scientific theories?']","['quantitative', 'prediction', 'gravity']"
1714,prediction,Science,"A scientific theory whose predictions are contradicted by observations and evidence will be rejected. New theories that generate many new predictions can more easily be supported or falsified (see predictive power). Notions that make no testable predictions are usually considered not to be part of science (protoscience or nescience) until testable predictions can be made.
",A scientific theory whose predictions are contradicted by observations and evidence will be rejected. New theories that generate many new predictions can more easily be supported or falsified (see predictive power).,"[' What will a scientific theory whose predictions are contradicted by observations and evidence be rejected for?', ' New theories that generate many new predictions can more easily be supported or falsified (see predictive power).']","['New theories that generate many new predictions can more easily be supported or falsified (see predictive power).', 'scientific theory whose predictions are contradicted by observations and evidence will be rejected']"
1715,prediction,Science,"Mathematical equations and models, and computer models, are frequently used to describe the past and future behaviour of a process within the boundaries of that model. In some cases the probability of an outcome, rather than a specific outcome, can be predicted, for example in much of quantum physics.
","Mathematical equations and models, and computer models, are frequently used to describe the past and future behaviour of a process within the boundaries of that model. In some cases the probability of an outcome, rather than a specific outcome, can be predicted, for example in much of quantum physics.","[' Mathematical equations and models describe what?', ' Computer models describe the past and future behaviour of a process within what model?', ' What can be predicted in quantum physics?']","['the past and future behaviour of a process within the boundaries of that model', 'the boundaries of that model', 'the probability of an outcome']"
1716,prediction,Science,"Accurate prediction and forecasting are very difficult in some areas, such as natural disasters, pandemics, demography, population dynamics and meteorology. For example, it is possible to predict the occurrence of solar cycles, but their exact timing and magnitude is much more difficult (see picture to right).
","Accurate prediction and forecasting are very difficult in some areas, such as natural disasters, pandemics, demography, population dynamics and meteorology. For example, it is possible to predict the occurrence of solar cycles, but their exact timing and magnitude is much more difficult (see picture to right).","[' What is very difficult in some areas?', ' What is possible to predict the occurrence of solar cycles but their timing and magnitude is much more difficult?']","['Accurate prediction and forecasting', 'it is possible']"
1717,prediction,Finance,"Mathematical models of stock market behaviour (and economic behaviour in general) are also unreliable in predicting future behaviour. Among other reasons, this is because economic events may span several years, and the world is changing over a similar time frame, thus invalidating the relevance of past observations to the present. Thus there are an extremely small number (of the order of 1) of relevant past data points from which to project the future. In addition, it is generally believed that stock market prices already take into account all the information available to predict the future, and subsequent movements must therefore be the result of unforeseen events. Consequently, it is extremely difficult for a stock investor to anticipate or predict a stock market boom, or a stock market crash. In contrast to predicting the actual stock return, forecasting of broad economic trends tends to have better accuracy. Such analysis is provided by both non-profit groups as well as by for-profit private institutions.","Mathematical models of stock market behaviour (and economic behaviour in general) are also unreliable in predicting future behaviour. Among other reasons, this is because economic events may span several years, and the world is changing over a similar time frame, thus invalidating the relevance of past observations to the present.","[' Mathematical models of stock market behaviour are also unreliable in what?', ' Why are mathematical models unreliability in predicting future behaviour?', ' Economic events may span several years and the world is changing over a similar time frame?']","['predicting future behaviour', 'economic events may span several years, and the world is changing over a similar time frame', 'invalidating the relevance of past observations to the present']"
1718,prediction,Finance,"An actuary uses actuarial science to assess and predict future business risk, such that the risk(s) can be mitigated. For example, in insurance an actuary would use a life table (which incorporates the historical experience of mortality rates and sometimes an estimate of future trends) to project life expectancy.
","An actuary uses actuarial science to assess and predict future business risk, such that the risk(s) can be mitigated. For example, in insurance an actuary would use a life table (which incorporates the historical experience of mortality rates and sometimes an estimate of future trends) to project life expectancy.","[' What type of science does an actuary use to assess and predict future business risk?', ' What is an example of a life table?']","['actuarial', 'insurance']"
1719,prediction,Sports,"Predicting the outcome of sporting events is a business which has grown in popularity in recent years.  Handicappers predict the outcome of games using a variety of mathematical formulas, simulation models or qualitative analysis.  Early, well known sports bettors, such as Jimmy the Greek, were believed to have access to information that gave them an edge.  Information ranged from personal issues, such as gambling or drinking to undisclosed injuries; anything that may affect the performance of a player on the field.
","Predicting the outcome of sporting events is a business which has grown in popularity in recent years. Handicappers predict the outcome of games using a variety of mathematical formulas, simulation models or qualitative analysis.","[' What is a business that has grown in popularity in recent years?', ' What do handicappers predict the outcome of games using?']","['Predicting the outcome of sporting events', 'mathematical formulas, simulation models or qualitative analysis']"
1720,prediction,Sports,"Recent times have changed the way sports are predicted.  Predictions now typically consist of two distinct approaches: Situational plays and statistical based models.  Situational plays are much more difficult to measure because they usually involve the motivation of a team.  Dan Gordon, noted handicapper, wrote ""Without an emotional edge in a game in addition to value in a line, I won't put my money on it"".  These types of plays consist of: Betting on the home underdog, betting against Monday Night winners if they are a favorite next week, betting the underdog in ""look ahead"" games etc.  As situational plays become more widely known they become less useful because they will impact the way the line is set.
",Recent times have changed the way sports are predicted. Predictions now typically consist of two distinct approaches: Situational plays and statistical based models.,"[' What have recent times changed the way sports are predicted?', ' Predictions now consist of what two distinct approaches?']","['Predictions now typically consist of two distinct approaches: Situational plays and statistical based models', 'Situational plays and statistical based models']"
1721,prediction,Sports,"The widespread use of technology has brought with it more modern sports betting systems.  These systems are typically algorithms and simulation models based on regression analysis. Jeff Sagarin, a sports statistician, has brought attention to sports by having the results of his models published in USA Today.  He is currently paid as a consultant by the Dallas Mavericks for his advice on lineups and the use of his Winval system, which evaluates free agents. Brian Burke, a former Navy fighter pilot turned sports statistician, has published his results of using regression analysis to predict the outcome of NFL games. Ken Pomeroy is widely accepted as a leading authority on college basketball statistics.  His website includes his College Basketball Ratings, a tempo based statistics system.  Some statisticians have become very famous for having successful prediction systems.  Dare wrote ""the effective odds for sports betting and horse racing are a direct result of human decisions and can therefore potentially exhibit consistent error"".  Unlike other games offered in a casino, prediction in sporting events can be both logical and consistent.
",The widespread use of technology has brought with it more modern sports betting systems. These systems are typically algorithms and simulation models based on regression analysis.,"[' What has the widespread use of technology brought with it more modern sports betting systems?', ' What are these systems typically algorithms and simulation models based on?']","['algorithms and simulation models based on regression analysis', 'regression analysis']"
1722,prediction,Sports,"Other more advance models include those based on Bayesian networks, which are causal probabilistic models commonly used for risk analysis and decision support. Based on this kind of mathematical modelling, Constantinou et al., have developed models for predicting the outcome of association football matches. What makes these models interesting is that, apart from taking into consideration relevant historical data, they also incorporate all these vague subjective factors, like availability of key players, team fatigue, team motivation and so on. They provide the user with the ability to include their best guesses about things that there are no hard facts available. This additional information is then combined with historical facts to provide a revised prediction for future match outcomes. The initial results based on these modelling practices are encouraging since they have demonstrated consistent profitability against published market odds.
","Other more advance models include those based on Bayesian networks, which are causal probabilistic models commonly used for risk analysis and decision support. Based on this kind of mathematical modelling, Constantinou et al., have developed models for predicting the outcome of association football matches.","[' What are Bayesian networks used for?', ' Constantinou et al. have developed models for predicting the outcome of association football matches?']","['risk analysis and decision support', 'Bayesian networks']"
1723,prediction,Sports,"Nowadays sport betting is a huge business; there are many websites (systems) alongside betting sites, which give tips or predictions for future games. Some of these prediction websites (tipsters) are based on human predictions, but others on computer software sometimes called prediction robots or bots. Prediction bots can use different amount of data and algorithms and because of that their accuracy may vary.
","Nowadays sport betting is a huge business; there are many websites (systems) alongside betting sites, which give tips or predictions for future games. Some of these prediction websites (tipsters) are based on human predictions, but others on computer software sometimes called prediction robots or bots.","[' What is a huge business nowadays?', ' What are many websites alongside betting sites that give tips or predictions for future games?', ' Some prediction websites are based on what?']","['sport betting', 'systems', 'human predictions']"
1724,prediction,Sports,"Sites such as Tzefi.com maintain that other sites' claim that they simulate the game 50,000 times before it's actually played on the field, is quite misleading and incorrect.
This is due to the fact that those sites don't consider the human element of the game, and that a batter with a .330 ERA may strike out when the bases are loaded, or a quarterback with a 100+ passer rating may throw an interception at a crucial time.
Tzefi.com boasts a 64.5% accuracy in predicting NFL games.
","Sites such as Tzefi.com maintain that other sites' claim that they simulate the game 50,000 times before it's actually played on the field, is quite misleading and incorrect. This is due to the fact that those sites don't consider the human element of the game, and that a batter with a .330 ERA may strike out when the bases are loaded, or a quarterback with a 100+ passer rating may throw an interception at a crucial time.","["" What site claims that they simulate the game 50,000 times before it's actually played on the field?"", ' What does Tzefi.com believe is misleading and incorrect?', ' What is the human element of the game?', ' A batter with a.330 ERA may strike out when the bases are loaded, or a quarterback with 100+ passer rating may throw what?']","['Tzefi.com', ""other sites' claim that they simulate the game 50,000 times before it's actually played on the field"", 'a batter with a .330 ERA may strike out when the bases are loaded', 'an interception']"
1725,prediction,Social science,"Prediction in the non-economic social sciences differs from the natural sciences and includes multiple alternative methods such as trend projection, forecasting, scenario-building and Delphi surveys. The oil company Shell is particularly well known for its scenario-building activities.","Prediction in the non-economic social sciences differs from the natural sciences and includes multiple alternative methods such as trend projection, forecasting, scenario-building and Delphi surveys. The oil company Shell is particularly well known for its scenario-building activities.","[' Prediction in the non-economic social sciences differs from what?', ' What is the oil company Shell known for?']","['the natural sciences', 'scenario-building activities']"
1726,prediction,Social science,"One reason for the peculiarity of societal prediction is that in the social sciences, ""predictors are part of the social context about which they are trying to make a prediction and may influence that context in the process"". As a consequence, societal predictions can become self-destructing. For example, a forecast that a large percentage of a population will become HIV infected based on existing trends may cause more people to avoid risky behavior and thus reduce the HIV infection rate, invalidating the forecast (which might have remained correct if it had not been publicly known). Or, a prediction that cybersecurity will become a major issue may cause organizations to implement more security cybersecurity measures, thus limiting the issue.","One reason for the peculiarity of societal prediction is that in the social sciences, ""predictors are part of the social context about which they are trying to make a prediction and may influence that context in the process"". As a consequence, societal predictions can become self-destructing.","[' What is one reason for the peculiarity of societal prediction?', ' What can become self-destructing?']","['in the social sciences', 'societal predictions']"
1727,prediction,Social science,"In politics it is common to attempt to predict the outcome of elections via political forecasting techniques (or assess the popularity of politicians) through the use of opinion polls. Prediction games have been used by many corporations and governments to learn about the most likely outcome of future events.
",In politics it is common to attempt to predict the outcome of elections via political forecasting techniques (or assess the popularity of politicians) through the use of opinion polls. Prediction games have been used by many corporations and governments to learn about the most likely outcome of future events.,"[' What are political forecasting techniques used for?', ' What are opinion polls used to assess the popularity of politicians?']","['predict the outcome of elections', 'political forecasting techniques']"
1728,prediction,Prophecy,"Predictions have often been made, from antiquity until the present, by using paranormal or supernatural means such as prophecy or by observing omens. Methods including water divining, astrology, numerology, fortune telling, interpretation of dreams, and many other forms of divination, have been used for millennia to attempt to predict the future. These means of prediction have not been proven by scientific experiments.
","Predictions have often been made, from antiquity until the present, by using paranormal or supernatural means such as prophecy or by observing omens. Methods including water divining, astrology, numerology, fortune telling, interpretation of dreams, and many other forms of divination, have been used for millennia to attempt to predict the future.","[' Predictions have often been made by using what?', ' What have been used for millennia to attempt to predict the future?', ' What has been used for millennia to try to predict the future?']","['paranormal or supernatural means such as prophecy or by observing omens', 'water divining', 'water divining']"
1729,prediction,Prophecy,"In literature, vision and prophecy are literary devices used to present a possible timeline of future events.  They can be distinguished by vision referring to what an individual sees happen. The book of Revelation, in the New Testament, thus uses vision as a literary device in this regard. It is also prophecy or prophetic literature when it is related by an individual in a sermon or other public forum.
","In literature, vision and prophecy are literary devices used to present a possible timeline of future events. They can be distinguished by vision referring to what an individual sees happen.","[' What are literary devices used to present a possible timeline of future events?', ' What can be distinguished by vision?']","['vision and prophecy', 'prophecy']"
1730,prediction,Prophecy,"Divination is the attempt to gain insight into a question or situation by way of an occultic standardized process or ritual. It is an integral part of witchcraft and has been used in various forms for thousands of years. Diviners ascertain their interpretations of how a querent should proceed by reading signs, events, or omens, or through alleged contact with a supernatural agency, most often describe as an angel or a god though viewed by Christians and Jews as a fallen angel or demon.",Divination is the attempt to gain insight into a question or situation by way of an occultic standardized process or ritual. It is an integral part of witchcraft and has been used in various forms for thousands of years.,"[' What is the attempt to gain insight into a question or situation by way of an occultic standardized process or ritual?', ' What is an integral part of witchcraft and has been used in various forms for thousands of years?']","['Divination', 'Divination']"
1731,prediction,Fiction,"In fantasy literature, predictions are often obtained through magic or prophecy, sometimes referring back to old traditions. For example, in J. R. R. Tolkien's The Lord of the Rings, many of the characters possess an awareness of events extending into the future, sometimes as prophecies, sometimes as more-or-less vague 'feelings'. The character Galadriel, in addition, employs a water ""mirror"" to show images, sometimes of possible future events.
","In fantasy literature, predictions are often obtained through magic or prophecy, sometimes referring back to old traditions. For example, in J. R. R. Tolkien's The Lord of the Rings, many of the characters possess an awareness of events extending into the future, sometimes as prophecies, sometimes as more-or-less vague 'feelings'.","[' How are predictions obtained in fantasy literature?', ' Who wrote The Lord of the Rings?', ' What does Tolkien call the characters in the book of lord of the rings?']","['magic or prophecy', 'J. R. R. Tolkien', 'feelings']"
1732,prediction,Fiction,"In some of Philip K. Dick's stories, mutant humans called precogs can foresee the future (ranging from days to years). In the story called The Golden Man, an exceptional mutant can predict the future to an indefinite range (presumably up to his death), and thus becomes completely non-human, an animal that follows the predicted paths automatically. Precogs also play an essential role in another of Dick's stories, The Minority Report, which was turned into a film by Steven Spielberg in 2002.
","In some of Philip K. Dick's stories, mutant humans called precogs can foresee the future (ranging from days to years). In the story called The Golden Man, an exceptional mutant can predict the future to an indefinite range (presumably up to his death), and thus becomes completely non-human, an animal that follows the predicted paths automatically.","[' Who wrote The Golden Man?', "" What are precogs in Dick's stories?"", ' How can a mutant human predict the future?', ' What is the name of the animal that follows the predicted paths?']","['Philip K. Dick', 'mutant humans', 'ranging from days to years). In the story called The Golden Man, an exceptional mutant can predict the future to an indefinite range', 'The Golden Man']"
1733,prediction,Fiction,"In the Foundation series by Isaac Asimov, a mathematician finds out that historical events (up to some detail) can be theoretically modelled using equations, and then spends years trying to put the theory in practice. The new science of psychohistory founded upon his success can simulate history and extrapolate the present into the future.
","In the Foundation series by Isaac Asimov, a mathematician finds out that historical events (up to some detail) can be theoretically modelled using equations, and then spends years trying to put the theory in practice. The new science of psychohistory founded upon his success can simulate history and extrapolate the present into the future.","[' Who wrote the Foundation series?', ' What can historical events be theoretically modelled using?', ' How long does Isaac Asimov spend trying to put the theory in practice?', ' Who can simulate history and extrapolate the present into the future?']","['Isaac Asimov', 'equations', 'years', 'psychohistory']"
1734,prediction,Fiction,"In Frank Herbert's sequels to 1965's Dune, his characters are dealing with the repercussions of being able to see the possible futures and select amongst them. Herbert sees this as a trap of stagnation, and his characters follow a so-called ""Golden Path"" out of the trap.
","In Frank Herbert's sequels to 1965's Dune, his characters are dealing with the repercussions of being able to see the possible futures and select amongst them. Herbert sees this as a trap of stagnation, and his characters follow a so-called ""Golden Path"" out of the trap.","[' In what year was Dune released?', ' Who wrote Dune?', ' In Dune, what did Herbert see as a trap of stagnation?', ' What did Herbert call the Golden Path?']","['1965', 'Frank Herbert', 'being able to see the possible futures and select amongst them', 'stagnation, and his characters follow a so-called ""Golden Path']"
1735,prediction,Fiction,"In Ursula K. Le Guin's The Left Hand of Darkness, the humanoid inhabitants of planet Gethen have mastered the art of prophecy and routinely produce data on past, present or future events on request. In this story, this was a minor plot device.
","In Ursula K. Le Guin's The Left Hand of Darkness, the humanoid inhabitants of planet Gethen have mastered the art of prophecy and routinely produce data on past, present or future events on request. In this story, this was a minor plot device.","[' Who wrote the book The Left Hand of Darkness?', ' What do humanoid inhabitants of planet Gethen produce on request?']","['Ursula K. Le Guin', 'data on past, present or future events']"
1736,prediction,Poetry,"For the ancients, prediction, prophesy, and poetry were often intertwined.  Prophecies were given in verse, and a word for poet in Latin is “vates” or prophet. Both poets and prophets claimed to be inspired by forces outside themselves. In contemporary cultures, theological revelation and poetry are typically seen as distinct and often even as opposed to each other. Yet the two still are often understood together as symbiotic in their origins, aims, and purposes.","For the ancients, prediction, prophesy, and poetry were often intertwined. Prophecies were given in verse, and a word for poet in Latin is “vates” or prophet.","[' What were often intertwined for the ancients?', ' What were prophecies given in?', ' A word for poet in Latin is what?']","['prediction, prophesy, and poetry', 'verse', 'vates']"
1737,random forest,Summary,"Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the mean or average prediction of the individual trees is returned. Random decision forests correct for decision trees' habit of overfitting to their training set.: 587–588  Random forests generally outperform decision trees, but their accuracy is lower than gradient boosted trees. However, data characteristics can affect their performance.","Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees.","[' Random forests operate by constructing a multitude of decision trees at what time?', ' For classification tasks, the output of a random forest is what?']","['training', 'the class selected by most trees']"
1738,random forest,Summary,"An extension of the algorithm was developed by Leo Breiman and Adele Cutler, who registered ""Random Forests"" as a trademark in 2006 (as of 2019, owned by Minitab, Inc.). The extension combines Breiman's ""bagging"" idea and random selection of features, introduced first by Ho and later independently by Amit and Geman in order to construct a collection of decision trees with controlled variance.
","An extension of the algorithm was developed by Leo Breiman and Adele Cutler, who registered ""Random Forests"" as a trademark in 2006 (as of 2019, owned by Minitab, Inc.). The extension combines Breiman's ""bagging"" idea and random selection of features, introduced first by Ho and later independently by Amit and Geman in order to construct a collection of decision trees with controlled variance.","[' Who developed an extension of the algorithm?', ' When did Leo Breiman and Adele Cutler register Random Forests as a trademark?', ' What is the name of the extension that combines the idea of ""bagging"" and random selection of features?', ' Who first introduced decision trees with controlled variance?', ' What did Ho and Geman later introduce?']","['Leo Breiman and Adele Cutler', '2006', 'Random Forests', 'Ho', 'random selection of features']"
1739,random forest,History,"The general method of random decision forests was first proposed by Ho in 1995. Ho established that forests of trees splitting with oblique hyperplanes can gain accuracy as they grow without suffering from overtraining, as long as the forests are randomly restricted to be sensitive to only selected feature dimensions.  A subsequent work along the same lines concluded that other splitting methods behave similarly, as long as they are randomly forced to be insensitive to some feature dimensions.  Note that this observation of a more complex classifier (a larger forest) getting more accurate nearly monotonically is in sharp contrast to the common belief that the complexity of a classifier can only grow to a certain level of accuracy before being hurt by overfitting.  The explanation of the forest method's resistance to overtraining can be found in Kleinberg's theory of stochastic discrimination.","The general method of random decision forests was first proposed by Ho in 1995. Ho established that forests of trees splitting with oblique hyperplanes can gain accuracy as they grow without suffering from overtraining, as long as the forests are randomly restricted to be sensitive to only selected feature dimensions.","[' When was the general method of random decision forests first proposed?', ' Ho established that forests of trees splitting with oblique hyperplanes can gain accuracy as they grow without suffering from what?']","['1995', 'overtraining']"
1740,random forest,History,"The early development of Breiman's notion of random forests was influenced by the work of Amit and
Geman who introduced the idea of searching over a random subset of the
available decisions when splitting a node, in the context of growing a single
tree.  The idea of random subspace selection from Ho was also influential in the design of random forests.  In this method a forest of trees is grown,
and variation among the trees is introduced by projecting the training data
into a randomly chosen subspace before fitting each tree or each node.  Finally, the idea of
randomized node optimization, where the decision at each node is selected by a
randomized procedure, rather than a deterministic optimization was first
introduced by Thomas G. Dietterich.","The early development of Breiman's notion of random forests was influenced by the work of Amit and
Geman who introduced the idea of searching over a random subset of the
available decisions when splitting a node, in the context of growing a single
tree. The idea of random subspace selection from Ho was also influential in the design of random forests.","["" Whose work influenced Breiman's idea of random forests?"", ' Who introduced the idea of searching over a random subset of the available decisions when splitting a node in the context of growing a single tree?', ' What was also influential in the design of random forests?', ' What was the idea of random subspace selection?']","['Amit and\nGeman', 'Amit and\nGeman', 'random subspace selection from Ho', 'Ho']"
1741,random forest,History,"The proper introduction of random forests was made in a paper
by Leo Breiman.  This paper describes a method of building a forest of
uncorrelated trees using a CART like procedure, combined with randomized node
optimization and bagging.  In addition, this paper combines several
ingredients, some previously known and some novel, which form the basis of the
modern practice of random forests, in particular:
","The proper introduction of random forests was made in a paper
by Leo Breiman. This paper describes a method of building a forest of
uncorrelated trees using a CART like procedure, combined with randomized node
optimization and bagging.","[' Who wrote a paper on the proper introduction of random forests?', ' What is the method of building a forest of uncorrelated trees using?']","['Leo Breiman', 'a CART like procedure']"
1742,random forest,Unsupervised learning with random forests,"As part of their construction, random forest predictors naturally lead to a dissimilarity measure among the observations. One can also define a random forest dissimilarity measure between unlabeled data: the idea is to construct a random forest predictor that distinguishes the ""observed"" data from suitably generated synthetic data.
The observed data are the original unlabeled data and the synthetic data are drawn from a reference distribution. A random forest dissimilarity can be attractive because it handles mixed variable types very well, is invariant to monotonic transformations of the input variables, and is robust to outlying observations. The random forest dissimilarity easily deals with a large number of semi-continuous variables due to its intrinsic variable selection; for example, the ""Addcl 1"" random forest dissimilarity weighs the contribution of each variable according to how dependent it is on other variables. The random forest dissimilarity has been used in a variety of applications, e.g. to find clusters of patients based on tissue marker data.","As part of their construction, random forest predictors naturally lead to a dissimilarity measure among the observations. One can also define a random forest dissimilarity measure between unlabeled data: the idea is to construct a random forest predictor that distinguishes the ""observed"" data from suitably generated synthetic data.","[' Random forest predictors naturally lead to what among the observations?', ' One can define a random forest dissimilarity measure between unlabeled data?', ' The idea is to construct a predictor that distinguishes the ""observed"" data from what?']","['dissimilarity measure', 'random forest predictors', 'suitably generated synthetic data']"
1743,random forest,Variants,"Instead of decision trees, linear models have been proposed and evaluated as base estimators in random forests, in particular multinomial logistic regression and naive Bayes classifiers. In cases that the relationship between the predictors and the target variable is linear, the base learners may have an equally high accuracy as the ensemble learner.","Instead of decision trees, linear models have been proposed and evaluated as base estimators in random forests, in particular multinomial logistic regression and naive Bayes classifiers. In cases that the relationship between the predictors and the target variable is linear, the base learners may have an equally high accuracy as the ensemble learner.","[' Instead of decision trees, what have been proposed and evaluated as base estimators in random forests?', ' Multinomial logistic regression and naive Bayes classifiers are examples of what?', ' In cases where the relationship between predictors and target variable is linear, what may have an equally high accuracy?', ' What may have an equally high accuracy as the ensemble learner?']","['linear models', 'linear models have been proposed and evaluated as base estimators in random forests', 'the base learners', 'base learners']"
1744,random forest,Kernel random forest,"In machine learning, kernel random forests (KeRF) establish the connection between random forests and kernel methods. By slightly modifying their definition, random forests can be rewritten as kernel methods, which are more interpretable and easier to analyze.","In machine learning, kernel random forests (KeRF) establish the connection between random forests and kernel methods. By slightly modifying their definition, random forests can be rewritten as kernel methods, which are more interpretable and easier to analyze.","[' What is the term for kernel random forests?', ' What does KeRF stand for?', ' How can kernel methods be rewritten?']","['KeRF', 'kernel random forests', 'By slightly modifying their definition']"
1745,random forest,Disadvantages,"While random forests often achieve higher accuracy than a single decision tree, they sacrifice the intrinsic interpretability present in decision trees. Decision trees are among a fairly small family of machine learning models that are easily interpretable along with linear models, rule-based models, and attention-based models. This interpretability is one of the most desirable qualities of decision trees. It allows developers to confirm that the model has learned realistic information from the data and allows end-users to have trust and confidence in the decisions made by the model. For example, following the path that a decision tree takes to make its decision is quite trivial, but following the paths of tens or hundreds of trees is much harder. To achieve both performance and interpretability, some model compression techniques allow transforming a random forest into a minimal ""born-again"" decision tree that faithfully reproduces the same decision function. If it is established that the predictive attributes are linearly correlated with the target variable, using random forest may not enhance the accuracy of the base learner. Furthermore, in problems with multiple categorical variables, random forest may not be able to increase the accuracy of the base learner.","While random forests often achieve higher accuracy than a single decision tree, they sacrifice the intrinsic interpretability present in decision trees. Decision trees are among a fairly small family of machine learning models that are easily interpretable along with linear models, rule-based models, and attention-based models.","[' Random forests often achieve higher accuracy than what?', ' Random forests sacrifice what intrinsic interpretability present in decision trees?', ' Decision trees are among a fairly small family of what kind of models?']","['a single decision tree', 'random forests', 'machine learning']"
1746,mobile device,Summary,"A mobile device (or handheld computer) is a computer small enough to hold and operate in the hand. Typically, any handheld computer device will have an LCD or OLED flat screen interface, providing a touchscreen interface with digital buttons and keyboard or physical buttons along with a physical keyboard. Many such devices can connect to the Internet and interconnect with other devices such as car entertainment systems or headsets via Wi-Fi, Bluetooth, cellular networks or near field communication (NFC). Integrated cameras, the ability to place and receive voice and video telephone calls, video games, and Global Positioning System (GPS) capabilities are common. Power is typically provided by a lithium-ion battery. Mobile devices may run mobile operating systems that allow third-party applications to be installed and run.
","A mobile device (or handheld computer) is a computer small enough to hold and operate in the hand. Typically, any handheld computer device will have an LCD or OLED flat screen interface, providing a touchscreen interface with digital buttons and keyboard or physical buttons along with a physical keyboard.","[' What is a mobile device?', ' What type of interface does a handheld computer typically have?']","['a computer small enough to hold and operate in the hand', 'touchscreen']"
1747,mobile device,Summary,"Early smartphones were joined in the late 2000s by larger, but otherwise essentially the same, tablets. Input and output is now usually via a touch-screen interface. Phones/tablets and personal digital assistants may provide much of the functionality of a laptop/desktop computer but more conveniently, in addition to exclusive features. Enterprise digital assistants can provide additional business functionality such as integrated data capture via barcode, RFID and smart card readers.
By 2010, mobile devices often contained sensors such as accelerometers, magnetometers and gyroscopes, allowing detection of orientation and motion. Mobile devices may provide biometric user authentication such as face recognition or fingerprint recognition.
","Early smartphones were joined in the late 2000s by larger, but otherwise essentially the same, tablets. Input and output is now usually via a touch-screen interface.","[' When were smartphones joined by larger tablets?', ' Input and output is now usually via what?']","['late 2000s', 'a touch-screen interface']"
1748,mobile device,Characteristics,"Strictly speaking, many so-called mobile devices are not mobile. It is the host that is mobile, i.e., a mobile human host carries a non-mobile smartphone device. An example of a true mobile computing device, where the device itself is mobile, is a robot. Another example is an autonomous vehicle. There are three basic ways mobile devices can be physically bound to mobile hosts: accompanied, surface-mounted or embedded into the fabric of a host, e.g., an embedded controller embedded in a host device. Accompanied refers to an object being loosely bound and accompanying a mobile host, e.g., a smartphone can be carried in a bag or pocket but can easily be misplaced. Hence, mobile hosts with embedded devices such as an autonomous vehicle can appear larger than pocket-sized.
","Strictly speaking, many so-called mobile devices are not mobile. It is the host that is mobile, i.e., a mobile human host carries a non-mobile smartphone device.","[' What are many so-called mobile devices not?', ' What is the host that is mobile?', ' A mobile human host carries what type of device?']","['mobile', 'a mobile human host carries a non-mobile smartphone device', 'non-mobile smartphone']"
1749,mobile device,Characteristics,"As stated earlier, the most common size of mobile computing device is pocket-sized that can be hand-held, but other sizes for mobile devices exist, too. Mark Weiser, known as the father of ubiquitous computing, computing everywhere, referred to device sizes that are tab-sized, pad and board sized, where tabs are defined as accompanied or wearable centimeter-sized devices, e.g. smartphones, phablets and pads are defined as hand-held decimeter-sized devices. If one changes the form of the mobile devices in terms of being non-planar, one can also have skin devices and tiny dust-sized devices. Dust refers to miniaturized devices without direct HCI interfaces, e.g., micro electro-mechanical systems (MEMS), ranging from nanometres through micrometers to millimeters. See also Smart dust. Skin: fabrics based upon light emitting and conductive polymers and organic computer devices. These can be formed into more flexible non-planar display surfaces and products such as clothes and curtains, see OLED display. Also see smart device.
","As stated earlier, the most common size of mobile computing device is pocket-sized that can be hand-held, but other sizes for mobile devices exist, too. Mark Weiser, known as the father of ubiquitous computing, computing everywhere, referred to device sizes that are tab-sized, pad and board sized, where tabs are defined as accompanied or wearable centimeter-sized devices, e.g.","[' What is the most common size of a mobile computing device?', ' Mark Weiser is known as the father of what?', ' What are tab-sized devices defined as?', ' What are pad and board sized devices?']","['pocket-sized', 'ubiquitous computing', 'accompanied or wearable centimeter-sized devices', 'tab-sized']"
1750,mobile device,Characteristics,"Although mobility is often regarded as synonymous with having wireless connectivity, these terms are different. Not all network access by mobile users, applications and devices need be via wireless networks and vice versa. Wireless access devices can be static and mobile users can move in between wired and wireless hotspots such as in Internet cafés. Some mobile devices can be used as mobile Internet devices to access the Internet while moving but they do not need to do this and many phone functions or applications are still operational even while disconnected to the Internet. What makes the mobile device unique compared to other technologies is the inherent flexibility in the hardware and also the software. Flexible applications include video chat, Web browsing, payment systems, NFC, audio recording etc. As mobile devices become ubiquitous there, will be a proliferation of services which include the use of the cloud. Although a common form of mobile device, a smartphone, has a display, another perhaps even more common form of smart computing device, the smart card, e.g., used as a bank card or travel card, does not have a display. This mobile device often has a CPU and memory but needs to connect, or be inserted into a reader in order to display its internal data or state.
","Although mobility is often regarded as synonymous with having wireless connectivity, these terms are different. Not all network access by mobile users, applications and devices need be via wireless networks and vice versa.","[' What is often regarded as synonymous with having wireless connectivity?', ' Mobility and wireless connectivity are not synonymous with what?', ' Not all network access by mobile users, applications and devices need to be via what network?']","['mobility', 'network access', 'wireless']"
1751,mobile device,Types,"There are many kinds of mobile devices, designed for different applications. They include:
","There are many kinds of mobile devices, designed for different applications. They include:","[' What kind of devices are there?', ' What are some types of mobile devices?']","['mobile', 'designed for different applications']"
1752,mobile device,Uses,"Handheld devices have become ruggedized for use in mobile field management. For instance, the uses are, digitizing notes, sending and receiving invoices, asset management, recording signatures, managing parts, and scanning barcodes.
","Handheld devices have become ruggedized for use in mobile field management. For instance, the uses are, digitizing notes, sending and receiving invoices, asset management, recording signatures, managing parts, and scanning barcodes.","[' What has become ruggedized for use in mobile field management?', ' What are some of the uses of handheld devices?']","['Handheld devices', 'digitizing notes, sending and receiving invoices, asset management, recording signatures, managing parts, and scanning barcodes']"
1753,mobile device,Uses,"In 2009, developments in mobile collaboration systems enabled the use of handheld devices that combine video, audio and on-screen drawing capabilities to enable multi-party conferencing in real-time, independent of location. Handheld computers are available in a variety of form factors, including smartphones on the low end, handheld PDAs, Ultra-Mobile PCs and Tablet PCs (Palm OS, WebOS). Users can watch television through Internet by IPTV on some mobile devices. Mobile television receivers have existed since the 1960s, and in the 21st century mobile phone providers began making television available on cellular phones.","In 2009, developments in mobile collaboration systems enabled the use of handheld devices that combine video, audio and on-screen drawing capabilities to enable multi-party conferencing in real-time, independent of location. Handheld computers are available in a variety of form factors, including smartphones on the low end, handheld PDAs, Ultra-Mobile PCs and Tablet PCs (Palm OS, WebOS).","[' What enabled the use of handheld devices in 2009?', ' What enabled multi-party conferencing in real-time, independent of location?', ' What is the low end of smartphones?', ' What are handheld PDAs, Ultra-Mobile PCs and Tablet PCs?']","['developments in mobile collaboration systems', 'handheld devices that combine video, audio and on-screen drawing capabilities', 'handheld PDAs', 'smartphones on the low end']"
1754,mobile device,Uses,"In the 2010s, mobile devices can sync and share a lot of data despite the distance or specifications of said devices. In the medical field, mobile devices are quickly becoming essential tools for accessing clinical information such as drugs, treatment, even medical calculation. Due to the popularity of mobile gaming, the gambling industry started offering casino games on mobile devices, which in turn lead to inclusion of these devices in anti hazard legislature as devices that could potentially be used in illegal gambling. Other potentially illegal activities might include the use of mobile devices in distributing child pornography and the legal sex industry use of mobile apps and hardware to promote its activities, as well as the possibility of using mobile devices to perform trans-border services, which are all issues that need to be regulated. In the military, mobile devices have created new opportunities for the armed forces to deliver training and educational materials to soldiers, regardless of where they are stationed.","In the 2010s, mobile devices can sync and share a lot of data despite the distance or specifications of said devices. In the medical field, mobile devices are quickly becoming essential tools for accessing clinical information such as drugs, treatment, even medical calculation.","[' In what decade were mobile devices able to sync and share a lot of data?', ' In what field are mobile devices becoming essential tools for accessing clinical information?']","['2010s', 'medical']"
1755,differential evolution,Summary,"In evolutionary computation, differential evolution (DE) is a method that optimizes a problem by iteratively trying to improve a candidate solution with regard to a given measure of quality. Such methods are commonly known as metaheuristics as they make few or no assumptions about the problem being optimized and can search very large spaces of candidate solutions. However, metaheuristics such as DE do not guarantee an optimal solution is ever found.
","In evolutionary computation, differential evolution (DE) is a method that optimizes a problem by iteratively trying to improve a candidate solution with regard to a given measure of quality. Such methods are commonly known as metaheuristics as they make few or no assumptions about the problem being optimized and can search very large spaces of candidate solutions.","[' What is differential evolution?', ' What is DE?', ' Metaheuristics make few or no assumptions about what?', ' What can search very large spaces of candidate solutions?']","['a method that optimizes a problem by iteratively trying to improve a candidate solution with regard to a given measure of quality', 'differential evolution', 'the problem being optimized', 'metaheuristics']"
1756,differential evolution,Summary,"DE is used for multidimensional real-valued functions but does not use the gradient of the problem being optimized, which means DE does not require the optimization problem to be differentiable, as is required by classic optimization methods such as gradient descent and quasi-newton methods. DE can therefore also be used on optimization problems that are not even continuous, are noisy, change over time, etc.","DE is used for multidimensional real-valued functions but does not use the gradient of the problem being optimized, which means DE does not require the optimization problem to be differentiable, as is required by classic optimization methods such as gradient descent and quasi-newton methods. DE can therefore also be used on optimization problems that are not even continuous, are noisy, change over time, etc.","[' What is used for multidimensional real-valued functions but does not use the gradient of the problem being optimized?', ' What does DE not require the optimization problem to be differentiable as required by classic optimization methods such as gradient descent and quasi-newton methods?', ' What methods can be used on optimization problems that are not even continuous, are noisy, or change over time?']","['DE', 'DE is used for multidimensional real-valued functions but does not use the gradient of the problem being optimized', 'gradient descent and quasi-newton methods']"
1757,differential evolution,Summary,"DE optimizes a problem by maintaining a population of candidate solutions and creating new candidate solutions by combining existing ones according to its simple formulae, and then keeping whichever candidate solution has the best score or fitness on the optimization problem at hand. In this way, the optimization problem is treated as a black box that merely provides a measure of quality given a candidate solution and the gradient is therefore not needed.
","DE optimizes a problem by maintaining a population of candidate solutions and creating new candidate solutions by combining existing ones according to its simple formulae, and then keeping whichever candidate solution has the best score or fitness on the optimization problem at hand. In this way, the optimization problem is treated as a black box that merely provides a measure of quality given a candidate solution and the gradient is therefore not needed.","[' How does DE optimize a problem?', ' What does DE create by combining existing solutions?', ' Which candidate solution has the best score on the optimization problem at hand?', ' What is treated as a black box that provides a measure of quality given a candidate solution?']","['by maintaining a population of candidate solutions', 'new candidate solutions', 'whichever candidate solution has the best score or fitness', 'the optimization problem']"
1758,differential evolution,Summary,"DE was introduced by Storn and Price in the 1990s. Books have been published on theoretical and practical aspects of using DE in parallel computing, multiobjective optimization, constrained optimization, and the books also contain surveys of application areas. Surveys on the multi-faceted research aspects of DE can be found in journal articles .","DE was introduced by Storn and Price in the 1990s. Books have been published on theoretical and practical aspects of using DE in parallel computing, multiobjective optimization, constrained optimization, and the books also contain surveys of application areas.","[' When was DE introduced by Storn and Price?', ' What has been published on theoretical and practical aspects of using DE in parallel computing?']","['1990s', 'Books']"
1759,differential evolution,"Algorithm <span id=""algo""></span>","A basic variant of the DE algorithm works by having a population of candidate solutions (called agents). These agents are moved around in the search-space by using simple mathematical formulae to combine the positions of existing agents from the population. If the new position of an agent is an improvement then it is accepted and forms part of the population, otherwise the new position is simply discarded. The process is repeated and by doing so it is hoped, but not guaranteed, that a satisfactory solution will eventually be discovered.
",A basic variant of the DE algorithm works by having a population of candidate solutions (called agents). These agents are moved around in the search-space by using simple mathematical formulae to combine the positions of existing agents from the population.,"[' What is a basic variant of the DE algorithm called?', ' What is the population of candidate solutions called in a DE algorithm?', ' How are these agents moved around in the search-space?']","['agents', 'agents', 'by using simple mathematical formulae to combine the positions of existing agents from the population']"
1760,differential evolution,"Algorithm <span id=""algo""></span>","Formally, let 



f
:


R


n


→

R



{\displaystyle f:\mathbb {R} ^{n}\to \mathbb {R} }
 be the fitness function which must be minimized (note that maximization can be performed by considering the function 



h
:=
−
f


{\displaystyle h:=-f}
 instead). The function takes a candidate solution as argument in the form of a vector of real numbers and produces a real number as output which indicates the fitness of the given candidate solution. The gradient of 



f


{\displaystyle f}
 is not known. The goal is to find a solution 




m



{\displaystyle \mathbf {m} }
 for which 



f
(

m

)
≤
f
(

p

)


{\displaystyle f(\mathbf {m} )\leq f(\mathbf {p} )}
 for all 




p



{\displaystyle \mathbf {p} }
 in the search-space, which means that 




m



{\displaystyle \mathbf {m} }
 is the global minimum.
","Formally, let 



f
:


R


n


→

R



{\displaystyle f:\mathbb {R} ^{n}\to \mathbb {R} }
 be the fitness function which must be minimized (note that maximization can be performed by considering the function 



h
:=
−
f


{\displaystyle h:=-f}
 instead). The function takes a candidate solution as argument in the form of a vector of real numbers and produces a real number as output which indicates the fitness of the given candidate solution.","[' What is the fitness function that must be minimized?', ' What can be performed by considering the function h := <unk> f <unk>displaystyle h:=-f<unk> instead?', ' The function takes a candidate solution as argument in the form of what?', ' What does the output of a real number indicate?']","['f\n:\n\n\nR\n\n\nn\n\n\n→\n\nR', 'maximization', 'a vector of real numbers', 'the fitness of the given candidate solution']"
1761,differential evolution,"Algorithm <span id=""algo""></span>","Let 




x

∈


R


n




{\displaystyle \mathbf {x} \in \mathbb {R} ^{n}}
 designate a candidate solution (agent) in the population. The basic DE algorithm can then be described as follows:
","Let 




x

∈


R


n




{\displaystyle \mathbf {x} \in \mathbb {R} ^{n}}
 designate a candidate solution (agent) in the population. The basic DE algorithm can then be described as follows:",[' What is the name of the basic DE algorithm?'],['\\mathbf']
1762,differential evolution,Parameter selection,"The choice of DE parameters 




NP



{\displaystyle {\text{NP}}}
, 




CR



{\displaystyle {\text{CR}}}
 and 



F


{\displaystyle F}
 can have a large impact on optimization performance.  Selecting the DE parameters that yield good performance has therefore been the subject of much research. Rules of thumb for parameter selection were devised by Storn et al. and Liu and Lampinen. Mathematical convergence analysis regarding parameter selection was done by Zaharie.","The choice of DE parameters 




NP



{\displaystyle {\text{NP}}}
, 




CR



{\displaystyle {\text{CR}}}
 and 



F


{\displaystyle F}
 can have a large impact on optimization performance. Selecting the DE parameters that yield good performance has therefore been the subject of much research.","[' The choice of DE parameters can have a large impact on what?', ' Selecting the DE parameters that yield good performance has been the subject of what research?']","['optimization performance', 'CR\n\n\n\n{\\displaystyle {\\text{CR}}}\n and \n\n\n\nF']"
1763,differential evolution,Variants,"Variants of the DE algorithm are continually being developed in an effort to improve optimization performance. Many different schemes for performing crossover and mutation of agents are possible in the basic algorithm given above, see e.g.","Variants of the DE algorithm are continually being developed in an effort to improve optimization performance. Many different schemes for performing crossover and mutation of agents are possible in the basic algorithm given above, see e.g.","[' What is constantly being developed in an effort to improve optimization performance?', ' What schemes are possible in the basic algorithm given above?']","['Variants of the DE algorithm', 'performing crossover and mutation of agents']"
1764,object detection,Summary,"Object detection is a computer technology related to computer vision and image processing that deals with detecting instances of semantic objects of a certain class (such as humans, buildings, or cars) in digital images and videos. Well-researched domains of object detection include face detection and pedestrian detection. Object detection has applications in many areas of computer vision, including image retrieval and video surveillance.
","Object detection is a computer technology related to computer vision and image processing that deals with detecting instances of semantic objects of a certain class (such as humans, buildings, or cars) in digital images and videos. Well-researched domains of object detection include face detection and pedestrian detection.","[' What is a computer technology related to computer vision and image processing?', ' What are two well-researched domains of object detection?']","['Object detection', 'face detection and pedestrian detection']"
1765,object detection,Uses,"It is widely used in computer vision tasks such as image annotation, vehicle counting, activity recognition, face detection, face recognition, video object co-segmentation. It is also used in tracking objects, for example tracking a ball during a football match, tracking movement of a cricket bat, or tracking a person in a video.
","It is widely used in computer vision tasks such as image annotation, vehicle counting, activity recognition, face detection, face recognition, video object co-segmentation. It is also used in tracking objects, for example tracking a ball during a football match, tracking movement of a cricket bat, or tracking a person in a video.","[' What are some of the tasks that it is widely used in?', ' What is another example of a task that it can be used for?', ' When is a ball tracked?', ' What is a cricket bat used for?', ' What is another term for tracking someone in a video?']","['image annotation, vehicle counting, activity recognition, face detection, face recognition, video object co-segmentation', 'tracking objects, for example tracking a ball during a football match', 'during a football match', 'tracking objects', 'tracking objects']"
1766,object detection,Concept,"Every object class has its own special features that helps in classifying the class – for example all circles are round.
Object class detection uses these special features. For example, when looking for circles, objects that are at a particular distance from a point (i.e. the center) are sought. Similarly, when looking for squares, objects that are perpendicular at corners and have equal side lengths are needed.  A similar approach is used for face identification where eyes, nose, and lips can be found and features like skin color and distance between eyes can be found.
",Every object class has its own special features that helps in classifying the class – for example all circles are round. Object class detection uses these special features.,"[' Every object class has its own special features that help in classifying the class?', ' All circles are what?', ' Object class detection uses what special features?']","['all circles are round', 'round', 'all circles are round']"
1767,object detection,Methods,"Methods for object detection generally fall into either neural network-based or non-neural approaches. For non-neural approaches, it becomes necessary to first define features using one of the methods below, then using a technique such as support vector machine (SVM) to do the classification. On the other hand, neural techniques are able to do end-to-end object detection without specifically defining features, and are typically based on convolutional neural networks (CNN).
","Methods for object detection generally fall into either neural network-based or non-neural approaches. For non-neural approaches, it becomes necessary to first define features using one of the methods below, then using a technique such as support vector machine (SVM) to do the classification.","[' What are the two main types of methods used for object detection?', ' What type of approach is used for non-neural approaches?']","['neural network-based or non-neural', 'support vector machine (SVM)']"
1768,service composition,Summary,"In computing, service composability is a design principle, applied within the service-orientation design paradigm, that encourages the design of services that can be reused in multiple solutions that are themselves made up of composed services. The ability to recompose the service is ideally independent of the size and complexity of the service composition.","In computing, service composability is a design principle, applied within the service-orientation design paradigm, that encourages the design of services that can be reused in multiple solutions that are themselves made up of composed services. The ability to recompose the service is ideally independent of the size and complexity of the service composition.","[' What is a design principle applied within the service-orientation design paradigm?', ' What encourages the design of services that can be reused in multiple solutions that are themselves made up of composed services?', ' The ability to recompose the service is ideally independent of what?', ' What is ideally independent of the size and complexity of the service composition?']","['service composability', 'service composability', 'the size and complexity of the service composition', 'The ability to recompose the service']"
1769,service composition,Purpose,"The concept of developing software out of independently existing components encourages the concept of composition. This is the underlying concept within object-orientation where the end product is composed of several interlinked objects that have the ability to become part of multiple software solutions, no matter how complex the solution is. The same composition concept is inherited by service-orientation, whereby a business process is automated by combining multiple services. However, within service-orientation there is even greater focus on building services that can be composed and recomposed within multiple solutions to provide the agility promised by the SOA. As a result of this emphasis, some guidelines are required to develop services that can be effectively aggregated into multiple solutions. 
","The concept of developing software out of independently existing components encourages the concept of composition. This is the underlying concept within object-orientation where the end product is composed of several interlinked objects that have the ability to become part of multiple software solutions, no matter how complex the solution is.","[' The concept of developing software out of independently existing components encourages what concept?', ' What is the underlying concept within object-orientation?']","['composition', 'composition']"
1770,service composition,Purpose,"The service composability principle provides design considerations that help towards designing composable services with a view to encourage service reuse as much as possible. The guidelines provided by this principle prepare the service so that it is ready to participate in service compositions without requiring any further design changes.
",The service composability principle provides design considerations that help towards designing composable services with a view to encourage service reuse as much as possible. The guidelines provided by this principle prepare the service so that it is ready to participate in service compositions without requiring any further design changes.,"[' What principle provides design considerations that help towards designing composable services?', ' The guidelines provided by the principle prepare the service so that it is ready to participate in what?']","['service composability principle', 'service compositions']"
1771,service composition,Application,"The application of the service composability principle requires designing services so that they can be used in a service composition either as a service that controls other services, i.e. a controller service, or as a service that provides functionality to other services in the composition without further composing other services, i.e. a composition member.","The application of the service composability principle requires designing services so that they can be used in a service composition either as a service that controls other services, i.e. a controller service, or as a service that provides functionality to other services in the composition without further composing other services, i.e.","[' What principle requires designing services so that they can be used in a service composition?', ' What is a controller service?', ' What is another service in the composition without further composing other services?']","['service composability', 'a service that controls other services', 'a service that provides functionality']"
1772,service composition,Application,"For the service to provide this dual functionality, the service contract must be designed so that it presents functionality based on varying levels of input and output data. In case if it is required to participate as a composition member, then usually the input parameters to the service would be more fine grained as compared to the situation when it is required to participate as a composition controller. A heavily reused service must be as stateless as possible (service statelessness principle) so that it can provide optimum performance when composed within multiple service compositions.  

The effectiveness of this principle depends upon the extent to which rest of the design principles have been applied successfully.  The application of the standardized service contract principle makes the services interoperable with others, and helps to keep the composition design simpler by avoiding the need to perform runtime data model transformation. By applying the service loose coupling principle, a service could be recomposed with the confidence that it would not create any form of negative coupling with the other service in the composition. The application of the service autonomy and the service statelessness principles increase the reliability and availability of the service so that it be reused in multiple service compositions with increased confidence.
","For the service to provide this dual functionality, the service contract must be designed so that it presents functionality based on varying levels of input and output data. In case if it is required to participate as a composition member, then usually the input parameters to the service would be more fine grained as compared to the situation when it is required to participate as a composition controller.","[' What must be designed for the service to provide dual functionality?', ' What must the service contract be designed so that it presents functionality based on?', ' What would the input parameters to the service be more fine grained as compared to the situation when it is required to participate as a composition controller?']","['the service contract', 'varying levels of input and output data', 'usually']"
1773,service composition,Considerations,"For the service to be an efficient service controller as well as a service member, the underlying technology architecture needs to provide a runtime environment that is scalable and can support the statelessness required by the service. Similarly as the service compositions increase in size, the storage and retrieval of the context data, related to the runtime interaction of the services, may need to be delegated to the runtime environment instead of the services managing this context data to make the service composition more efficient.
","For the service to be an efficient service controller as well as a service member, the underlying technology architecture needs to provide a runtime environment that is scalable and can support the statelessness required by the service. Similarly as the service compositions increase in size, the storage and retrieval of the context data, related to the runtime interaction of the services, may need to be delegated to the runtime environment instead of the services managing this context data to make the service composition more efficient.","[' What must the underlying technology architecture provide for the service to be an efficient service controller as well as a service member?', ' What needs to provide a runtime environment that is scalable and can support the statelessness required by the service?', ' As the service compositions increase in size, the storage and retrieval of what increases in size?', ' What may need to be delegated to the runtime environment instead of the services managing context data?']","['a runtime environment', 'the underlying technology architecture', 'context data', 'the storage and retrieval of the context data']"
1774,service composition,Considerations,"As more and more service compositions are built, there is a tendency of getting dependent on a service that is highly reused. This requires careful analysis during the design of the service compositions and considering alternate standby services for critical functionality. On the other hand, it may become difficult to evolve a service that is now become a part of multiple service compositions. This could be addressed by the application of the Concurrent Contracts design pattern that advocates maintaining multiple concurrent contracts for a service. This way the service can evolve while providing backward compatibility.
","As more and more service compositions are built, there is a tendency of getting dependent on a service that is highly reused. This requires careful analysis during the design of the service compositions and considering alternate standby services for critical functionality.","[' As more service compositions are built, there is a tendency to get dependent on a service that is highly reused.', ' What requires careful analysis during the design of the service composition?']","['careful analysis during the design of the service compositions and considering alternate standby services for critical functionality', 'As more and more service compositions are built, there is a tendency of getting dependent on a service that is highly reused']"
1775,education,Summary,"Education is the process of facilitating learning, or the acquisition of knowledge, skills, values, morals, beliefs, habits, and personal development. Education originated as transmission of cultural heritage from one generation to the next. Today, educational goals increasingly encompass new ideas such as liberation of learners, critical thinking about presented information, skills needed for the modern society, empathy and complex vocational skills.
","Education is the process of facilitating learning, or the acquisition of knowledge, skills, values, morals, beliefs, habits, and personal development. Education originated as transmission of cultural heritage from one generation to the next.","[' What is the process of facilitating learning?', ' What originated as transmission of cultural heritage from one generation to the next?']","['Education', 'Education']"
1776,education,Summary,"UNESCO defines three main learning settings. Formal education takes place in education and training institutions, is usually structured by curricular aims and objectives, and learning is typically guided by a teacher. In most regions, formal education is compulsory up to a certain age and commonly divided into educational stages such as kindergarten, primary school and secondary school. Nonformal learning occurs as addition or alternative to formal education.  It may be structured according to educational arrangements, but in a more flexible manner, and usually takes place in community-based, workplace-based or civil society-based settings. Lastly, informal settings occurs in daily life, in the family, any experience that has a formative effect on the way one thinks, feels, or acts may be considered educational, whether unintentional or intentional. In practice there is a continuum from the highly formalized to the highly informalized, and informal learning can occur in all three settings. For instance, homeschooling can be classified as nonformal or informal, depending upon the structure. 
","UNESCO defines three main learning settings. Formal education takes place in education and training institutions, is usually structured by curricular aims and objectives, and learning is typically guided by a teacher.","[' How many main learning settings does UNESCO define?', ' Where does formal education take place?', ' Who guides learning?']","['three', 'education and training institutions', 'a teacher']"
1777,education,Summary,"Regardless of setting, educational methods include teaching, training, storytelling, discussion, and directed research. The methodology of teaching is called pedagogy. Education is supported by a variety of different philosophies, theories and empirical research agendas. 
","Regardless of setting, educational methods include teaching, training, storytelling, discussion, and directed research. The methodology of teaching is called pedagogy.",[' What is the methodology of teaching called?'],['pedagogy']
1778,education,Summary,"There are movements for education reforms, such as for improving quality and efficiency of education towards relevance in students' lives and efficient problem solving in modern or future society at large, or for evidence-based education methodologies. A right to education has been recognized by some governments and the United Nations. Global initiatives aim at achieving the Sustainable Development Goal 4, which promotes quality education for all.
","There are movements for education reforms, such as for improving quality and efficiency of education towards relevance in students' lives and efficient problem solving in modern or future society at large, or for evidence-based education methodologies. A right to education has been recognized by some governments and the United Nations.","[' What is a movement for education reforms?', ' What has been recognized by some governments and the United Nations?']","['improving quality and efficiency of education', 'A right to education']"
1779,education,History,"
Education began in prehistory, as adults trained the young in the knowledge and skills deemed necessary in their society. In pre-literate societies, this was achieved orally and through imitation. Story-telling passed knowledge, values, and skills from one generation to the next. As cultures began to extend their knowledge beyond skills that could be readily learned through imitation, formal education developed. Schools existed in Egypt at the time of the Middle Kingdom. ","
Education began in prehistory, as adults trained the young in the knowledge and skills deemed necessary in their society. In pre-literate societies, this was achieved orally and through imitation.","[' When did education begin?', ' Who trained the young in the knowledge and skills deemed necessary in their society?', ' In pre-literate societies, how was education achieved?']","['prehistory', 'adults', 'orally and through imitation']"
1780,education,History,"Plato founded the Academy in Athens, the first institution of higher learning in Europe. The city of Alexandria in Egypt, established in 330 BCE, became the successor to Athens as the intellectual cradle of Ancient Greece. There, the great Library of Alexandria was built in the 3rd century BCE. European civilizations suffered a collapse of literacy and organization following the fall of Rome in CE 476.","Plato founded the Academy in Athens, the first institution of higher learning in Europe. The city of Alexandria in Egypt, established in 330 BCE, became the successor to Athens as the intellectual cradle of Ancient Greece.","[' Who founded the Academy in Athens?', ' What was the first institution of higher learning in Europe?', ' When was the city of Alexandria established?']","['Plato', 'the Academy in Athens', '330 BCE']"
1781,education,History,"In China, Confucius (551–479 BCE), of the State of Lu, was the country's most influential ancient philosopher, whose educational outlook continues to influence the societies of China and neighbours like Korea, Japan, and Vietnam. Confucius gathered disciples and searched in vain for a ruler who would adopt his ideals for good governance, but his Analects were written down by followers and have continued to influence education in East Asia into the modern era.","In China, Confucius (551–479 BCE), of the State of Lu, was the country's most influential ancient philosopher, whose educational outlook continues to influence the societies of China and neighbours like Korea, Japan, and Vietnam. Confucius gathered disciples and searched in vain for a ruler who would adopt his ideals for good governance, but his Analects were written down by followers and have continued to influence education in East Asia into the modern era.","[' Who was the most influential ancient philosopher in China?', ' When was Confucius born?', ' What was the name of the State of Lu?', ' What did Analects do for education in East Asia?']","['Confucius', '551', 'Confucius', 'written down by followers and have continued to influence education in East Asia into the modern era']"
1782,education,History,"The Aztecs had schools for the noble youths called Calmecac where they would receive rigorous religious and military training. The Aztecs also had a well-developed theory about education, which has an equivalent word in Nahuatl called tlacahuapahualiztli. It means ""the art of raising or educating a person"", or ""the art of strengthening or bringing up men"". This was a broad conceptualization of education, which prescribed that it begins at home, supported by formal schooling, and reinforced by community living. Historians cite that formal education was mandatory for everyone regardless of social class and gender. There was also the word neixtlamachiliztli, which is ""the act of giving wisdom to the face."" These concepts underscore a complex set of educational practices, which was oriented towards communicating to the next generation the experience and intellectual heritage of the past for the purpose of individual development and his integration into the community.","The Aztecs had schools for the noble youths called Calmecac where they would receive rigorous religious and military training. The Aztecs also had a well-developed theory about education, which has an equivalent word in Nahuatl called tlacahuapahualiztli.","[' What school did the Aztecs have for noble youths called?', ' What did the Calmecac school provide?', ' Who had a well-developed theory about education?']","['Calmecac', 'rigorous religious and military training', 'The Aztecs']"
1783,education,History,"After the Fall of Rome, the Catholic Church became the sole preserver of literate scholarship in Western Europe. The church established cathedral schools in the Early Middle Ages as centres of advanced education. Some of these establishments ultimately evolved into medieval universities and forebears of many of Europe's modern universities. During the High Middle Ages, Chartres Cathedral operated the famous and influential Chartres Cathedral School. The medieval universities of Western Christendom were well-integrated across all of Western Europe, encouraged freedom of inquiry, and produced a great variety of fine scholars and natural philosophers, including Thomas Aquinas of the University of Naples, Robert Grosseteste of the University of Oxford, an early expositor of a systematic method of scientific experimentation, and Saint Albert the Great, a pioneer of biological field research. Founded in 1088, the University of Bologne is considered the first, and the oldest continually operating university.","After the Fall of Rome, the Catholic Church became the sole preserver of literate scholarship in Western Europe. The church established cathedral schools in the Early Middle Ages as centres of advanced education.","[' What was the sole preserver of literate scholarship in Western Europe?', ' When did the Catholic Church establish cathedral schools as centres of advanced education?']","['the Catholic Church', 'Early Middle Ages']"
1784,education,History,"The Renaissance in Europe ushered in a new age of scientific and intellectual inquiry and appreciation of ancient Greek and Roman civilizations. Around 1450, Johannes Gutenberg developed a printing press, which allowed works of literature to spread more quickly. The European Age of Empires saw European ideas of education in philosophy, religion, arts and sciences spread out across the globe. Missionaries and scholars also brought back new ideas from other civilizations – as with the Jesuit China missions who played a significant role in the transmission of knowledge, science, and culture between China and Europe, translating works from Europe like Euclid's Elements for Chinese scholars and the thoughts of Confucius for European audiences. The Enlightenment saw the emergence of a more secular educational outlook in Europe. Much of modern traditional Western and Eastern education is based on the Prussian education system.","The Renaissance in Europe ushered in a new age of scientific and intellectual inquiry and appreciation of ancient Greek and Roman civilizations. Around 1450, Johannes Gutenberg developed a printing press, which allowed works of literature to spread more quickly.","[' What ushered in a new age of scientific and intellectual inquiry and appreciation of ancient Greek and Roman civilizations?', ' When did Johannes Gutenberg develop a printing press?']","['The Renaissance in Europe', '1450']"
1785,education,History,"In most countries today, full-time education, whether at school or otherwise, is compulsory for all children up to a certain age. Due to this the proliferation of compulsory education, combined with population growth, UNESCO has calculated that in the next 30 years more people will receive formal education than in all of human history thus far.","In most countries today, full-time education, whether at school or otherwise, is compulsory for all children up to a certain age. Due to this the proliferation of compulsory education, combined with population growth, UNESCO has calculated that in the next 30 years more people will receive formal education than in all of human history thus far.","[' What is compulsory for all children up to a certain age in most countries today?', ' In the next 30 years, how many people will receive formal education than in all countries?', ' How many years more people will receive formal education than in all of human history?']","['full-time education', 'more', '30']"
1786,education,Formal,"Formal education occurs in a structured environment whose explicit purpose is teaching students. Usually, formal education takes place in a school environment with classrooms of multiple students learning together with a trained, certified teacher of the subject. Most school systems are designed around a set of values or ideals that govern all educational choices in that system. Such choices include curriculum, organizational models, design of the physical learning spaces (e.g. classrooms), student-teacher interactions, methods of assessment, class size, educational activities, and more.","Formal education occurs in a structured environment whose explicit purpose is teaching students. Usually, formal education takes place in a school environment with classrooms of multiple students learning together with a trained, certified teacher of the subject.","[' What is the explicit purpose of formal education?', ' Where does formal education usually take place?']","['teaching students', 'a school environment']"
1787,education,Formal,"The International Standard Classification of Education (ISCED) was created by UNESCO as a statistical base to compare education systems. In 1997, it defined 7 levels of education and 25 fields, though the fields were later separated out to form a different project. The current version ISCED 2011 has 9 rather than 7 levels, created by dividing the tertiary pre-doctorate level into three levels. It also extended the lowest level (ISCED 0) to cover a new sub-category of early childhood educational development programmes, which target children below the age of 3 years.","The International Standard Classification of Education (ISCED) was created by UNESCO as a statistical base to compare education systems. In 1997, it defined 7 levels of education and 25 fields, though the fields were later separated out to form a different project.","[' Who created the International Standard Classification of Education?', ' How many levels of education did the ISCED define in 1997?']","['UNESCO', '7']"
1788,education,Public schooling,"The education sector or education system is a group of institutions (ministries of education, local educational authorities, teacher training institutions, schools, universities, etc.) whose primary purpose is to provide education to children and young people in educational settings. It involves a wide range of people (curriculum developers, inspectors, school principals, teachers, school nurses, students, etc.). These institutions can vary according to different contexts.","The education sector or education system is a group of institutions (ministries of education, local educational authorities, teacher training institutions, schools, universities, etc.) whose primary purpose is to provide education to children and young people in educational settings.","[' What is a group of institutions called?', ' What is the primary purpose of the education sector?']","['The education sector or education system', 'to provide education to children and young people in educational settings']"
1789,education,Public schooling,"Schools deliver education, with support from the rest of the education system through various elements such as education policies and guidelines – to which school policies can refer – curricula and learning materials, as well as pre- and in-service teacher training programmes. The school environment – both physical (infrastructures) and psychological (school climate) – is also guided by school policies that should ensure the well-being of students when they are in school. The Organisation for Economic Co-operation and Development has found that schools tend to perform best when principals have full authority and responsibility for ensuring that students are proficient in core subjects upon graduation. They must also seek feedback from students for quality-assurance and improvement. Governments should limit themselves to monitoring student proficiency.","Schools deliver education, with support from the rest of the education system through various elements such as education policies and guidelines – to which school policies can refer – curricula and learning materials, as well as pre- and in-service teacher training programmes. The school environment – both physical (infrastructures) and psychological (school climate) – is also guided by school policies that should ensure the well-being of students when they are in school.","[' How do schools deliver education?', ' What do school policies and guidelines refer to?', ' Pre- and in-service teacher training programmes are examples of what?', ' What are the two types of school environments?', ' What should ensure the well-being of students when they are in school?']","['through various elements such as education policies and guidelines – to which school policies can refer – curricula and learning materials, as well as pre- and in-service teacher training programmes', 'curricula and learning materials', 'education policies and guidelines', 'physical (infrastructures) and psychological', 'The school environment – both physical (infrastructures) and psychological (school climate) – is also guided by school policies']"
1790,education,Public schooling,"The education sector is fully integrated into society, through interactions with numerous stakeholders and other sectors. These include parents, local communities, religious leaders, NGOs, stakeholders involved in health, child protection, justice and law enforcement (police), media and political leadership.","The education sector is fully integrated into society, through interactions with numerous stakeholders and other sectors. These include parents, local communities, religious leaders, NGOs, stakeholders involved in health, child protection, justice and law enforcement (police), media and political leadership.","[' What sector is fully integrated into society?', ' Parents, local communities, religious leaders, NGOs, stakeholders involved in health, child protection, justice, and law enforcement (police), media and political leadership are examples of what?']","['education', 'parents']"
1791,education,Development goals,"Joseph Chimombo pointed out education's role as a policy instrument, capable of instilling social change and economic advancement in developing countries by giving communities the opportunity to take control of their destinies. The 2030 Agenda for Sustainable Development, adopted by the United Nations (UN) General Assembly in September 2015, calls for a new vision to address the environmental, social and economic concerns facing the world today. The Agenda includes 17 Sustainable Development Goals (SDGs), including SDG 4 on education.","Joseph Chimombo pointed out education's role as a policy instrument, capable of instilling social change and economic advancement in developing countries by giving communities the opportunity to take control of their destinies. The 2030 Agenda for Sustainable Development, adopted by the United Nations (UN) General Assembly in September 2015, calls for a new vision to address the environmental, social and economic concerns facing the world today.","["" Who pointed out education's role as a policy instrument?"", ' Who adopted the 2030 Agenda for Sustainable Development in September 2015?', ' What did Joseph Chimombo say education can do in developing countries?', ' When did the UN General Assembly convene?', ' What does the UN call for?']","['Joseph Chimombo', 'United Nations (UN) General Assembly', 'instilling social change and economic advancement', 'September 2015', 'a new vision to address the environmental, social and economic concerns facing the world today']"
1792,education,Development goals,"Since 1909, the percentage of children in the developing world attending school has increased. Before then, a small minority of boys attended school. By the start of the twenty-first century, the majority of children in most regions of the world attended some form of school. By 2016, over 91 percent of children are enrolled in formal primary schooling. However, a learning crisis has emerged across the globe, due to the fact that a large proportion of students enrolled in school are not learning. A World Bank study found that ""53 percent of children in low- and middle-income countries cannot read and understand a simple story by the end of primary school."" While schooling has increased rapidly over the last few decades, learning has not followed suit. 
","Since 1909, the percentage of children in the developing world attending school has increased. Before then, a small minority of boys attended school.","[' Since what year has the percentage of children in the developing world attending school increased?', ' Before 1909, what percentage of boys attended school?']","['1909', 'a small minority']"
1793,education,Development goals,"Universal Primary Education was one of the eight international Millennium Development Goals, towards which progress has been made in the past decade, though barriers still remain. Securing charitable funding from prospective donors is one particularly persistent problem. Researchers at the Overseas Development Institute have indicated that the main obstacles to funding for education include conflicting donor priorities, an immature aid architecture, and a lack of evidence and advocacy for the issue. Additionally, Transparency International has identified corruption in the education sector as a major stumbling block to achieving Universal Primary Education in Africa. Furthermore, demand in the developing world for improved educational access is not as high as foreigners have expected. Indigenous governments are reluctant to take on the ongoing costs involved. There is also economic pressure from some parents, who prefer their children to earn money in the short term rather than work towards the long-term benefits of education.","Universal Primary Education was one of the eight international Millennium Development Goals, towards which progress has been made in the past decade, though barriers still remain. Securing charitable funding from prospective donors is one particularly persistent problem.","[' What was one of the eight international Millennium Development Goals?', ' What is one particularly persistent problem?']","['Universal Primary Education', 'Securing charitable funding from prospective donors']"
1794,education,Development goals,"A study conducted by the UNESCO International Institute for Educational Planning indicates that stronger capacities in educational planning and management may have an important spill-over effect on the system as a whole. Sustainable capacity development requires complex interventions at the institutional, organizational and individual levels that could be based on some foundational principles:","A study conducted by the UNESCO International Institute for Educational Planning indicates that stronger capacities in educational planning and management may have an important spill-over effect on the system as a whole. Sustainable capacity development requires complex interventions at the institutional, organizational and individual levels that could be based on some foundational principles:","[' Who conducted the study?', ' What may stronger capacities in educational planning and management have an important spill-over effect on the system as a whole?', ' Sustainable capacity development requires complex interventions at what levels?', ' What could be based on some foundational principles?']","['UNESCO International Institute for Educational Planning', 'A study conducted by the UNESCO International Institute for Educational Planning', 'institutional, organizational and individual', 'Sustainable capacity development requires complex interventions']"
1795,education,Economics,"It has been argued that high rates of education are essential for countries to be able to achieve high levels of economic growth. Empirical analyses tend to support the theoretical prediction that poor countries should grow faster than rich countries because they can adopt cutting-edge technologies already tried and tested by rich countries. However, technology transfer requires knowledgeable managers and engineers who are able to operate new machines or production practices borrowed from the leader in order to close the gap through imitation. Therefore, a country's ability to learn from the leader is a function of its stock of ""human capital"". Recent study of the determinants of aggregate economic growth have stressed the importance of fundamental economic institutions and the role of cognitive skills.",It has been argued that high rates of education are essential for countries to be able to achieve high levels of economic growth. Empirical analyses tend to support the theoretical prediction that poor countries should grow faster than rich countries because they can adopt cutting-edge technologies already tried and tested by rich countries.,"[' What has been argued is essential for countries to be able to achieve high levels of economic growth?', ' Empirical analyses tend to support the theoretical prediction that poor countries should grow faster than rich countries because they can adopt cutting edge technologies already tried and tested?', ' Because they can adopt cutting-edge technologies already tried and tested by rich countries?']","['high rates of education', 'high rates of education are essential for countries to be able to achieve high levels of economic growth.', 'poor countries']"
1796,education,Economics,"At the level of the individual, there is a large literature, generally related to the work of Jacob Mincer, on how earnings are related to the schooling and other human capital. This work has motivated many studies, but is also controversial. The chief controversies revolve around how to interpret the impact of schooling. Some students who have indicated a high potential for learning, by testing with a high intelligence quotient, may not achieve their full academic potential, due to financial difficulties.","At the level of the individual, there is a large literature, generally related to the work of Jacob Mincer, on how earnings are related to the schooling and other human capital. This work has motivated many studies, but is also controversial.","[' Who wrote a large amount of literature on how earnings are related to schooling and other human capital?', ' What has motivated many studies?']","['Jacob Mincer', 'Jacob Mincer']"
1797,education,Development,"The world is changing at an ever quickening rate, which means that a lot of knowledge becomes obsolete and inaccurate more quickly. The emphasis is therefore shifting to teaching the skills of learning: to picking up new knowledge quickly and in as agile a way as possible. Finnish schools have begun to move away from the regular subject-focused curricula, introducing instead developments like phenomenon-based learning, where students study concepts like climate change instead. There are also active educational interventions to implement programs and paths specific to non-traditional students, such as first generation students.
","The world is changing at an ever quickening rate, which means that a lot of knowledge becomes obsolete and inaccurate more quickly. The emphasis is therefore shifting to teaching the skills of learning: to picking up new knowledge quickly and in as agile a way as possible.","[' At what rate is the world changing?', ' A lot of knowledge becomes obsolete and inaccurate more quickly what?', ' What is the emphasis shifting to teaching?']","['ever quickening rate', 'The world is changing at an ever quickening rate', 'skills of learning']"
1798,education,Development,"Education is also becoming a commodity no longer reserved for children; adults need it too. Some governmental bodies, like the Finnish Innovation Fund Sitra in Finland, have proposed compulsory lifelong education.","Education is also becoming a commodity no longer reserved for children; adults need it too. Some governmental bodies, like the Finnish Innovation Fund Sitra in Finland, have proposed compulsory lifelong education.","[' What is becoming a commodity no longer reserved for children?', ' What does the Finnish Innovation Fund Sitra propose?']","['Education', 'compulsory lifelong education']"
1799,education,Development,"Studies found that automation is likely to eliminate nearly half the jobs in developed countries during roughly the next two decades. Automation is therefore considered to be a major factor in a ""race between education and technology"". Automation technologies and their application may render certain currently taught skills and knowledge redundant while increasing the need for other curricula – such as material related to the application of automation. It has been argued that formal education is ""teaching workers the wrong things, and that deep reform is essential to facilitate the development of digital knowledge and technical skills, as well as nonroutine cognitive and noncognitive (or ""soft"") skills"" and that the formal state-organized education system – which is built on the Industrial Revolution model and focuses on IQ and memorization is losing relevance. FSchools were found rarely teach in forms of ""learning by doing"", and many children above a certain age ""hate school"" in terms of the material and subjects being taught, with much of it being a ""waste of time"" that gets forgotten quickly and is useless in modern society. Moreover, the material currently being taught may not be taught in a highly time-efficient manner and analyzing educational issues over time and using relevant forms of student feedback in efficiency analysis were found to be important. Some research investigates how education can facilitate students' interest in topics – and jobs – that scientific research, data, economic players, financial markets, and other economic mechanisms consider important to contemporary and future human civilization and states.","Studies found that automation is likely to eliminate nearly half the jobs in developed countries during roughly the next two decades. Automation is therefore considered to be a major factor in a ""race between education and technology"".","[' How many jobs are likely to be eliminated by automation in developed countries in the next two decades?', ' What is considered a major factor in a race between education and technology?']","['nearly half', 'Automation']"
1800,education,Development,"Research and data indicate future environmental conditions will be ""far more dangerous than currently believed"", with a review concluding that the current challenges that humanity faces are enormous. The effective resolval of such challenges may require novel lesson plans tailored towards skills and knowledge found to be both required and reasonable to be taught at the respective age with the respective methodology despite novel technological computation and information retrieval technologies such as smartphones, mathematical software and the World Wide Web. Environmental education is not widely taught extensively or facilitated while being potentially important to the protection and generation of – often unquantified – economic value such as clean air that agents of the economy can breathe. Education is often considered to be a national investment which may not always optimize for cost-efficiency while optimizing only in terms of contemporary economic value metrics or evaluations such as of finance and GDP without consideration of economic values or priorizations beyond these tools such as minimized marine pollution and maximized climate change mitigation. Researchers found that there is a growing disconnect between humans and nature and that schools ""are not properly preparing students to become the scientists of tomorrow"". They also find that critical thought, social responsibility, health and safety are often neglected. According to UNESCO, ""for a country to meet the basic needs of its people, the teaching of science is a strategic imperative"".","Research and data indicate future environmental conditions will be ""far more dangerous than currently believed"", with a review concluding that the current challenges that humanity faces are enormous. The effective resolval of such challenges may require novel lesson plans tailored towards skills and knowledge found to be both required and reasonable to be taught at the respective age with the respective methodology despite novel technological computation and information retrieval technologies such as smartphones, mathematical software and the World Wide Web.","[' Research and data indicate future environmental conditions will be ""far more dangerous than currently believed""?', ' A review concludes that the current challenges that humanity faces are what?', ' The effective resolval of such challenges may require what kind of lesson plans?', ' At what age are skills and knowledge required and reasonable to be taught?', ' Smartphones, mathematical software and the World Wide Web are examples of what?']","['Research', 'enormous', 'novel', 'respective age', 'novel technological computation and information retrieval technologies']"
1801,education,Development,"Studies have shown that active learning rarely applied in schools is highly efficacious. Studies found that massive open online courses offer a pathway to employment that currently bypasses conventional universities and their degree programs while often being more relevant to contemporary economic activities and the students' interests. Such online courses are not commonly part of formal education but are typically both completed and selected entirely on behalf of the student, sometimes with the support of peers over online forums. In contrast, blended learning merges online education with forms of face‐to‐face communication and traditional class-based education in classrooms, revealing itself to have the general capacity for increasingly relevant, resource-efficient and effective approaches to education. Deploying, using, and managing various tools or platforms for education typically imply an increase in economic investment. Expenses for education are often large with many calling for further increases. Potential policies for the development of international open source educational software using latest technologies may minimize costs, hardware requirements, problem-resolval efforts and deployment-times while increasing robustness, security and functional features of the software.",Studies have shown that active learning rarely applied in schools is highly efficacious. Studies found that massive open online courses offer a pathway to employment that currently bypasses conventional universities and their degree programs while often being more relevant to contemporary economic activities and the students' interests.,"[' Studies have shown that active learning rarely applied in schools is highly efficacious what?', ' Massive open online courses offer a pathway to employment that bypasses conventional universities and their degree programs?']","['massive open online courses', 'massive']"
1802,linear programming,Summary,"Linear programming (LP, also called linear optimization) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships. Linear programming is a special case of mathematical programming (also known as mathematical optimization).
","Linear programming (LP, also called linear optimization) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships. Linear programming is a special case of mathematical programming (also known as mathematical optimization).","[' What is linear programming also known as?', ' What is a method to achieve the best outcome in a mathematical model?']","['linear optimization', 'Linear programming']"
1803,linear programming,Summary,"More formally, linear programming is a technique for the optimization of a linear objective function, subject to linear equality and linear inequality constraints. Its feasible region is a convex polytope, which is a set defined as the intersection of finitely many half spaces, each of which is defined by a linear inequality. Its objective function is a real-valued affine (linear) function defined on this polyhedron. A linear programming algorithm finds a point in the polytope where this function has the smallest (or largest) value if such a point exists.
","More formally, linear programming is a technique for the optimization of a linear objective function, subject to linear equality and linear inequality constraints. Its feasible region is a convex polytope, which is a set defined as the intersection of finitely many half spaces, each of which is defined by a linear inequality.","[' What is a technique for the optimization of a linear objective function?', ' What is the feasible region of linear programming?', ' A convex polytope is defined as the intersection of how many half spaces?', ' How is each half space defined?']","['linear programming', 'convex polytope', 'finitely', 'a linear inequality']"
1804,linear programming,Summary,"Here the components of x are the variables to be determined, c and b are given vectors (with 





c


T




{\displaystyle \mathbf {c} ^{T}}
 indicating that the coefficients of c are used as a single-row matrix for the purpose of forming the matrix product), and A is a given matrix. The function whose value is to be maximized or minimized (




x

↦


c


T



x



{\displaystyle \mathbf {x} \mapsto \mathbf {c} ^{T}\mathbf {x} }
 in this case) is called the objective function. The inequalities Ax ≤ b and x ≥ 0 are the constraints which specify a convex polytope over which the objective function is to be optimized. In this context, two vectors are comparable when they have the same dimensions. If every entry in the first is less-than or equal-to the corresponding entry in the second, then it can be said that the first vector is less-than or equal-to the second vector.
","Here the components of x are the variables to be determined, c and b are given vectors (with 





c


T




{\displaystyle \mathbf {c} ^{T}}
 indicating that the coefficients of c are used as a single-row matrix for the purpose of forming the matrix product), and A is a given matrix. The function whose value is to be maximized or minimized (




x

↦


c


T



x



{\displaystyle \mathbf {x} \mapsto \mathbf {c} ^{T}\mathbf {x} }
 in this case) is called the objective function.","[' What are the components of x?', ' What are c and b given vectors?', ' What is A?', ' What is the function whose value is to be maximized or minimized called?']","['variables to be determined', 'c\n\n\nT\n\n\n\n\n{\\displaystyle \\mathbf {c} ^{T}}\n indicating that the coefficients of c are used as a single-row matrix for the purpose of forming the matrix product), and A is a given matrix. The function whose value is to be maximized or minimized (\n\n\n\n\nx\n\n↦\n\n\nc\n\n\nT\n\n\n\nx\n\n\n\n{\\displaystyle \\mathbf {x} \\mapsto \\mathbf {c} ^{T}\\mathbf {x}', 'a given matrix', 'the objective function']"
1805,linear programming,Summary,"Linear programming can be applied to various fields of study. It is widely used in mathematics, and to a lesser extent in business, economics, and for some engineering problems. Industries that use linear programming models include transportation, energy, telecommunications, and manufacturing. It has proven useful in modeling diverse types of problems in planning, routing, scheduling, assignment, and design.
","Linear programming can be applied to various fields of study. It is widely used in mathematics, and to a lesser extent in business, economics, and for some engineering problems.","[' What can be applied to various fields of study?', ' Linear programming is widely used in mathematics, and what other field?']","['Linear programming', 'business, economics']"
1806,linear programming,History,"In 1939 a linear programming formulation of a problem that is equivalent to the general linear programming problem was given by the Soviet mathematician and economist Leonid Kantorovich, who also proposed a method for solving it. It is a way he developed, during World War II, to plan expenditures and returns in order to reduce costs of the army and to increase losses imposed on the enemy. Kantorovich's work was initially neglected in the USSR. About the same time as Kantorovich, the Dutch-American economist T. C. Koopmans formulated classical economic problems as linear programs. Kantorovich and Koopmans later shared the 1975 Nobel prize in economics. In 1941, Frank Lauren Hitchcock also formulated transportation  problems as linear programs and gave a solution very similar to the later simplex method. Hitchcock had died in 1957 and the Nobel prize is not awarded posthumously.
","In 1939 a linear programming formulation of a problem that is equivalent to the general linear programming problem was given by the Soviet mathematician and economist Leonid Kantorovich, who also proposed a method for solving it. It is a way he developed, during World War II, to plan expenditures and returns in order to reduce costs of the army and to increase losses imposed on the enemy.","["" What was Leonid Kantorovich's profession?"", ' When was Kantovich born?', ' What was the name of the Soviet mathematician and economist who gave a linear programming formulation?', ' During what war did he plan expenditures and returns?', ' What was he planning to reduce costs of?']","['mathematician and economist', 'Soviet', 'Leonid Kantorovich', 'World War II', 'the army']"
1807,linear programming,History,"During 1946–1947, George B. Dantzig independently developed general linear programming formulation to use for planning problems in the US Air Force. In 1947, Dantzig also invented the simplex method that for the first time efficiently tackled the linear programming problem in most cases. When Dantzig arranged a meeting with John von Neumann to discuss his simplex method, Neumann immediately conjectured the theory of duality by realizing that the problem he had been working in game theory was equivalent. Dantzig provided formal proof in an unpublished report ""A Theorem on Linear Inequalities"" on January 5, 1948. Dantzig's work was made available to public in 1951. In the post-war years, many industries applied it in their daily planning.
","During 1946–1947, George B. Dantzig independently developed general linear programming formulation to use for planning problems in the US Air Force. In 1947, Dantzig also invented the simplex method that for the first time efficiently tackled the linear programming problem in most cases.","[' In what year did George B. Dantzig develop a general linear programming formulation?', ' Who developed the simplex method in 1947?']","['1946', 'George B. Dantzig']"
1808,linear programming,History,"Dantzig's original example was to find the best assignment of 70 people to 70 jobs. The computing power required to test all the permutations to select the best assignment is vast; the number of possible configurations exceeds the number of particles in the observable universe. However, it takes only a moment to find the optimum solution by posing the problem as a linear program and applying the simplex algorithm. The theory behind linear programming drastically reduces the number of possible solutions that must be checked.
",Dantzig's original example was to find the best assignment of 70 people to 70 jobs. The computing power required to test all the permutations to select the best assignment is vast; the number of possible configurations exceeds the number of particles in the observable universe.,"["" How many people to 70 jobs was Dantzig's original example?"", ' How much computing power is required to test all the permutations to select the best assignment?', ' What exceeds the number of particles in the observable universe?']","['70', 'vast', 'the number of possible configurations']"
1809,linear programming,Uses,"Linear programming is a widely used field of optimization for several reasons. Many practical problems in operations research can be expressed as linear programming problems. Certain special cases of linear programming, such as network flow problems and multicommodity flow problems are considered important enough to have generated much research on specialized algorithms for their solution. A number of algorithms for other types of optimization problems work by solving LP problems as sub-problems. Historically, ideas from linear programming have inspired many of the central concepts of optimization theory, such as duality, decomposition, and the importance of convexity and its generalizations. Likewise, linear programming was heavily used in the early formation of microeconomics and it is currently utilized in company management, such as planning, production, transportation, technology and other issues. Although the modern management issues are ever-changing, most companies would like to maximize profits and minimize costs with limited resources. Google uses linear programming to stabilize YouTube videos. Therefore, many issues can be characterized as linear programming problems.
",Linear programming is a widely used field of optimization for several reasons. Many practical problems in operations research can be expressed as linear programming problems.,"[' What is a widely used field of optimization?', ' What can be expressed as linear programming problems?']","['Linear programming', 'practical problems in operations research']"
1810,linear programming,Standard form,"Standard form is the usual and most intuitive form of describing a linear programming problem. It consists of the following three parts:
",Standard form is the usual and most intuitive form of describing a linear programming problem. It consists of the following three parts:,"[' What is the most intuitive form of describing a linear programming problem?', ' How many parts does standard form consist of?']","['Standard form', 'three']"
1811,linear programming,Augmented form (slack form),"Linear programming problems can be converted into an augmented form in order to apply the common form of the simplex algorithm. This form introduces non-negative slack variables to replace inequalities with equalities in the constraints. The problems can then be written in the following block matrix form:
",Linear programming problems can be converted into an augmented form in order to apply the common form of the simplex algorithm. This form introduces non-negative slack variables to replace inequalities with equalities in the constraints.,"[' What can be converted into an augmented form?', ' What is introduced to replace inequalities with equalities in the constraints?']","['Linear programming problems', 'non-negative slack variables']"
1812,linear programming,Duality,"Every linear programming problem, referred to as a primal problem, can be converted into a dual problem, which provides an upper bound to the optimal value of the primal problem. In matrix form, we can express the primal problem as:
","Every linear programming problem, referred to as a primal problem, can be converted into a dual problem, which provides an upper bound to the optimal value of the primal problem. In matrix form, we can express the primal problem as:","[' What is a linear programming problem referred to as?', ' What can be converted into a dual problem?', ' In what form can the primal problem be expressed?']","['primal problem', 'Every linear programming problem, referred to as a primal problem', 'matrix']"
1813,linear programming,Duality,"There are two ideas fundamental to duality theory. One is the fact that (for the symmetric dual) the dual of a dual linear program is the original primal linear program. Additionally, every feasible solution for a linear program gives a bound on the optimal value of the objective function of its dual.  The weak duality theorem states that the objective function value of the dual at any feasible solution is always greater than or equal to the objective function value of the primal at any feasible solution. The strong duality theorem states that if the primal has an optimal solution, x*, then the dual also has an optimal solution, y*, and cTx*=bTy*.
",There are two ideas fundamental to duality theory. One is the fact that (for the symmetric dual) the dual of a dual linear program is the original primal linear program.,"[' What are the two ideas fundamental to duality theory?', ' What is the other idea fundamental?', ' For the symmetric dual, what is the original primal linear program?']","['the fact that (for the symmetric dual) the dual of a dual linear program is the original primal linear program', 'the fact that (for the symmetric dual) the dual of a dual linear program is the original primal linear program', 'the dual of a dual linear program']"
1814,linear programming,Duality,"A linear program can also be unbounded or infeasible. Duality theory tells us that if the primal is unbounded then the dual is infeasible by the weak duality theorem. Likewise, if the dual is unbounded, then the primal must be infeasible. However, it is possible for both the dual and the primal to be infeasible.  See dual linear program for details and several more examples.
",A linear program can also be unbounded or infeasible. Duality theory tells us that if the primal is unbounded then the dual is infeasible by the weak duality theorem.,"[' A linear program can also be what?', ' What theory tells us that if the primal is unbounded then the dual is infeasible by the weak duality theorem?']","['unbounded or infeasible', 'Duality theory']"
1815,linear programming,Complementary slackness,"It is possible to obtain an optimal solution to the dual when only an optimal solution to the primal is known using the complementary slackness theorem. The theorem states:
",It is possible to obtain an optimal solution to the dual when only an optimal solution to the primal is known using the complementary slackness theorem. The theorem states:,"[' How is it possible to obtain an optimal solution to the dual?', ' What does the complementary slackness theorem state?']","['when only an optimal solution to the primal is known using the complementary slackness theorem', 'It is possible to obtain an optimal solution to the dual']"
1816,linear programming,Complementary slackness,"Suppose that x = (x1, x2, ... , xn) is primal feasible and that y = (y1, y2, ... , ym) is dual feasible. Let (w1, w2, ..., wm) denote the corresponding primal slack variables, and let (z1, z2, ... , zn) denote the corresponding dual slack variables. Then x and y are optimal for their respective problems if and only if
","Suppose that x = (x1, x2, ... , xn) is primal feasible and that y = (y1, y2, ... , ym) is dual feasible. Let (w1, w2, ..., wm) denote the corresponding primal slack variables, and let (z1, z2, ... , zn) denote the corresponding dual slack variables.","[' Suppose that x = (x1, x2,..., xn) is primal feasible and that y = (y1, y2,...) is dual feasible?', ' Let (w1, w2,..., wm) denote the corresponding primal slack variables and what?']","['y', 'dual slack variables']"
1817,linear programming,Complementary slackness,"So if the i-th slack variable of the primal is not zero, then the i-th variable of the dual is equal to zero. Likewise, if the j-th slack variable of the dual is not zero, then the j-th variable of the primal is equal to zero.
","So if the i-th slack variable of the primal is not zero, then the i-th variable of the dual is equal to zero. Likewise, if the j-th slack variable of the dual is not zero, then the j-th variable of the primal is equal to zero.","[' If the i-th slack variable of the primal is not zero, then what is equal to zero?', ' What is the j-th variable of a primal equal to?']","['the i-th variable of the dual', 'zero']"
1818,linear programming,Complementary slackness,"This necessary condition for optimality conveys a fairly simple economic principle.  In standard form (when maximizing), if there is slack in a constrained primal resource (i.e., there are ""leftovers""), then additional quantities of that resource must have no value.  Likewise, if there is slack in the dual (shadow) price non-negativity constraint requirement, i.e., the price is not zero, then there must be scarce supplies (no ""leftovers"").
","This necessary condition for optimality conveys a fairly simple economic principle. In standard form (when maximizing), if there is slack in a constrained primal resource (i.e., there are ""leftovers""), then additional quantities of that resource must have no value.","[' What conveys a fairly simple economic principle?', ' If there is slack in a constrained primal resource, what must have no value?']","['necessary condition for optimality', 'additional quantities']"
1819,linear programming,Open problems and recent work,"This closely related set of problems has been cited by Stephen Smale as among the 18 greatest unsolved problems of the 21st century.  In Smale's words, the third version of the problem ""is the main unsolved problem of linear programming theory.""  While algorithms exist to solve linear programming in weakly polynomial time, such as the ellipsoid methods and interior-point techniques, no algorithms have yet been found that allow strongly polynomial-time performance in the number of constraints and the number of variables.  The development of such algorithms would be of great theoretical interest, and perhaps allow practical gains in solving large LPs as well.
","This closely related set of problems has been cited by Stephen Smale as among the 18 greatest unsolved problems of the 21st century. In Smale's words, the third version of the problem ""is the main unsolved problem of linear programming theory.""","[' Stephen Smale cited the third version of the problem as what?', ' What is the main unsolved problem of linear programming theory?']","['the main unsolved problem of linear programming theory', 'the third version of the problem']"
1820,linear programming,Open problems and recent work,"These questions relate to the performance analysis and development of simplex-like methods.  The immense efficiency of the simplex algorithm in practice despite its exponential-time theoretical performance hints that there may be variations of simplex that run in polynomial or even strongly polynomial time.  It would be of great practical and theoretical significance to know whether any such variants exist, particularly as an approach to deciding if LP can be solved in strongly polynomial time.
",These questions relate to the performance analysis and development of simplex-like methods. The immense efficiency of the simplex algorithm in practice despite its exponential-time theoretical performance hints that there may be variations of simplex that run in polynomial or even strongly polynomial time.,"[' What does the efficiency of the simplex algorithm in practice hint at?', ' What may be variations of simplex that run in polynomial time?']","['there may be variations of simplex that run in polynomial or even strongly polynomial time', 'The immense efficiency of the simplex algorithm']"
1821,linear programming,Open problems and recent work,"The simplex algorithm and its variants fall in the family of edge-following algorithms, so named because they solve linear programming problems by moving from vertex to vertex along edges of a polytope.  This means that their theoretical performance is limited by the maximum number of edges between any two vertices on the LP polytope.  As a result, we are interested in knowing the maximum graph-theoretical diameter of polytopal graphs.  It has been proved that all polytopes have subexponential diameter. The recent disproof of the Hirsch conjecture is the first step to prove whether any polytope has superpolynomial diameter. If any such polytopes exist, then no edge-following variant can run in polynomial time. Questions about polytope diameter are of independent mathematical interest.
","The simplex algorithm and its variants fall in the family of edge-following algorithms, so named because they solve linear programming problems by moving from vertex to vertex along edges of a polytope. This means that their theoretical performance is limited by the maximum number of edges between any two vertices on the LP polytope.","[' What algorithm is in the family of edge-following algorithms?', ' Why are the simplex algorithm and its variants so named?', ' How do simplex algorithms solve linear programming problems?', ' What limits their theoretical performance?', ' By the maximum number of edges between any two vertices on the LP polytope?']","['simplex algorithm', 'they solve linear programming problems by moving from vertex to vertex along edges of a polytope', 'by moving from vertex to vertex along edges of a polytope', 'the maximum number of edges between any two vertices on the LP polytope', 'The simplex algorithm']"
1822,linear programming,Open problems and recent work,"Simplex pivot methods preserve primal (or dual) feasibility.  On the other hand, criss-cross pivot methods do not preserve (primal or dual) feasibility – they may visit primal feasible, dual feasible or primal-and-dual infeasible bases in any order.  Pivot methods of this type have been studied since the 1970s.  Essentially, these methods attempt to find the shortest pivot path on the arrangement polytope under the linear programming problem.  In contrast to polytopal graphs, graphs of arrangement polytopes are known to have small diameter, allowing the possibility of strongly polynomial-time criss-cross pivot algorithm without resolving questions about the diameter of general polytopes.","Simplex pivot methods preserve primal (or dual) feasibility. On the other hand, criss-cross pivot methods do not preserve (primal or dual) feasibility – they may visit primal feasible, dual feasible or primal-and-dual infeasible bases in any order.","[' Simplex pivot methods preserve what kind of feasibility?', ' Criss-cross pivot methods do not preserve what type of feasibility – they may visit primal feasible, dual feasible or primal-and-dual infeasible bases?']","['primal', 'primal or dual)']"
1823,linear programming,Integer unknowns,"If all of the unknown variables are required to be integers, then the problem is called an integer programming (IP) or integer linear programming (ILP) problem.  In contrast to linear programming, which can be solved efficiently in the worst case, integer programming problems are in many practical situations (those with bounded variables) NP-hard. 0–1 integer programming or binary integer programming (BIP) is the special case of integer programming where variables are required to be 0 or 1 (rather than arbitrary integers). This problem is also classified as NP-hard, and in fact the decision version was one of Karp's 21 NP-complete problems.
","If all of the unknown variables are required to be integers, then the problem is called an integer programming (IP) or integer linear programming (ILP) problem. In contrast to linear programming, which can be solved efficiently in the worst case, integer programming problems are in many practical situations (those with bounded variables) NP-hard.","[' What is the name of the problem in which all unknown variables are required to be integers?', ' What is another name for the integer linear programming problem?', ' In contrast to linear programming, in which case can linear programming be solved efficiently?', ' What type of programming problems are in many practical situations NP-hard?']","['integer programming (IP) or integer linear programming (ILP) problem', 'ILP', 'worst case', 'integer']"
1824,linear programming,Integer unknowns,"If only some of the unknown variables are required to be integers, then the problem is called a mixed integer programming (MIP) problem.  These are generally also NP-hard because they are even more general than ILP programs.
","If only some of the unknown variables are required to be integers, then the problem is called a mixed integer programming (MIP) problem. These are generally also NP-hard because they are even more general than ILP programs.","[' What is the problem called if only some of the unknown variables are required to be integers?', ' What is a mixed integer programming problem?']","['a mixed integer programming (MIP) problem', 'If only some of the unknown variables are required to be integers']"
1825,linear programming,Integral linear programs,"A linear program in real variables is said to be integral if it has at least one optimal solution which is integral. Likewise, a polyhedron 



P
=
{
x
∣
A
x
≥
0
}


{\displaystyle P=\{x\mid Ax\geq 0\}}
 is said to be integral if for all bounded feasible objective functions c, the linear program 



{
max
c
x
∣
x
∈
P
}


{\displaystyle \{\max cx\mid x\in P\}}
 has an optimum 




x

∗




{\displaystyle x^{*}}
 with integer coordinates. As observed by Edmonds and Giles in 1977, one can equivalently say that the polyhedron 



P


{\displaystyle P}
 is integral if for every bounded feasible integral objective function c, the optimal value of the linear program 



{
max
c
x
∣
x
∈
P
}


{\displaystyle \{\max cx\mid x\in P\}}
 is an integer.
","A linear program in real variables is said to be integral if it has at least one optimal solution which is integral. Likewise, a polyhedron 



P
=
{
x
∣
A
x
≥
0
}


{\displaystyle P=\{x\mid Ax\geq 0\}}
 is said to be integral if for all bounded feasible objective functions c, the linear program 



{
max
c
x
∣
x
∈
P
}


{\displaystyle \{\max cx\mid x\in P\}}
 has an optimum 




x

∗




{\displaystyle x^{*}}
 with integer coordinates.","[' A linear program in real variables is said to be integral if it has at least one optimal solution which is what?', ' A polyhedron P = <unk> x <unk> A x and 0 <unk>displaystyle P=<unk>x<unk>mid Ax<unk>geq 0<unk> is integral for all bounded feasible objective functions c, what is the linear program?', ' What has an optimum x <unk> <unk>displaystyle x<unk>*<unk> with integer coordinates?']","['integral', 'max\nc\nx\n∣\nx\n∈\nP\n}', 'max\nc\nx\n∣\nx\n∈\nP\n}\n\n\n{\\displaystyle \\{\\max cx\\mid x\\in P\\']"
1826,linear programming,Integral linear programs,"Integral linear programs are of central importance in the polyhedral aspect of combinatorial optimization since they provide an alternate characterization of a problem. Specifically, for any problem, the convex hull of the solutions is an integral polyhedron; if this polyhedron has a nice/compact description, then we can efficiently find the optimal feasible solution under any linear objective. Conversely, if we can prove that a linear programming relaxation is integral, then it is the desired description of the convex hull of feasible (integral) solutions.
","Integral linear programs are of central importance in the polyhedral aspect of combinatorial optimization since they provide an alternate characterization of a problem. Specifically, for any problem, the convex hull of the solutions is an integral polyhedron; if this polyhedron has a nice/compact description, then we can efficiently find the optimal feasible solution under any linear objective.","[' What are integral linear programs of central importance in the polyhedral aspect of combinatorial optimization?', ' What provide an alternate characterization of a problem?', ' For any problem, the convex hull of the solutions is an integral polyhedron.', ' What kind of description can we use to efficiently find the optimal feasible solution?']","['they provide an alternate characterization of a problem', 'Integral linear programs', 'Integral linear programs', 'nice/compact']"
1827,linear programming,Integral linear programs,"One common way of proving that a polyhedron is integral is to show that it is totally unimodular. There are other general methods including the integer decomposition property and total dual integrality. Other specific well-known integral LPs include the matching polytope, lattice polyhedra, submodular flow polyhedra, and the intersection of two generalized polymatroids/g-polymatroids – e.g. see Schrijver 2003.
",One common way of proving that a polyhedron is integral is to show that it is totally unimodular. There are other general methods including the integer decomposition property and total dual integrality.,"[' What is a common way of proving that a polyhedron is integral?', ' What is another general method?', ' The integer decomposition property and total dual integrality are examples of what?']","['to show that it is totally unimodular', 'the integer decomposition property and total dual integrality', 'general methods']"
1828,social network analysis,Summary,"Social network analysis (SNA) is the process of investigating social structures  through the use of networks and graph theory. It characterizes networked structures in terms of nodes (individual actors, people, or things within the network) and the ties, edges, or links (relationships or interactions) that connect them.  Examples of social structures commonly visualized through social network analysis include social media networks, memes spread, information circulation, friendship and acquaintance networks, business networks, knowledge networks, difficult working relationships, social networks, collaboration graphs, kinship, disease transmission, and sexual relationships. These networks are often visualized through sociograms in which nodes are represented as points and ties are represented as lines. These visualizations provide a means of qualitatively assessing networks by varying the visual representation of their nodes and edges to reflect attributes of interest.","Social network analysis (SNA) is the process of investigating social structures  through the use of networks and graph theory. It characterizes networked structures in terms of nodes (individual actors, people, or things within the network) and the ties, edges, or links (relationships or interactions) that connect them.","[' What is the process of investigating social structures through the use of networks and graph theory?', ' Social network analysis characterizes networked structures in terms of what?']","['Social network analysis', 'nodes']"
1829,social network analysis,Summary,"Social network analysis has emerged as a key technique in modern sociology.  It has also gained significant popularity in the following - anthropology, biology, demography, communication studies, economics, geography, history, information science, organizational studies, political science, public health,  social psychology, development studies, sociolinguistics, and computer science and is now commonly available as a consumer tool (see the list of SNA software).","Social network analysis has emerged as a key technique in modern sociology. It has also gained significant popularity in the following - anthropology, biology, demography, communication studies, economics, geography, history, information science, organizational studies, political science, public health,  social psychology, development studies, sociolinguistics, and computer science and is now commonly available as a consumer tool (see the list of SNA software).","[' What has emerged as a key technique in modern sociology?', ' Social network analysis has gained significant popularity in what fields?', ' Psychology, development studies, sociolinguistics, and computer science are examples of what?', ' What type of tool is SNA now available as?']","['Social network analysis', 'anthropology, biology, demography, communication studies, economics, geography, history, information science, organizational studies, political science, public health,  social psychology, development studies, sociolinguistics, and computer science', 'Social network analysis', 'consumer']"
1830,social network analysis,Summary,"The advantages of SNA are twofold. Firstly, it can process a large amount of relational data and describe the overall relational network structure. tem and parameter selection to confirm the influential nodes in the network, such as in-degree and out-degree centrality. SNA context and choose which parameters to define the “center” according to the characteristics of the network. Through analyzing nodes, clusters and relations, the communication structure and position of individuals can be clearly described.","The advantages of SNA are twofold. Firstly, it can process a large amount of relational data and describe the overall relational network structure.","[' What are the advantages of SNA?', ' What can SNA process a large amount of?']","['it can process a large amount of relational data and describe the overall relational network structure', 'relational data']"
1831,social network analysis,History,"Social network analysis has its theoretical roots in the work of early sociologists such as Georg Simmel and Émile Durkheim, who wrote about the importance of studying patterns of relationships that connect social actors. Social scientists have used the concept of ""social networks"" since early in the 20th century to connote complex sets of relationships between members of social systems at all scales, from interpersonal to international.","Social network analysis has its theoretical roots in the work of early sociologists such as Georg Simmel and Émile Durkheim, who wrote about the importance of studying patterns of relationships that connect social actors. Social scientists have used the concept of ""social networks"" since early in the 20th century to connote complex sets of relationships between members of social systems at all scales, from interpersonal to international.","[' Who wrote about the importance of studying patterns of relationships that connect social actors?', ' Social scientists have used the concept of ""social networks"" since early in what century?', ' When did social networks begin to be used?', ' What do social networks mean?']","['Georg Simmel and Émile Durkheim', '20th', 'early in the 20th century', 'complex sets of relationships between members of social systems at all scales']"
1832,social network analysis,History,"In the 1930s Jacob Moreno and Helen Jennings introduced basic analytical methods. In 1954, John Arundel Barnes started using the term systematically to denote patterns of ties, encompassing concepts traditionally used by the public and those used by social scientists: bounded groups (e.g., tribes, families) and social categories (e.g., gender, ethnicity). Scholars such as Ronald Burt, Kathleen Carley, Mark Granovetter, David Krackhardt, Edward Laumann, Anatol Rapoport, Barry Wellman, Douglas R. White, and Harrison White expanded the use of systematic social network analysis.","In the 1930s Jacob Moreno and Helen Jennings introduced basic analytical methods. In 1954, John Arundel Barnes started using the term systematically to denote patterns of ties, encompassing concepts traditionally used by the public and those used by social scientists: bounded groups (e.g., tribes, families) and social categories (e.g., gender, ethnicity).","[' When did Jacob Moreno and Helen Jennings introduce basic analytical methods?', ' When did John Arundel Barnes start using the term systematically to denote patterns of ties?', ' What are bounded groups?', ' What are social categories?']","['1930s', '1954', 'tribes, families', 'gender, ethnicity']"
1833,social network analysis,History,"SNA has been extensively used in research on study abroad second language acquisition. Even in the study of literature, network analysis has been applied by Anheier, Gerhards and Romo, Wouter De Nooy, and Burgert Senekal. Indeed, social network analysis has found applications in various academic disciplines, as well as practical applications such as countering money laundering and terrorism.
","SNA has been extensively used in research on study abroad second language acquisition. Even in the study of literature, network analysis has been applied by Anheier, Gerhards and Romo, Wouter De Nooy, and Burgert Senekal.","[' What has been extensively used in research on study abroad second language acquisition?', ' Who has applied network analysis in the study of literature?']","['SNA', 'Anheier, Gerhards and Romo, Wouter De Nooy, and Burgert Senekal']"
1834,social network analysis,Modelling and visualization of networks,"Visual representation of social networks is important to understand the network data and convey the result of the analysis. Numerous methods of visualization for data produced by social network analysis have been presented. Many of the analytic software have modules for network visualization. Exploration of the data is done through displaying nodes and ties in various layouts, and attributing colors, size and other advanced properties to nodes. Visual representations of networks may be a powerful method for conveying complex information, but care should be taken in interpreting node and graph properties from visual displays alone, as they may misrepresent structural properties better captured through quantitative analyses.",Visual representation of social networks is important to understand the network data and convey the result of the analysis. Numerous methods of visualization for data produced by social network analysis have been presented.,"[' What is important to understand the network data and convey the result of the analysis?', ' Numerous methods of visualization have been presented for what?']","['Visual representation of social networks', 'data produced by social network analysis']"
1835,social network analysis,Modelling and visualization of networks,"Signed graphs can be used to illustrate good and bad relationships between humans. A positive edge between two nodes denotes a positive relationship (friendship, alliance, dating) and a negative edge between two nodes denotes a negative relationship (hatred, anger). Signed social network graphs can be used to predict the future evolution of the graph. In signed social networks, there is the concept of ""balanced"" and ""unbalanced"" cycles. A balanced cycle is defined as a cycle where the product of all the signs are positive. According to balance theory, balanced graphs represent a group of people who are unlikely to change their opinions of the other people in the group. Unbalanced graphs represent a group of people who are very likely to change their opinions of the people in their group. For example, a group of 3 people (A, B, and C) where A and B have a positive relationship, B and C have a positive relationship, but C and A have a negative relationship is an unbalanced cycle. This group is very likely to morph into a balanced cycle, such as one where B only has a good relationship with A, and both A and B have a negative relationship with C. By using the concept of balanced and unbalanced cycles, the evolution of signed social network graphs can be predicted.","Signed graphs can be used to illustrate good and bad relationships between humans. A positive edge between two nodes denotes a positive relationship (friendship, alliance, dating) and a negative edge between two nodes denotes a negative relationship (hatred, anger).","[' What can be used to illustrate good and bad relationships between humans?', ' A positive edge between two nodes denotes what type of relationship?']","['Signed graphs', 'positive']"
1836,social network analysis,Modelling and visualization of networks,"Especially when using social network analysis as a tool for facilitating change, different approaches of participatory network mapping have proven useful. Here participants / interviewers provide network data by actually mapping out the network (with pen and paper or digitally) during the data collection session. An example of a pen-and-paper network mapping approach, which also includes the collection of some actor attributes (perceived influence and goals of actors) is the * Net-map toolbox. One benefit of this approach is that it allows researchers to collect qualitative data and ask clarifying questions while the network data is collected.","Especially when using social network analysis as a tool for facilitating change, different approaches of participatory network mapping have proven useful. Here participants / interviewers provide network data by actually mapping out the network (with pen and paper or digitally) during the data collection session.","[' What has proven useful when using social network analysis as a tool for facilitating change?', ' What do participants / interviewers provide by actually mapping out the network?']","['participatory network mapping', 'network data']"
1837,social network analysis,Practical applications,"Social network analysis is used extensively in a wide range of applications and disciplines.  Some common network analysis applications include data aggregation and mining, network propagation modeling, network modeling and sampling, user attribute and behavior analysis, community-maintained resource support, location-based interaction analysis, social sharing and filtering, recommender systems development, and link prediction and entity resolution. In the private sector, businesses use social network analysis to support activities such as customer interaction and analysis, information system development analysis, marketing, and business intelligence needs (see social media analytics).  Some public sector uses include development of leader engagement strategies, analysis of individual and group engagement and media use, and community-based problem solving.
","Social network analysis is used extensively in a wide range of applications and disciplines. Some common network analysis applications include data aggregation and mining, network propagation modeling, network modeling and sampling, user attribute and behavior analysis, community-maintained resource support, location-based interaction analysis, social sharing and filtering, recommender systems development, and link prediction and entity resolution.","[' What is used extensively in a wide range of applications and disciplines?', ' Data aggregation and mining, network propagation modeling, network modeling and sampling, user attribute and behavior analysis, community-maintained resource support, location-based interaction analysis, social sharing and filtering, and recommender systems development are examples of what?', ' What is social sharing and filtering?', ' What is link prediction and entity resolution?']","['Social network analysis', 'network analysis applications', 'network analysis', 'network analysis applications']"
1838,uncertainty,Summary,"Uncertainty refers to epistemic situations involving imperfect or unknown information. It applies to predictions of future events, to physical measurements that are already made, or to the unknown. Uncertainty arises in partially observable or stochastic environments, as well as due to ignorance, indolence, or both. It arises in any number of fields, including insurance, philosophy, physics, statistics, economics, finance, medicine, psychology, sociology, engineering, metrology, meteorology, ecology and information science.
","Uncertainty refers to epistemic situations involving imperfect or unknown information. It applies to predictions of future events, to physical measurements that are already made, or to the unknown.","[' What is the term for epistemic situations involving imperfect or unknown information?', ' What does uncertainty apply to predictions of future events, physical measurements that are already made, or the unknown?']","['Uncertainty', 'epistemic situations involving imperfect or unknown information']"
1839,uncertainty,Measurements,"The most commonly used procedure for calculating measurement uncertainty is described in the ""Guide to the Expression of Uncertainty in Measurement"" (GUM) published by ISO. A derived work is for example the National Institute of Standards and Technology (NIST) Technical Note 1297, ""Guidelines for Evaluating and Expressing the Uncertainty of NIST Measurement Results"", and the Eurachem/Citac publication ""Quantifying Uncertainty in Analytical Measurement"". The uncertainty of the result of a measurement generally consists of several components. The components are regarded as random variables, and may be grouped into two categories according to the method used to estimate their numerical values:
","The most commonly used procedure for calculating measurement uncertainty is described in the ""Guide to the Expression of Uncertainty in Measurement"" (GUM) published by ISO. A derived work is for example the National Institute of Standards and Technology (NIST) Technical Note 1297, ""Guidelines for Evaluating and Expressing the Uncertainty of NIST Measurement Results"", and the Eurachem/Citac publication ""Quantifying Uncertainty in Analytical Measurement"".","[' What is the most commonly used procedure for calculating measurement uncertainty?', ' Who publishes the ""Guide to the Expression of Uncertainty in Measurement""?', ' What does NIST stand for?', ' What is Note 1297?', ' What is the Eurachem/Citac publication called?']","['Guide to the Expression of Uncertainty in Measurement', 'ISO', 'National Institute of Standards and Technology', 'Guidelines for Evaluating and Expressing the Uncertainty of NIST Measurement Results', 'Quantifying Uncertainty in Analytical Measurement']"
1840,uncertainty,Measurements,"By propagating the variances of the components through a function relating the components to the measurement result, the combined measurement uncertainty is given as the square root of the resulting variance. The simplest form is the standard deviation of a repeated observation.
","By propagating the variances of the components through a function relating the components to the measurement result, the combined measurement uncertainty is given as the square root of the resulting variance. The simplest form is the standard deviation of a repeated observation.","[' What is given as the square root of the resulting variance?', ' What is the simplest form of measurement uncertainty?']","['combined measurement uncertainty', 'standard deviation of a repeated observation']"
1841,uncertainty,Measurements,"In metrology, physics, and engineering, the uncertainty or margin of error of a measurement, when explicitly stated, is given by a range of values likely to enclose the true value. This may be denoted by error bars on a graph, or by the following notations:","In metrology, physics, and engineering, the uncertainty or margin of error of a measurement, when explicitly stated, is given by a range of values likely to enclose the true value. This may be denoted by error bars on a graph, or by the following notations:","[' In metrology, physics, and engineering, the uncertainty or margin of error of a measurement is given by a range of values likely to enclose the true value of what?', ' On a graph, what may be denoted by error bars?']","['when explicitly stated', 'the uncertainty or margin of error']"
1842,uncertainty,Measurements," 
In the last notation, parentheses are the concise notation for the ± notation. For example, applying 10 1⁄2 meters in a scientific or engineering application, it could be written 10.5 m or 10.50 m, by convention meaning accurate to within one tenth of a meter, or one hundredth. The precision is symmetric around the last digit. In this case it's half a tenth up and half a tenth down, so 10.5 means between 10.45 and 10.55. Thus it is understood that 10.5 means 10.5±0.05, and 10.50 means 10.50±0.005, also written 10.50(5) and 10.500(5) respectively.  But if the accuracy is within two tenths, the uncertainty is ± one tenth, and it is required to be explicit: 10.5±0.1 and 10.50±0.01 or 10.5(1) and 10.50(1).   The numbers in parentheses apply to the numeral left of themselves, and are not part of that number, but part of a notation of uncertainty. They apply to the least significant digits. For instance, 1.00794(7) stands for 1.00794±0.00007, while 1.00794(72) stands for 1.00794±0.00072. This concise notation is used for example by IUPAC in stating the atomic mass of elements.
"," 
In the last notation, parentheses are the concise notation for the ± notation. For example, applying 10 1⁄2 meters in a scientific or engineering application, it could be written 10.5 m or 10.50 m, by convention meaning accurate to within one tenth of a meter, or one hundredth.","[' In the last notation, what is the concise notation for the <unk> notation?', ' What is the convention of accurate to within one tenth of a meter?']","['parentheses', 'parentheses']"
1843,uncertainty,Measurements,"The middle notation is used when the error is not symmetrical about the value – for example 3.4+0.3−0.2. This can occur when using a logarithmic scale, for example.
","The middle notation is used when the error is not symmetrical about the value – for example 3.4+0.3−0.2. This can occur when using a logarithmic scale, for example.","[' What is used when the error is not symmetrical about the value?', ' What can occur when using a logarithmic scale?']","['The middle notation', 'The middle notation']"
1844,uncertainty,Measurements,"Uncertainty of a measurement can be determined by repeating a measurement to arrive at an estimate of the standard deviation of the values. Then, any single value has an uncertainty equal to the standard deviation. However, if the values are averaged, then the mean measurement value has a much smaller uncertainty, equal to the standard error of the mean, which is the standard deviation divided by the square root of the number of measurements. This procedure neglects systematic errors, however.","Uncertainty of a measurement can be determined by repeating a measurement to arrive at an estimate of the standard deviation of the values. Then, any single value has an uncertainty equal to the standard deviation.","[' What can be determined by repeating a measurement to arrive at an estimate of the standard deviation of values?', ' Any single value has an uncertainty equal to what?']","['Uncertainty of a measurement', 'the standard deviation']"
1845,uncertainty,Measurements,"When the uncertainty represents the standard error of the measurement, then about 68.3% of the time, the true value of the measured quantity falls within the stated uncertainty range. For example, it is likely that for 31.7% of the atomic mass values given on the list of elements by atomic mass, the true value lies outside of the stated range. If the width of the interval is doubled, then probably only 4.6% of the true values lie outside the doubled interval, and if the width is tripled, probably only 0.3% lie outside. These values follow from the properties of the normal distribution, and they apply only if the measurement process produces normally distributed errors. In that case, the quoted standard errors are easily converted to 68.3% (""one sigma""), 95.4% (""two sigma""), or 99.7% (""three sigma"") confidence intervals.","When the uncertainty represents the standard error of the measurement, then about 68.3% of the time, the true value of the measured quantity falls within the stated uncertainty range. For example, it is likely that for 31.7% of the atomic mass values given on the list of elements by atomic mass, the true value lies outside of the stated range.","[' What percentage of the time does the true value of the measured quantity fall within the stated uncertainty range?', ' How many percent of the atomic mass values given on the list of elements by atomic are likely to fall within this range of uncertainty?', ' What is the true value of atomic mass?', ' What does the list of elements contain?']","['68.3%', '31.7%', 'outside of the stated range', 'atomic mass values']"
1846,uncertainty,Measurements,"In this context, uncertainty depends on both the accuracy and precision of the measurement instrument. The lower the accuracy and precision of an instrument, the larger the measurement uncertainty is. Precision is often determined as the standard deviation of the repeated measures of a given value, namely using the same method described above to assess measurement uncertainty. However, this method is correct only when the instrument is accurate. When it is inaccurate, the uncertainty is larger than the standard deviation of the repeated measures, and it appears evident that the uncertainty does not depend only on instrumental precision.
","In this context, uncertainty depends on both the accuracy and precision of the measurement instrument. The lower the accuracy and precision of an instrument, the larger the measurement uncertainty is.","[' What depends on both the accuracy and precision of the measurement instrument?', ' What is the larger the measurement uncertainty?']","['uncertainty', 'The lower the accuracy and precision of an instrument']"
1847,uncertainty,In the media,"Uncertainty in science, and science in general, may be interpreted differently in the public sphere than in the scientific community.  This is due in part to the diversity of the public audience, and the tendency for scientists to misunderstand lay audiences and therefore not communicate ideas clearly and effectively. One example is explained by the information deficit model. Also, in the public realm, there are often many scientific voices giving input on a single topic. For example, depending on how an issue is reported in the public sphere, discrepancies between outcomes of multiple scientific studies due to methodological differences could be interpreted by the public as a lack of consensus in a situation where a consensus does in fact exist. This interpretation may have even been intentionally promoted, as scientific uncertainty may be managed to reach certain goals.  For example, climate change deniers took the advice of Frank Luntz to frame global warming as an issue of scientific uncertainty, which was a precursor to the conflict frame used by journalists when reporting the issue.","Uncertainty in science, and science in general, may be interpreted differently in the public sphere than in the scientific community. This is due in part to the diversity of the public audience, and the tendency for scientists to misunderstand lay audiences and therefore not communicate ideas clearly and effectively.","[' What may be interpreted differently in the public sphere than in the scientific community?', ' What can cause scientists to misunderstand lay audiences and not communicate ideas clearly and effectively?']","['Uncertainty in science', 'diversity of the public audience']"
1848,uncertainty,In the media,"""Indeterminacy can be loosely said to apply to situations in which not all the parameters of the system and their interactions are fully known, whereas ignorance refers to situations in which it is not known what is not known."" These unknowns, indeterminacy and ignorance, that exist in science are often ""transformed"" into uncertainty when reported to the public in order to make issues more manageable, since scientific indeterminacy and ignorance are difficult concepts for scientists to convey without losing credibility. Conversely, uncertainty is often interpreted by the public as ignorance. The transformation of indeterminacy and ignorance into uncertainty may be related to the public's misinterpretation of uncertainty as ignorance.
","""Indeterminacy can be loosely said to apply to situations in which not all the parameters of the system and their interactions are fully known, whereas ignorance refers to situations in which it is not known what is not known."" These unknowns, indeterminacy and ignorance, that exist in science are often ""transformed"" into uncertainty when reported to the public in order to make issues more manageable, since scientific indeterminacy and ignorance are difficult concepts for scientists to convey without losing credibility.","[' What can be loosely said to apply to situations where not all the parameters of the system and their interactions are fully known?', ' What refers to situations in which it is not known what?', ' What are unknowns, indeterminacy and ignorance often ""transformed"" into when reported to the public?', ' What are difficult concepts for scientists to convey without losing credibility?']","['Indeterminacy', 'ignorance', 'uncertainty', 'scientific indeterminacy and ignorance']"
1849,uncertainty,In the media,"Journalists may inflate uncertainty (making the science seem more uncertain than it really is) or downplay uncertainty (making the science seem more certain than it really is). One way that journalists inflate uncertainty is by describing new research that contradicts past research without providing context for the change. Journalists may give scientists with minority views equal weight as scientists with majority views, without adequately describing or explaining the state of scientific consensus on the issue. In the same vein, journalists may give non-scientists the same amount of attention and importance as scientists.",Journalists may inflate uncertainty (making the science seem more uncertain than it really is) or downplay uncertainty (making the science seem more certain than it really is). One way that journalists inflate uncertainty is by describing new research that contradicts past research without providing context for the change.,"[' What do journalists do to make the science seem more uncertain than it really is?', ' How do journalists inflate uncertainty?']","['inflate uncertainty', 'by describing new research that contradicts past research without providing context for the change']"
1850,uncertainty,In the media,"Journalists may downplay uncertainty by eliminating ""scientists' carefully chosen tentative wording, and by losing these caveats the information is skewed and presented as more certain and conclusive than it really is"". Also, stories with a single source or without any context of previous research mean that the subject at hand is presented as more definitive and certain than it is in reality. There is often a ""product over process"" approach to science journalism that aids, too, in the downplaying of uncertainty. Finally, and most notably for this investigation, when science is framed by journalists as a triumphant quest, uncertainty is erroneously framed as ""reducible and resolvable"".","Journalists may downplay uncertainty by eliminating ""scientists' carefully chosen tentative wording, and by losing these caveats the information is skewed and presented as more certain and conclusive than it really is"". Also, stories with a single source or without any context of previous research mean that the subject at hand is presented as more definitive and certain than it is in reality.","[' Journalists may downplay uncertainty by eliminating what?', ' By losing these caveats the information is skewed and presented as more certain and conclusive than it really is?', ' Stories with a single source or without any context of previous research mean that what is the subject at hand?', ' What does context of previous research mean?', ' What is presented as more definitive and certain than it is in reality?']","[""scientists' carefully chosen tentative wording"", ""scientists' carefully chosen tentative wording"", 'more definitive and certain', 'the subject at hand is presented as more definitive and certain than it is in reality', 'the subject at hand']"
1851,uncertainty,In the media,"Some media routines and organizational factors affect the overstatement of uncertainty; other media routines and organizational factors help inflate the certainty of an issue. Because the general public (in the United States) generally trusts scientists, when science stories are covered without alarm-raising cues from special interest organizations (religious groups, environmental organizations, political factions, etc.) they are often covered in a business related sense, in an economic-development frame or a social progress frame. The nature of these frames is to downplay or eliminate uncertainty, so when economic and scientific promise are focused on early in the issue cycle, as has happened with coverage of plant biotechnology and nanotechnology in the United States, the matter in question seems more definitive and certain.","Some media routines and organizational factors affect the overstatement of uncertainty; other media routines and organizational factors help inflate the certainty of an issue. Because the general public (in the United States) generally trusts scientists, when science stories are covered without alarm-raising cues from special interest organizations (religious groups, environmental organizations, political factions, etc.)","[' What affects the overstatement of uncertainty?', ' What do media routines and organizational factors help inflate?', ' Why do the general public trust scientists?', ' What do special interest groups do without alarm-raising cues?']","['media routines and organizational factors', 'the certainty of an issue', 'when science stories are covered without alarm-raising cues from special interest organizations', 'science stories are covered']"
1852,uncertainty,Philosophy,"In Western philosophy the first philosopher to embrace uncertainty was Pyrrho resulting in the Hellenistic philosophies of Pyrrhonism and Academic Skepticism, the first schools of philosophical skepticism. Aporia and acatalepsy represent key concepts in ancient Greek philosophy regarding uncertainty.
","In Western philosophy the first philosopher to embrace uncertainty was Pyrrho resulting in the Hellenistic philosophies of Pyrrhonism and Academic Skepticism, the first schools of philosophical skepticism. Aporia and acatalepsy represent key concepts in ancient Greek philosophy regarding uncertainty.","[' Who was the first Western philosopher to embrace uncertainty?', ' What were the Hellenistic philosophies of Pyrrhonism and Academic Skepticism?', ' Aporia and acatalepsy represent key concepts in ancient Greek philosophy regarding what?']","['Pyrrho', 'the first schools of philosophical skepticism', 'uncertainty']"
1853,resource description framework,Summary,"The Resource Description Framework (RDF) is a World Wide Web Consortium (W3C) standard originally designed as a data model for metadata. It has come to be used as a general method for description and exchange of graph data. RDF provides a variety of syntax notations and data serialization formats with Turtle (Terse RDF Triple Language) currently being the most widely used notation. 
",The Resource Description Framework (RDF) is a World Wide Web Consortium (W3C) standard originally designed as a data model for metadata. It has come to be used as a general method for description and exchange of graph data.,"[' What is RDF?', ' What was RDF originally designed as?', ' RDF has come to be used as what?']","['The Resource Description Framework', 'a data model for metadata', 'a general method for description and exchange of graph data']"
1854,resource description framework,Summary,"RDF is a directed graph composed of triple statements. An RDF graph statement is represented by: 1) a node for the subject, 2) an arc that goes from a subject to an object for the predicate and 3) a node for the object. Each of the three parts of the statement can be identified by a URI. An object can also be a literal value. This simple, flexible data model has a lot of expressive power to represent complex situations, relationships, and other things of interest, while also being appropriately abstract.
","RDF is a directed graph composed of triple statements. An RDF graph statement is represented by: 1) a node for the subject, 2) an arc that goes from a subject to an object for the predicate and 3) a node for the object.","[' What is a directed graph composed of?', ' What is an RDF graph statement represented by?', ' An arc that goes from a subject to an object for what?']","['triple statements', 'a node for the subject, 2) an arc that goes from a subject to an object for the predicate and 3) a node for the object', 'the predicate']"
1855,resource description framework,Summary,"RDF was adopted as a W3C recommendation in 1999. The RDF 1.0 specification was published in 2004, the RDF 1.1 specification in 2014. SPARQL is a standard query language for RDF graphs. RDFS, OWL and SHACL are ontology languages that are used to describe RDF data.
","RDF was adopted as a W3C recommendation in 1999. The RDF 1.0 specification was published in 2004, the RDF 1.1 specification in 2014.","[' When was RDF adopted as a W3C recommendation?', ' What specification was published in 2004?', ' When was the RDF 1.1 specification published?']","['1999', 'RDF 1.0', '2014']"
1856,resource description framework,Overview,"The RDF data model is similar to classical conceptual modeling approaches (such as entity–relationship or class diagrams). It is based on the idea of making statements about resources (in particular web resources) in expressions of the form subject–predicate–object, known as triples. The subject denotes the resource, and the predicate denotes traits or aspects of the resource, and expresses a relationship between the subject and the object.
","The RDF data model is similar to classical conceptual modeling approaches (such as entity–relationship or class diagrams). It is based on the idea of making statements about resources (in particular web resources) in expressions of the form subject–predicate–object, known as triples.","[' What is the RDF data model similar to?', ' What are the expressions of the form subject-predicate-object called?']","['classical conceptual modeling approaches', 'triples']"
1857,resource description framework,Overview,"For example, one way to represent the notion ""The sky has the color blue"" in RDF is as the triple: a subject denoting ""the sky"", a predicate denoting ""has the color"", and an object denoting ""blue"". Therefore, RDF uses subject instead of object (or entity) in contrast to the typical approach of an entity–attribute–value model in object-oriented design: entity (sky), attribute (color), and value (blue).
","For example, one way to represent the notion ""The sky has the color blue"" in RDF is as the triple: a subject denoting ""the sky"", a predicate denoting ""has the color"", and an object denoting ""blue"". Therefore, RDF uses subject instead of object (or entity) in contrast to the typical approach of an entity–attribute–value model in object-oriented design: entity (sky), attribute (color), and value (blue).","[' What is one way to represent the notion ""The sky has the color blue"" in RDF?', ' What does RDF use instead of object (or entity)?', ' What is another name for entity?', ' What is the other name for attribute in object-oriented design?']","['the triple', 'subject', 'sky', 'color']"
1858,resource description framework,Overview,"This mechanism for describing resources is a major component in the W3C's Semantic Web activity: an evolutionary stage of the World Wide Web in which automated software can store, exchange, and use machine-readable information distributed throughout the Web, in turn enabling users to deal with the information with greater efficiency and certainty. RDF's simple data model and ability to model disparate, abstract concepts has also led to its increasing use in knowledge management applications unrelated to Semantic Web activity.
","This mechanism for describing resources is a major component in the W3C's Semantic Web activity: an evolutionary stage of the World Wide Web in which automated software can store, exchange, and use machine-readable information distributed throughout the Web, in turn enabling users to deal with the information with greater efficiency and certainty. RDF's simple data model and ability to model disparate, abstract concepts has also led to its increasing use in knowledge management applications unrelated to Semantic Web activity.","["" What is a major component of the W3C's Semantic Web activity?"", ' What can automated software store, exchange, and use distributed throughout the Web?', "" RDF's ability to model disparate, abstract concepts has led to its increasing use in what?""]","['mechanism for describing resources', 'machine-readable information', 'knowledge management applications']"
1859,resource description framework,Overview,"A collection of RDF statements intrinsically represents a labeled, directed multi-graph. This makes an RDF data model better suited to certain kinds of knowledge representation than are other relational or ontological models. 
","A collection of RDF statements intrinsically represents a labeled, directed multi-graph. This makes an RDF data model better suited to certain kinds of knowledge representation than are other relational or ontological models.","[' A collection of RDF statements intrinsically represents what?', ' What makes an RDF data model better suited to certain kinds of knowledge representation?']","['a labeled, directed multi-graph', 'A collection of RDF statements intrinsically represents a labeled, directed multi-graph']"
1860,resource description framework,History,"In 1999, the W3C published the first recommended RDF specification, the Model and Syntax Specification (""RDF M&S""). This described RDF's data model and an XML serialization.","In 1999, the W3C published the first recommended RDF specification, the Model and Syntax Specification (""RDF M&S""). This described RDF's data model and an XML serialization.","[' In what year was the Model and Syntax Specification published?', ' What was the name of the first recommended RDF specification?']","['1999', 'the Model and Syntax Specification']"
1861,resource description framework,History,"Two persistent misunderstandings about RDF developed at this time: firstly, due to the MCF influence and the RDF ""Resource Description"" initialism, the idea that RDF was specifically for use in representing metadata; secondly that RDF was an XML format rather than a data model, and only the RDF/XML serialisation being XML-based. RDF saw little take-up in this period, but there was significant work done in Bristol, around ILRT at Bristol University and HP Labs, and in Boston at MIT. RSS 1.0 and FOAF became exemplar applications for RDF in this period.
","Two persistent misunderstandings about RDF developed at this time: firstly, due to the MCF influence and the RDF ""Resource Description"" initialism, the idea that RDF was specifically for use in representing metadata; secondly that RDF was an XML format rather than a data model, and only the RDF/XML serialisation being XML-based. RDF saw little take-up in this period, but there was significant work done in Bristol, around ILRT at Bristol University and HP Labs, and in Boston at MIT.","[' What did the MCF influence and the RDF ""Resource Description"" initialism create?', ' What was RDF an XML format rather than a data model?', ' What was the only data model that was XML-based?', ' Where did RDF see little take-up in this period?', ' What was significant work done in Bristol?']","['the idea that RDF was specifically for use in representing metadata', 'RDF/XML serialisation being XML-based', 'RDF/XML serialisation', 'significant work done in Bristol, around ILRT at Bristol University and HP Labs, and in Boston at MIT', 'ILRT']"
1862,resource description framework,Applications,"Some uses of RDF include research into social networking. It will also help people in business fields understand better their relationships with members of industries that could be of use for product placement.  It will also help scientists understand how people are connected to one another.
",Some uses of RDF include research into social networking. It will also help people in business fields understand better their relationships with members of industries that could be of use for product placement.,"[' What are some uses of RDF?', ' What will help people in business fields understand better their relationships with industry members?']","['research into social networking', 'RDF']"
1863,resource description framework,Applications,"RDF is being used to have a better understanding of road traffic patterns.  This is because the information regarding traffic patterns is  on different websites, and RDF is used to integrate information from different sources on the web. Before, the common methodology was using keyword searching, but this method is problematic because it does not  consider synonyms. This is why ontologies are useful in this situation. But one of the issues that comes up when trying to efficiently study traffic is that to fully understand traffic,  concepts related to people, streets, and roads must be well understood. Since these are human  concepts, they require the addition of fuzzy logic. This is because values that are useful  when describing roads, like slipperiness, are not precise concepts and cannot be measured. This would imply that the best solution would incorporate both fuzzy logic and ontology.","RDF is being used to have a better understanding of road traffic patterns. This is because the information regarding traffic patterns is  on different websites, and RDF is used to integrate information from different sources on the web.","[' What is being used to have a better understanding of road traffic patterns?', ' What is RDF used to integrate information from different sources on the web?']","['RDF', 'road traffic patterns']"
1864,routing,Summary,"Routing is the process of selecting a path for traffic in a network or between or across multiple networks. Broadly, routing is performed in many types of networks, including circuit-switched networks, such as the public switched telephone network (PSTN), and computer networks, such as the Internet.
","Routing is the process of selecting a path for traffic in a network or between or across multiple networks. Broadly, routing is performed in many types of networks, including circuit-switched networks, such as the public switched telephone network (PSTN), and computer networks, such as the Internet.","[' What is the process of selecting a path for traffic in a network or between or across multiple networks?', ' Routing is performed in many types of networks, including circuit-switched networks, such as what?', ' What network is the public switched telephone network?']","['Routing', 'the public switched telephone network', 'circuit-switched networks']"
1865,routing,Summary,"In packet switching networks, routing is the higher-level decision making that directs network packets from their source toward their destination through intermediate network nodes by specific packet forwarding mechanisms. Packet forwarding is the transit of network packets from one network interface to another.  Intermediate nodes are typically network hardware devices such as routers, gateways, firewalls, or switches. General-purpose computers also forward packets and perform routing, although they have no specially optimized hardware for the task.
","In packet switching networks, routing is the higher-level decision making that directs network packets from their source toward their destination through intermediate network nodes by specific packet forwarding mechanisms. Packet forwarding is the transit of network packets from one network interface to another.","[' What is the higher-level decision making that directs network packets from their source toward their destination?', ' What is packet forwarding?']","['routing', 'the transit of network packets from one network interface to another']"
1866,routing,Summary,"The routing process usually directs forwarding on the basis of routing tables. Routing tables maintain a record of the routes to various network destinations. Routing tables may be specified by an administrator, learned by observing network traffic or built with the assistance of routing protocols.
",The routing process usually directs forwarding on the basis of routing tables. Routing tables maintain a record of the routes to various network destinations.,"[' The routing process usually directs forwarding on the basis of what?', ' Routing tables maintain a record of the routes to various network destinations?']","['routing tables', 'routing process usually directs forwarding on the basis of routing tables']"
1867,routing,Summary,"Routing, in a narrower sense of the term, often refers to IP routing and is contrasted with bridging. IP routing assumes that network addresses are structured and that similar addresses imply proximity within the network. Structured addresses allow a single routing table entry to represent the route to a group of devices.  In large networks, structured addressing (routing, in the narrow sense) outperforms unstructured addressing (bridging). Routing has become the dominant form of addressing on the Internet. Bridging is still widely used within local area networks.
","Routing, in a narrower sense of the term, often refers to IP routing and is contrasted with bridging. IP routing assumes that network addresses are structured and that similar addresses imply proximity within the network.","[' What does routing refer to in a narrower sense of the term?', ' Routing is contrasted with what?', ' IP routing assumes that network addresses are structured and that similar addresses imply proximity within the network?']","['IP routing', 'bridging', 'Routing']"
1868,routing,Delivery schemes,"Unicast is the dominant form of message delivery on the Internet.  This article focuses on unicast routing algorithms.
",Unicast is the dominant form of message delivery on the Internet. This article focuses on unicast routing algorithms.,"[' What is the dominant form of message delivery on the internet?', ' What does this article focus on?']","['Unicast', 'unicast routing algorithms']"
1869,routing,Topology distribution,"With static routing, small networks may use manually configured routing tables. Larger networks have complex topologies that can change rapidly, making the manual construction of routing tables unfeasible. Nevertheless, most of the public switched telephone network (PSTN) uses pre-computed routing tables, with fallback routes if the most direct route becomes blocked (see routing in the PSTN).
","With static routing, small networks may use manually configured routing tables. Larger networks have complex topologies that can change rapidly, making the manual construction of routing tables unfeasible.","[' Small networks may use what?', ' Large networks have complex topologies that can change rapidly, making what unfavorable?']","['manually configured routing tables', 'manual construction of routing tables unfeasible']"
1870,routing,Topology distribution,"Dynamic routing attempts to solve this problem by constructing routing tables automatically, based on information carried by routing protocols, allowing the network to act nearly autonomously in avoiding network failures and blockages. Dynamic routing dominates the Internet. Examples of dynamic-routing protocols and algorithms include Routing Information Protocol (RIP), Open Shortest Path First (OSPF) and Enhanced Interior Gateway Routing Protocol (EIGRP).
","Dynamic routing attempts to solve this problem by constructing routing tables automatically, based on information carried by routing protocols, allowing the network to act nearly autonomously in avoiding network failures and blockages. Dynamic routing dominates the Internet.","[' Dynamic routing attempts to solve what problem by constructing routing tables automatically?', ' What does dynamic routing dominate?']","['network failures and blockages', 'the Internet']"
1871,routing,Path selection,"Path selection involves applying a routing metric to multiple routes to select (or predict) the best route. Most routing algorithms use only one network path at a time. Multipath routing and specifically equal-cost multi-path routing techniques enable the use of multiple alternative paths.
",Path selection involves applying a routing metric to multiple routes to select (or predict) the best route. Most routing algorithms use only one network path at a time.,"[' What involves applying a routing metric to multiple routes to select (or predict) the best route?', ' Most routing algorithms use how many network paths at a time?']","['Path selection', 'one']"
1872,routing,Path selection,"In computer networking, the metric is computed by a routing algorithm, and can cover information such as bandwidth, network delay, hop count, path cost, load, maximum transmission unit, reliability, and communication cost. The routing table stores only the best possible routes, while link-state or topological databases may store all other information as well.
","In computer networking, the metric is computed by a routing algorithm, and can cover information such as bandwidth, network delay, hop count, path cost, load, maximum transmission unit, reliability, and communication cost. The routing table stores only the best possible routes, while link-state or topological databases may store all other information as well.","[' What is computed by a routing algorithm in computer networking?', ' What can cover information such as bandwidth, network delay, hop count, load, maximum transmission unit, reliability, and communication cost?', ' The routing table stores only the best possible routes, while what may store all other information?', ' What types of databases may store all other information?', ' What type of database may store routes?']","['the metric', 'the metric', 'link-state or topological databases', 'link-state or topological', 'link-state or topological databases']"
1873,routing,Path selection,"Because a routing metric is specific to a given routing protocol, multi-protocol routers must use some external heuristic to select between routes learned from different routing protocols. Cisco routers, for example, attribute a value known as the administrative distance to each route, where smaller administrative distances indicate routes learned from a protocol assumed to be more reliable.
","Because a routing metric is specific to a given routing protocol, multi-protocol routers must use some external heuristic to select between routes learned from different routing protocols. Cisco routers, for example, attribute a value known as the administrative distance to each route, where smaller administrative distances indicate routes learned from a protocol assumed to be more reliable.","[' What must multi-protocol routers use to select between routes learned from different routing protocols?', ' What does Cisco routers attribute a value known as to each route?', ' What indicate routes learned from a protocol assumed to be more reliable?']","['external heuristic', 'the administrative distance', 'smaller administrative distances']"
1874,routing,Path selection,"A local administrator can set up host-specific routes that provide more control over network usage, permits testing, and better overall security. This is useful for debugging network connections or routing tables.
","A local administrator can set up host-specific routes that provide more control over network usage, permits testing, and better overall security. This is useful for debugging network connections or routing tables.","[' What can a local administrator set up that provide more control over network usage?', ' What does setting up host-specific routes provide?']","['host-specific routes', 'more control over network usage, permits testing, and better overall security']"
1875,routing,Path selection,"In some small systems, a single central device decides ahead of time the complete path of every packet. In some other small systems, whichever edge device injects a packet into the network decides ahead of time the complete path of that particular packet. In either case, the route-planning device needs to know a lot of information about what devices are connected to the network and how they are connected to each other. Once it has this information, it can use an algorithm such as A* search algorithm to find the best path.
","In some small systems, a single central device decides ahead of time the complete path of every packet. In some other small systems, whichever edge device injects a packet into the network decides ahead of time the complete path of that particular packet.","[' What decides ahead of time the complete path of every packet in some small systems?', ' In some smaller systems, whichever edge device injects a packet into the network decides what?']","['a single central device', 'the complete path of that particular packet']"
1876,routing,Path selection,"In high-speed systems, there are so many packets transmitted every second that it is infeasible for a single device to calculate the complete path for each and every packet. Early high-speed systems dealt with this with circuit switching by setting up a path once for the first packet between some source and some destination; later packets between that same source and that same destination continue to follow the same path without recalculating until the circuit teardown. Later high-speed systems inject packets into the network without any one device ever calculating a complete path for packets.
","In high-speed systems, there are so many packets transmitted every second that it is infeasible for a single device to calculate the complete path for each and every packet. Early high-speed systems dealt with this with circuit switching by setting up a path once for the first packet between some source and some destination; later packets between that same source and that same destination continue to follow the same path without recalculating until the circuit teardown.","[' In high-speed systems, how many packets are transmitted every second?', ' What did early high speed systems do to deal with this problem?', ' How did early systems deal with circuit switching?', ' How many times do packets between some source and some destination follow the same path?']","['so many', 'circuit switching by setting up a path once for the first packet between some source and some destination', 'by setting up a path once for the first packet between some source and some destination', 'later']"
1877,routing,Path selection,"In large systems, there are so many connections between devices, and those connections change so frequently, that it is infeasible for any one device to even know how all the devices are connected to each other, much less calculate a complete path through them. Such systems generally use next-hop routing.
","In large systems, there are so many connections between devices, and those connections change so frequently, that it is infeasible for any one device to even know how all the devices are connected to each other, much less calculate a complete path through them. Such systems generally use next-hop routing.","[' How often do connections between devices change in large systems?', ' What type of routing do large systems generally use?']","['frequently', 'next-hop']"
1878,routing,Path selection,"Most systems use a deterministic dynamic routing algorithm. When a device chooses a path to a particular final destination, that device always chooses the same path to that destination until it receives information that makes it think some other path is better.
","Most systems use a deterministic dynamic routing algorithm. When a device chooses a path to a particular final destination, that device always chooses the same path to that destination until it receives information that makes it think some other path is better.","[' Most systems use what type of routing algorithm?', ' When a device chooses a path to a particular final destination, that device always chooses the same path to that destination until it receives information that makes it think another path is better?']","['deterministic dynamic', 'deterministic dynamic routing algorithm']"
1879,routing,Path selection,"A few routing algorithms do not use a deterministic algorithm to find the best link for a packet to get from its original source to its final destination. Instead, to avoid congestion hot spots in packet systems, a few algorithms use a randomized algorithm—Valiant's paradigm—that routes a path to a randomly picked intermediate destination, and from there to its true final destination. In many early telephone switches, a randomizer was often used to select the start of a path through a multistage switching fabric.
","A few routing algorithms do not use a deterministic algorithm to find the best link for a packet to get from its original source to its final destination. Instead, to avoid congestion hot spots in packet systems, a few algorithms use a randomized algorithm—Valiant's paradigm—that routes a path to a randomly picked intermediate destination, and from there to its true final destination.","[' A few routing algorithms do not use what to find the best link for a packet to get from its original source to its final destination?', ' To avoid congestion hot spots in packet systems, a few algorithms use what?', "" What is Valiant's paradigm?"", ' What is the randomized algorithm that routes a path to?']","['a deterministic algorithm', 'a randomized algorithm', 'a randomized algorithm', ""Valiant's paradigm""]"
1880,routing,Path selection,"Depending on the application for which path selection is performed, different metrics can be used. For example, for web requests one can use minimum latency paths to minimize web page load time, or for bulk data transfers one can choose the least utilized path to balance load across the network and increase throughput. A popular path selection objective is to reduce the average completion times of traffic flows and the total network bandwidth consumption. Recently, a path selection metric was proposed that computes the total number of bytes scheduled on the edges per path as selection metric. An empirical analysis of several path selection metrics, including this new proposal, has been made available.","Depending on the application for which path selection is performed, different metrics can be used. For example, for web requests one can use minimum latency paths to minimize web page load time, or for bulk data transfers one can choose the least utilized path to balance load across the network and increase throughput.","[' What metrics can be used depending on the application for which path selection is performed?', ' For web requests, what can one use to minimize web page load time?', ' What can one choose to balance load across the network?', ' The least utilized path to balance load across the network and increase throughput?']","['different metrics', 'minimum latency paths', 'the least utilized path', 'bulk data transfers']"
1881,routing,Multiple agents,"In some networks, routing is complicated by the fact that no single entity is responsible for selecting paths; instead, multiple entities are involved in selecting paths or even parts of a single path. Complications or inefficiency can result if these entities choose paths to optimize their own objectives, which may conflict with the objectives of other participants.
","In some networks, routing is complicated by the fact that no single entity is responsible for selecting paths; instead, multiple entities are involved in selecting paths or even parts of a single path. Complications or inefficiency can result if these entities choose paths to optimize their own objectives, which may conflict with the objectives of other participants.","[' Why is routing complicated in some networks?', ' What can result if entities choose paths to optimize their own objectives?', ' What do entities choose to optimize their own objectives?', ' What may conflict with the objectives of other participants?']","['no single entity is responsible for selecting paths', 'Complications or inefficiency', 'paths', 'choose paths to optimize their own objectives']"
1882,routing,Multiple agents,"A classic example involves traffic in a road system, in which each driver picks a path that minimizes their travel time. With such routing, the equilibrium routes can be longer than optimal for all drivers. In particular, Braess's paradox shows that adding a new road can lengthen travel times for all drivers.
","A classic example involves traffic in a road system, in which each driver picks a path that minimizes their travel time. With such routing, the equilibrium routes can be longer than optimal for all drivers.","[' What is a classic example of traffic in a road system?', ' What can be longer than optimal for all drivers?']","['each driver picks a path that minimizes their travel time', 'equilibrium routes']"
1883,routing,Multiple agents,"In a single-agent model used, for example, for routing automated guided vehicles (AGVs) on a terminal, reservations are made for each vehicle to prevent simultaneous use of the same part of an infrastructure. This approach is also referred to as context-aware routing.","In a single-agent model used, for example, for routing automated guided vehicles (AGVs) on a terminal, reservations are made for each vehicle to prevent simultaneous use of the same part of an infrastructure. This approach is also referred to as context-aware routing.","[' What is a single agent model used for routing on a terminal?', ' What is this approach also referred to as?']","['automated guided vehicles', 'context-aware routing']"
1884,routing,Multiple agents,"The Internet is partitioned into autonomous systems (ASs) such as internet service providers (ISPs), each of which controls routes involving its network. Routing occurs at multiple levels. First, AS-level paths are selected via the BGP protocol that produces a sequence of ASs through which packets flow. Each AS may have multiple paths, offered by neighboring ASs, from which to choose. These routing decisions often correlate with business relationships with these neighboring ASs, which may be unrelated to path quality or latency. Second, once an AS-level path has been selected, there are often multiple corresponding router-level paths to choose from. This is due, in part, because two ISPs may be connected through multiple connections. In choosing the single router-level path, it is common practice for each ISP to employ hot-potato routing: sending traffic along the path that minimizes the distance through the ISP's own network—even if that path lengthens the total distance to the destination.
","The Internet is partitioned into autonomous systems (ASs) such as internet service providers (ISPs), each of which controls routes involving its network. Routing occurs at multiple levels.","[' What is an autonomous system?', ' What does an ISP stand for?']","['internet service providers', 'internet service providers']"
1885,routing,Multiple agents,"For example, consider two ISPs, A and B. Each has a presence in New York, connected by a fast link with latency 5 ms—and each has a presence in London connected by a 5 ms link. Suppose both ISPs have trans-Atlantic links that connect their two networks, but A's link has latency 100 ms and B's has latency 120 ms. When routing a message from a source in A's London network to a destination in B's New York network, A may choose to immediately send the message to B in London. This saves A the work of sending it along an expensive trans-Atlantic link, but causes the message to experience latency 125 ms when the other route would have been 20 ms faster.
","For example, consider two ISPs, A and B. Each has a presence in New York, connected by a fast link with latency 5 ms—and each has a presence in London connected by a 5 ms link.","[' How many ISPs have a presence in New York?', ' How many ms latency does each ISP have in London?', ' What is the latency of a fast link between A and B?']","['two', '5', '5\xa0ms']"
1886,routing,Multiple agents,"A 2003 measurement study of Internet routes found that, between pairs of neighboring ISPs, more than 30% of paths have inflated latency due to hot-potato routing, with 5% of paths being delayed by at least 12 ms. Inflation due to AS-level path selection, while substantial, was attributed primarily to BGP's lack of a mechanism to directly optimize for latency, rather than to selfish routing policies. It was also suggested that, were an appropriate mechanism in place, ISPs would be willing to cooperate to reduce latency rather than use hot-potato routing. Such a mechanism was later published by the same authors, first for the case of two ISPs and then for the global case.","A 2003 measurement study of Internet routes found that, between pairs of neighboring ISPs, more than 30% of paths have inflated latency due to hot-potato routing, with 5% of paths being delayed by at least 12 ms. Inflation due to AS-level path selection, while substantial, was attributed primarily to BGP's lack of a mechanism to directly optimize for latency, rather than to selfish routing policies. It was also suggested that, were an appropriate mechanism in place, ISPs would be willing to cooperate to reduce latency rather than use hot-potato routing.","[' How much latency does hot-potato routing cause?', ' What percentage of paths are delayed by at least 12 ms?', ' Why is latency inflated?', "" Why was path selection attributed to BGP's lack of a mechanism to directly optimize for latency?"", ' What would ISPs be willing to cooperate to reduce latency rather than using hot-potato routing?']","['more than 30% of paths have inflated latency', '5%', 'hot-potato routing', 'Inflation', 'an appropriate mechanism in place']"
1887,routing,Route analytics,"As the Internet and IP networks have become mission critical business tools, there has been increased interest in techniques and methods to monitor the routing posture of networks. Incorrect routing or routing issues cause undesirable performance degradation, flapping or downtime. Monitoring routing in a network is achieved using route analytics tools and techniques.","As the Internet and IP networks have become mission critical business tools, there has been increased interest in techniques and methods to monitor the routing posture of networks. Incorrect routing or routing issues cause undesirable performance degradation, flapping or downtime.","[' What have the Internet and IP networks become?', ' What has increased interest in techniques and methods to monitor the routing posture of networks?', ' Incorrect routing or routing issues cause undesirable performance degradation, flapping or downtime?']","['mission critical business tools', 'Internet and IP networks', 'Internet']"
1888,routing,Centralized routing,"In networks where a logically centralized control is available over the forwarding state, for example, using software-defined networking, routing techniques can be used that aim to optimize global and network-wide performance metrics. This has been used by large internet companies that operate many data centers in different geographical locations attached using private optical links, examples of which include Microsoft's Global WAN, Facebook's Express Backbone, and Google's B4.","In networks where a logically centralized control is available over the forwarding state, for example, using software-defined networking, routing techniques can be used that aim to optimize global and network-wide performance metrics. This has been used by large internet companies that operate many data centers in different geographical locations attached using private optical links, examples of which include Microsoft's Global WAN, Facebook's Express Backbone, and Google's B4.","[' What can be used in networks where a logical centralized control is available over the forwarding state?', ' What can routing techniques aim to optimize?', ' Large internet companies operate many data centers in different geographical locations attached to what?', ' What is the name of the company that operates many data centers in different geographical locations attached using private optical links?', "" What is Facebook's Express Backbone?""]","['routing techniques', 'global and network-wide performance metrics', 'private optical links', ""Microsoft's Global WAN"", ""Microsoft's Global WAN""]"
1889,routing,Centralized routing,"Global performance metrics to optimize include maximizing network utilization, minimizing traffic flow completion times, maximizing the traffic delivered prior to specific deadlines and reducing the completion times of flows. Work on the later over private WAN discusses modeling routing as a graph optimization problem by pushing all the queuing to the end-points. The authors also propose a heuristic to solve the problem efficiently while sacrificing negligible performance.","Global performance metrics to optimize include maximizing network utilization, minimizing traffic flow completion times, maximizing the traffic delivered prior to specific deadlines and reducing the completion times of flows. Work on the later over private WAN discusses modeling routing as a graph optimization problem by pushing all the queuing to the end-points.","[' What are some of the metrics to optimize?', ' What is a graph optimization problem by pushing all the queuing to the top of the queue?', ' What is a graph optimization problem by pushing all the queuing to the end-points?']","['maximizing network utilization, minimizing traffic flow completion times, maximizing the traffic delivered prior to specific deadlines and reducing the completion times of flows', 'routing', 'routing']"
1890,inductive logic programming,Summary,"Inductive logic programming (ILP) is a subfield of symbolic artificial intelligence  which uses logic programming as a uniform representation for examples, background knowledge and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesised logic program which entails all the positive and none of the negative examples.
","Inductive logic programming (ILP) is a subfield of symbolic artificial intelligence  which uses logic programming as a uniform representation for examples, background knowledge and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesised logic program which entails all the positive and none of the negative examples.","[' Inductive logic programming is a subfield of what?', ' ILP uses logic programming as a uniform representation for examples, background knowledge and hypotheses?', ' What will an ILP system derive as a logical database of facts?', ' What is the hypothesised logic program?']","['symbolic artificial intelligence', 'Inductive logic programming', 'a hypothesised logic program', 'all the positive and none of the negative examples']"
1891,inductive logic programming,Summary,"Inductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting. Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples. The term Inductive Logic Programming was first introduced in a paper by Stephen Muggleton in 1991. Muggleton also founded the annual international conference on Inductive Logic Programming, introduced the theoretical ideas of Predicate Invention, Inverse resolution, and Inverse entailment. Muggleton implemented Inverse entailment first in the PROGOL system. The term ""inductive"" here refers to philosophical (i.e. suggesting a theory to explain observed facts) rather than mathematical (i.e. proving a property for all members of a well-ordered set) induction.
",Inductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting.,"[' What is particularly useful in bioinformatics and natural language processing?', ' Who laid the initial theoretical foundation for inductive machine learning in a logical setting?']","['Inductive logic programming', 'Gordon Plotkin and Ehud Shapiro']"
1892,inductive logic programming,Formal definition,"The background knowledge is given as a logic theory B, commonly in the form of Horn clauses used in logic programming.
The positive and negative examples are given as a conjunction 




E

+




{\displaystyle E^{+}}
 and 




E

−




{\displaystyle E^{-}}
 of unnegated and negated ground literals, respectively.
A correct hypothesis h is a logic proposition satisfying the following requirements.","The background knowledge is given as a logic theory B, commonly in the form of Horn clauses used in logic programming. The positive and negative examples are given as a conjunction 




E

+




{\displaystyle E^{+}}
 and 




E

−




{\displaystyle E^{-}}
 of unnegated and negated ground literals, respectively.","[' What is the background knowledge given as?', ' What are Horn clauses used in logic programming?', ' How are positive and negative examples given?']","['logic theory B', 'background knowledge', 'as a conjunction \n\n\n\n\nE\n\n+\n\n\n\n\n{\\displaystyle E^{+}}\n and \n\n\n\n\nE\n\n−\n\n\n\n\n{\\displaystyle E^{-}}\n of unnegated and negated ground literals']"
1893,inductive logic programming,Formal definition,"""Necessity"" does not impose a restriction on h, but forbids any generation of a hypothesis as long as the positive facts are explainable without it.
""Sufficiency"" requires any generated hypothesis h to explain all positive examples 




E

+




{\displaystyle E^{+}}
.
""Weak consistency"" forbids generation of any hypothesis h that contradicts the background knowledge B.
""Strong consistency"" also forbids generation of any hypothesis h that is inconsistent with the negative examples 




E

−




{\displaystyle E^{-}}
, given the background knowledge B; it implies ""Weak consistency""; if no negative examples are given, both requirements coincide. Džeroski  requires only ""Sufficiency"" (called ""Completeness"" there) and ""Strong consistency"".
","""Necessity"" does not impose a restriction on h, but forbids any generation of a hypothesis as long as the positive facts are explainable without it. ""Sufficiency"" requires any generated hypothesis h to explain all positive examples 




E

+




{\displaystyle E^{+}}
.","[' ""Necessity"" does not impose a restriction on what?', ' ""Sufficiency"" requires any generated hypothesis h to explain all positive examples E + <unk>displaystyle E<unk>+<unk>?']","['h', '{\\displaystyle E^{+}}\n.']"
1894,inductive logic programming,Example,"It starts from the background knowledge (cf. picture)
",It starts from the background knowledge (cf. picture),"[' What does it start from?', ' What is the background knowledge?']","['background knowledge', 'It starts from the background knowledge (cf. picture']"
1895,inductive logic programming,Example,"The resulting Horn clause is the hypothesis h obtained by the rlgg approach. Ignoring the background knowledge facts, the clause informally reads ""




x

m
e




{\displaystyle x_{me}}
 is called a daughter of 




x

h
t




{\displaystyle x_{ht}}
 if 




x

h
t




{\displaystyle x_{ht}}
 is the parent of 




x

m
e




{\displaystyle x_{me}}
 and 




x

m
e




{\displaystyle x_{me}}
 is female"", which is a commonly accepted definition.
","The resulting Horn clause is the hypothesis h obtained by the rlgg approach. Ignoring the background knowledge facts, the clause informally reads ""




x

m
e




{\displaystyle x_{me}}
 is called a daughter of 




x

h
t




{\displaystyle x_{ht}}
 if 




x

h
t




{\displaystyle x_{ht}}
 is the parent of 




x

m
e




{\displaystyle x_{me}}
 and 




x

m
e




{\displaystyle x_{me}}
 is female"", which is a commonly accepted definition.","[' What is the hypothesis h obtained by the rlgg approach?', ' Ignoring the background knowledge facts, what does the clause informally read?', ' What is a commonly accepted accepted view of the Horn clause?', ' What is x m e <unk>displaystyle x_<unk>me<unk> female?', ' What is the commonly accepted definition?']","['Horn clause', 'x\n\nm\ne', 'female', 'x\n\nm\ne', 'female']"
1896,inductive logic programming,Example,"Concerning the above requirements, ""Necessity"" was satisfied because the predicate dau doesn't appear in the background knowledge, which hence cannot imply any property containing this predicate, such as the positive examples are.
""Sufficiency"" is satisfied by the computed hypothesis h, since it, together with 





par


(
h
,
m
)
∧


fem


(
m
)


{\displaystyle {\textit {par}}(h,m)\land {\textit {fem}}(m)}
 from the background knowledge, implies the first positive example 





dau


(
m
,
h
)


{\displaystyle {\textit {dau}}(m,h)}
, and similarly  h and 





par


(
t
,
e
)
∧


fem


(
e
)


{\displaystyle {\textit {par}}(t,e)\land {\textit {fem}}(e)}
 from the background knowledge implies the second  positive example 





dau


(
e
,
t
)


{\displaystyle {\textit {dau}}(e,t)}
. ""Weak consistency"" is satisfied by h, since h holds in the (finite) Herbrand structure described by the background knowledge; similar for ""Strong consistency"".
","Concerning the above requirements, ""Necessity"" was satisfied because the predicate dau doesn't appear in the background knowledge, which hence cannot imply any property containing this predicate, such as the positive examples are. ""Sufficiency"" is satisfied by the computed hypothesis h, since it, together with 





par


(
h
,
m
)
∧


fem


(
m
)


{\displaystyle {\textit {par}}(h,m)\land {\textit {fem}}(m)}
 from the background knowledge, implies the first positive example 





dau


(
m
,
h
)


{\displaystyle {\textit {dau}}(m,h)}
, and similarly  h and 





par


(
t
,
e
)
∧


fem


(
e
)


{\displaystyle {\textit {par}}(t,e)\land {\textit {fem}}(e)}
 from the background knowledge implies the second  positive example 





dau


(
e
,
t
)


{\displaystyle {\textit {dau}}(e,t)}
.","["" What was satisfied because the predicate dau doesn't appear in the background knowledge?"", ' What is satisfied by the computed hypothesis h?', ' Why was ""Necessity"" satisfied?', ' What implies the first positive example dau ( m, h ) <unk>displaystyle <unk>textit <unk>dau<unk>(m,h)<unk>?', ' Similarly h and par ( t, e )<unk> <unk> fem ( e)?']","['Necessity', 'Sufficiency', ""the predicate dau doesn't appear in the background knowledge"", 'fem\n\n\n(\nm\n)\n\n\n{\\displaystyle {\\textit {par}}(h,m)\\land {\\textit {fem}}(m)}\n from the background knowledge', 'fem']"
1897,inductive logic programming,Example,"The common definition of the grandmother relation, viz. 





gra


(
x
,
z
)
←


fem


(
x
)
∧


par


(
x
,
y
)
∧


par


(
y
,
z
)


{\displaystyle {\textit {gra}}(x,z)\leftarrow {\textit {fem}}(x)\land {\textit {par}}(x,y)\land {\textit {par}}(y,z)}
, cannot be learned using the above approach, since the variable y occurs in the clause body only; the corresponding literals would have been deleted in the 4th step of the approach. To overcome this flaw, that step has to be modified such that it can be parametrized with different literal post-selection heuristics. Historically, the GOLEM implementation is based on the rlgg approach.
","The common definition of the grandmother relation, viz. gra


(
x
,
z
)
←


fem


(
x
)
∧


par


(
x
,
y
)
∧


par


(
y
,
z
)


{\displaystyle {\textit {gra}}(x,z)\leftarrow {\textit {fem}}(x)\land {\textit {par}}(x,y)\land {\textit {par}}(y,z)}
, cannot be learned using the above approach, since the variable y occurs in the clause body only; the corresponding literals would have been deleted in the 4th step of the approach.","[' What is the common definition of the grandmother relation?', ' Where does the variable y occur?', ' What would have been deleted in the 4th step of the approach?']","['gra', 'in the clause body only', 'the corresponding literals']"
1898,inductive logic programming,Inductive Logic Programming system,"Inductive Logic Programming system is a program that takes as an input logic theories 



B
,

E

+


,

E

−




{\displaystyle B,E^{+},E^{-}}
 and outputs a correct hypothesis H wrt theories 



B
,

E

+


,

E

−




{\displaystyle B,E^{+},E^{-}}
 An algorithm of an ILP system consists of two parts: hypothesis search and hypothesis selection. First a hypothesis is searched with an inductive logic programming procedure, then a subset of the found hypotheses (in most systems one hypothesis) is chosen by a selection algorithm. A selection algorithm scores each of the found hypotheses and returns the ones with the highest score. An example of score function include minimal compression length where a hypothesis with a lowest Kolmogorov complexity has the highest score and is returned. An ILP system is complete iff for any input logic theories 



B
,

E

+


,

E

−




{\displaystyle B,E^{+},E^{-}}
 any correct hypothesis H wrt to these input theories can be found with its hypothesis search procedure.
","Inductive Logic Programming system is a program that takes as an input logic theories 



B
,

E

+


,

E

−




{\displaystyle B,E^{+},E^{-}}
 and outputs a correct hypothesis H wrt theories 



B
,

E

+


,

E

−




{\displaystyle B,E^{+},E^{-}}
 An algorithm of an ILP system consists of two parts: hypothesis search and hypothesis selection. First a hypothesis is searched with an inductive logic programming procedure, then a subset of the found hypotheses (in most systems one hypothesis) is chosen by a selection algorithm.","[' What is a program that takes as input logic theories B, E +, E <unk> <unk>displaystyle B,E+<unk>,E<unk>-<unk> and outputs a correct hypothesis H?', ' What are the two parts of an ILP system?', ' What type of programming procedure is used to search a hypothesis?', ' How is a subset of the found hypotheses selected?', ' What is the most common type of selection?']","['Inductive Logic Programming system', 'hypothesis search and hypothesis selection', 'inductive', 'by a selection algorithm', 'hypothesis']"
1899,abstract interpretation,Summary,"In computer science, abstract interpretation is a theory of sound approximation of the semantics of computer programs, based on monotonic functions over ordered sets, especially lattices. It can be viewed as a partial execution of a computer program which gains information about its semantics (e.g., control-flow, data-flow) without performing all the calculations.
","In computer science, abstract interpretation is a theory of sound approximation of the semantics of computer programs, based on monotonic functions over ordered sets, especially lattices. It can be viewed as a partial execution of a computer program which gains information about its semantics (e.g., control-flow, data-flow) without performing all the calculations.","[' Abstract interpretation is a theory of sound approximation of the semantics of what?', ' Abstract interpretation can be viewed as a partial execution of a computer program which gains information about its semantics?', ' What does control-flow and data-flow do?']","['computer programs', 'abstract interpretation', 'gains information about its semantics']"
1900,abstract interpretation,Intuition,"Consider the people in a conference room.  Assume a unique identifier for each person in the room, like a social security number in the United States.  To prove that someone is not present, all one needs to do is see if their social security number is not on the list. Since two different people cannot have the same number, it is possible to prove or disprove the presence of a participant simply by looking up his or her number.
","Consider the people in a conference room. Assume a unique identifier for each person in the room, like a social security number in the United States.","[' What is a unique identifier for each person in a conference room?', ' What is the social security number in the United States?']","['a social security number in the United States', 'unique identifier']"
1901,abstract interpretation,Intuition,"However it is possible that only the names of attendees were registered. If the name of a person is not found in the list, we may safely conclude that that person was not present; but if it is, we cannot conclude definitely without further inquiries, due to the possibility of homonyms (for example, two people named John Smith).  Note that this imprecise information will still be adequate for most purposes, because homonyms are rare in practice. However, in all rigor, we cannot say for sure that somebody was present in the room; all we can say is that he or she was possibly here. If the person we are looking up is a criminal, we will issue an alarm; but there is of course the possibility of issuing a false alarm. Similar phenomena will occur in the analysis of programs.
","However it is possible that only the names of attendees were registered. If the name of a person is not found in the list, we may safely conclude that that person was not present; but if it is, we cannot conclude definitely without further inquiries, due to the possibility of homonyms (for example, two people named John Smith).","[' What is the possibility that only the names of attendees were registered?', ' If the name of a person is not found in the list, what can we conclude?', ' What is a possible homonym for John Smith?']","['it is possible that only the names of attendees were registered. If the name of a person is not found in the list, we may safely conclude that that person was not present', 'that that person was not present', 'two people named John Smith']"
1902,abstract interpretation,Intuition,"If we are only interested in some specific information, say, ""was there a person of age n in the room?"", keeping a list of all names and dates of births is unnecessary. We may safely and without loss of precision restrict ourselves to keeping a list of the participants' ages. If this is already too much to handle, we might keep only the age of the youngest, m and oldest person, M. If the question is about an age strictly lower than m or strictly higher than M, then we may safely respond that no such participant was present. Otherwise, we may only be able to say that we do not know.
","If we are only interested in some specific information, say, ""was there a person of age n in the room? "", keeping a list of all names and dates of births is unnecessary.",[' Is keeping a list of all names and dates of births necessary?'],['unnecessary']
1903,abstract interpretation,Intuition,"In the case of computing, concrete, precise information is in general not computable within finite time and memory (see Rice's theorem and the halting problem). Abstraction is used to allow for generalized answers to questions (for example, answering ""maybe"" to a yes/no question, meaning ""yes or no"", when we (an algorithm of abstract interpretation) cannot compute the precise answer with certainty); this simplifies the problems, making them amenable to automatic solutions. One crucial requirement is to add enough vagueness so as to make problems manageable while still retaining enough precision for answering the important questions (such as ""might the program crash?"").
","In the case of computing, concrete, precise information is in general not computable within finite time and memory (see Rice's theorem and the halting problem). Abstraction is used to allow for generalized answers to questions (for example, answering ""maybe"" to a yes/no question, meaning ""yes or no"", when we (an algorithm of abstract interpretation) cannot compute the precise answer with certainty); this simplifies the problems, making them amenable to automatic solutions.","[' What type of information is not computable within finite time and memory?', ' What is used to allow for generalized answers to questions?', ' What does a yes/no question mean?', ' What is the meaning of ""yes or no?""', ' What simplifies the problems?']","['concrete, precise information', 'Abstraction', 'yes or no', 'when we (an algorithm of abstract interpretation) cannot compute the precise answer with certainty', 'Abstraction']"
1904,abstract interpretation,Abstract interpretation of computer programs,"Given a programming or specification language, abstract interpretation consists of giving several semantics linked by relations of abstraction. A semantics is a mathematical characterization of a possible behavior of the program. The most precise semantics, describing very closely the actual execution of the program, are called the concrete semantics. For instance, the concrete semantics of an imperative programming language may associate to each program the set of execution traces it may produce – an execution trace being a sequence of possible consecutive states of the execution of the program; a state typically consists of the value of the program counter and the memory locations (globals, stack and heap). More abstract semantics are then derived; for instance, one may consider only the set of reachable states in the executions (which amounts to considering the last states in finite traces).
","Given a programming or specification language, abstract interpretation consists of giving several semantics linked by relations of abstraction. A semantics is a mathematical characterization of a possible behavior of the program.","[' What is a mathematical characterization of a possible behavior of the program?', ' What is given a programming or specification language?']","['A semantics', 'abstract interpretation']"
1905,abstract interpretation,Abstract interpretation of computer programs,"The goal of static analysis is to derive a computable semantic interpretation  at some point. For instance, one may choose to represent the state of a program manipulating integer variables by forgetting the actual values of the variables and only keeping their signs (+, − or 0). For some elementary operations, such as multiplication, such an abstraction does not lose any precision: to get the sign of a product, it is sufficient to know the sign of the operands. For some other operations, the abstraction may lose precision: for instance, it is impossible to know the sign of a sum whose operands are respectively positive and negative.
","The goal of static analysis is to derive a computable semantic interpretation  at some point. For instance, one may choose to represent the state of a program manipulating integer variables by forgetting the actual values of the variables and only keeping their signs (+, − or 0).","[' What is the goal of static analysis?', ' What may one choose to represent the state of a program manipulating integer variables?']","['to derive a computable semantic interpretation', 'by forgetting the actual values of the variables and only keeping their signs (+, − or 0).']"
1906,abstract interpretation,Abstract interpretation of computer programs,"Sometimes a loss of precision is necessary to make the semantics decidable (see Rice's theorem and the halting problem). In general, there is a compromise to be made between the precision of the analysis and its decidability (computability), or tractability (computational cost).
","Sometimes a loss of precision is necessary to make the semantics decidable (see Rice's theorem and the halting problem). In general, there is a compromise to be made between the precision of the analysis and its decidability (computability), or tractability (computational cost).","[' What is a compromise to be made between the precision of the analysis and its decidability?', ' What is the term for computational cost?']","['computability', 'tractability']"
1907,abstract interpretation,Abstract interpretation of computer programs,"In practice the abstractions that are defined are tailored to both the program properties one desires to analyze, and to the set of target programs. The first large scale automated analysis of computer programs with abstract interpretation was motivated by the accident that resulted in the destruction of the first flight of the Ariane 5 rocket in 1996.","In practice the abstractions that are defined are tailored to both the program properties one desires to analyze, and to the set of target programs. The first large scale automated analysis of computer programs with abstract interpretation was motivated by the accident that resulted in the destruction of the first flight of the Ariane 5 rocket in 1996.","[' What motivated the first large scale automated analysis of computer programs with abstract interpretation?', ' What caused the destruction of the first computer program?', ' In what year was the first flight of the Ariane 5 rocket destroyed?']","['the accident that resulted in the destruction of the first flight of the Ariane 5 rocket in 1996', 'the accident that resulted in the destruction of the first flight of the Ariane 5 rocket', '1996']"
1908,abstract interpretation,Formalization,"Let L be an ordered set, called a concrete set and let L′ be another ordered set, called an abstract set. These two sets are related to each other by defining total functions that map elements from one to the other.
","Let L be an ordered set, called a concrete set and let L′ be another ordered set, called an abstract set. These two sets are related to each other by defining total functions that map elements from one to the other.","[' What is a concrete set called?', ' What is an abstract set?', ' How are the two sets related?']","['L be an ordered set', 'L′', 'by defining total functions that map elements from one to the other']"
1909,abstract interpretation,Formalization,"A  function α is called an abstraction function if it maps an element x in the concrete set L to an element α(x) in the abstract set L′. That is, element  α(x) in L′ is the abstraction of x in L.
","A  function α is called an abstraction function if it maps an element x in the concrete set L to an element α(x) in the abstract set L′. That is, element  α(x) in L′ is the abstraction of x in L.","[' What is a function <unk> called if it maps an element x in the concrete set L?', ' What is an abstraction function if the element <unk>(x) in L′ is the abstraction of x?']","['α', 'α']"
1910,abstract interpretation,Formalization,"Let L1, L2, L′1 and L′2 be ordered sets. The concrete semantics f is a monotonic function from L1 to L2. A function f′ from L′1 to L′2 is said to be a valid abstraction of f if for all x′ in L′1, (f ∘ γ)(x′) ≤ (γ ∘ f′)(x′).
","Let L1, L2, L′1 and L′2 be ordered sets. The concrete semantics f is a monotonic function from L1 to L2.","[' Let L1, L2, and L′1 be ordered sets?', ' The concrete semantics f is a monotonic function from what to what?']","['L1, L2, L′1 and L′2 be ordered sets.', 'L1 to L2']"
1911,abstract interpretation,Formalization,"Program semantics are generally described using fixed points in the presence of loops or recursive procedures. Let us suppose that L is a complete lattice and let f be a monotonic function from L into L. Then, any x′ such that f(x′) ≤ x′ is an abstraction of the least fixed-point of f, which exists, according to the Knaster–Tarski theorem.
","Program semantics are generally described using fixed points in the presence of loops or recursive procedures. Let us suppose that L is a complete lattice and let f be a monotonic function from L into L. Then, any x′ such that f(x′) ≤ x′ is an abstraction of the least fixed-point of f, which exists, according to the Knaster–Tarski theorem.","[' Program semantics are generally described using fixed points in the presence of what?', ' What is a complete lattice and let f be a monotonic function from L into L?', ' What theorem states that f(x′) <unk> x′ is an abstraction of the least fixed-point of f?']","['loops or recursive procedures', 'L', 'Knaster–Tarski']"
1912,abstract interpretation,Formalization,"The difficulty is now to obtain such an x′. If L′ is of finite height, or at least verifies the ascending chain condition (all ascending sequences are ultimately stationary), then such an x′ may be obtained as the stationary limit of the ascending sequence x′n defined by induction as follows: x′0=⊥ (the least element of L′) and x′n+1=f′(x′n).
","The difficulty is now to obtain such an x′. If L′ is of finite height, or at least verifies the ascending chain condition (all ascending sequences are ultimately stationary), then such an x′ may be obtained as the stationary limit of the ascending sequence x′n defined by induction as follows: x′0=⊥ (the least element of L′) and x′n+1=f′(x′n).","[' What is the difficulty in obtaining such an x′?', ' If L′ is of finite height, or at least verifies the ascending chain condition, what condition does all ascending sequences ultimately be stationary?', ' What is the least element of L′?', ' What is f′(x′n)?']","['L′ is of finite height', 'x′', 'x′0', 'x′n+1']"
1913,abstract interpretation,Formalization,"In other cases, it is still possible to obtain such an x′ through a widening operator ∇: for all x and y, x ∇ y should be greater or equal than both x and y, and for any sequence y′n, the sequence defined by x′0=⊥ and x′n+1=x′n ∇ y′n is ultimately stationary. We can then take y′n=f′(x′n).
","In other cases, it is still possible to obtain such an x′ through a widening operator ∇: for all x and y, x ∇ y should be greater or equal than both x and y, and for any sequence y′n, the sequence defined by x′0=⊥ and x′n+1=x′n ∇ y′n is ultimately stationary. We can then take y′n=f′(x′n).","[' What is another way to obtain an x′?', ' What should be greater or equal than both x and y?', ' For any sequence y′n, what is the sequence defined by?', ' What is the final state of the sequence defined by x′0=<unk>?', ' What does y′n mean?']","['widening operator ∇', 'x ∇ y', 'x′0=⊥', 'stationary', 'stationary']"
1914,abstract interpretation,Formalization,"In some cases, it is possible to define abstractions using Galois connections (α, γ) where α is from L to L′ and γ is from L′ to L. This supposes the existence of best abstractions, which is not necessarily the case. For instance, if we abstract sets of couples (x, y) of real numbers by enclosing convex polyhedra, there is no optimal abstraction to the disc defined by x2+y2 ≤ 1.
","In some cases, it is possible to define abstractions using Galois connections (α, γ) where α is from L to L′ and γ is from L′ to L. This supposes the existence of best abstractions, which is not necessarily the case. For instance, if we abstract sets of couples (x, y) of real numbers by enclosing convex polyhedra, there is no optimal abstraction to the disc defined by x2+y2 ≤ 1.","[' In some cases, it is possible to define abstractions using what?', ' What supposes the existence of best abstractions?', ' If we abstract sets of couples (x, x), what would we do?', ' What is the optimal abstraction to the disc defined by x2+y2 <unk> 1?']","['Galois connections', 'Galois connections', 'there is no optimal abstraction to the disc defined by x2+y2 ≤ 1', 'no']"
1915,abstract interpretation,Examples of abstract domains,"One can assign to each variable x available at a given program point an interval [Lx, Hx]. A state assigning the value v(x) to variable x will be a concretization of these intervals if for all x, v(x) is in [Lx, Hx]. From the intervals [Lx, Hx] and [Ly, Hy] for variables x and y, one can easily obtain intervals for x+y ([Lx+Ly, Hx+Hy]) and for x−y ([Lx−Hy, Hx−Ly]); note that these are exact abstractions, since the set of possible outcomes for, say, x+y, is precisely the interval ([Lx+Ly, Hx+Hy]). More complex formulas can be derived for multiplication, division, etc., yielding so-called interval arithmetics.","One can assign to each variable x available at a given program point an interval [Lx, Hx]. A state assigning the value v(x) to variable x will be a concretization of these intervals if for all x, v(x) is in [Lx, Hx].","[' What can be assigned to each variable x available at a given program point?', ' What will be a concretization of these intervals if for all x, v(x) is in [Lx, Hx]?']","['an interval', 'A state assigning the value v(x) to variable x']"
1916,abstract interpretation,Examples of abstract domains,"With reasonable arithmetic types, the result for z should be zero. But if we do interval arithmetic starting from x in [0, 1], one gets z in [−1, +1]. While each of the operations taken individually was exactly abstracted, their composition isn't.
","With reasonable arithmetic types, the result for z should be zero. But if we do interval arithmetic starting from x in [0, 1], one gets z in [−1, +1].",[' What should the result for z be with reasonable arithmetic types?'],['zero']
1917,abstract interpretation,Examples of abstract domains,"The problem is evident: we did not keep track of the equality relationship between x and y; actually, this domain of intervals does not take into account any relationships between variables, and is thus a non-relational domain. Non-relational domains tend to be fast and simple to implement, but imprecise.
","The problem is evident: we did not keep track of the equality relationship between x and y; actually, this domain of intervals does not take into account any relationships between variables, and is thus a non-relational domain. Non-relational domains tend to be fast and simple to implement, but imprecise.","[' What does the non-relational domain of intervals not take into account?', ' What is fast and simple to implement, but imprecise?']","['any relationships between variables', 'Non-relational domains']"
1918,abstract interpretation,Examples of abstract domains,"and combinations thereof (such as the reduced product, cf. right picture).
","and combinations thereof (such as the reduced product, cf. right picture).",[' What is one example of a reduced product?'],['combinations']
1919,computational complexity,Summary,"In computer science, the computational complexity or simply complexity of an algorithm is the amount of resources required to run it. Particular focus is given to time and memory requirements. The complexity of a problem is the complexity of the best algorithms that allow solving the problem.
","In computer science, the computational complexity or simply complexity of an algorithm is the amount of resources required to run it. Particular focus is given to time and memory requirements.","[' In computer science, what is the computational complexity of an algorithm?', ' What is the amount of resources required to run it?', ' A special focus is given to what?']","['the amount of resources required to run it', 'computational complexity', 'time and memory requirements']"
1920,computational complexity,Summary,"The study of the complexity of explicitly given algorithms is called analysis of algorithms, while the study of the complexity of problems is called computational complexity theory. Both areas are highly related, as the complexity of an algorithm is always an upper bound on the complexity of the problem solved by this algorithm. Moreover, for designing efficient algorithms, it is often fundamental to compare the complexity of a specific algorithm to the complexity of the problem to be solved. Also, in most cases, the only thing that is known about the complexity of a problem is that it is lower than the complexity of the most efficient known algorithms. Therefore, there is a large overlap between analysis of algorithms and complexity theory. 
","The study of the complexity of explicitly given algorithms is called analysis of algorithms, while the study of the complexity of problems is called computational complexity theory. Both areas are highly related, as the complexity of an algorithm is always an upper bound on the complexity of the problem solved by this algorithm.","[' What is the study of the complexity of explicitly given algorithms called?', ' What is another name for computational complexity theory?', ' The complexity of an algorithm is always an upper bound on what?', ' What is the upper bound on the complexity of the problem solved by this algorithm?']","['analysis of algorithms', 'the study of the complexity of problems', 'the complexity of the problem solved by this algorithm', 'the complexity of an algorithm']"
1921,computational complexity,Summary,"As the amount of resources required to run an algorithm generally varies with the size of the input, the complexity is typically expressed as a function n → f(n), where n is the size of the input and f(n) is either the worst-case complexity (the maximum of the amount of resources that are needed over all inputs of size n) or the average-case complexity (the average of the amount of resources over all inputs of size n). Time complexity is generally expressed as the number of required elementary operations on an input of size n, where elementary operations are assumed to take a constant amount of time on a given computer and change only by a constant factor when run on a different computer. Space complexity is generally expressed as the amount of memory required by an algorithm on an input of size n.
","As the amount of resources required to run an algorithm generally varies with the size of the input, the complexity is typically expressed as a function n → f(n), where n is the size of the input and f(n) is either the worst-case complexity (the maximum of the amount of resources that are needed over all inputs of size n) or the average-case complexity (the average of the amount of resources over all inputs of size n). Time complexity is generally expressed as the number of required elementary operations on an input of size n, where elementary operations are assumed to take a constant amount of time on a given computer and change only by a constant factor when run on a different computer.","[' The amount of resources required to run an algorithm generally varies with what?', ' What is typically expressed as a function?', ' Where n is the size of the input and f(n) is either the worst-case complexity or the maximum of the amount of the complexity?', ' What is the worst-case complexity?', ' How is time complexity expressed?', ' Time complexity is generally expressed as the number of required elementary operations on what?', ' What is generally expressed as the number of required elementary operations on an input of size n?', ' Elementary operations are assumed to take a constant amount of time on what computer?']","['the size of the input', 'the complexity', 'worst-case complexity', 'the maximum of the amount of resources that are needed over all inputs of size n', 'as the number of required elementary operations on an input of size n', 'an input of size n', 'Time complexity', 'a given computer and change only by a constant factor when run on a different computer']"
1922,computational complexity,Complexity as a function of input size,"It is impossible to count the number of steps of an algorithm on all possible inputs. As the complexity generally increases with the size of the input, the complexity is typically expressed as a function of the size n (in bits) of the input, and therefore, the complexity is a function of n. However, the complexity of an algorithm may vary dramatically for different inputs of the same size. Therefore, several complexity functions are commonly used.
","It is impossible to count the number of steps of an algorithm on all possible inputs. As the complexity generally increases with the size of the input, the complexity is typically expressed as a function of the size n (in bits) of the input, and therefore, the complexity is a function of n. However, the complexity of an algorithm may vary dramatically for different inputs of the same size.","[' Is it possible to count the number of steps of an algorithm on all possible inputs?', ' What is the complexity typically expressed as a function of the size n of the input?', ' What is a function of n?', ' The complexity of an algorithm can vary dramatically for different inputs of the same size?']","['It is impossible', 'the complexity is a function of n', 'the complexity', 'complexity']"
1923,computational complexity,Complexity as a function of input size,"The worst-case complexity is the maximum of the complexity over all inputs of size n, and the average-case complexity is the average of the complexity over all inputs of size n (this makes sense, as the number of possible inputs of a given size is finite). Generally, when ""complexity"" is used without being further specified, this is the worst-case time complexity that is considered.
","The worst-case complexity is the maximum of the complexity over all inputs of size n, and the average-case complexity is the average of the complexity over all inputs of size n (this makes sense, as the number of possible inputs of a given size is finite). Generally, when ""complexity"" is used without being further specified, this is the worst-case time complexity that is considered.","[' What is the worst-case complexity over all inputs of size n?', ' The average-case complex is the average of what?', ' How many possible inputs are there of a given size?', ' What is the worst-case time complexity that is considered?']","['maximum', 'the complexity over all inputs of size n', 'finite', 'complexity']"
1924,computational complexity,Asymptotic complexity,"It is generally difficult to compute precisely the worst-case and the average-case complexity. In addition, these exact values provide little practical application, as any change of computer or of model of computation would change the complexity somewhat. Moreover, the resource use is not critical for small values of n, and this makes that, for small n, the ease of implementation is generally more interesting than a low complexity.
","It is generally difficult to compute precisely the worst-case and the average-case complexity. In addition, these exact values provide little practical application, as any change of computer or of model of computation would change the complexity somewhat.","[' How difficult is it to compute the worst-case and average-case complexity?', ' What would change the complexity of computation?']","['generally difficult', 'any change of computer or of model of computation']"
1925,computational complexity,Asymptotic complexity,"For these reasons, one generally focuses on the behavior of the complexity for large n, that is on its asymptotic behavior when n tends to the infinity. Therefore, the complexity is generally expressed by using big O notation.
","For these reasons, one generally focuses on the behavior of the complexity for large n, that is on its asymptotic behavior when n tends to the infinity. Therefore, the complexity is generally expressed by using big O notation.","[' What is the behavior of the complexity for large n?', ' What does big O notation express?']","['asymptotic', 'complexity']"
1926,computational complexity,Asymptotic complexity,"For example, the usual algorithm for integer multiplication has a complexity of 



O
(

n

2


)
,


{\displaystyle O(n^{2}),}
 this means that there is a constant 




c

u




{\displaystyle c_{u}}
 such that the multiplication of two integers of at most n digits may be done in a time less than 




c

u



n

2


.


{\displaystyle c_{u}n^{2}.}
 This bound is sharp in the sense that the worst-case complexity and the average-case complexity are 



Ω
(

n

2


)
,


{\displaystyle \Omega (n^{2}),}
 which means that there is a constant 




c

l




{\displaystyle c_{l}}
 such that these complexities are larger than 




c

l



n

2


.


{\displaystyle c_{l}n^{2}.}
 The radix does not appear in these complexity, as changing of radix changes only the constants 




c

u




{\displaystyle c_{u}}
 and 




c

l


.


{\displaystyle c_{l}.}

","For example, the usual algorithm for integer multiplication has a complexity of 



O
(

n

2


)
,


{\displaystyle O(n^{2}),}
 this means that there is a constant 




c

u




{\displaystyle c_{u}}
 such that the multiplication of two integers of at most n digits may be done in a time less than 




c

u



n

2


. {\displaystyle c_{u}n^{2}.}","[' What is the complexity of the usual algorithm for integer multiplication?', ' What is a constant for the multiplication of two integers of at most n digits?']","['O', 'c\n\nu']"
1927,computational complexity,Models of computation,"The evaluation of the complexity relies on the choice of a model of computation, which consists in defining the basic operations that are done in a unit of time. When the model of computation is not explicitly specified, this is generally meant as being multitape Turing machine.
","The evaluation of the complexity relies on the choice of a model of computation, which consists in defining the basic operations that are done in a unit of time. When the model of computation is not explicitly specified, this is generally meant as being multitape Turing machine.","[' The evaluation of complexity relies on the choice of what?', ' What consists in defining the basic operations that are done in a unit of time?', ' When the model of computation is not explicitly specified, what is meant as being multitape Turing machine?']","['a model of computation', 'model of computation', 'model of computation']"
1928,computational complexity,Problem complexity (lower bounds),"The complexity of a problem is the infimum of the complexities of the algorithms that may solve the problem, including unknown algorithms. Thus the complexity of a problem is not greater than the complexity of any algorithm that solves the problems.
","The complexity of a problem is the infimum of the complexities of the algorithms that may solve the problem, including unknown algorithms. Thus the complexity of a problem is not greater than the complexity of any algorithm that solves the problems.","[' What is the infimum of the complexities of the algorithms that may solve the problem?', ' What is not greater than the complexity of any algorithm that solves the problems?']","['The complexity of a problem', 'the complexity of a problem']"
1929,computational complexity,Problem complexity (lower bounds),"For solving most problems, it is required to read all input data, which, normally, needs a time proportional to the size of the data. Thus, such problems have a complexity that is at least linear, that is, using big omega notation, a complexity 



Ω
(
n
)
.


{\displaystyle \Omega (n).}

","For solving most problems, it is required to read all input data, which, normally, needs a time proportional to the size of the data. Thus, such problems have a complexity that is at least linear, that is, using big omega notation, a complexity 



Ω
(
n
)
.","[' For solving most problems, it is required to read all input data.', ' What normally needs a time proportional to the size of the data?', ' What does big omega notation use?']","['Ω\n(\nn', 'input data', 'at least linear']"
1930,computational complexity,Problem complexity (lower bounds),"The solution of some problems, typically in computer algebra and computational algebraic geometry, may be very large. In such a case, the complexity is lower bounded by the maximal size of the output, since the output must be written. For example, a system of n polynomial equations of degree d in n indeterminates may have up to 




d

n




{\displaystyle d^{n}}
 complex solutions, if the number of solutions is finite (this is Bézout's theorem). As these solutions must be written down, the complexity of this problem is 



Ω
(

d

n


)
.


{\displaystyle \Omega (d^{n}).}
 For this problem, an algorithm of complexity 




d

O
(
n
)




{\displaystyle d^{O(n)}}
 is known, which may thus be considered as asymptotically quasi-optimal.
","The solution of some problems, typically in computer algebra and computational algebraic geometry, may be very large. In such a case, the complexity is lower bounded by the maximal size of the output, since the output must be written.","[' The solution of some problems in computer algebra and computational algebraic geometry may be what?', ' What is lower bounded by the maximum size of the output?']","['very large', 'complexity']"
1931,computational complexity,Problem complexity (lower bounds),"A nonlinear lower bound of 



Ω
(
n
log
⁡
n
)


{\displaystyle \Omega (n\log n)}
 is known for the number of comparisons needed for a sorting algorithm. Thus the best sorting algorithms are optimal, as their complexity is 



O
(
n
log
⁡
n
)
.


{\displaystyle O(n\log n).}
 This lower bound results from the fact that there are n! ways of ordering n objects. As each comparison splits in two parts this set of n! orders, the number of N of comparisons that are needed for distinguishing all orders must verify 




2

N


>
n
!
,


{\displaystyle 2^{N}>n!,}
 which implies 



N
=
Ω
(
n
log
⁡
n
)
,


{\displaystyle N=\Omega (n\log n),}
 by Stirling's formula.
","A nonlinear lower bound of 



Ω
(
n
log
⁡
n
)


{\displaystyle \Omega (n\log n)}
 is known for the number of comparisons needed for a sorting algorithm. Thus the best sorting algorithms are optimal, as their complexity is 



O
(
n
log
⁡
n
)
.","[' What is known for the number of comparisons needed for a sorting algorithm?', "" What is the best sorting algorithms' complexity?""]","['Ω\n(\nn\nlog\n\u2061\nn\n)\n\n\n{\\displaystyle \\Omega (n\\log n)}', 'O']"
1932,computational complexity,Problem complexity (lower bounds),"A standard method for getting lower bounds of complexity consists of reducing a problem to another problem. More precisely, suppose that one may encode a problem A of size n into a subproblem of size f(n) of a problem B, and that the complexity of A is 



Ω
(
g
(
n
)
)
.


{\displaystyle \Omega (g(n)).}
 Without loss of generality, one may suppose that the function f increases with n and has an inverse function h. Then the complexity of the problem B is 



Ω
(
g
(
h
(
n
)
)
)
.


{\displaystyle \Omega (g(h(n))).}
 This is the method that is used to prove that, if P ≠ NP (an unsolved conjecture), the complexity of every NP-complete problem is 



Ω
(

n

k


)
,


{\displaystyle \Omega (n^{k}),}
 for every positive integer k.
","A standard method for getting lower bounds of complexity consists of reducing a problem to another problem. More precisely, suppose that one may encode a problem A of size n into a subproblem of size f(n) of a problem B, and that the complexity of A is 



Ω
(
g
(
n
)
)
.",[' A standard method for getting lower bounds of complexity consists of reducing a problem to what?'],['another problem']
1933,computational complexity,Use in algorithm design,"It is a common misconception that the evaluation of the complexity of algorithms will become less important as a result of Moore's law, which posits the exponential growth of the power of modern computers. This is wrong because this power increase allows working with large input data (big data). For example, when one wants to sort alphabetically a list of a few hundreds of entries, such as the bibliography of a book, any algorithm should work well in less than a second. On the other hand, for a list of a million of entries (the phone numbers of a large town, for example), the elementary algorithms that require 



O
(

n

2


)


{\displaystyle O(n^{2})}
 comparisons would have to do a trillion of comparisons, which would need around three hours at the speed of 10 million of comparisons per second. On the other hand, the quicksort and merge sort require only 



n

log

2


⁡
n


{\displaystyle n\log _{2}n}
 comparisons (as average-case complexity for the former, as worst-case complexity for the latter). For n = 1,000,000, this gives approximately 30,000,000 comparisons, which would only take 3 seconds at 10 million comparisons per second.
","It is a common misconception that the evaluation of the complexity of algorithms will become less important as a result of Moore's law, which posits the exponential growth of the power of modern computers. This is wrong because this power increase allows working with large input data (big data).","[' What law posits the exponential growth of the power of modern computers?', "" What does Moore's law allow working with large input data?""]","[""Moore's law"", 'power increase']"
1934,computational complexity,Use in algorithm design,"Thus the evaluation of the complexity may allow eliminating many inefficient algorithms before any implementation. This may also be used for tuning complex algorithms without testing all variants. By determining the most costly steps of a complex algorithm, the study of complexity allows also focusing on these steps the effort for improving the efficiency of an implementation.
",Thus the evaluation of the complexity may allow eliminating many inefficient algorithms before any implementation. This may also be used for tuning complex algorithms without testing all variants.,[' The evaluation of complexity may allow eliminating many inefficient algorithms before what?'],['any implementation']
1935,gpu,Summary,"A graphics processing unit (GPU) is a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device. GPUs are used in embedded systems, mobile phones, personal computers, workstations, and game consoles.
","A graphics processing unit (GPU) is a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device. GPUs are used in embedded systems, mobile phones, personal computers, workstations, and game consoles.","[' What is a graphics processing unit?', ' What does a GPU do?', ' Where are GPUs used?']","['a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device', 'rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device', 'embedded systems, mobile phones, personal computers, workstations, and game consoles']"
1936,gpu,Summary,"Modern GPUs are very efficient at manipulating computer graphics and image processing. Their highly parallel structure makes them more efficient than general-purpose central processing units (CPUs) for algorithms that process large blocks of data in parallel. In a personal computer, a GPU can be present on a video card or embedded on the motherboard. In certain CPUs, they are embedded on the CPU die.",Modern GPUs are very efficient at manipulating computer graphics and image processing. Their highly parallel structure makes them more efficient than general-purpose central processing units (CPUs) for algorithms that process large blocks of data in parallel.,"[' Modern GPUs are very efficient at manipulating what?', ' What makes modern GPUs more efficient than CPUs?']","['computer graphics and image processing', 'highly parallel structure']"
1937,gpu,Summary,"In the 1970s, the term ""GPU"" originally stood for graphics processor unit and described a programmable processing unit independently working from the CPU and responsible for graphics manipulation and output. Later, in 1994, Sony used the term (now standing for graphics processing unit) in reference to the PlayStation console's Toshiba-designed Sony GPU in 1994. The term was popularized by Nvidia in 1999, who marketed the GeForce 256 as ""the world's first GPU"". It was presented as a ""single-chip processor with integrated transform, lighting, triangle setup/clipping, and rendering engines"". Rival ATI Technologies coined the term ""visual processing unit"" or VPU with the release of the Radeon 9700 in 2002.","In the 1970s, the term ""GPU"" originally stood for graphics processor unit and described a programmable processing unit independently working from the CPU and responsible for graphics manipulation and output. Later, in 1994, Sony used the term (now standing for graphics processing unit) in reference to the PlayStation console's Toshiba-designed Sony GPU in 1994.","[' What did the term ""GPU"" stand for in the 1970s?', ' What was the term used to describe a programmable processing unit working independently from the CPU?', "" In what year did Sony use the term GPU in reference to the PlayStation console's Toshiba-designed graphics processor?"", ' What was the name of the graphics processing unit on the PlayStation console in 1994?']","['graphics processor unit', 'graphics processor unit', '1994', 'Sony GPU']"
1938,gpu,Computational functions,"Modern GPUs use most of their transistors to do calculations related to 3D computer graphics. In addition to the 3D hardware, today's GPUs include basic 2D acceleration and framebuffer capabilities (usually with a VGA compatibility mode). Newer cards such as AMD/ATI HD5000-HD7000 even lack 2D acceleration; it has to be emulated by 3D hardware. GPUs were initially used to accelerate the memory-intensive work of texture mapping and rendering polygons, later adding units to accelerate geometric calculations such as the rotation and translation of vertices into different coordinate systems. Recent developments in GPUs include support for programmable shaders which can manipulate vertices and textures with many of the same operations supported by CPUs, oversampling and interpolation techniques to reduce aliasing, and very high-precision color spaces. Given that most of these computations involve matrix and vector operations, engineers and scientists have increasingly studied the use of GPUs for non-graphical calculations; they are especially suited to other embarrassingly parallel problems.
","Modern GPUs use most of their transistors to do calculations related to 3D computer graphics. In addition to the 3D hardware, today's GPUs include basic 2D acceleration and framebuffer capabilities (usually with a VGA compatibility mode).","[' What do modern GPUs use to do calculations related to 3D computer graphics?', ' Modern GPUs include basic 2D acceleration and framebuffer capabilities with what mode?']","['transistors', 'VGA compatibility']"
1939,gpu,Computational functions,"
Several factors of the GPU's construction enter into the performance of the card for real-time rendering. Common factors can include the size of the connector pathways in the semiconductor device fabrication, the clock signal frequency, and the number and size of various on-chip memory caches. Additionally, the number of Streaming Multiprocessors (SM) for NVidia GPUs, or Compute Units (CU) for AMD GPUs, which describe the number of core on-silicon processor units within the GPU chip that perform the core calculations, typically working in parallel with other SM/CUs on the GPU. Performance of GPUs are typically measured in floating point operations per second or FLOPS, with GPUs in the 2010s and 2020s typically delivering performance measured in teraflops (TFLOPS). This is an estimated performance measure as other factors can impact the actual display rate.","
Several factors of the GPU's construction enter into the performance of the card for real-time rendering. Common factors can include the size of the connector pathways in the semiconductor device fabrication, the clock signal frequency, and the number and size of various on-chip memory caches.","["" Several factors of the GPU's construction enter into what?"", ' The size of the connector pathways in the semiconductor device fabrication, the clock signal frequency, and the number and size of various on-chip memory caches are some of the common factors that affect the performance of the card for what purpose?']","['performance of the card for real-time rendering', 'real-time rendering']"
1940,gpu,Computational functions,"With the emergence of deep learning, the importance of GPUs has increased. In research done by Indigo, it was found that while training deep learning neural networks, GPUs can be 250 times faster than CPUs. There has been some level of competition in this area with ASICs, most prominently the Tensor Processing Unit (TPU) made by Google. However, ASICs require changes to existing code and GPUs are still very popular.
","With the emergence of deep learning, the importance of GPUs has increased. In research done by Indigo, it was found that while training deep learning neural networks, GPUs can be 250 times faster than CPUs.","[' What has the importance of GPUs increased with the emergence of deep learning?', ' Indigo found that while training deep learning neural networks, GPUs can be 250 times faster than what?']","['250 times faster than CPUs', 'CPUs']"
1941,time series,Summary,"In mathematics, a time series is a series of data points indexed (or listed or graphed) in time order.  Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of discrete-time data. Examples of time series are heights of ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.
","In mathematics, a time series is a series of data points indexed (or listed or graphed) in time order. Most commonly, a time series is a sequence taken at successive equally spaced points in time.","[' In mathematics, a time series is a series of data points indexed (or listed or graphed) in what order?']",['time']
1942,time series,Summary,"A Time series is very frequently plotted via a run chart (which is a temporal line chart). Time series are used in statistics, signal processing, pattern recognition, econometrics, mathematical finance, weather forecasting, earthquake prediction, electroencephalography, control engineering, astronomy, communications engineering, and largely in any domain of applied science and engineering which involves temporal measurements.
","A Time series is very frequently plotted via a run chart (which is a temporal line chart). Time series are used in statistics, signal processing, pattern recognition, econometrics, mathematical finance, weather forecasting, earthquake prediction, electroencephalography, control engineering, astronomy, communications engineering, and largely in any domain of applied science and engineering which involves temporal measurements.","[' What is a run chart?', ' Time series are used in statistics, signal processing, pattern recognition, what?', ' In what domain of applied science and engineering does temporal measurements occur?']","['temporal line chart', 'econometrics', 'any']"
1943,time series,Summary,"Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values. While regression analysis is often employed in such a way as to test relationships between one or more different time series, this type of analysis is not usually called ""time series analysis"", which refers in particular to relationships between different points in time within a single series. Interrupted time series analysis is used to detect changes in the evolution of a time series from before to after some intervention which may affect the underlying variable.
",Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values.,"[' What is time series analysis?', ' What is the use of a model to predict future values based on previously observed values?']","['methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data', 'Time series forecasting']"
1944,time series,Summary,"Time series data have a natural temporal ordering.  This makes time series analysis distinct from cross-sectional studies, in which there is no natural ordering of the observations (e.g. explaining people's wages by reference to their respective education levels, where the individuals' data could be entered in any order).  Time series analysis is also distinct from spatial data analysis where the observations typically relate to geographical locations (e.g. accounting for house prices by the location as well as the intrinsic characteristics of the houses). A stochastic model for a time series will generally reflect the fact that observations close together in time will be more closely related than observations further apart. In addition, time series models will often make use of the natural one-way ordering of time so that values for a given period will be expressed as deriving in some way from past values, rather than from future values (see time reversibility).
","Time series data have a natural temporal ordering. This makes time series analysis distinct from cross-sectional studies, in which there is no natural ordering of the observations (e.g.","[' What do time series data have a natural temporal ordering?', ' What makes time series analysis distinct from cross-sectional studies?']","['cross-sectional studies', 'natural temporal ordering']"
1945,time series,Summary,"Time series analysis can be applied to real-valued, continuous data, discrete numeric data, or discrete symbolic data (i.e. sequences of characters, such as letters and words in the English language).
","Time series analysis can be applied to real-valued, continuous data, discrete numeric data, or discrete symbolic data (i.e. sequences of characters, such as letters and words in the English language).","[' How can time series analysis be applied to real-valued, continuous data, numeric data, or symbolic data?', ' What are sequences of characters such as letters and words in the English language?']","['sequences of characters', 'discrete symbolic data']"
1946,time series,Methods for analysis,"Methods for time series analysis may be divided into two classes: frequency-domain methods and time-domain methods. The former include spectral analysis and wavelet analysis; the latter include auto-correlation and cross-correlation analysis. In the time domain, correlation and analysis can be made in a filter-like manner using scaled correlation, thereby mitigating the need to operate in the frequency domain.
",Methods for time series analysis may be divided into two classes: frequency-domain methods and time-domain methods. The former include spectral analysis and wavelet analysis; the latter include auto-correlation and cross-correlation analysis.,"[' Methods for time series analysis may be divided into how many classes?', ' What are the first two classes of methods?', ' The second class of methods includes what two types of analysis?']","['two', 'frequency-domain methods and time-domain methods', 'auto-correlation and cross-correlation analysis']"
1947,time series,Methods for analysis,"Additionally, time series analysis techniques may be divided into parametric and non-parametric methods. The parametric approaches assume that the underlying stationary stochastic process has a certain structure which can be described using a small number of parameters (for example, using an autoregressive or moving average model). In these approaches, the task is to estimate the parameters of the model that describes the stochastic process. By contrast, non-parametric approaches explicitly estimate the covariance or the spectrum of the process without assuming that the process has any particular structure.
","Additionally, time series analysis techniques may be divided into parametric and non-parametric methods. The parametric approaches assume that the underlying stationary stochastic process has a certain structure which can be described using a small number of parameters (for example, using an autoregressive or moving average model).","[' What are time series analysis techniques divided into?', ' The parametric approaches assume that the underlying stationary stochastic process has a certain structure which can be described using what?']","['parametric and non-parametric methods', 'a small number of parameters']"
1948,time series,Panel data,"A time series is one type of panel data. Panel data is the general class, a multidimensional data set, whereas a time series data set is a one-dimensional panel (as is a cross-sectional dataset).  A data set may exhibit characteristics of both panel data and time series data.  One way to tell is to ask what makes one data record unique from the other records.  If the answer is the time data field, then this is a time series data set candidate.  If determining a unique record requires a time data field and an additional identifier which is unrelated to time (e.g. student ID, stock symbol, country code), then it is panel data candidate.  If the differentiation lies on the non-time identifier, then the data set is a cross-sectional data set candidate.
","A time series is one type of panel data. Panel data is the general class, a multidimensional data set, whereas a time series data set is a one-dimensional panel (as is a cross-sectional dataset).","[' What is one type of panel data?', ' What is the general class?', ' A time series data set is what?']","['time series', 'Panel data', 'a one-dimensional panel']"
1949,time series,Models,"Models for time series data can have many forms and represent different stochastic processes. When modeling variations in the level of a process, three broad classes of practical importance are the autoregressive (AR) models, the integrated (I) models, and the moving average (MA) models. These three classes depend linearly on previous data points. Combinations of these ideas produce autoregressive moving average (ARMA) and autoregressive integrated moving average (ARIMA) models. The autoregressive fractionally integrated moving average (ARFIMA) model generalizes the former three. Extensions of these classes to deal with vector-valued data are available under the heading of multivariate time-series models and sometimes the preceding acronyms are extended by including an initial ""V"" for ""vector"", as in VAR for vector autoregression. An additional set of extensions of these models is available for use where the observed time-series is driven by some ""forcing"" time-series (which may not have a causal effect on the observed series): the distinction from the multivariate case is that the forcing series may be deterministic or under the experimenter's control. For these models, the acronyms are extended with a final ""X"" for ""exogenous"".
","Models for time series data can have many forms and represent different stochastic processes. When modeling variations in the level of a process, three broad classes of practical importance are the autoregressive (AR) models, the integrated (I) models, and the moving average (MA) models.","[' Models for time series data can have many forms and represent what?', ' When modeling variations in the level of a process, what are three broad classes of practical importance?']","['different stochastic processes', 'the autoregressive (AR) models, the integrated (I) models, and the moving average (MA) models']"
1950,time series,Models,"Non-linear dependence of the level of a series on previous data points is of interest, partly because of the possibility of producing a chaotic time series. However, more importantly, empirical investigations can indicate the advantage of using predictions derived from non-linear models, over those from linear models, as for example in nonlinear autoregressive exogenous models. Further references on nonlinear time series analysis: (Kantz and Schreiber), and (Abarbanel)","Non-linear dependence of the level of a series on previous data points is of interest, partly because of the possibility of producing a chaotic time series. However, more importantly, empirical investigations can indicate the advantage of using predictions derived from non-linear models, over those from linear models, as for example in nonlinear autoregressive exogenous models.","[' Non-linear dependence of the level of a series on previous data points is of interest, partly because of the possibility of producing what?', ' What can empirical investigations indicate the advantage of using predictions derived from non linear models, over those from linear models?', ' What are non-linear models compared to?', ' What is an example of nonlinear autoregressive exogenous models?']","['a chaotic time series', 'nonlinear autoregressive exogenous models', 'linear models', 'linear models']"
1951,time series,Models,"Among other types of non-linear time series models, there are models to represent the changes of variance over time (heteroskedasticity). These models represent autoregressive conditional heteroskedasticity (ARCH) and the collection comprises a wide variety of representation (GARCH, TARCH, EGARCH, FIGARCH, CGARCH, etc.). Here changes in variability are related to, or predicted by, recent past values of the observed series. This is in contrast to other possible representations of locally varying variability, where the variability might be modelled as being driven by a separate time-varying process, as in a doubly stochastic model.
","Among other types of non-linear time series models, there are models to represent the changes of variance over time (heteroskedasticity). These models represent autoregressive conditional heteroskedasticity (ARCH) and the collection comprises a wide variety of representation (GARCH, TARCH, EGARCH, FIGARCH, CGARCH, etc.).","[' What are non-linear time series models used for?', ' What are the models used to represent the changes of variance over time called?']","['represent the changes of variance over time', 'heteroskedasticity']"
1952,time series,Models,"In recent work on model-free analyses, wavelet transform based methods (for example locally stationary wavelets and wavelet decomposed neural networks) have gained favor. Multiscale (often referred to as multiresolution) techniques decompose a given time series, attempting to illustrate time dependence at multiple scales. See also Markov switching multifractal (MSMF) techniques for modeling volatility evolution.
","In recent work on model-free analyses, wavelet transform based methods (for example locally stationary wavelets and wavelet decomposed neural networks) have gained favor. Multiscale (often referred to as multiresolution) techniques decompose a given time series, attempting to illustrate time dependence at multiple scales.","[' What have wavelet transform based methods gained favor in recent work on model-free analyses?', ' What are locally stationary wavelets and wavelet decomposed neural networks called?', ' Multiscale techniques decompose a given time series, trying to illustrate time dependence at multiple scales?']","['locally stationary wavelets and wavelet decomposed neural networks', 'wavelet transform based methods', 'multiresolution']"
1953,time series,Models,"A Hidden Markov model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (hidden) states. An HMM can be considered as the simplest dynamic Bayesian network. HMM models are widely used in speech recognition, for translating a time series of spoken words into text.
",A Hidden Markov model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process with unobserved (hidden) states. An HMM can be considered as the simplest dynamic Bayesian network.,"[' What is HMM?', ' What is the simplest dynamic Bayesian network?']","['Hidden Markov model', 'Hidden Markov model']"
1954,time series,Visualization,Time series can be visualized with two categories of chart: Overlapping Charts and Separated Charts. Overlapping Charts display all-time series on the same layout while Separated Charts presents them on different layouts (but aligned for comparison purpose),Time series can be visualized with two categories of chart: Overlapping Charts and Separated Charts. Overlapping Charts display all-time series on the same layout while Separated Charts presents them on different layouts (but aligned for comparison purpose),"[' How many categories of charts can time series be visualized with?', ' Overlapping Charts display all-time series on the same layout what?', ' Separated Charts presents them on different layouts but aligned for comparison purpose?']","['two', 'Separated Charts', 'Overlapping Charts']"
1955,parallel computing,Summary,"Parallel computing is a type of computation in which many calculations or processes are carried out simultaneously. Large problems can often be divided into smaller ones, which can then be solved at the same time. There are several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism. Parallelism has long been employed in high-performance computing, but has gained broader interest due to the physical constraints preventing frequency scaling. As power consumption (and consequently heat generation) by computers has become a concern in recent years, parallel computing has become the dominant paradigm in computer architecture, mainly in the form of multi-core processors.","Parallel computing is a type of computation in which many calculations or processes are carried out simultaneously. Large problems can often be divided into smaller ones, which can then be solved at the same time.","[' Parallel computing is a type of what?', ' Large problems can often be divided into smaller ones which can then be solved at the same time?']","['computation', 'Parallel computing']"
1956,parallel computing,Summary,"Parallel computing is closely related to concurrent computing—they are frequently used together, and often conflated, though the two are distinct: it is possible to have parallelism without concurrency (such as bit-level parallelism), and concurrency without parallelism (such as multitasking by time-sharing on a single-core CPU). In parallel computing, a computational task is typically broken down into several, often many, very similar sub-tasks that can be processed independently and whose results are combined afterwards, upon completion. In contrast, in concurrent computing, the various processes often do not address related tasks; when they do, as is typical in distributed computing, the separate tasks may have a varied nature and often require some inter-process communication during execution.
","Parallel computing is closely related to concurrent computing—they are frequently used together, and often conflated, though the two are distinct: it is possible to have parallelism without concurrency (such as bit-level parallelism), and concurrency without parallelism (such as multitasking by time-sharing on a single-core CPU). In parallel computing, a computational task is typically broken down into several, often many, very similar sub-tasks that can be processed independently and whose results are combined afterwards, upon completion.","[' Parallel computing is closely related to what?', ' Parallel computing and concurrent computing are often used together and often conflated.', ' What is possible to have without concurrency (bit-level parallelism)?', ' What type of CPU is used for time-sharing?', ' In parallel computing, a computational task is typically broken down into many, many, very similar sub-tasks that can be processed independently and whose results are combined?']","['concurrent computing', 'Parallel computing', 'parallelism', 'single-core', 'upon completion']"
1957,parallel computing,Summary,"Parallel computers can be roughly classified according to the level at which the hardware supports parallelism, with multi-core and multi-processor computers having multiple processing elements within a single machine, while clusters, MPPs, and grids use multiple computers to work on the same task. Specialized parallel computer architectures are sometimes used alongside traditional processors, for accelerating specific tasks.
","Parallel computers can be roughly classified according to the level at which the hardware supports parallelism, with multi-core and multi-processor computers having multiple processing elements within a single machine, while clusters, MPPs, and grids use multiple computers to work on the same task. Specialized parallel computer architectures are sometimes used alongside traditional processors, for accelerating specific tasks.","[' Parallel computers can be classified according to the level at which the hardware supports what?', ' Multi-core and multi-processor computers have multiple processing elements within a single machine.', ' Clusters, MPPs, and grids use multiple computers to work on what task?', ' Specialized parallel computer architectures are sometimes used for what purpose?', ' Parallel computer architectures are sometimes used alongside traditional processors for what purpose?']","['parallelism', 'Parallel computers', 'the same task', 'accelerating specific tasks', 'accelerating specific tasks']"
1958,parallel computing,Summary,"In some cases parallelism is transparent to the programmer, such as in bit-level or instruction-level parallelism, but explicitly parallel algorithms, particularly those that use concurrency, are more difficult to write than sequential ones, because concurrency introduces several new classes of potential software bugs, of which race conditions are the most common. Communication and synchronization between the different subtasks are typically some of the greatest obstacles to getting optimal parallel program performance.
","In some cases parallelism is transparent to the programmer, such as in bit-level or instruction-level parallelism, but explicitly parallel algorithms, particularly those that use concurrency, are more difficult to write than sequential ones, because concurrency introduces several new classes of potential software bugs, of which race conditions are the most common. Communication and synchronization between the different subtasks are typically some of the greatest obstacles to getting optimal parallel program performance.","[' What type of parallelism is transparent to the programmer?', ' Why are explicitly parallel algorithms more difficult to write than sequential ones?', ' What introduces several new classes of potential software bugs?', ' What are the most common race conditions?', ' Communication and synchronization between the different subtasks are typically some of the greatest obstacles to getting optimal parallel program performance?']","['bit-level or instruction-level parallelism', 'concurrency introduces several new classes of potential software bugs', 'concurrency', 'software bugs', 'parallelism is transparent to the programmer, such as in bit-level or instruction-level parallelism, but explicitly parallel algorithms, particularly those that use concurrency, are more difficult to write than sequential ones, because concurrency']"
1959,parallel computing,Background,"Traditionally, computer software has been written for serial computation. To solve a problem, an algorithm is constructed and implemented as a serial stream of instructions. These instructions are executed on a central processing unit on one computer. Only one instruction may execute at a time—after that instruction is finished, the next one is executed.","Traditionally, computer software has been written for serial computation. To solve a problem, an algorithm is constructed and implemented as a serial stream of instructions.","[' What has been written for serial computation?', ' To solve a problem, what is constructed and implemented as a serial stream of instructions?']","['computer software', 'an algorithm']"
1960,parallel computing,Background,"Parallel computing, on the other hand, uses multiple processing elements simultaneously to solve a problem. This is accomplished by breaking the problem into independent parts so that each processing element can execute its part of the algorithm simultaneously with the others. The processing elements can be diverse and include resources such as a single computer with multiple processors, several networked computers, specialized hardware, or any combination of the above. Historically parallel computing was used for scientific computing and the simulation of scientific problems, particularly in the natural and engineering sciences, such as meteorology. This led to the design of parallel hardware and software, as well as high performance computing.","Parallel computing, on the other hand, uses multiple processing elements simultaneously to solve a problem. This is accomplished by breaking the problem into independent parts so that each processing element can execute its part of the algorithm simultaneously with the others.","[' What type of computing uses multiple processing elements simultaneously to solve a problem?', ' How is parallel computing accomplished?', ' What can each processing element execute its part of the algorithm simultaneously with?']","['Parallel computing', 'by breaking the problem into independent parts', 'the others']"
1961,parallel computing,Background,"Frequency scaling was the dominant reason for improvements in computer performance from the mid-1980s until 2004. The runtime of a program is equal to the number of instructions multiplied by the average time per instruction. Maintaining everything else constant, increasing the clock frequency decreases the average time it takes to execute an instruction. An increase in frequency thus decreases runtime for all compute-bound programs. However, power consumption P by a chip is given by the equation P = C × V 2 × F, where C is the capacitance being switched per clock cycle (proportional to the number of transistors whose inputs change), V is voltage, and F is the processor frequency (cycles per second). Increases in frequency increase the amount of power used in a processor. Increasing processor power consumption led ultimately to Intel's May 8, 2004 cancellation of its Tejas and Jayhawk processors, which is generally cited as the end of frequency scaling as the dominant computer architecture paradigm.",Frequency scaling was the dominant reason for improvements in computer performance from the mid-1980s until 2004. The runtime of a program is equal to the number of instructions multiplied by the average time per instruction.,"[' What was the dominant reason for improvements in computer performance from the mid-1980s until 2004?', ' The runtime of a program is equal to how many instructions multiplied by the average time per instruction?']","['Frequency scaling', 'number of instructions']"
1962,parallel computing,Background,"To deal with the problem of power consumption and overheating the major central processing unit (CPU or processor) manufacturers started to produce power efficient processors with multiple cores. The core is the computing unit of the processor and in multi-core processors each core is independent and can access the same memory concurrently. Multi-core processors have brought parallel computing to desktop computers. Thus parallelisation of serial programmes has become a mainstream programming task. In 2012 quad-core processors became standard for desktop computers, while servers have 10 and 12 core processors. From Moore's law it can be predicted that the number of cores per processor will double every 18–24 months. This could mean that after 2020 a typical processor will have dozens or hundreds of cores.",To deal with the problem of power consumption and overheating the major central processing unit (CPU or processor) manufacturers started to produce power efficient processors with multiple cores. The core is the computing unit of the processor and in multi-core processors each core is independent and can access the same memory concurrently.,"[' What is another term for a central processing unit?', ' What is the computing unit of a processor?', ' In multi-core processors, which core is independent?', ' How many cores of a processor are independent?']","['CPU or processor', 'The core', 'each core', 'multi-core']"
1963,parallel computing,Background,"An operating system can ensure that different tasks and user programmes are run in parallel on the available cores. However, for a serial software programme to take full advantage of the multi-core architecture the programmer needs to restructure and parallelise the code. A speed-up of application software runtime will no longer be achieved through frequency scaling, instead programmers will need to parallelise their software code to take advantage of the increasing computing power of multicore architectures.","An operating system can ensure that different tasks and user programmes are run in parallel on the available cores. However, for a serial software programme to take full advantage of the multi-core architecture the programmer needs to restructure and parallelise the code.","[' What can ensure that different tasks and user programmes are run in parallel on the available cores?', ' What needs to restructure and parallelise the code for a serial software programme to take full advantage of the multi-core architecture?']","['An operating system', 'programmer']"
1964,parallel computing,Algorithmic methods,"As parallel computers become larger and faster, we are now able to solve problems that had previously taken too long to run. Fields as varied as bioinformatics (for protein folding and sequence analysis) and economics (for mathematical finance) have taken advantage of parallel computing. Common types of problems in parallel computing applications include:","As parallel computers become larger and faster, we are now able to solve problems that had previously taken too long to run. Fields as varied as bioinformatics (for protein folding and sequence analysis) and economics (for mathematical finance) have taken advantage of parallel computing.","[' What are two fields that have taken advantage of parallel computing?', ' What is bioinformatics?']","['bioinformatics (for protein folding and sequence analysis) and economics', 'protein folding and sequence analysis']"
1965,parallel computing,Fault tolerance,"Parallel computing can also be applied to the design of fault-tolerant computer systems, particularly via lockstep systems performing the same operation in parallel. This provides redundancy in case one component fails, and also allows automatic error detection and error correction if the results differ. These methods can be used to help prevent single-event upsets caused by transient errors. Although additional measures may be required in embedded or specialized systems, this method can provide a cost-effective approach to achieve n-modular redundancy in commercial off-the-shelf systems.
","Parallel computing can also be applied to the design of fault-tolerant computer systems, particularly via lockstep systems performing the same operation in parallel. This provides redundancy in case one component fails, and also allows automatic error detection and error correction if the results differ.","[' What type of computing can be used to design fault-tolerant computer systems?', ' What provides redundancy in case one component fails?']","['Parallel computing', 'Parallel computing']"
1966,parallel computing,History,"In April 1958, Stanley Gill (Ferranti) discussed parallel programming and the need for branching and waiting. Also in 1958, IBM researchers John Cocke and Daniel Slotnick discussed the use of parallelism in numerical calculations for the first time. Burroughs Corporation introduced the D825 in 1962, a four-processor computer that accessed up to 16 memory modules through a crossbar switch. In 1967, Amdahl and Slotnick published a debate about the feasibility of parallel processing at American Federation of Information Processing Societies Conference. It was during this debate that Amdahl's law was coined to define the limit of speed-up due to parallelism.
","In April 1958, Stanley Gill (Ferranti) discussed parallel programming and the need for branching and waiting. Also in 1958, IBM researchers John Cocke and Daniel Slotnick discussed the use of parallelism in numerical calculations for the first time.","[' In what year did Stanley Gill discuss parallel programming?', ' Who discussed the need for branching and waiting?', ' What IBM researchers discussed the use of parallelism in numerical calculations for the first time?']","['1958', 'Stanley Gill (Ferranti)', 'John Cocke and Daniel Slotnick']"
1967,parallel computing,History,"In 1969, Honeywell introduced its first Multics system, a symmetric multiprocessor system capable of running up to eight processors in parallel. C.mmp, a multi-processor project at Carnegie Mellon University in the 1970s, was among the first multiprocessors with more than a few processors. The first bus-connected multiprocessor with snooping caches was the Synapse N+1 in 1984.","In 1969, Honeywell introduced its first Multics system, a symmetric multiprocessor system capable of running up to eight processors in parallel. C.mmp, a multi-processor project at Carnegie Mellon University in the 1970s, was among the first multiprocessors with more than a few processors.","[' In what year did Honeywell introduce its first Multics system?', ' What was the name of the multi-processor project at Carnegie Mellon University in the 1970s?', ' How many processors were C.mmp capable of running in parallel?']","['1969', 'C.mmp', 'more than a few processors']"
1968,parallel computing,History,"SIMD parallel computers can be traced back to the 1970s. The motivation behind early SIMD computers was to amortize the gate delay of the processor's control unit over multiple instructions. In 1964, Slotnick had proposed building a massively parallel computer for the Lawrence Livermore National Laboratory. His design was funded by the US Air Force, which was the earliest SIMD parallel-computing effort, ILLIAC IV. The key to its design was a fairly high parallelism, with up to 256 processors, which allowed the machine to work on large datasets in what would later be known as vector processing. However, ILLIAC IV was called ""the most infamous of supercomputers"", because the project was only one-fourth completed, but took 11 years and cost almost four times the original estimate. When it was finally ready to run its first real application in 1976, it was outperformed by existing commercial supercomputers such as the Cray-1.
",SIMD parallel computers can be traced back to the 1970s. The motivation behind early SIMD computers was to amortize the gate delay of the processor's control unit over multiple instructions.,"[' SIMD parallel computers can be traced back to what decade?', ' The motivation behind early SIMD computers was to amortize what?']","['1970s', ""the gate delay of the processor's control unit over multiple instructions""]"
1969,parallel computing,Biological brain as massively parallel computer,"In the early 1970s, at the MIT Computer Science and Artificial Intelligence Laboratory, Marvin Minsky and Seymour Papert started developing the Society of Mind theory, which views the biological brain as massively parallel computer. In 1986, Minsky published The Society of Mind, which claims that “mind is formed from many little agents, each mindless by itself”. The theory attempts to explain how what we call intelligence could be a product of the interaction of non-intelligent parts. Minsky says that the biggest source of ideas about the theory came from his work in trying to create a machine that uses a robotic arm, a video camera, and a computer to build with children's blocks.","In the early 1970s, at the MIT Computer Science and Artificial Intelligence Laboratory, Marvin Minsky and Seymour Papert started developing the Society of Mind theory, which views the biological brain as massively parallel computer. In 1986, Minsky published The Society of Mind, which claims that “mind is formed from many little agents, each mindless by itself”.","[' In what decade did Marvin Minsky and Seymour Papert start developing the Society of Mind theory?', ' What does the society of mind theory view the biological brain as?', ' In what year did Minsky publish The Society Of Mind?', ' How is the mind formed?', ' What does Mind claim is formed from many little agents?']","['1970s', 'massively parallel computer', '1986', 'from many little agents', 'mind']"
1970,support vector machines,Summary,"In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995,  Vapnik et al., 1997) SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.
","In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995,  Vapnik et al., 1997) SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974).","[' What are support-vector machines also known as?', ' What are SVMs?', ' Where were SVM developed?', ' What is one of the most robust prediction methods?', ' What is based on statistical learning frameworks or VC theory?']","['SVMs', 'support-vector machines', 'AT&T Bell Laboratories', 'SVMs', 'SVMs']"
1971,support vector machines,Summary,"When data are unlabelled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The support-vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data.","When data are unlabelled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The support-vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data.","[' When data are unlabelled, what is not possible?', ' What is required when data are not labelled?', ' Who created the support-vector clustering algorithm?', ' Who created the support vector machines algorithm?', ' What is the name of the algorithm that uses support vectors to categorize unlabeled data?']","['supervised learning', 'unsupervised learning approach', 'Hava Siegelmann and Vladimir Vapnik', 'Hava Siegelmann and Vladimir Vapnik', 'support-vector clustering algorithm']"
1972,support vector machines,Motivation,"Classifying data is a common task in machine learning.
Suppose some given data points each belong to one of two classes, and the goal is to decide which class a new data point will be in. In the case of support-vector machines, a data point is viewed as a 



p


{\displaystyle p}
-dimensional vector (a list of 



p


{\displaystyle p}
 numbers), and we want to know whether we can separate such points with a 



(
p
−
1
)


{\displaystyle (p-1)}
-dimensional hyperplane. This is called a linear classifier. There are many hyperplanes that might classify the data. One reasonable choice as the best hyperplane is the one that represents the largest separation, or margin, between the two classes. So we choose the hyperplane so that the distance from it to the nearest data point on each side is maximized. If such a hyperplane exists, it is known as the maximum-margin hyperplane and the linear classifier it defines is known as a maximum-margin classifier; or equivalently, the perceptron of optimal stability.","Classifying data is a common task in machine learning. Suppose some given data points each belong to one of two classes, and the goal is to decide which class a new data point will be in.","[' What is a common task in machine learning?', ' What is the goal of classifying data?']","['Classifying data', 'to decide which class']"
1973,support vector machines,Definition,"More formally, a support-vector machine constructs a hyperplane or set of hyperplanes in a high- or infinite-dimensional space, which can be used for classification, regression, or other tasks like outliers detection. Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class (so-called functional margin), since in general the larger the margin, the lower the generalization error of the classifier.","More formally, a support-vector machine constructs a hyperplane or set of hyperplanes in a high- or infinite-dimensional space, which can be used for classification, regression, or other tasks like outliers detection. Intuitively, a good separation is achieved by the hyperplane that has the largest distance to the nearest training-data point of any class (so-called functional margin), since in general the larger the margin, the lower the generalization error of the classifier.","[' What does a support-vector machine construct in a high- or infinite-dimensional space?', ' What can be used for classification, regression, or other tasks like outliers detection?', ' What is the largest distance to the nearest training-data point of any class?', ' The larger the margin, the lower the generalization error of the classifier?']","['a hyperplane or set of hyperplanes', 'a support-vector machine constructs a hyperplane or set of hyperplanes in a high- or infinite-dimensional space', 'functional margin', 'larger']"
1974,support vector machines,Definition,"Whereas the original problem may be stated in a finite-dimensional space, it often happens that the sets to discriminate are not linearly separable in that space. For this reason, it was proposed that the original finite-dimensional space be mapped into a much higher-dimensional space, presumably making the separation easier in that space. To keep the computational load reasonable, the mappings used by SVM schemes are designed to ensure that dot products of pairs of input data vectors may be computed easily in terms of the variables in the original space, by defining them in terms of a kernel function 



k
(
x
,
y
)


{\displaystyle k(x,y)}
 selected to suit the problem. The hyperplanes in the higher-dimensional space are defined as the set of points whose dot product with a vector in that space is constant, where such a set of vectors is an orthogonal (and thus minimal) set of vectors that defines a hyperplane. The vectors defining the hyperplanes can be chosen to be linear combinations with parameters 




α

i




{\displaystyle \alpha _{i}}
 of images of feature vectors 




x

i




{\displaystyle x_{i}}
 that occur in the data base. With this choice of a hyperplane, the points 



x


{\displaystyle x}
 in the feature space that are mapped into the hyperplane are defined by the relation 





∑

i



α

i


k
(

x

i


,
x
)
=

constant

.



{\displaystyle \textstyle \sum _{i}\alpha _{i}k(x_{i},x)={\text{constant}}.}
  Note that if 



k
(
x
,
y
)


{\displaystyle k(x,y)}
 becomes small as 



y


{\displaystyle y}
 grows further away from 



x


{\displaystyle x}
, each term in the sum measures the degree of closeness of the test point 



x


{\displaystyle x}
 to the corresponding data base point 




x

i




{\displaystyle x_{i}}
. In this way, the sum of kernels above can be used to measure the relative nearness of each test point to the data points originating in one or the other of the sets to be discriminated. Note the fact that the set of points 



x


{\displaystyle x}
 mapped into any hyperplane can be quite convoluted as a result, allowing much more complex discrimination between sets that are not convex at all in the original space.
","Whereas the original problem may be stated in a finite-dimensional space, it often happens that the sets to discriminate are not linearly separable in that space. For this reason, it was proposed that the original finite-dimensional space be mapped into a much higher-dimensional space, presumably making the separation easier in that space.","[' Where is the original problem stated?', ' Why was it proposed that the original finite-dimensional space be mapped into a much higher dimensional space?', ' What does a much higher-dimensional space make separation easier in?']","['finite-dimensional space', 'making the separation easier in that space', 'finite-dimensional space, it often happens that the sets to discriminate are not linearly separable in that space. For this reason, it was proposed that the original finite-dimensional space be mapped into a much higher-dimensional space, presumably making the separation easier in that space']"
1975,support vector machines,History,"The original SVM algorithm was invented by Vladimir N. Vapnik and Alexey Ya. Chervonenkis in 1963. In 1992, Bernhard Boser, Isabelle Guyon and Vladimir Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick to maximum-margin hyperplanes. The ""soft margin"" incarnation, as is commonly used software packages, was proposed by Corinna Cortes and Vapnik in 1993 and published in 1995.",The original SVM algorithm was invented by Vladimir N. Vapnik and Alexey Ya. Chervonenkis in 1963.,"[' Who invented the original SVM algorithm?', ' In what year was the original algorithm invented?']","['Vladimir N. Vapnik and Alexey Ya. Chervonenkis', '1963']"
1976,support vector machines,Linear SVM,"where the 




y

i




{\displaystyle y_{i}}
 are either 1 or −1, each indicating the class to which the point 





x


i




{\displaystyle \mathbf {x} _{i}}
 belongs. Each 





x


i




{\displaystyle \mathbf {x} _{i}}
 is a 



p


{\displaystyle p}
-dimensional real vector. We want to find the ""maximum-margin hyperplane"" that divides the group of points 





x


i




{\displaystyle \mathbf {x} _{i}}
 for which 




y

i


=
1


{\displaystyle y_{i}=1}
 from the group of points for which 




y

i


=
−
1


{\displaystyle y_{i}=-1}
, which is defined so that the distance between the hyperplane and the nearest point 





x


i




{\displaystyle \mathbf {x} _{i}}
 from either group is maximized.
","where the 




y

i




{\displaystyle y_{i}}
 are either 1 or −1, each indicating the class to which the point 





x


i




{\displaystyle \mathbf {x} _{i}}
 belongs. Each 





x


i




{\displaystyle \mathbf {x} _{i}}
 is a 



p


{\displaystyle p}
-dimensional real vector.",[' What is a p <unk>displaystyle p<unk> -dimensional real vector?'],['x\n\n\ni']
1977,support vector machines,Linear SVM,"where 




w



{\displaystyle \mathbf {w} }
 is the (not necessarily normalized) normal vector to the hyperplane. This is much like Hesse normal form, except that 




w



{\displaystyle \mathbf {w} }
 is not necessarily a unit vector. The parameter 






b

‖

w

‖






{\displaystyle {\tfrac {b}{\|\mathbf {w} \|}}}
 determines the offset of the hyperplane from the origin along the normal vector 




w



{\displaystyle \mathbf {w} }
.
","where 




w



{\displaystyle \mathbf {w} }
 is the (not necessarily normalized) normal vector to the hyperplane. This is much like Hesse normal form, except that 




w



{\displaystyle \mathbf {w} }
 is not necessarily a unit vector.","[' Where w <unk>displaystyle <unk>mathbf <unk>w<unk> <unk> is the (not necessarily normalized) normal vector to the hyperplane?', ' How is this similar to Hesse normal form?', ' What is not necessarily a unit vector?']","['w', 'w\n\n\n\n{\\displaystyle \\mathbf {w} }\n is the (not necessarily normalized) normal vector to the hyperplane', 'w\n\n\n\n{\\displaystyle \\mathbf {w} }\n is the (not necessarily normalized) normal vector to the hyperplane']"
1978,support vector machines,Nonlinear Kernels,"The original maximum-margin hyperplane algorithm proposed by Vapnik in 1963 constructed a linear classifier. However, in 1992, Bernhard Boser, Isabelle Guyon and Vladimir Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick (originally proposed by Aizerman et al.) to maximum-margin hyperplanes. The resulting algorithm is formally similar, except that every dot product is replaced by a nonlinear kernel function. This allows the algorithm to fit the maximum-margin hyperplane in a transformed feature space. The transformation may be nonlinear and the transformed space high-dimensional; although the classifier is a hyperplane in the transformed feature space, it may be nonlinear in the original input space.
","The original maximum-margin hyperplane algorithm proposed by Vapnik in 1963 constructed a linear classifier. However, in 1992, Bernhard Boser, Isabelle Guyon and Vladimir Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick (originally proposed by Aizerman et al.)","[' In what year was the original maximum-margin hyperplane algorithm proposed by Vapnik?', ' Who proposed a way to create nonlinear classifiers by applying the kernel trick?']","['1963', 'Bernhard Boser, Isabelle Guyon and Vladimir Vapnik']"
1979,support vector machines,Nonlinear Kernels,"The kernel is related to the transform 



φ
(




x

i


→



)


{\displaystyle \varphi ({\vec {x_{i}}})}
 by the equation 



k
(




x

i


→



,




x

j


→



)
=
φ
(




x

i


→



)
⋅
φ
(




x

j


→



)


{\displaystyle k({\vec {x_{i}}},{\vec {x_{j}}})=\varphi ({\vec {x_{i}}})\cdot \varphi ({\vec {x_{j}}})}
. The value w is also in the transformed space, with 







w
→



=

∑

i



α

i



y

i


φ
(




x
→




i


)



{\displaystyle \textstyle {\vec {w}}=\sum _{i}\alpha _{i}y_{i}\varphi ({\vec {x}}_{i})}
. Dot products with w for classification can again be computed by the kernel trick, i.e. 







w
→



⋅
φ
(



x
→



)
=

∑

i



α

i



y

i


k
(




x
→




i


,



x
→



)



{\displaystyle \textstyle {\vec {w}}\cdot \varphi ({\vec {x}})=\sum _{i}\alpha _{i}y_{i}k({\vec {x}}_{i},{\vec {x}})}
.
","The kernel is related to the transform 



φ
(




x

i


→



)


{\displaystyle \varphi ({\vec {x_{i}}})}
 by the equation 



k
(




x

i


→



,




x

j


→



)
=
φ
(




x

i


→



)
⋅
φ
(




x

j


→



)


{\displaystyle k({\vec {x_{i}}},{\vec {x_{j}}})=\varphi ({\vec {x_{i}}})\cdot \varphi ({\vec {x_{j}}})}
. The value w is also in the transformed space, with 







w
→



=

∑

i



α

i



y

i


φ
(




x
→




i


)



{\displaystyle \textstyle {\vec {w}}=\sum _{i}\alpha _{i}y_{i}\varphi ({\vec {x}}_{i})}
.","[' The kernel is related to the transform what?', ' The value w is also in the transformed space?']","['φ\n(\n\n\n\n\nx\n\ni\n\n\n→\n\n\n\n)\n⋅\nφ\n(\n\n\n\n\nx\n\nj\n\n\n→', 'w']"
1980,support vector machines,Computing the SVM classifier,"We focus on the soft-margin classifier since, as noted above, choosing a sufficiently small value for 



λ


{\displaystyle \lambda }
 yields the hard-margin classifier for linearly classifiable input data. The classical approach, which involves reducing (2) to a quadratic programming problem, is detailed below. Then, more recent approaches such as sub-gradient descent and coordinate descent will be discussed.
","We focus on the soft-margin classifier since, as noted above, choosing a sufficiently small value for 



λ


{\displaystyle \lambda }
 yields the hard-margin classifier for linearly classifiable input data. The classical approach, which involves reducing (2) to a quadratic programming problem, is detailed below.","[' What is the hard-margin classifier for linearly classifiable input data?', ' What approach involves reducing (2) to a quadratic programming problem?']","['soft-margin classifier', 'classical']"
1981,support vector machines,Empirical risk minimization,"The soft-margin support vector machine described above is an example of an empirical risk minimization (ERM) algorithm for the hinge loss. Seen this way, support vector machines belong to a natural class of algorithms for statistical inference, and many of its unique features are due to the behavior of the hinge loss. This perspective can provide further insight into how and why SVMs work, and allow us to better analyze their statistical properties.
","The soft-margin support vector machine described above is an example of an empirical risk minimization (ERM) algorithm for the hinge loss. Seen this way, support vector machines belong to a natural class of algorithms for statistical inference, and many of its unique features are due to the behavior of the hinge loss.","[' What is an example of an empirical risk minimization (ERM) algorithm for the hinge loss?', ' Support vector machines belong to what class of algorithms for statistical inference?', ' The unique features of the hinge are due to the behavior of what?']","['soft-margin support vector machine', 'natural', 'loss']"
1982,support vector machines,Properties,"SVMs belong to a family of generalized linear classifiers and can be interpreted as an extension of the perceptron. They can also be considered a special case of Tikhonov regularization. A special property is that they simultaneously minimize the empirical classification error and maximize the geometric margin; hence they are also known as maximum margin classifiers.
",SVMs belong to a family of generalized linear classifiers and can be interpreted as an extension of the perceptron. They can also be considered a special case of Tikhonov regularization.,"[' What type of classifiers do SVMs belong to?', ' What can be interpreted as an extension of the perceptron?']","['generalized linear classifiers', 'SVMs']"
1983,support vector machines,Implementation,"The parameters of the maximum-margin hyperplane are derived by solving the optimization. There exist several specialized algorithms for quickly solving the quadratic programming (QP) problem that arises from SVMs, mostly relying on heuristics for breaking the problem down into smaller, more manageable chunks.
","The parameters of the maximum-margin hyperplane are derived by solving the optimization. There exist several specialized algorithms for quickly solving the quadratic programming (QP) problem that arises from SVMs, mostly relying on heuristics for breaking the problem down into smaller, more manageable chunks.","[' The parameters of the maximum-margin hyperplane are derived by solving what?', ' There exist several specialized algorithms for quickly solving the quadratic programming (QP) problem that arises from SVMs.']","['the optimization', 'The parameters of the maximum-margin hyperplane are derived by solving the optimization']"
1984,support vector machines,Implementation,"Another approach is to use an interior-point method that uses Newton-like iterations to find a solution of the Karush–Kuhn–Tucker conditions of the primal and dual problems.
Instead of solving a sequence of broken-down problems, this approach directly solves the problem altogether. To avoid solving a linear system involving the large kernel matrix, a low-rank approximation to the matrix is often used in the kernel trick.
","Another approach is to use an interior-point method that uses Newton-like iterations to find a solution of the Karush–Kuhn–Tucker conditions of the primal and dual problems. Instead of solving a sequence of broken-down problems, this approach directly solves the problem altogether.","[' What method uses Newton-like iterations to find a solution of the Karush–Kuhn–Tucker conditions of the primal and dual problems?', ' Instead of solving a sequence of broken-down problems, what solves the problem altogether?']","['interior-point method', 'an interior-point method']"
1985,support vector machines,Implementation,"Another common method is Platt's sequential minimal optimization (SMO) algorithm, which breaks the problem down into 2-dimensional sub-problems that are solved analytically, eliminating the need for a numerical optimization algorithm and matrix storage. This algorithm is conceptually simple, easy to implement, generally faster, and has better scaling properties for difficult SVM problems.","Another common method is Platt's sequential minimal optimization (SMO) algorithm, which breaks the problem down into 2-dimensional sub-problems that are solved analytically, eliminating the need for a numerical optimization algorithm and matrix storage. This algorithm is conceptually simple, easy to implement, generally faster, and has better scaling properties for difficult SVM problems.","["" What is Platt's sequential minimal optimization (SMO) algorithm?"", ' What does SMO eliminate the need for?', ' How is SMO implemented?', ' What kind of problem does implement generally have better scaling properties for?']","['breaks the problem down into 2-dimensional sub-problems that are solved analytically', 'a numerical optimization algorithm and matrix storage', 'easy to implement', 'difficult SVM']"
1986,support vector machines,Implementation,"The special case of linear support-vector machines can be solved more efficiently by the same kind of algorithms used to optimize its close cousin, logistic regression; this class of algorithms includes sub-gradient descent (e.g., PEGASOS) and coordinate descent (e.g., LIBLINEAR). LIBLINEAR has some attractive training-time properties. Each convergence iteration takes time linear in the time taken to read the train data, and the iterations also have a Q-linear convergence property, making the algorithm extremely fast.
","The special case of linear support-vector machines can be solved more efficiently by the same kind of algorithms used to optimize its close cousin, logistic regression; this class of algorithms includes sub-gradient descent (e.g., PEGASOS) and coordinate descent (e.g., LIBLINEAR). LIBLINEAR has some attractive training-time properties.","[' What type of machines can be solved more efficiently by the same kind of algorithms used to optimize its close cousin, logistic regression?', ' What class of algorithms includes sub-gradient descent (e.g., PEGASOS)?']","['linear support-vector', 'logistic regression']"
1987,support vector machines,Implementation,"The general kernel SVMs can also be solved more efficiently using sub-gradient descent (e.g. P-packSVM), especially when parallelization is allowed.
","The general kernel SVMs can also be solved more efficiently using sub-gradient descent (e.g. P-packSVM), especially when parallelization is allowed.",[' What type of SVMs can be solved more efficiently using sub-gradient descent?'],['general kernel']
1988,support vector machines,Implementation,"Preprocessing of data (standardization) is highly recommended to enhance accuracy of classification. There are a few methods of standardization, such as min-max, normalization by decimal scaling, Z-score. Subtraction of mean and division by variance of each feature is usually used for SVM.","Preprocessing of data (standardization) is highly recommended to enhance accuracy of classification. There are a few methods of standardization, such as min-max, normalization by decimal scaling, Z-score.","[' What is highly recommended to enhance accuracy of classification?', ' What are a few methods of standardization?']","['Preprocessing of data', 'min-max, normalization by decimal scaling, Z-score']"
1989,execution time,Summary,"In computer science, runtime, run time, or execution time is the final phase of a computer program's life cycle, in which the code is being executed on the computer's central processing unit (CPU) as machine code. In other words, ""runtime"" is the running phase of a program.
","In computer science, runtime, run time, or execution time is the final phase of a computer program's life cycle, in which the code is being executed on the computer's central processing unit (CPU) as machine code. In other words, ""runtime"" is the running phase of a program.","["" What is the final phase of a computer program's life cycle?"", "" What is executed on the computer's central processing unit (CPU) as machine code?""]","['runtime', 'the code']"
1990,execution time,Summary,"A runtime error is detected after or during the execution (running state) of a program, whereas a compile-time error is detected by the compiler before the program is ever executed. Type checking, register allocation, code generation, and code optimization are typically done at compile time, but may be done at runtime depending on the particular language and compiler. Many other runtime errors exist and are handled differently by different programming languages, such as division by zero errors, domain errors, array subscript out of bounds errors, arithmetic underflow errors, several types of underflow and overflow errors, and many other runtime errors generally considered as software bugs which may or may not be caught and handled by any particular computer language. 
","A runtime error is detected after or during the execution (running state) of a program, whereas a compile-time error is detected by the compiler before the program is ever executed. Type checking, register allocation, code generation, and code optimization are typically done at compile time, but may be done at runtime depending on the particular language and compiler.","[' What is detected after or during the execution of a program?', ' Who detects a compile-time error before the program is executed?', ' Type checking, register allocation, code generation, and code optimization are typically done at what time?', ' What is typically done at compile time?', ' What may be done at runtime depending on the particular language and compiler?']","['runtime error', 'compiler', 'compile time', 'Type checking', 'Type checking, register allocation, code generation, and code optimization']"
1991,execution time,Implementation details,"When a program is to be executed, a loader first performs the necessary memory setup and links the program with any dynamically linked libraries it needs, and then the execution begins starting from the program's entry point. In some cases, a language or implementation will have these tasks done by the language runtime instead, though this is unusual in mainstream languages on common consumer operating systems.
","When a program is to be executed, a loader first performs the necessary memory setup and links the program with any dynamically linked libraries it needs, and then the execution begins starting from the program's entry point. In some cases, a language or implementation will have these tasks done by the language runtime instead, though this is unusual in mainstream languages on common consumer operating systems.","[' When a program is to be executed, a loader first performs what?', ' A loader links the program with any libraries it needs and then the execution begins from where?', ' What does a language or implementation do instead of the language runtime?', ' What is unusual in mainstream languages?']","['the necessary memory setup', ""the program's entry point"", 'memory setup and links the program with any dynamically linked libraries it needs', 'a language or implementation will have these tasks done by the language runtime']"
1992,execution time,Implementation details,"Some program debugging can only be performed (or is more efficient or accurate when performed) at runtime. Logic errors and array bounds checking are examples. For this reason, some programming bugs are not discovered until the program is tested in a production environment with real data, despite sophisticated compile-time checking and pre-release testing. In this case, the end-user may encounter a ""runtime error"" message. 
",Some program debugging can only be performed (or is more efficient or accurate when performed) at runtime. Logic errors and array bounds checking are examples.,"[' Some program debugging can only be performed at what time?', ' Logic errors and array bounds checking are examples of what?']","['runtime', 'program debugging']"
1993,execution time,Application errors (exceptions),"Exception handling is one language feature designed to handle runtime errors, providing a structured way to catch completely unexpected situations as well as predictable errors or unusual results without the amount of inline error checking required of languages without it. More recent advancements in runtime engines enable automated exception handling which provides ""root-cause"" debug information for every exception of interest and is implemented independent of the source code, by attaching a special software product to the runtime engine.
","Exception handling is one language feature designed to handle runtime errors, providing a structured way to catch completely unexpected situations as well as predictable errors or unusual results without the amount of inline error checking required of languages without it. More recent advancements in runtime engines enable automated exception handling which provides ""root-cause"" debug information for every exception of interest and is implemented independent of the source code, by attaching a special software product to the runtime engine.","[' What is one language feature designed to handle runtime errors?', ' Exception handling provides a structured way to catch completely unexpected situations without the amount of what?', ' More recent advancements in runtime engines enable automated exception handling?', ' What do recent advancements in runtime engines enable?', ' What provides ""root-cause"" debug information for every exception of interest?', ' How is automated exception handling implemented?']","['Exception handling', 'inline error checking', 'provides ""root-cause"" debug information for every exception of interest and is implemented independent of the source code', 'automated exception handling', 'automated exception handling', 'by attaching a special software product to the runtime engine']"
1994,rfid,Summary,"Radio-frequency identification (RFID) uses electromagnetic fields to automatically identify and track tags attached to objects. An RFID system consists of a tiny radio transponder, a radio receiver and transmitter. When triggered by an electromagnetic interrogation pulse from a nearby RFID reader device, the tag transmits digital data, usually an identifying inventory number, back to the reader. This number can be used to track inventory goods.
","Radio-frequency identification (RFID) uses electromagnetic fields to automatically identify and track tags attached to objects. An RFID system consists of a tiny radio transponder, a radio receiver and transmitter.","[' What does RFID stand for?', ' What uses electromagnetic fields to identify and track tags attached to objects?', ' An RFID system consists of what?']","['Radio-frequency identification', 'Radio-frequency identification', 'a tiny radio transponder, a radio receiver and transmitter']"
1995,rfid,Summary,"Passive tags are powered by energy from the RFID reader's interrogating radio waves. Active tags are powered by a battery and thus can be read at a greater range from the RFID reader, up to hundreds of meters.
","Passive tags are powered by energy from the RFID reader's interrogating radio waves. Active tags are powered by a battery and thus can be read at a greater range from the RFID reader, up to hundreds of meters.","[' How are passive tags powered?', ' How are active tags powered by a battery?', "" What is the range of the RFID reader's range?""]","[""energy from the RFID reader's interrogating radio waves"", ""energy from the RFID reader's interrogating radio waves"", 'up to hundreds of meters']"
1996,rfid,Summary,"Unlike a barcode, the tag does not need to be within the line of sight of the reader, so it may be embedded in the tracked object. RFID is one method of automatic identification and data capture (AIDC).","Unlike a barcode, the tag does not need to be within the line of sight of the reader, so it may be embedded in the tracked object. RFID is one method of automatic identification and data capture (AIDC).","[' What type of tag does not need to be within the line of sight of the reader?', ' What is one method of automatic identification and data capture?']","['barcode', 'RFID']"
1997,rfid,Summary,"RFID tags are used in many industries. For example, an RFID tag attached to an automobile during production can be used to track its progress through the assembly line, RFID-tagged pharmaceuticals can be tracked through warehouses, and implanting RFID microchips in livestock and pets enables positive identification of animals. Tags can also be used in shops to expedite checkout, and to prevent theft by customers and employees.
","RFID tags are used in many industries. For example, an RFID tag attached to an automobile during production can be used to track its progress through the assembly line, RFID-tagged pharmaceuticals can be tracked through warehouses, and implanting RFID microchips in livestock and pets enables positive identification of animals.","[' What is an example of an RFID tag attached to an automobile during production?', ' What can be tracked through warehouses?', ' How can RFID microchips be implanted in livestock and pets?']","['to track its progress through the assembly line', 'RFID-tagged pharmaceuticals', 'positive identification of animals']"
1998,rfid,Summary,"Since RFID tags can be attached to physical money, clothing, and possessions, or implanted in animals and people, the possibility of reading personally-linked information without consent has raised serious privacy concerns. These concerns resulted in standard specifications development addressing privacy and security issues.
","Since RFID tags can be attached to physical money, clothing, and possessions, or implanted in animals and people, the possibility of reading personally-linked information without consent has raised serious privacy concerns. These concerns resulted in standard specifications development addressing privacy and security issues.","[' What can RFID tags be attached to?', ' What has the possibility of reading personally-linked information without consent raised?']","['physical money, clothing, and possessions', 'serious privacy concerns']"
1999,rfid,Summary,"In 2014, the world RFID market was worth US$8.89 billion, up from US$7.77 billion in 2013 and US$6.96 billion in 2012. This figure includes tags, readers, and software/services for RFID cards, labels, fobs, and all other form factors. The market value is expected to rise from US$12.08 billion in 2020 to US$16.23 billion by 2029.","In 2014, the world RFID market was worth US$8.89 billion, up from US$7.77 billion in 2013 and US$6.96 billion in 2012. This figure includes tags, readers, and software/services for RFID cards, labels, fobs, and all other form factors.","[' What was the value of the world RFID market in 2014?', ' How much was the global RFID market worth in 2013?', ' In 2012, what was the market value for RFID cards, labels, fobs, and all other form factors?']","['US$8.89 billion', '$7.77 billion', 'US$6.96 billion']"
2000,rfid,History,"In 1945, Léon Theremin invented the ""Thing"", a listening device for the Soviet Union which retransmitted incident radio waves with the added audio information. Sound waves vibrated a diaphragm which slightly altered the shape of the resonator, which modulated the reflected radio frequency. Even though this device was a covert listening device, rather than an identification tag, it is considered to be a predecessor of RFID because it was passive, being energized and activated by waves from an outside source.","In 1945, Léon Theremin invented the ""Thing"", a listening device for the Soviet Union which retransmitted incident radio waves with the added audio information. Sound waves vibrated a diaphragm which slightly altered the shape of the resonator, which modulated the reflected radio frequency.","[' In what year was the ""Thing"" invented?', ' What was the listening device for the Soviet Union called?', ' How did sound waves affect the shape of the resonator?']","['1945', 'Thing', 'slightly altered the shape of the resonator, which modulated the reflected radio frequency']"
2001,rfid,History,"Similar technology, such as the Identification friend or foe transponder, was routinely used by the Allies and Germany in World War II to identify aircraft as friendly or hostile. Transponders are still used by most powered aircraft. An early work exploring RFID is the landmark 1948 paper by Harry Stockman, who predicted that ""Considerable research and development work has to be done before the remaining basic problems in reflected-power communication are solved, and before the field of useful applications is explored.""
","Similar technology, such as the Identification friend or foe transponder, was routinely used by the Allies and Germany in World War II to identify aircraft as friendly or hostile. Transponders are still used by most powered aircraft.","[' What technology was routinely used by the Allies and Germany in World War II?', ' What is still used by most powered aircraft?']","['Identification friend or foe transponder', 'Transponders']"
2002,rfid,History,"Mario Cardullo's device, patented on January 23, 1973, was the first true ancestor of modern RFID, as it was a passive radio transponder with memory. The initial device was passive, powered by the interrogating signal, and was demonstrated in 1971 to the New York Port Authority and other potential users. It consisted of a transponder with 16 bit memory for use as a toll device. The basic Cardullo patent covers the use of RF, sound and light as transmission carriers. The original business plan presented to investors in 1969 showed uses in transportation (automotive vehicle identification, automatic toll system, electronic license plate, electronic manifest, vehicle routing, vehicle performance monitoring), banking (electronic checkbook, electronic credit card), security (personnel identification, automatic gates, surveillance) and medical (identification, patient history).","Mario Cardullo's device, patented on January 23, 1973, was the first true ancestor of modern RFID, as it was a passive radio transponder with memory. The initial device was passive, powered by the interrogating signal, and was demonstrated in 1971 to the New York Port Authority and other potential users.","["" When was Mario Cardullo's device patented?"", ' What was the first true ancestor of modern RFID?', ' When was the device demonstrated to the New York Port Authority?']","['January 23, 1973', ""Mario Cardullo's device"", '1971']"
2003,rfid,History,"In 1973, an early demonstration of reflected power (modulated backscatter) RFID tags, both passive and semi-passive, was performed by Steven Depp, Alfred Koelle and Robert Frayman at the Los Alamos National Laboratory. The portable system operated at 915 MHz and used 12-bit tags. This technique is used by the majority of today's UHFID and microwave RFID tags.","In 1973, an early demonstration of reflected power (modulated backscatter) RFID tags, both passive and semi-passive, was performed by Steven Depp, Alfred Koelle and Robert Frayman at the Los Alamos National Laboratory. The portable system operated at 915 MHz and used 12-bit tags.","[' In what year did Steven Depp, Alfred Koelle, and Robert Frayman perform an early demonstration of RFID tags?', ' At what frequency did the portable system operate at?', ' What did the system use?', ' How many bits did the tags used?']","['1973', '915\xa0MHz', '12-bit tags', '12']"
2004,rfid,Design,"A radio-frequency identification system uses tags, or labels attached to the objects to be identified. Two-way radio transmitter-receivers called interrogators or readers send a signal to the tag and read its response.","A radio-frequency identification system uses tags, or labels attached to the objects to be identified. Two-way radio transmitter-receivers called interrogators or readers send a signal to the tag and read its response.","[' What does a radio-frequency identification system use?', ' What are two-way radio transmitter-receiver called?']","['tags', 'interrogators']"
2005,rfid,Uses,"RFID offers advantages over manual systems or use of barcodes. The tag can be read if passed near a reader, even if it is covered by the object or not visible. The tag can be read inside a case, carton, box or other container, and unlike barcodes, RFID tags can be read hundreds per second; barcodes can only be read one at a time using current devices. Some RFID tags, such as battery-assisted passive tags, are also able to monitor temperature and humidity.","RFID offers advantages over manual systems or use of barcodes. The tag can be read if passed near a reader, even if it is covered by the object or not visible.","[' RFID offers advantages over what?', ' RFID can be read if passed near a reader, even if it is covered by what object?']","['manual systems or use of barcodes', 'the object or not visible']"
2006,rfid,Uses,"In 2011, the cost of passive tags started at US$0.09 each; special tags, meant to be mounted on metal or withstand gamma sterilization, could cost up to US$5. Active tags for tracking containers, medical assets, or monitoring environmental conditions in data centers started at US$50 and could be over US$100 each. Battery-Assisted Passive (BAP) tags were in the US$3–10 range.","In 2011, the cost of passive tags started at US$0.09 each; special tags, meant to be mounted on metal or withstand gamma sterilization, could cost up to US$5. Active tags for tracking containers, medical assets, or monitoring environmental conditions in data centers started at US$50 and could be over US$100 each.","[' What was the cost of passive tags in 2011?', ' What could special tags cost up to?', ' How much did passive tags cost in 2011, and could be over US$100?', ' How much did data centers start at?', ' How much could a data center be worth?']","['US$0.09 each', 'US$5', '$0.09 each', 'US$50', 'over US$100']"
2007,rfid,Uses,"In 2010, three factors drove a significant increase in RFID usage: decreased cost of equipment and tags, increased performance to a reliability of 99.9%, and a stable international standard around HF and UHF passive RFID. The adoption of these standards were driven by EPCglobal, a joint venture between GS1 and GS1 US, which were responsible for driving global adoption of the barcode in the 1970s and 1980s. The EPCglobal Network was developed by the Auto-ID Center.","In 2010, three factors drove a significant increase in RFID usage: decreased cost of equipment and tags, increased performance to a reliability of 99.9%, and a stable international standard around HF and UHF passive RFID. The adoption of these standards were driven by EPCglobal, a joint venture between GS1 and GS1 US, which were responsible for driving global adoption of the barcode in the 1970s and 1980s.","[' How many factors drove a significant increase in RFID usage in 2010?', ' What was the reliability of the RFID standard?', ' Who drove the adoption of the standards?', ' Who was responsible for driving global adoption of the barcode in the 1970s and 1980s?']","['three', '99.9%,', 'EPCglobal', 'EPCglobal']"
2008,rfid,Regulation and standardization,"To avoid injuries to humans and animals, RF transmission needs to be controlled.
A number of organizations have set standards for RFID, including the International Organization for Standardization (ISO), the International Electrotechnical Commission (IEC), ASTM International, the DASH7 Alliance and EPCglobal.","To avoid injuries to humans and animals, RF transmission needs to be controlled. A number of organizations have set standards for RFID, including the International Organization for Standardization (ISO), the International Electrotechnical Commission (IEC), ASTM International, the DASH7 Alliance and EPCglobal.","[' What needs to be controlled to avoid injuries to humans and animals?', ' What organizations have set standards for RFID?', ' The International Organization for Standardization (ISO), the International Electrotechical Commission (IEC), ASTM International, the DASH7 Alliance and EPCglobal are examples of what?']","['RF transmission', 'the International Organization for Standardization (ISO), the International Electrotechnical Commission (IEC), ASTM International, the DASH7 Alliance and EPCglobal', 'organizations have set standards for RFID']"
2009,rfid,Regulation and standardization,"Every country can set its own rules for frequency allocation for RFID tags, and not all radio bands are available in all countries. These frequencies are known as the ISM bands (Industrial Scientific and Medical bands). The return signal of the tag may still cause interference for other radio users.","Every country can set its own rules for frequency allocation for RFID tags, and not all radio bands are available in all countries. These frequencies are known as the ISM bands (Industrial Scientific and Medical bands).","[' How can each country set its own rules for frequency allocation for RFID tags?', ' What are these frequencies known as?']","['Every country can set its own rules for frequency allocation for RFID tags', 'ISM bands']"
2010,rfid,Regulation and standardization,"In North America, UHF can be used unlicensed for 902–928 MHz (±13 MHz from the 915 MHz center frequency), but restrictions exist for transmission power. In Europe, RFID and other low-power radio applications are regulated by ETSI recommendations EN 300 220 and EN 302 208, and ERO recommendation 70 03, allowing RFID operation with somewhat complex band restrictions from 865–868 MHz. Readers are required to monitor a channel before transmitting (""Listen Before Talk""); this requirement has led to some restrictions on performance, the resolution of which is a subject of current research. The North American UHF standard is not accepted in France as it interferes with its military bands. On July 25, 2012, Japan changed its UHF band to 920 MHz, more closely matching the United States’ 915 MHz band, establishing an international standard environment for RFID.","In North America, UHF can be used unlicensed for 902–928 MHz (±13 MHz from the 915 MHz center frequency), but restrictions exist for transmission power. In Europe, RFID and other low-power radio applications are regulated by ETSI recommendations EN 300 220 and EN 302 208, and ERO recommendation 70 03, allowing RFID operation with somewhat complex band restrictions from 865–868 MHz.","[' Where can UHF be used unlicensed?', ' What is the transmission power limit in North America?', ' Where are RFID and other low-power radio applications regulated by ETSI recommendations?', ' What recommendation allows RFID operation with somewhat complex band restrictions from 865-868 MHz?']","['North America', '902–928\xa0MHz', 'Europe', 'ERO recommendation 70 03']"
2011,rfid,Regulation and standardization,"In order to ensure global interoperability of products, several organizations have set up additional standards for RFID testing. These standards include conformance, performance and interoperability tests.","In order to ensure global interoperability of products, several organizations have set up additional standards for RFID testing. These standards include conformance, performance and interoperability tests.","[' In order to ensure global interoperability of products, what have several organizations set up additional standards for RFID testing?']","['conformance, performance and interoperability tests']"
2012,decision tree,Summary,"A decision tree  is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.
","A decision tree  is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.","[' What is a decision support tool that uses a tree-like model of decisions and their possible consequences?', ' What is one way to display an algorithm that only contains conditional control statements?']","['A decision tree', 'A decision tree']"
2013,decision tree,Overview,"A decision tree is a flowchart-like structure in which each internal node represents a ""test"" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules.
","A decision tree is a flowchart-like structure in which each internal node represents a ""test"" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes).","[' What is a decision tree like?', ' What does each internal node represent?', ' Which branch represents the outcome of the test?']","['flowchart-like structure', 'a ""test"" on an attribute', 'each']"
2014,decision tree,Overview,"Decision trees are commonly used in operations research and operations management. If, in practice, decisions have to be taken online with no recall under incomplete knowledge, a decision tree should be paralleled by a probability model as a best choice model or online selection model algorithm. Another use of decision trees is as a descriptive means for calculating conditional probabilities.
","Decision trees are commonly used in operations research and operations management. If, in practice, decisions have to be taken online with no recall under incomplete knowledge, a decision tree should be paralleled by a probability model as a best choice model or online selection model algorithm.","[' Decision trees are commonly used in operations research and operations management.', ' What should a decision tree be paralleled by?', ' What is a best choice model?']","['Decision trees are commonly used in operations research and operations management.', 'a probability model', 'probability model']"
2015,decision tree,Association rule induction,"Decision trees can also be seen as generative models of induction rules from empirical data. An optimal decision tree is then defined as a tree that accounts for most of the data, while minimizing the number of levels (or ""questions""). Several algorithms to generate such optimal trees have been devised, such as ID3/4/5, CLS, ASSISTANT, and CART.
","Decision trees can also be seen as generative models of induction rules from empirical data. An optimal decision tree is then defined as a tree that accounts for most of the data, while minimizing the number of levels (or ""questions"").","[' Decision trees can also be seen as generative models of what?', ' An optimal decision tree is defined as a tree that accounts for most of the data while minimizing how many levels?']","['induction rules', 'number']"
2016,decision tree,Advantages and disadvantages,"Among decision support tools, decision trees (and influence diagrams) have several advantages. Decision trees:
","Among decision support tools, decision trees (and influence diagrams) have several advantages. Decision trees:",[' Decision trees and influence diagrams have several advantages compared to decision support tools.'],['decision support tools']
2017,decision tree,Optimizing Decision Tree,"A few things should be considered when improving the accuracy of the decision tree classifier. The following are some possible optimizations to consider when looking to make sure the decision tree model produced makes the correct decision or classification. Note that these things are not the only things to consider but only some.
",A few things should be considered when improving the accuracy of the decision tree classifier. The following are some possible optimizations to consider when looking to make sure the decision tree model produced makes the correct decision or classification.,"[' What should be considered when improving the accuracy of a decision tree classifier?', ' What are some possible optimizations to consider when looking to make sure the decision tree model produces the correct decision?']","['A few things should be considered when improving the accuracy of the decision tree classifier. The following are some possible optimizations', 'The following']"
2018,decision tree,Optimizing Decision Tree,"The accuracy of the decision tree can change based on the depth of the decision tree. In many cases, the tree’s leaves are pure nodes. When a node is pure it means that all the data in that node belongs to a single class. For example, if the classes in the data set are Cancer and Non-Cancer a leaf node would be considered pure when all the sample data in a leaf node is part of only one class, either cancer or non-cancer. It is important to note that a deeper tree is not always better when optimizing the decision tree. A deeper tree can influence the runtime in a negative way. If a certain classification algorithm is being used, then a deeper tree could mean the runtime of this classification algorithm being significantly slower. There is also the possibility that the actual algorithm building the decision tree will get significantly slower as the tree gets deeper. If the tree-building algorithm being used splits pure nodes, then a decrease in the overall accuracy of the tree classifier could be experienced. Occasionally, going deeper in the tree can cause an accuracy decrease in general, so it is very important to test modifying the depth of the decision tree and selecting the depth that produces the best results. To summarize observe the points below, we will define the number D as the depth of the tree.
","The accuracy of the decision tree can change based on the depth of the decision tree. In many cases, the tree’s leaves are pure nodes.","[' The accuracy of the decision tree can change based on the depth of what?', ' In many cases, what are the leaves of a tree?']","['decision tree', 'pure nodes']"
2019,decision tree,Optimizing Decision Tree,"
The ability to test the differences in classification results when changing D is imperative. We must be able to easily change and test the variables that could affect the accuracy and reliability of the decision tree-model.
","
The ability to test the differences in classification results when changing D is imperative. We must be able to easily change and test the variables that could affect the accuracy and reliability of the decision tree-model.","[' The ability to test the differences in classification results when changing D is what?', ' What must we be able to easily change and test?']","['imperative', 'variables that could affect the accuracy and reliability of the decision tree-model']"
2020,decision tree,Optimizing Decision Tree,"The node splitting function used can have an impact on improving the accuracy of the decision tree. For example, using the Information gain function may yield better results than using the phi function. The phi function is known as a measure of “goodness” of a candidate split  at a node in the decision tree. The information gain function is known as measure of the “reduction in entropy”. In the following we will build two decision trees. One decision tree will be built using the phi function to split the nodes and one decision tree will be built using the information gain function to split the nodes.
","The node splitting function used can have an impact on improving the accuracy of the decision tree. For example, using the Information gain function may yield better results than using the phi function.","[' What function can have an impact on the accuracy of the decision tree?', ' What function may yield better results than using the phi function?']","['node splitting', 'Information gain function']"
2021,decision tree,Optimizing Decision Tree,"This is the information gain function formula. The formula states the information gain is a function of the entropy of a node of the decision tree minus the entropy of a candidate split at node t of a decision tree. 
",This is the information gain function formula. The formula states the information gain is a function of the entropy of a node of the decision tree minus the entropy of a candidate split at node t of a decision tree.,"[' What is the information gain function formula?', ' What does the formula state?', ' The information gain is a function of what?']","['a function of the entropy of a node of the decision tree minus the entropy of a candidate split at node t of a decision tree', 'the information gain is a function of the entropy of a node of the decision tree minus the entropy of a candidate split at node t of a decision tree', 'the entropy of a node of the decision tree']"
2022,decision tree,Optimizing Decision Tree,"The is the phi function formula. The phi function is maximized when the feature chosen splits the samples in a way that produces homogenous splits and have around the same number of samples in each split.
",The is the phi function formula. The phi function is maximized when the feature chosen splits the samples in a way that produces homogenous splits and have around the same number of samples in each split.,"[' What is the phi function formula?', ' What is maximized when the feature chosen splits the samples in a way that produces homogenous splits?', ' How many samples are in each split?']","['maximized when the feature chosen splits the samples in a way that produces homogenous splits and have around the same number of samples in each split', 'The phi function', 'around the same number']"
2023,decision tree,Optimizing Decision Tree,"We will set D ,which is the depth of the decision tree we are building, to three (D = 3). We also have the following data set of Cancer and Non-Cancer samples and the mutation features that the samples either have or do not have. If a sample has a feature mutation then the sample is positive for that mutation, will be represented by one. If a sample does not have a feature mutation then the sample is negative for that mutation, will be represented as zero.   
","We will set D ,which is the depth of the decision tree we are building, to three (D = 3). We also have the following data set of Cancer and Non-Cancer samples and the mutation features that the samples either have or do not have.","[' What is the depth of the decision tree we are building?', ' How many samples do we have?']","['D ,which is the depth of the decision tree we are building, to three', 'three']"
2024,decision tree,Optimizing Decision Tree,"To summarize, C stands for Cancer and NC stands for Non-Cancer. The letter M stands for mutation ,and if a sample has a particular mutation it will show up in the table as a one and otherwise zero.
","To summarize, C stands for Cancer and NC stands for Non-Cancer. The letter M stands for mutation ,and if a sample has a particular mutation it will show up in the table as a one and otherwise zero.","[' What stands for Cancer and NC stands for Non-Cancer?', ' What does the letter M stand for?']","['C', 'mutation']"
2025,decision tree,Optimizing Decision Tree,"
Now, we can use the formulas to calculate the phi function values and information gain values for each M in the dataset. Once all the values are calculated the tree can be produced. The first thing to be done is to select the root node. In information gain and the phi function we consider the optimal split to be the mutation that produces the highest value for information gain or the phi function. Now assume that M1  has the highest phi function value and M4 has the highest information gain value. The M1 mutation will be the root of our phi function tree and M4 will be the root of our information gain tree. You can observe the root nodes below 
","
Now, we can use the formulas to calculate the phi function values and information gain values for each M in the dataset. Once all the values are calculated the tree can be produced.",[' What can be used to calculate the phi function values and information gain values for each M in the dataset?'],['formulas']
2026,decision tree,Optimizing Decision Tree,"
Now, once we have chosen the root node we can split the samples into two groups based on whether a sample is positive or negative for the root node mutation. The groups will be called group A and group B. For example, if we use M1 to split the samples in the root node we get NC2 and C2 samples in group A and the rest of the samples NC4, NC3, NC1, C1 in group B.
","
Now, once we have chosen the root node we can split the samples into two groups based on whether a sample is positive or negative for the root node mutation. The groups will be called group A and group B.","[' How many groups can a sample be split into?', ' What are the groups called?']","['two', 'group A and group B']"
2027,decision tree,Optimizing Decision Tree,"Disregarding  the mutation chosen for the root node,  proceed to place the next best features that have the highest values for information gain or the phi function in the left or right child nodes of the decision tree. Once we choose the root node and the two child nodes for the tree of depth = 3 we can just add the leaves. The leaves will represent the final classification decision the model has produced based on the mutations a sample either has or does not have. The left tree is the decision tree we obtain from using information gain to split the nodes and the right tree is what we obtain from using the phi function to split the nodes.
","Disregarding  the mutation chosen for the root node,  proceed to place the next best features that have the highest values for information gain or the phi function in the left or right child nodes of the decision tree. Once we choose the root node and the two child nodes for the tree of depth = 3 we can just add the leaves.","[' Where do we place the next best features that have the highest values for information gain or phi function?', ' How many child nodes are there in the tree of depth = 3?']","['in the left or right child nodes of the decision tree', 'two']"
2028,decision tree,Optimizing Decision Tree,"The tree using information gain has  the same results when using the phi function when calculating the accuracy. When we classify the samples based on the model using information gain we get one true positive, one false positive, zero false negatives, and four true negatives. For the model using the phi function we get two true positives, zero false positives, one false negative, and three true negatives. The next step is to evaluate the effectiveness of the decision tree using some key metrics that will be discussed in the Evaluating a Decision Tree section below. The metrics that will be discussed below can help determine the next steps to be taken when optimizing the decision tree.
","The tree using information gain has  the same results when using the phi function when calculating the accuracy. When we classify the samples based on the model using information gain we get one true positive, one false positive, zero false negatives, and four true negatives.","[' The tree using information gain has the same results when using what function when calculating the accuracy?', ' When we classify samples based on the model using what?']","['phi', 'information gain']"
2029,decision tree,Optimizing Decision Tree,"The above information is not where it ends for building and optimizing a decision tree. There are many techniques for improving the decision tree classification models we build. One of the techniques is making our decision tree model from a bootstrapped dataset. The bootstrapped dataset helps remove the bias that occurs when building a decision tree model with the same data the model is tested with. The ability to leverage the power of random forests can also help significantly improve the overall accuracy of the model being built. This method generates many decisions from many decision trees and tallies up the votes from each decision tree to make the final classification. There are many techniques, but the main objective is to test building your decision tree model in different ways to make sure it reaches the highest performance level possible.
",The above information is not where it ends for building and optimizing a decision tree. There are many techniques for improving the decision tree classification models we build.,"[' What is not where it ends for building and optimizing a decision tree?', ' There are many techniques for improving what?']","['The above information', 'decision tree classification models we build']"
2030,decision tree,Evaluating a Decision Tree,"           It is important to know the measurements used to evaluate decision trees. The main metrics used are accuracy, sensitivity, specificity, precision, miss rate, false discovery rate, and false omission rate. All these measurements are derived from the number of true positives, false positives, true negatives, and false negatives obtained when running a set of samples through the decision tree classification model. Also, a confusion matrix can be made to display these results. All these main metrics tell something different about the strengths and weaknesses of the classification model built based on your decision tree. For example, A low sensitivity with high specificity could indicate the classification model built from the decision tree does not do well identifying cancer samples over non-cancer samples.
","           It is important to know the measurements used to evaluate decision trees. The main metrics used are accuracy, sensitivity, specificity, precision, miss rate, false discovery rate, and false omission rate.",[' What are the main metrics used to evaluate decision trees?'],"['accuracy, sensitivity, specificity, precision, miss rate, false discovery rate, and false omission rate']"
2031,decision tree,Evaluating a Decision Tree,"
Let us take the confusion matrix below. The confusion matrix shows us the decision tree model classifier built gave 11 true positives, 1 false positive, 45 false negatives, and 105 true negatives.
","
Let us take the confusion matrix below. The confusion matrix shows us the decision tree model classifier built gave 11 true positives, 1 false positive, 45 false negatives, and 105 true negatives.","[' What does the confusion matrix show?', ' How many false positives did the decision tree model classifier give?']","['the decision tree model classifier built gave 11 true positives, 1 false positive, 45 false negatives, and 105 true negatives', '1']"
2032,decision tree,Evaluating a Decision Tree,"Once we have calculated the key metrics we can make some initial conclusions on the performance of the decision tree model built. The accuracy that we calculated was 71.60%. The accuracy value is good to start but we would like to get our models as accurate as possible while maintaining the overall performance. The sensitivity value of 19.64% means that out of everyone who was actually positive for Cancer tested positive. If we look at the specificity value of 99.06% we know that out of all the samples that were negative for cancer actually tested negative. When it comes to sensitivity and specificity it is important to have a balance between the two values ,so if we can decrease our specificity to increase the sensitivity that would prove to be beneficial. These are just a few examples on how to use these values and the meanings behind them to evaluate the decision tree model and improve upon the next iteration.
",Once we have calculated the key metrics we can make some initial conclusions on the performance of the decision tree model built. The accuracy that we calculated was 71.60%.,[' What was the accuracy of the decision tree model calculated?'],['71.60%.']
2033,accessibility,Summary,"Accessibility is the design of products, devices, services, vehicles, or environments so as to be usable by people with disabilities. The concept of accessible design and practice of accessible development ensures both ""direct access"" (i.e. unassisted) and ""indirect access"" meaning compatibility with a person's assistive technology (for example, computer screen readers).
","Accessibility is the design of products, devices, services, vehicles, or environments so as to be usable by people with disabilities. The concept of accessible design and practice of accessible development ensures both ""direct access"" (i.e.","[' What is the design of products, devices, services, vehicles, or environments so as to be usable by people with disabilities?', ' The concept of accessible design and practice of accessible development ensures both what?']","['Accessibility', 'direct access']"
2034,accessibility,Summary,"Accessibility can be viewed as the ""ability to access"" and benefit from some system or entity. The concept focuses on enabling access for people with disabilities, or enabling access through the use of assistive technology; however, research and development in accessibility brings benefits to everyone.","Accessibility can be viewed as the ""ability to access"" and benefit from some system or entity. The concept focuses on enabling access for people with disabilities, or enabling access through the use of assistive technology; however, research and development in accessibility brings benefits to everyone.","[' What can be viewed as the ""ability to access"" and benefit from some system or entity?', ' What focuses on enabling access for people with disabilities?', ' Research and development in what brings benefits to everyone?']","['Accessibility', 'Accessibility', 'accessibility']"
2035,accessibility,Summary,"Accessibility is strongly related to universal design which is the process of creating products that are usable by people with the widest possible range of abilities, operating within the widest possible range of situations. This is about making things accessible to all people (whether they have a disability or not).
","Accessibility is strongly related to universal design which is the process of creating products that are usable by people with the widest possible range of abilities, operating within the widest possible range of situations. This is about making things accessible to all people (whether they have a disability or not).","[' What is accessibility strongly related to?', ' What is the process of creating products that are usable by people with the widest possible range of abilities?']","['universal design', 'universal design']"
2036,accessibility,Legislation,"The disability rights movement advocates equal access to social, political, and economic life which includes not only physical access but access to the same tools, services, organizations and facilities as non-disabled people (e.g., museums). Article 9 of the United Nations Convention on the Rights of Persons with Disabilities commits signatories to provide for full accessibility in their countries.","The disability rights movement advocates equal access to social, political, and economic life which includes not only physical access but access to the same tools, services, organizations and facilities as non-disabled people (e.g., museums). Article 9 of the United Nations Convention on the Rights of Persons with Disabilities commits signatories to provide for full accessibility in their countries.","[' What does the disability rights movement advocate?', ' What does Article 9 of the United Nations Convention on the Rights of Persons with Disabilities commit signatories to?', ' Convention on the Rights of Persons with Disabilities commits signatories to what?']","['equal access to social, political, and economic life', 'provide for full accessibility in their countries', 'provide for full accessibility in their countries']"
2037,accessibility,Legislation,"While it is often used to describe facilities or amenities to assist people with impaired mobility, through the provision of facilities like wheelchair ramps, the term can extend include other types of disability. Accessible facilities therefore extend to areas such as Braille signage, elevators, audio signals at pedestrian crossings, walkway contours, website accessibility and accessible publishing.","While it is often used to describe facilities or amenities to assist people with impaired mobility, through the provision of facilities like wheelchair ramps, the term can extend include other types of disability. Accessible facilities therefore extend to areas such as Braille signage, elevators, audio signals at pedestrian crossings, walkway contours, website accessibility and accessible publishing.","[' What does the term accessibility refer to?', ' What is an example of a facility that can be used to assist people with disabilities?', ' What type of signage is used at pedestrian crossings?', ' What kind of signage does Braille use?']","['facilities or amenities to assist people with impaired mobility', 'wheelchair ramps', 'audio signals', 'audio signals']"
2038,accessibility,Assistive technology and adaptive technology,"Assistive technology is the creation of a new device that assists a person in completing a task that would otherwise be impossible. Some examples include new computer software programs like screen readers, and inventions such as assistive listening devices, including hearing aids, and traffic lights with a standard color code that enables colorblind individuals to understand the correct signal.
","Assistive technology is the creation of a new device that assists a person in completing a task that would otherwise be impossible. Some examples include new computer software programs like screen readers, and inventions such as assistive listening devices, including hearing aids, and traffic lights with a standard color code that enables colorblind individuals to understand the correct signal.","[' Assistive technology is the creation of a new device that assists a person in completing a task that would otherwise be impossible?', ' What are some examples of new computer software programs?', ' Hearing aids and traffic lights with a standard color code are examples of what?', ' What is a standard color code that enables colorblind people to understand the correct signal?']","['Assistive technology is the creation of a new device that assists a person in completing a task that would otherwise be impossible.', 'screen readers', 'assistive listening devices', 'traffic lights']"
2039,accessibility,Assistive technology and adaptive technology,"Adaptive technology is the modification, or adaptation, of existing devices, methods, or the creation of new uses for existing devices, to enable a person to complete a task. Examples include the use of remote controls, and the autocomplete (word completion) feature in computer word processing programs, which both help individuals with mobility impairments to complete tasks. Adaptations to wheelchair tires are another example; widening the tires enables wheelchair users to move over soft surfaces, such as deep snow on ski hills, and sandy beaches.
","Adaptive technology is the modification, or adaptation, of existing devices, methods, or the creation of new uses for existing devices, to enable a person to complete a task. Examples include the use of remote controls, and the autocomplete (word completion) feature in computer word processing programs, which both help individuals with mobility impairments to complete tasks.","[' What is adaptive technology?', ' What does autocomplete stand for?', ' What do word processing programs help people with mobility impairments do?']","['the modification, or adaptation, of existing devices, methods, or the creation of new uses for existing devices', 'word completion', 'complete tasks']"
2040,accessibility,Assistive technology and adaptive technology,"Assistive technology and adaptive technology have a key role in developing the means for people with disabilities to live more independently, and to more fully participate in mainstream society. In order to have access to assistive or adaptive technology, however, educating the public and even legislating requirements to incorporate this technology have been necessary.
","Assistive technology and adaptive technology have a key role in developing the means for people with disabilities to live more independently, and to more fully participate in mainstream society. In order to have access to assistive or adaptive technology, however, educating the public and even legislating requirements to incorporate this technology have been necessary.","[' Assistive technology and adaptive technology have a key role in developing the means for people with disabilities to live more independently and to more fully participate in what society?', ' What has been necessary to educate the public?', ' What have been required to incorporate technology?']","['mainstream', 'legislating requirements', 'legislating']"
2041,accessibility,Employment,"Employment rates for workers with disabilities are lower than for the general workforce. Workers in Western countries fare relatively well, having access to more services and training as well as legal protections against employment discrimination. Despite this, in the United States the 2012 unemployment rate for workers with disabilities was 12.9%, while it was 7.3% for workers without disabilities. More than half of workers with disabilities (52%) earned less than $25,000 in the previous year, compared with just 38% of workers with no disabilities. This translates into an earnings gap where individuals with disabilities earn about 25 percent less of what workers without disabilities earn. Among occupations with 100,000 or more people, dishwashers had the highest disability rate (14.3%), followed by refuse and recyclable material collectors (12.7%), personal care aides (11.9%), and janitors and building cleaners (11.8%). The rates for refuse and recyclable material collectors, personal care aides, and janitors and building cleaners were not statistically different from one another.","Employment rates for workers with disabilities are lower than for the general workforce. Workers in Western countries fare relatively well, having access to more services and training as well as legal protections against employment discrimination.","[' What are the employment rates for workers with disabilities lower than for the general workforce?', ' Workers in Western countries have access to more services and what?']","['Employment rates for workers with disabilities are lower', 'training']"
2042,accessibility,Employment,"Surveys of non-Western countries are limited, but the available statistics also indicate fewer jobs being filled by workers with disabilities. In India, a large 1999 survey found that ""of the 'top 100 multinational companies' in the country [...] the employment rate of persons with disabilities in the private sector was a mere 0.28%, 0.05% in multinational companies and only 0.58% in the top 100 IT companies in the country"". India, like much of the world, has large sections of the economy that are without strong regulation or social protections, such as the informal economy. Other factors have been cited as contributing to the high unemployment rate, such as public service regulations. Although employment for workers with disabilities is higher in the public sector due to hiring programs targeting persons with disabilities, regulations currently restrict types of work available to persons with disabilities:
""Disability-specific employment reservations are limited to the public sector and a large number of the reserved positions continue to be vacant despite nearly two decades of enactment of the PWD Act"".","Surveys of non-Western countries are limited, but the available statistics also indicate fewer jobs being filled by workers with disabilities. In India, a large 1999 survey found that ""of the 'top 100 multinational companies' in the country [...] the employment rate of persons with disabilities in the private sector was a mere 0.28%, 0.05% in multinational companies and only 0.58% in the top 100 IT companies in the country"".","[' In India, in 1999, what was the employment rate of persons with disabilities in the private sector?', ' What is limited in non-Western countries?', ' What was the rate of persons with disabilities in the private sector?', ' What percentage of people with disabilities were in the top 100 IT companies?']","['0.28%,', 'Surveys', '0.28%,', '0.58%']"
2043,accessibility,Housing,"Most existing and new housing, even in the wealthiest nations, lack basic accessibility features unless the designated, immediate occupant of a home currently has a disability. However, there are some initiatives to change typical residential practices so that new homes incorporate basic access features such as zero-step entries and door widths adequate for wheelchairs to pass through. Occupational Therapists are a professional group skilled in the assessment and making of recommendations to improve access to homes. They are involved in both the adaptation of existing housing to improve accessibility, and in the design of future housing.","Most existing and new housing, even in the wealthiest nations, lack basic accessibility features unless the designated, immediate occupant of a home currently has a disability. However, there are some initiatives to change typical residential practices so that new homes incorporate basic access features such as zero-step entries and door widths adequate for wheelchairs to pass through.","[' What does most existing and new housing lack?', ' What are some initiatives to change typical residential practices?', ' What are some basic access features?', ' What are door widths adequate for?']","['basic accessibility features', 'zero-step entries and door widths adequate for wheelchairs to pass through', 'zero-step entries and door widths adequate for wheelchairs to pass through', 'wheelchairs to pass through']"
2044,accessibility,Housing,"The broad concept of Universal design is relevant to housing, as it is to all aspects of the built environment. Furthermore, a Visitability movement begun by grass roots disability advocates in the 1980s focuses specifically on changing construction practices in new housing. This movement, a network of interested people working in their locales, works on educating, passing laws, and spurring voluntary home access initiatives with the intention that basic access become a routine part of new home construction.
","The broad concept of Universal design is relevant to housing, as it is to all aspects of the built environment. Furthermore, a Visitability movement begun by grass roots disability advocates in the 1980s focuses specifically on changing construction practices in new housing.","[' What concept is relevant to housing as it is to all aspects of the built environment?', ' What movement started by grass roots disability advocates in the 1980s focuses specifically on changing construction practices in new housing?']","['Universal design', 'Visitability']"
2045,accessibility,Voting,"Under the Convention on the Rights of Persons with Disabilities, states parties are bound to assure accessible elections, voting, and voting procedures. In 2018, the United Nations Committee on the Rights of Persons with Disabilities issued an opinion that all polling stations should be fully accessible. At the European Court of Human Rights, there are currently two ongoing cases about the accessibility of polling places and voting procedures. They were brought against Slovenia by two voters and the Slovenian Disability Rights Association. As of January 2020, the case, called Toplak and Mrak v. Slovenia, was ongoing. The aim of the court procedure is to make accessible all polling places in Europe.","Under the Convention on the Rights of Persons with Disabilities, states parties are bound to assure accessible elections, voting, and voting procedures. In 2018, the United Nations Committee on the Rights of Persons with Disabilities issued an opinion that all polling stations should be fully accessible.","[' Under what convention are states parties bound to assure accessible elections, voting, and voting procedures?', ' In 2018, the United Nations Committee on the Rights of Persons with Disabilities issued an opinion that all polling stations should be fully accessible?']","['Convention on the Rights of Persons with Disabilities', 'Convention on the Rights of Persons with Disabilities']"
2046,accessibility,"Disability, information technology (IT) and telecommunications","Advances in information technology and telecommunications have represented a leap forward for accessibility. Access to the technology is restricted to those who can afford it, but it has become more widespread in Western countries in recent years. For those who use it, it provides the ability to access information and services by minimizing the barriers of distance and cost as well as the accessibility and usability of the interface. In many countries this has led to initiatives, laws and/or regulations that aim toward providing universal access to the internet and to phone systems at reasonable cost to citizens.","Advances in information technology and telecommunications have represented a leap forward for accessibility. Access to the technology is restricted to those who can afford it, but it has become more widespread in Western countries in recent years.","[' Advances in information technology and telecommunications have represented a leap forward for what?', ' Access to technology is restricted to those who can afford it, but it has become more widespread in what countries in recent years?']","['accessibility', 'Western countries']"
2047,accessibility,"Disability, information technology (IT) and telecommunications","A major advantage of advanced technology is its flexibility. Some technologies can be used at home, in the workplace, and in school, expanding the ability of the user to participate in various spheres of daily life. Augmentative and alternative communication technology is one such area of IT progress. It includes inventions such as speech-generating devices, teletypewriter devices, adaptive pointing devices to replace computer mouse devices, and many others. Mobile telecommunications devices and computer applications are also equipped with accessibility features. They can be adapted to create accessibility to a range of tasks, and may be suitable for different kinds of disability.
","A major advantage of advanced technology is its flexibility. Some technologies can be used at home, in the workplace, and in school, expanding the ability of the user to participate in various spheres of daily life.","[' What is a major advantage of advanced technology?', ' What can some technologies be used at home, in the workplace, and in school?']","['its flexibility', 'expanding the ability of the user to participate in various spheres of daily life']"
2048,accessibility,Education and accessibility for students,"Equal access to education for students with disabilities is supported in some countries by legislation. It is still challenging for some students with disabilities to fully participate in mainstream education settings, but many adaptive technologies and assistive programs are making improvements. In India, the Medical Council of India has now passed the directives to all the medical institutions to make them accessible to persons with disabilities. This happened due to a petition by Dr Satendra Singh founder of Infinite Ability.","Equal access to education for students with disabilities is supported in some countries by legislation. It is still challenging for some students with disabilities to fully participate in mainstream education settings, but many adaptive technologies and assistive programs are making improvements.","[' What is supported in some countries by legislation?', ' What are making improvements for students with disabilities?']","['Equal access to education for students with disabilities', 'adaptive technologies and assistive programs']"
2049,accessibility,Education and accessibility for students,"Students with a physical or mental impairment or learning disability may require note-taking assistance, which may be provided by a business offering such services, as with tutoring services. Talking books in the form of talking textbooks are available in Canadian secondary and post-secondary schools. Also, students may require adaptive technology to access computers and the Internet. These may be tax-exempt expenses in some jurisdictions with a medical prescription.
","Students with a physical or mental impairment or learning disability may require note-taking assistance, which may be provided by a business offering such services, as with tutoring services. Talking books in the form of talking textbooks are available in Canadian secondary and post-secondary schools.","[' What can students with a physical or mental impairment or learning disability need?', ' What type of books are available in Canadian secondary and post-secondary schools?']","['note-taking assistance', 'Talking books']"
2050,bipartite graph,Summary,"In the mathematical field of graph theory, a bipartite graph (or bigraph) is a graph whose vertices can be divided into two disjoint and independent sets 



U


{\displaystyle U}
 and 



V


{\displaystyle V}
 such that every edge connects a vertex in 



U


{\displaystyle U}
 to one in 



V


{\displaystyle V}
. Vertex sets 



U


{\displaystyle U}
 and 



V


{\displaystyle V}
 are usually called the parts of the graph. Equivalently, a bipartite graph is a graph that does not contain any odd-length cycles.","In the mathematical field of graph theory, a bipartite graph (or bigraph) is a graph whose vertices can be divided into two disjoint and independent sets 



U


{\displaystyle U}
 and 



V


{\displaystyle V}
 such that every edge connects a vertex in 



U


{\displaystyle U}
 to one in 



V


{\displaystyle V}
. Vertex sets 



U


{\displaystyle U}
 and 



V


{\displaystyle V}
 are usually called the parts of the graph.","[' What is the mathematical field of graph theory?', ' What is a graph whose vertices can be divided into two disjoint and independent sets?', ' Every edge connects a vertex in U <unk>displaystyle U<unk> to what?', ' What are Vertex sets U <unk>displaystyle U<unk> and V <unk>Displaystyle V<unk> usually called?']","['bipartite graph', 'bipartite graph', 'one in \n\n\n\nV\n\n\n{\\displaystyle V}', 'the parts of the graph']"
2051,bipartite graph,Summary,"The two sets 



U


{\displaystyle U}
 and 



V


{\displaystyle V}
 may be thought of as a coloring of the graph with two colors: if one colors all nodes in 



U


{\displaystyle U}
 blue, and all nodes in 



V


{\displaystyle V}
 green, each edge has endpoints of differing colors, as is required in the graph coloring problem. In contrast, such a coloring is impossible in the case of a non-bipartite graph, such as a triangle: after one node is colored blue and another green, the third vertex of the triangle is connected to vertices of both colors, preventing it from being assigned either color.
","The two sets 



U


{\displaystyle U}
 and 



V


{\displaystyle V}
 may be thought of as a coloring of the graph with two colors: if one colors all nodes in 



U


{\displaystyle U}
 blue, and all nodes in 



V


{\displaystyle V}
 green, each edge has endpoints of differing colors, as is required in the graph coloring problem. In contrast, such a coloring is impossible in the case of a non-bipartite graph, such as a triangle: after one node is colored blue and another green, the third vertex of the triangle is connected to vertices of both colors, preventing it from being assigned either color.","[' What are the two sets U <unk>displaystyle U<unk> and V <unk>Displaystyle V<unk> thought of as?', ' How many colors does each edge have?', ' What is required in the graph?', ' What is required in the graph coloring problem?', ' What is impossible in the case of a non-bipartite graph?', ' Which vertex of the triangle is connected to vertices of both colors?', ' What prevents it from being assigned either color?']","['a coloring of the graph with two colors', 'differing colors', 'each edge has endpoints of differing colors', 'each edge has endpoints of differing colors', 'a triangle', 'third', 'the third vertex of the triangle is connected to vertices of both colors']"
2052,bipartite graph,Summary,"One often writes 



G
=
(
U
,
V
,
E
)


{\displaystyle G=(U,V,E)}
 to denote a bipartite graph whose partition has the parts 



U


{\displaystyle U}
 and 



V


{\displaystyle V}
, with 



E


{\displaystyle E}
 denoting the edges of the graph. If a bipartite graph is not connected, it may have more than one bipartition; in this case, the 



(
U
,
V
,
E
)


{\displaystyle (U,V,E)}
 notation is helpful in specifying one particular bipartition that may be of importance in an application.  If 




|

U

|

=

|

V

|



{\displaystyle |U|=|V|}
, that is, if the two subsets have equal cardinality, then 



G


{\displaystyle G}
 is called a balanced bipartite graph. If all vertices on the same side of the bipartition have the same degree, then 



G


{\displaystyle G}
 is called biregular.
","One often writes 



G
=
(
U
,
V
,
E
)


{\displaystyle G=(U,V,E)}
 to denote a bipartite graph whose partition has the parts 



U


{\displaystyle U}
 and 



V


{\displaystyle V}
, with 



E


{\displaystyle E}
 denoting the edges of the graph. If a bipartite graph is not connected, it may have more than one bipartition; in this case, the 



(
U
,
V
,
E
)


{\displaystyle (U,V,E)}
 notation is helpful in specifying one particular bipartition that may be of importance in an application.","[' What does one often write to denote a bipartite graph?', ' What does E <unk>displaystyle E<unk> denote?', ' What is helpful in specifying one particular bipartition that may be of importance in an application?']","['G\n=\n(\nU\n,\nV\n,\nE\n)\n\n\n{\\displaystyle G=(U,V,E)}', 'the edges of the graph', '(\nU\n,\nV\n,\nE\n)\n\n\n{\\displaystyle (U,V,E)}\n notation']"
2053,bipartite graph,Examples,"When modelling relations between two different classes of objects, bipartite graphs very often arise naturally. For instance, a graph of football players and clubs, with an edge between a player and a club if the player has played for that club, is a natural example of an affiliation network, a type of bipartite graph used in social network analysis.","When modelling relations between two different classes of objects, bipartite graphs very often arise naturally. For instance, a graph of football players and clubs, with an edge between a player and a club if the player has played for that club, is a natural example of an affiliation network, a type of bipartite graph used in social network analysis.","[' When modelling relations between two different classes of objects, bipartite graphs arise naturally what?', ' A graph of football players and clubs with an edge between a player and a club if the player has played for that club is a natural example of an affiliation network?', ' What is a natural example of an affiliation network?', ' What is club a type of?']","['naturally. For instance, a graph of football players and clubs, with an edge between a player and a club if the player has played for that club, is a natural example of an affiliation network', 'bipartite graphs', 'a graph of football players and clubs', 'affiliation network']"
2054,bipartite graph,Examples,"Another example where bipartite graphs appear naturally is in the (NP-complete) railway optimization problem, in which the input is a schedule of trains and their stops, and the goal is to find a set of train stations as small as possible such that every train visits at least one of the chosen stations. This problem can be modeled as a dominating set problem in a bipartite graph that has a vertex for each train and each station and an edge for each pair of a station and a train that stops at that station.","Another example where bipartite graphs appear naturally is in the (NP-complete) railway optimization problem, in which the input is a schedule of trains and their stops, and the goal is to find a set of train stations as small as possible such that every train visits at least one of the chosen stations. This problem can be modeled as a dominating set problem in a bipartite graph that has a vertex for each train and each station and an edge for each pair of a station and a train that stops at that station.","[' What is an example of a problem where bipartite graphs appear naturally?', ' What is the input to the railway optimization problem?', ' The goal is to find a set of train stations as small as possible.', ' What is possible such that every train visits at least one of the chosen stations?', ' What can be modeled as a dominating set problem in a bipartite graph?', ' For each pair of a station and a train that stops at that station?']","['railway optimization problem', 'a schedule of trains and their stops', 'railway optimization problem', 'a set of train stations as small', 'railway optimization problem', 'an edge']"
2055,bipartite graph,Examples,A third example is in the academic field of numismatics. Ancient coins are made using two positive impressions of the design (the obverse and reverse). The charts numismatists produce to represent the production of coins are bipartite graphs.,A third example is in the academic field of numismatics. Ancient coins are made using two positive impressions of the design (the obverse and reverse).,"[' What is the third field of study in numismatics?', ' Ancient coins are made using how many positive impressions of the design?']","['academic', 'two']"
2056,bipartite graph,Additional applications,"Bipartite graphs are extensively used in modern coding theory, especially to decode codewords received from the channel. Factor graphs and Tanner graphs are examples of this. A Tanner graph is a bipartite graph in which the vertices on one side of the bipartition represent digits of a codeword, and the vertices on the other side represent combinations of digits that are expected to sum to zero in a codeword without errors. A factor graph is a closely related belief network used for probabilistic decoding of LDPC and turbo codes.","Bipartite graphs are extensively used in modern coding theory, especially to decode codewords received from the channel. Factor graphs and Tanner graphs are examples of this.","[' What is extensively used in modern coding theory?', ' What are examples of bipartite graphs?']","['Bipartite graphs', 'Factor graphs and Tanner graphs']"
2057,bipartite graph,Additional applications,"In computer science, a Petri net is a mathematical modeling tool used in analysis and simulations of concurrent systems. A system is modeled as a bipartite directed graph with two sets of nodes:  A set of ""place"" nodes that contain resources, and a set of ""event"" nodes which generate and/or consume resources.  There are additional constraints on the nodes and edges that constrain the behavior of the system.  Petri nets utilize the properties of bipartite directed graphs and other properties to allow mathematical proofs of the behavior of systems while also allowing easy implementation of simulations of the system.","In computer science, a Petri net is a mathematical modeling tool used in analysis and simulations of concurrent systems. A system is modeled as a bipartite directed graph with two sets of nodes:  A set of ""place"" nodes that contain resources, and a set of ""event"" nodes which generate and/or consume resources.","[' What is a mathematical modeling tool used in computer science?', ' A system is modeled as a bipartite directed graph with how many sets of nodes?', ' What are the set of ""place"" nodes that contain resources?', ' What is a set of ""event"" nodes that generate and consume resources?']","['Petri net', 'two', 'A set of ""place"" nodes that contain resources, and a set of ""event"" nodes', 'place']"
2058,bipartite graph,Additional applications,"In projective geometry, Levi graphs are a form of bipartite graph used to model the incidences between points and lines in a configuration. Corresponding to the geometric property of points and lines that every two lines meet in at most one point and every two points be connected with a single line, Levi graphs necessarily do not contain any cycles of length four, so their girth must be six or more.","In projective geometry, Levi graphs are a form of bipartite graph used to model the incidences between points and lines in a configuration. Corresponding to the geometric property of points and lines that every two lines meet in at most one point and every two points be connected with a single line, Levi graphs necessarily do not contain any cycles of length four, so their girth must be six or more.","[' Levi graphs are a form of what type of graph used to model the incidences between points and lines in a configuration?', ' How many lines meet in at most one point?', ' How many points must be connected with a single line?', ' Levi graphs do not contain cycles of what length?']","['bipartite', 'every two', 'two', 'four']"
2059,evolutionary algorithms,Summary,"In computational intelligence (CI), an evolutionary algorithm (EA) is a subset of evolutionary computation, a generic population-based metaheuristic optimization algorithm. An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection. Candidate solutions to the optimization problem play the role of individuals in a population, and the fitness function determines the quality of the solutions (see also loss function). Evolution of the population then takes place after the repeated application of the above operators.
","In computational intelligence (CI), an evolutionary algorithm (EA) is a subset of evolutionary computation, a generic population-based metaheuristic optimization algorithm. An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection.","[' What is an EA subset of in computational intelligence?', ' What is EA a generic population-based metaheuristic optimization algorithm?']","['evolutionary computation', 'evolutionary algorithm']"
2060,evolutionary algorithms,Summary,"Evolutionary algorithms often perform well approximating solutions to all types of problems because they ideally do not make any assumption about the underlying fitness landscape. Techniques from evolutionary algorithms applied to the modeling of biological evolution are generally limited to explorations of microevolutionary processes and planning models based upon cellular processes. In most real applications of EAs, computational complexity is a prohibiting factor. In fact, this computational complexity is due to fitness function evaluation. Fitness approximation is one of the solutions to overcome this difficulty. However, seemingly simple EA can solve often complex problems; therefore, there may be no direct link between algorithm complexity and problem complexity.
",Evolutionary algorithms often perform well approximating solutions to all types of problems because they ideally do not make any assumption about the underlying fitness landscape. Techniques from evolutionary algorithms applied to the modeling of biological evolution are generally limited to explorations of microevolutionary processes and planning models based upon cellular processes.,"[' Why do evolutionary algorithms perform well approximating solutions to all types of problems?', ' Evolutionary algorithms do not make any assumption about what?', ' Techniques from evolutionary algorithms applied to the modeling of biological evolution are limited to explorations of what processes?', ' What are explorations of?', ' What are planning models based upon?']","['they ideally do not make any assumption about the underlying fitness landscape', 'the underlying fitness landscape', 'microevolutionary processes', 'microevolutionary processes and planning models based upon cellular processes', 'cellular processes']"
2061,evolutionary algorithms,Implementation,"Step One: Generate the initial population of individuals randomly. (First generation)
",Step One: Generate the initial population of individuals randomly. (First generation),[' Step One: Generate the initial population of individuals randomly.'],['First generation']
2062,evolutionary algorithms,Comparison to biological processes,"A possible limitation of many evolutionary algorithms is their lack of a clear genotype–phenotype distinction. In nature, the fertilized egg cell undergoes a complex process known as embryogenesis to become a mature phenotype. This indirect encoding is believed to make the genetic search more robust (i.e. reduce the probability of fatal mutations), and also may improve the evolvability of the organism. Such indirect (also known as generative or developmental) encodings also enable evolution to exploit the regularity in the environment. Recent work in the field of artificial embryogeny, or artificial developmental systems, seeks to address these concerns. And gene expression programming successfully explores a genotype–phenotype system, where the genotype consists of linear multigenic chromosomes of fixed length and the phenotype consists of multiple expression trees or computer programs of different sizes and shapes.","A possible limitation of many evolutionary algorithms is their lack of a clear genotype–phenotype distinction. In nature, the fertilized egg cell undergoes a complex process known as embryogenesis to become a mature phenotype.","[' What is a possible limitation of many evolutionary algorithms?', ' What is the complex process that the fertilized egg cell undergoes to become a mature phenotype?']","['lack of a clear genotype–phenotype distinction', 'embryogenesis']"
2063,query language,Summary,"Query languages, data query languages or database query languages (DQLs) are computer languages used to make queries in databases and information systems.
A well known example is the Structured Query Language (SQL).
","Query languages, data query languages or database query languages (DQLs) are computer languages used to make queries in databases and information systems. A well known example is the Structured Query Language (SQL).","[' What are computer languages used to make queries in databases and information systems?', ' What is a well known example of the Structured Query Language?']","['Query languages, data query languages or database query languages (DQLs)', 'SQL']"
2064,query language,Types,"Broadly, query languages can be classified according to whether they are database query languages or information retrieval query languages. The difference is that a database query language attempts to give factual answers to factual questions, while an information retrieval query language attempts to find documents containing information that is relevant to an area of inquiry.
","Broadly, query languages can be classified according to whether they are database query languages or information retrieval query languages. The difference is that a database query language attempts to give factual answers to factual questions, while an information retrieval query language attempts to find documents containing information that is relevant to an area of inquiry.","[' What can query languages be classified according to?', ' What is the difference between a database query language and an information retrieval query language?', ' Language attempts to find documents containing information that is relevant to what?']","['whether they are database query languages or information retrieval query languages', 'attempts to give factual answers to factual questions', 'an area of inquiry']"
2065,artificial neural networks,Summary,"
An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron receives a signal then processes it and can signal neurons connected to it. The ""signal"" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.","
An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons.","[' What is an ANN based on?', ' What do artificial neurons loosely model?']","['a collection of connected units or nodes called artificial neurons', 'the neurons in a biological brain']"
2066,artificial neural networks,Training,"Neural networks learn (or are trained) by processing examples, each of which contains a known ""input"" and ""result,"" forming probability-weighted associations between the two, which are stored within the data structure of the net itself. The training of a neural network from a given example is usually conducted by determining the difference between the processed output of the network (often a prediction) and a target output. This difference is the error. The network then adjusts its weighted associations according to a learning rule and using this error value. Successive adjustments will cause the neural network to produce output which is increasingly similar to the target output. After a sufficient number of these adjustments the training can be terminated based upon certain criteria. This is known as supervised learning.
","Neural networks learn (or are trained) by processing examples, each of which contains a known ""input"" and ""result,"" forming probability-weighted associations between the two, which are stored within the data structure of the net itself. The training of a neural network from a given example is usually conducted by determining the difference between the processed output of the network (often a prediction) and a target output.","[' What do neural networks learn by processing?', ' What are the associations between input and result stored in?', ' How are neural networks trained?', ' What is usually conducted by determining the difference between the processed output of the network and a target output?']","['examples', 'the data structure of the net itself', 'by processing examples', 'The training of a neural network']"
2067,artificial neural networks,Training,"Such systems ""learn"" to perform tasks by considering examples, generally without being programmed with task-specific rules. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as ""cat"" or ""no cat"" and using the results to identify cats in other images. They do this without any prior knowledge of cats, for example, that they have fur, tails, whiskers, and cat-like faces. Instead, they automatically generate identifying characteristics from the examples that they process.
","Such systems ""learn"" to perform tasks by considering examples, generally without being programmed with task-specific rules. For example, in image recognition, they might learn to identify images that contain cats by analyzing example images that have been manually labeled as ""cat"" or ""no cat"" and using the results to identify cats in other images.","[' How do image recognition systems learn to perform tasks?', ' What does an image recognition system do without being programmed with?', ' What is another name for a cat?', ' What is a way to identify cats?']","['by considering examples', 'learn to identify images that contain cats by analyzing example images that have been manually labeled as ""cat"" or ""no cat"" and using the results to identify cats in other images', 'no cat', 'image recognition']"
2068,artificial neural networks,History,"Warren McCulloch and Walter Pitts (1943) opened the subject by creating a computational model for neural networks. In the late 1940s, D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning. Farley and Wesley A. Clark (1954) first used computational machines, then called ""calculators"", to simulate a Hebbian network. In 1958, psychologist Frank Rosenblatt invented the perceptron, the first artificial neural network, funded by the United States Office of Naval Research. The first functional networks with many layers were published by Ivakhnenko and Lapa in 1965, as the Group Method of Data Handling. The basics of continuous backpropagation were derived in the context of control theory by Kelley in 1960 and by Bryson in 1961, using principles of dynamic programming. Thereafter research stagnated following Minsky and Papert (1969), who discovered that basic perceptrons were incapable of processing the exclusive-or circuit and that computers lacked sufficient power to process useful neural networks.
","Warren McCulloch and Walter Pitts (1943) opened the subject by creating a computational model for neural networks. In the late 1940s, D. O. Hebb created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning.","[' In what year did Warren McCulloch and Walter Pitts create a computational model for neural networks?', ' Who created a learning hypothesis based on the mechanism of neural plasticity that became known as Hebbian learning?']","['1943', 'D. O. Hebb']"
2069,artificial neural networks,History,"In 1970, Seppo Linnainmaa published the general method for automatic differentiation (AD) of discrete connected networks of nested differentiable functions. In 1973, Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients. Werbos's (1975) backpropagation algorithm enabled practical training of multi-layer networks. In 1982, he applied Linnainmaa's AD method to neural networks in the way that became widely used.","In 1970, Seppo Linnainmaa published the general method for automatic differentiation (AD) of discrete connected networks of nested differentiable functions. In 1973, Dreyfus used backpropagation to adapt parameters of controllers in proportion to error gradients.","[' In what year did Seppo Linnainmaa publish the general method for automatic differentiation of discrete connected networks of nested differentiable functions?', ' Who used backpropagation to adapt parameters of controllers in proportion to error gradients?']","['1970', 'Dreyfus']"
2070,artificial neural networks,History,"The development of metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI), in the form of complementary MOS (CMOS) technology, enabled increasing MOS transistor counts in digital electronics. This provided more processing power for the development of practical artificial neural networks in the 1980s.","The development of metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI), in the form of complementary MOS (CMOS) technology, enabled increasing MOS transistor counts in digital electronics. This provided more processing power for the development of practical artificial neural networks in the 1980s.","[' What is another name for metal-oxide-semiconductor (MOS) very-large-scale integration?', ' What technology enabled increasing MOS transistor counts in digital electronics?', ' In what decade did the development of practical artificial neural networks occur?']","['VLSI', 'metal–oxide–semiconductor (MOS) very-large-scale integration (VLSI), in the form of complementary MOS (CMOS)', '1980s']"
2071,artificial neural networks,History,"In 1992, max-pooling was introduced to help with least-shift invariance and tolerance to deformation to aid 3D object recognition. Schmidhuber adopted a multi-level hierarchy of networks (1992) pre-trained one level at a time by unsupervised learning and fine-tuned by backpropagation.","In 1992, max-pooling was introduced to help with least-shift invariance and tolerance to deformation to aid 3D object recognition. Schmidhuber adopted a multi-level hierarchy of networks (1992) pre-trained one level at a time by unsupervised learning and fine-tuned by backpropagation.","[' When was max-pooling introduced?', ' What was introduced to help with least-shift invariance and tolerance to deformation to aid 3D object recognition?', ' Who adopted a multi-level hierarchy of networks (1992) pre-trained one level at a time by unsupervised learning?']","['1992', 'max-pooling', 'Schmidhuber']"
2072,artificial neural networks,History,"Geoffrey Hinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine to model each layer. In 2012, Ng and Dean created a network that learned to recognize higher-level concepts, such as cats, only from watching unlabeled images. Unsupervised pre-training and increased computing power from GPUs and distributed computing allowed the use of larger networks, particularly in image and visual recognition problems, which became known as ""deep learning"".",Geoffrey Hinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine to model each layer.,[' Geoffrey Hinton et al. (2006) proposed learning a high-level representation using successive layers of binary or real-valued latent variables with a restricted Boltzmann machine to model each layer?'],['binary or real-valued latent variables with a restricted Boltzmann machine to model each layer.']
2073,artificial neural networks,History,"Ciresan and colleagues (2010) showed that despite the vanishing gradient problem, GPUs make backpropagation feasible for many-layered feedforward neural networks. Between 2009 and 2012, ANNs began winning prizes in image recognition contests, approaching human level performance on various tasks, initially in pattern recognition and handwriting recognition. For example, the bi-directional and multi-dimensional long short-term memory (LSTM) of Graves et al. won three competitions in connected handwriting recognition in 2009 without any prior knowledge about the three languages to be learned.","Ciresan and colleagues (2010) showed that despite the vanishing gradient problem, GPUs make backpropagation feasible for many-layered feedforward neural networks. Between 2009 and 2012, ANNs began winning prizes in image recognition contests, approaching human level performance on various tasks, initially in pattern recognition and handwriting recognition.","[' Who showed that GPUs make backpropagation feasible for many-layered feedforward neural networks?', ' When did ANNs begin winning prizes in image recognition contests?']","['Ciresan and colleagues', 'Between 2009 and 2012']"
2074,artificial neural networks,Models,"ANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with. They soon reoriented towards improving empirical results, mostly abandoning attempts to remain true to their biological precursors. Neurons are connected to each other in various patterns, to allow the output of some neurons to become the input of others. The network forms a directed, weighted graph.","ANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with. They soon reoriented towards improving empirical results, mostly abandoning attempts to remain true to their biological precursors.","[' ANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with.', ' ANN reoriented towards what?']","['ANNs began as an attempt to exploit the architecture of the human brain to perform tasks that conventional algorithms had little success with.', 'improving empirical results']"
2075,artificial neural networks,Models,"An artificial neural network consists of a collection of simulated neurons. Each neuron is a node which is connected to other nodes via links that correspond to biological axon-synapse-dendrite connections. Each link has a weight, which determines the strength of one node's influence on another.",An artificial neural network consists of a collection of simulated neurons. Each neuron is a node which is connected to other nodes via links that correspond to biological axon-synapse-dendrite connections.,"[' What does an artificial neural network consist of?', ' What is each neuron a node connected to?', ' How are links connected to other nodes?']","['simulated neurons', 'other nodes', 'biological axon-synapse-dendrite connections']"
2076,artificial neural networks,Types,"ANNs have evolved into a broad family of techniques that have advanced the state of the art across multiple domains.  The simplest types have one or more static components, including number of units, number of layers, unit weights and topology. Dynamic types allow one or more of these to evolve via learning. The latter are much more complicated, but can shorten learning periods and produce better results. Some types allow/require learning to be ""supervised"" by the operator, while others operate independently. Some types operate purely in hardware, while others are purely software and run on general purpose computers.
","ANNs have evolved into a broad family of techniques that have advanced the state of the art across multiple domains. The simplest types have one or more static components, including number of units, number of layers, unit weights and topology.","[' What has evolved into a broad family of techniques that have advanced the state of the art across multiple domains?', ' The simplest types have one or more static components, including number of units, number of layers, what?']","['ANNs', 'unit weights and topology']"
2077,artificial neural networks,Network design,"Neural architecture search (NAS) uses machine learning to automate ANN design. Various approaches to NAS have designed networks that compare well with hand-designed systems. The basic search algorithm is to propose a candidate model, evaluate it against a dataset and use the results as feedback to teach the NAS network. Available systems include AutoML and AutoKeras.",Neural architecture search (NAS) uses machine learning to automate ANN design. Various approaches to NAS have designed networks that compare well with hand-designed systems.,"[' Neural architecture search uses machine learning to automate ANN design.', ' Various approaches to NAS have designed networks that compare well with what?']","['NAS', 'hand-designed systems']"
2078,artificial neural networks,Applications,"Because of their ability to reproduce and model nonlinear processes, artificial neural networks have found applications in many disciplines. Application areas include system identification and control (vehicle control, trajectory prediction, process control, natural resource management), quantum chemistry, general game playing, pattern recognition (radar systems, face identification, signal classification, 3D reconstruction, object recognition and more), sensor data analysis, sequence recognition (gesture, speech, handwritten and printed text recognition), medical diagnosis, finance (e.g. automated trading systems), data mining, visualization, machine translation, social network filtering and e-mail spam filtering. ANNs have been used to diagnose several types of cancers and to distinguish highly invasive cancer cell lines from less invasive lines using only cell shape information.","Because of their ability to reproduce and model nonlinear processes, artificial neural networks have found applications in many disciplines. Application areas include system identification and control (vehicle control, trajectory prediction, process control, natural resource management), quantum chemistry, general game playing, pattern recognition (radar systems, face identification, signal classification, 3D reconstruction, object recognition and more), sensor data analysis, sequence recognition (gesture, speech, handwritten and printed text recognition), medical diagnosis, finance (e.g.","[' Artificial neural networks are able to reproduce and model what kind of processes?', ' What are some application areas of artificial neural networks?', ' What are some examples of pattern recognition?', ' What is one example of a pattern recognition system?']","['nonlinear', 'system identification and control (vehicle control, trajectory prediction, process control, natural resource management),', 'radar systems, face identification, signal classification, 3D reconstruction, object recognition', 'radar systems']"
2079,artificial neural networks,Applications,"ANNs have been used to accelerate reliability analysis of infrastructures subject to natural disasters and to predict foundation settlements. ANNs have also been used for building black-box models in geoscience: hydrology, ocean modelling and coastal engineering, and geomorphology. ANNs have been employed in cybersecurity, with the objective to discriminate between legitimate activities and malicious ones. For example, machine learning has been used for classifying Android malware, for identifying domains belonging to threat actors and for detecting URLs posing a security risk. Research is underway on ANN systems designed for penetration testing, for detecting botnets, credit cards frauds and network intrusions.
","ANNs have been used to accelerate reliability analysis of infrastructures subject to natural disasters and to predict foundation settlements. ANNs have also been used for building black-box models in geoscience: hydrology, ocean modelling and coastal engineering, and geomorphology.","[' What has been used to accelerate reliability analysis of infrastructures subject to natural disasters?', ' What have ANNs also been used for building black-box models in?']","['ANNs', 'geoscience']"
2080,artificial neural networks,Applications,"ANNs have been proposed as a tool to solve partial differential equations in physics and simulate the properties of many-body open quantum systems. In brain research ANNs have studied short-term behavior of individual neurons, the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems. Studies considered long-and short-term plasticity of neural systems and their relation to learning and memory from the individual neuron to the system level.
","ANNs have been proposed as a tool to solve partial differential equations in physics and simulate the properties of many-body open quantum systems. In brain research ANNs have studied short-term behavior of individual neurons, the dynamics of neural circuitry arise from interactions between individual neurons and how behavior can arise from abstract neural modules that represent complete subsystems.","[' What has been proposed as a tool to solve partial differential equations in physics?', ' What have ANNs studied in brain research?', ' The dynamics of neural circuitry arise from interactions between what?', ' How can behavior arise from abstract neural modules that represent complete subsystems?']","['ANNs', 'short-term behavior of individual neurons', 'individual neurons', 'interactions between individual neurons']"
2081,cryptography,Summary,"Cryptography, or cryptology (from Ancient Greek: κρυπτός, romanized: kryptós ""hidden, secret""; and γράφειν graphein, ""to write"", or -λογία -logia, ""study"", respectively), is the practice and study of techniques for secure communication in the presence of adversarial behavior. More generally, cryptography is about constructing and analyzing protocols that prevent third parties or the public from reading private messages; various aspects in information security such as data confidentiality, data integrity, authentication, and non-repudiation are central to modern cryptography. Modern cryptography exists at the intersection of the disciplines of mathematics, computer science, electrical engineering, communication science, and physics. Applications of cryptography include electronic commerce, chip-based payment cards, digital currencies, computer passwords, and military communications.
","Cryptography, or cryptology (from Ancient Greek: κρυπτός, romanized: kryptós ""hidden, secret""; and γράφειν graphein, ""to write"", or -λογία -logia, ""study"", respectively), is the practice and study of techniques for secure communication in the presence of adversarial behavior. More generally, cryptography is about constructing and analyzing protocols that prevent third parties or the public from reading private messages; various aspects in information security such as data confidentiality, data integrity, authentication, and non-repudiation are central to modern cryptography.","[' What is the Greek word for cryptography?', ' What does kryptós mean in Greek?', ' What prevents third parties from reading private messages?', ' What aspects of information security are central to modern cryptography?']","['cryptology', 'hidden, secret', 'cryptography is about constructing and analyzing protocols', 'data confidentiality, data integrity, authentication, and non-repudiation']"
2082,cryptography,Summary,"Cryptography prior to the modern age was effectively synonymous with encryption, converting information from a readable state to unintelligible nonsense. The sender of an encrypted message shares the decoding technique only with intended recipients to preclude access from adversaries. The cryptography literature often uses the names Alice (""A"") for the sender, Bob (""B"") for the intended recipient, and Eve (""eavesdropper"") for the adversary. Since the development of rotor cipher machines in World War I and the advent of computers in World War II, cryptography methods have become increasingly complex and its applications more varied.
","Cryptography prior to the modern age was effectively synonymous with encryption, converting information from a readable state to unintelligible nonsense. The sender of an encrypted message shares the decoding technique only with intended recipients to preclude access from adversaries.","[' Prior to the modern age, what was cryptography synonymous with?', ' What did encryption convert information from a readable state to?', ' Who shares the decoding technique with the intended recipients?']","['encryption', 'unintelligible nonsense', 'The sender of an encrypted message']"
2083,cryptography,Summary,"Modern cryptography is heavily based on mathematical theory and computer science practice; cryptographic algorithms are designed around computational hardness assumptions, making such algorithms hard to break in actual practice by any adversary. While it is theoretically possible to break into a well-designed system, it is infeasible in actual practice to do so. Such schemes, if well designed, are therefore termed ""computationally secure""; theoretical advances (e.g., improvements in integer factorization algorithms) and faster computing technology require these designs to be continually reevaluated, and if necessary, adapted.  Information-theoretically secure schemes that provably cannot be broken even with unlimited computing power, such as the one-time pad, are much more difficult to use in practice than the best theoretically breakable, but computationally secure, schemes.
","Modern cryptography is heavily based on mathematical theory and computer science practice; cryptographic algorithms are designed around computational hardness assumptions, making such algorithms hard to break in actual practice by any adversary. While it is theoretically possible to break into a well-designed system, it is infeasible in actual practice to do so.","[' Modern cryptography is heavily based on what?', ' Cryptographic algorithms are designed around what assumptions?', ' What makes cryptographic algorithms hard to break in actual practice by any adversary?', ' What is a well-designed system infeasible in practice?']","['mathematical theory and computer science practice', 'computational hardness', 'computational hardness assumptions', 'break into a well-designed system, it is infeasible in actual practice to do so']"
2084,cryptography,Summary,"The growth of cryptographic technology has raised a number of legal issues in the Information Age. Cryptography's potential for use as a tool for espionage and sedition has led many governments to classify it as a weapon and to limit or even prohibit its use and export. In some jurisdictions where the use of cryptography is legal, laws permit investigators to compel the disclosure of encryption keys for documents relevant to an investigation. Cryptography also plays a major role in digital rights management and copyright infringement disputes in regard to digital media.",The growth of cryptographic technology has raised a number of legal issues in the Information Age. Cryptography's potential for use as a tool for espionage and sedition has led many governments to classify it as a weapon and to limit or even prohibit its use and export.,"[' The growth of what technology has raised a number of legal issues in the Information Age?', "" Cryptography's potential for use as a tool for espionage and sedition has led many governments to classify it as what?""]","['cryptographic', 'a weapon']"
2085,cryptography,Terminology,"Until modern times, cryptography referred almost exclusively to ""encryption"", which is the process of converting ordinary information (called plaintext) into unintelligible form (called ciphertext). Decryption is the reverse, in other words, moving from the unintelligible ciphertext back to plaintext. A cipher (or cypher) is a pair of algorithms that carry out the encryption and the reversing decryption. The detailed operation of a cipher is controlled both by the algorithm and, in each instance, by a ""key"". The key is a secret (ideally known only to the communicants), usually a string of characters (ideally short so it can be remembered by the user), which is needed to decrypt the ciphertext.  In formal mathematical terms, a ""cryptosystem"" is the ordered list of elements of finite possible plaintexts, finite possible cyphertexts, finite possible keys, and the encryption and decryption algorithms which correspond to each key.  Keys are important both formally and in actual practice, as ciphers without variable keys can be trivially broken with only the knowledge of the cipher used and are therefore useless (or even counter-productive) for most purposes. Historically, ciphers were often used directly for encryption or decryption without additional procedures such as authentication or integrity checks. 
","Until modern times, cryptography referred almost exclusively to ""encryption"", which is the process of converting ordinary information (called plaintext) into unintelligible form (called ciphertext). Decryption is the reverse, in other words, moving from the unintelligible ciphertext back to plaintext.","[' What is the process of converting ordinary information into unintelligible form called?', ' What is decryption?']","['encryption', 'the process of converting ordinary information (called plaintext) into unintelligible form']"
2086,cryptography,Terminology,"There are two main types of cryptosystems: symmetric and asymmetric. In symmetric systems, the only ones known until the 1970s, the same secret key encrypts and decrypts a message. Data manipulation in symmetric systems is significantly faster than in asymetric systems. Asymmetric systems use a ""public key"" to encrypt a message and a related ""private key"" to decrypt it. The advantage of asymmetric systems is that the public key can be freely published, allowing parties to establish secure communication without having a shared secret key.  In practice, asymmetric systems are used to first exchange a secret key, and then secure communication proceeds via a more efficient symmetric system using that key. Examples of asymmetric systems include Diffie–Hellman key exchange, RSA (Rivest–Shamir–Adleman), ECC (Elliptic Curve Cryptography), and Post-quantum cryptography. Secure symmetric algorithms include the commonly used AES (Advanced Encryption Standard) which replaced the older DES (Data Encryption Standard).  Insecure symmetric algorithms include children's language tangling schemes such as Pig Latin or other cant, and all historical cryptographic schemes, however seriously intended, prior to the invention of the one-time pad early in the 20th century.
","There are two main types of cryptosystems: symmetric and asymmetric. In symmetric systems, the only ones known until the 1970s, the same secret key encrypts and decrypts a message.","[' What are the two main types of cryptosystems?', ' When were the only known symmetric systems known?', ' What is the main difference between symmetric and asymmetric systems?']","['symmetric and asymmetric', 'until the 1970s', 'the same secret key encrypts and decrypts a message']"
2087,cryptography,Terminology,"In colloquial use, the term ""code"" is often used to mean any method of encryption or concealment of meaning. However, in cryptography, code has a more specific meaning: the replacement of a unit of plaintext (i.e., a meaningful word or phrase) with a code word (for example, ""wallaby"" replaces ""attack at dawn"").  A cypher, in contrast, is a scheme for changing or substituting an element below such a level (a letter, or a syllable or a pair of letters, etc.) in order to produce a cyphertext.
","In colloquial use, the term ""code"" is often used to mean any method of encryption or concealment of meaning. However, in cryptography, code has a more specific meaning: the replacement of a unit of plaintext (i.e., a meaningful word or phrase) with a code word (for example, ""wallaby"" replaces ""attack at dawn"").","[' What term is often used to mean any method of encryption or concealment of meaning?', ' In cryptography, code has a more specific meaning: the replacement of a unit of plaintext with a code word?', ' What does ""wallaby"" replace?']","['code', 'code', 'attack at dawn']"
2088,cryptography,Terminology,"Some use the terms ""cryptography"" and ""cryptology"" interchangeably in English, while others (including US military practice generally) use ""cryptography"" to refer specifically to the use and practice of cryptographic techniques and ""cryptology"" to refer to the combined study of cryptography and cryptanalysis. English is more flexible than several other languages in which ""cryptology"" (done by cryptologists) is always used in the second sense above. RFC 2828 advises that steganography is sometimes included in cryptology.","Some use the terms ""cryptography"" and ""cryptology"" interchangeably in English, while others (including US military practice generally) use ""cryptography"" to refer specifically to the use and practice of cryptographic techniques and ""cryptology"" to refer to the combined study of cryptography and cryptanalysis. English is more flexible than several other languages in which ""cryptology"" (done by cryptologists) is always used in the second sense above.","[' What do some people use the terms ""cryptography"" and ""cryptology"" interchangeably in English?', ' What does cryptography refer specifically to?', ' How is English flexible than other languages?', ' What language is more flexible than other languages in which ""cryptology"" is always used in the second sense?']","['cryptographic techniques', 'the use and practice of cryptographic techniques', 'more flexible', 'English']"
2089,cryptography,Terminology,"The study of characteristics of languages that have some application in cryptography or cryptology (e.g. frequency data, letter combinations, universal patterns, etc.) is called cryptolinguistics.
","The study of characteristics of languages that have some application in cryptography or cryptology (e.g. frequency data, letter combinations, universal patterns, etc.)","[' What is the study of characteristics of languages that have some application in cryptography or cryptology?', ' Frequency data, letter combinations, universal patterns are examples of what?']","['frequency data, letter combinations, universal patterns', 'languages that have some application in cryptography or cryptology']"
2090,cryptography,History of cryptography and cryptanalysis,"Before the modern era, cryptography focused on message confidentiality (i.e., encryption)—conversion of messages from a comprehensible form into an incomprehensible one and back again at the other end, rendering it unreadable by interceptors or eavesdroppers without secret knowledge (namely the key needed for decryption of that message). Encryption attempted to ensure secrecy in communications, such as those of spies, military leaders, and diplomats. In recent decades, the field has expanded beyond confidentiality concerns to include techniques for message integrity checking, sender/receiver identity authentication, digital signatures, interactive proofs and secure computation, among others.
","Before the modern era, cryptography focused on message confidentiality (i.e., encryption)—conversion of messages from a comprehensible form into an incomprehensible one and back again at the other end, rendering it unreadable by interceptors or eavesdroppers without secret knowledge (namely the key needed for decryption of that message). Encryption attempted to ensure secrecy in communications, such as those of spies, military leaders, and diplomats.","[' What did cryptography focus on before the modern era?', ' What did encryption attempt to do to make a message unreadable?', ' What did encryption try to ensure in communications?', ' What was the purpose of encryption?']","['message confidentiality', 'conversion of messages from a comprehensible form into an incomprehensible one and back again at the other end', 'secrecy', 'to ensure secrecy in communications']"
2091,object recognition,Summary,"Object recognition – technology in the field of computer vision for finding and identifying objects in an image or video sequence. Humans recognize a multitude of objects in images with little effort, despite the fact that the image of the objects may vary somewhat in different view points, in many different sizes and scales or even when they are translated or rotated. Objects can even be recognized when they are partially obstructed from view. This task is still a challenge for computer vision systems. Many approaches to the task have been implemented over multiple decades.
","Object recognition – technology in the field of computer vision for finding and identifying objects in an image or video sequence. Humans recognize a multitude of objects in images with little effort, despite the fact that the image of the objects may vary somewhat in different view points, in many different sizes and scales or even when they are translated or rotated.","[' Object recognition is a technology in the field of computer vision for finding and identifying what in an image or video sequence?', ' Humans recognize a multitude of objects in images with what amount of effort?', ' What may vary in different view points, in many different sizes and scales or even when they are translated or rotated?']","['objects', 'little', 'the image of the objects']"
2092,object recognition,Genetic algorithm,"Genetic algorithms can operate without prior knowledge of a given dataset and can develop recognition procedures without human intervention. A recent project achieved 100 percent accuracy on the benchmark motorbike, face, airplane and car image datasets from Caltech and 99.4 percent accuracy on fish species image datasets.","Genetic algorithms can operate without prior knowledge of a given dataset and can develop recognition procedures without human intervention. A recent project achieved 100 percent accuracy on the benchmark motorbike, face, airplane and car image datasets from Caltech and 99.4 percent accuracy on fish species image datasets.","[' How can genetic algorithms operate without prior knowledge of a given dataset?', ' Genetic algorithms can develop recognition procedures without what?', ' How much accuracy did a recent project achieve on the benchmark motorbike, face, airplane and car image datasets from Caltech?']","['can develop recognition procedures without human intervention', 'human intervention', '100 percent']"
2093,computational geometry,Summary,"Computational geometry is a branch of computer science devoted to the study of algorithms which can be stated in terms of geometry. Some purely geometrical problems arise out of the study of computational geometric algorithms, and such problems are also considered to be part of computational geometry. While modern computational geometry is a recent development, it is one of the oldest fields of computing with a history stretching back to antiquity.
","Computational geometry is a branch of computer science devoted to the study of algorithms which can be stated in terms of geometry. Some purely geometrical problems arise out of the study of computational geometric algorithms, and such problems are also considered to be part of computational geometry.","[' What is a branch of computer science devoted to the study of algorithms?', ' What are some purely geometrical problems arise out of?']","['Computational geometry', 'the study of computational geometric algorithms']"
2094,computational geometry,Summary,"Computational complexity is central to computational geometry, with great practical significance if algorithms are used on very large datasets containing tens or hundreds of millions of points. For such sets, the difference between O(n2) and O(n log n) may be the difference between days and seconds of computation.
","Computational complexity is central to computational geometry, with great practical significance if algorithms are used on very large datasets containing tens or hundreds of millions of points. For such sets, the difference between O(n2) and O(n log n) may be the difference between days and seconds of computation.","[' What is central to computational geometry?', ' What is of great practical significance if algorithms are used on very large datasets containing tens or hundreds of millions of points?', ' The difference between O(n2) and O(log n) may be the difference between days and what?']","['Computational complexity', 'Computational complexity', 'seconds of computation']"
2095,computational geometry,Summary,"Although most algorithms of computational geometry have been developed (and are being developed) for electronic computers, some algorithms were developed for unconventional computers (e.g. optical computers )
","Although most algorithms of computational geometry have been developed (and are being developed) for electronic computers, some algorithms were developed for unconventional computers (e.g. optical computers )","[' Most algorithms of computational geometry have been developed for what type of computers?', ' Some algorithms were developed for which type of computer?', ' What kind of computers were some algorithms developed for?']","['electronic', 'unconventional', 'unconventional']"
2096,computational geometry,Combinatorial computational geometry,"Some of these problems seem so simple that they were not regarded as problems at all until the advent of computers. Consider, for example, the Closest pair problem:
","Some of these problems seem so simple that they were not regarded as problems at all until the advent of computers. Consider, for example, the Closest pair problem:","[' When were some of these problems regarded as problems?', ' What is the Closest pair problem?']","['until the advent of computers', 'Some of these problems seem so simple']"
2097,computational geometry,Combinatorial computational geometry,"One could compute the distances between all the pairs of points, of which there are n(n-1)/2, then pick the pair with the smallest distance. This brute-force algorithm takes O(n2) time; i.e. its execution time is proportional to the square of the number of points. A classic result in computational geometry was the formulation of an algorithm that takes O(n log n). Randomized algorithms that take O(n) expected time, as well as a deterministic algorithm that takes O(n log log n) time, have also been discovered.
","One could compute the distances between all the pairs of points, of which there are n(n-1)/2, then pick the pair with the smallest distance. This brute-force algorithm takes O(n2) time; i.e.","[' How long does the brute force algorithm take?', ' How many pairs of points exist?']","['O(n2) time', 'n(n-1)/2']"
2098,computational geometry,Numerical computational geometry,"The most important instruments here are parametric curves and parametric surfaces, such as Bézier curves, spline curves and surfaces. An important non-parametric approach is the level-set method.
","The most important instruments here are parametric curves and parametric surfaces, such as Bézier curves, spline curves and surfaces. An important non-parametric approach is the level-set method.","[' What are the most important instruments for parametric curves and parametric surfaces?', ' What is an important non-parametric approach?']","['Bézier curves, spline curves and surfaces', 'level-set method']"
2099,collaborative filtering,Summary,"Collaborative filtering (CF) is a technique used by recommender systems. Collaborative filtering has two senses, a narrow one and a more general one.","Collaborative filtering (CF) is a technique used by recommender systems. Collaborative filtering has two senses, a narrow one and a more general one.","[' What is CF?', ' How many senses does CF have?']","['Collaborative filtering', 'two']"
2100,collaborative filtering,Summary,"In the newer, narrower sense, collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B's opinion on a different issue than that of a randomly chosen person. For example, a collaborative filtering recommendation system for preferences in television programming could make predictions about which television show a user should like given a partial list of that user's tastes (likes or dislikes). Note that these predictions are specific to the user, but use information gleaned from many users. This differs from the simpler approach of giving an average (non-specific) score for each item of interest, for example based on its number of votes.
","In the newer, narrower sense, collaborative filtering is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B's opinion on a different issue than that of a randomly chosen person.","[' What is a method of making automatic predictions about the interests of a user by collecting preferences or taste information from many users?', ' What is the underlying assumption of collaborative filtering?', ' Who has the same opinion on an issue as a person B?', "" What is more likely to have B's opinion on a different issue than that of a randomly chosen person?""]","['collaborative filtering', 'if a person A has the same opinion as a person B on an issue', 'A', 'A']"
2101,collaborative filtering,Summary,"In the more general sense, collaborative filtering is the process of filtering for information or patterns using techniques involving collaboration among multiple agents, viewpoints, data sources, etc. Applications of collaborative filtering typically involve very large data sets. Collaborative filtering methods have been applied to many different kinds of data including: sensing and monitoring data, such as in mineral exploration, environmental sensing over large areas or multiple sensors; financial data, such as financial service institutions that integrate many financial sources; or in electronic commerce and web applications where the focus is on user data, etc. The remainder of this discussion focuses on collaborative filtering for user data, although some of the methods and approaches may apply to the other major applications as well.
","In the more general sense, collaborative filtering is the process of filtering for information or patterns using techniques involving collaboration among multiple agents, viewpoints, data sources, etc. Applications of collaborative filtering typically involve very large data sets.","[' What is the process of filtering for information or patterns using techniques involving collaboration among multiple agents, viewpoints, data sources, etc?', ' Applications of collaborative filtering typically involve what kind of data sets?']","['collaborative filtering', 'very large']"
2102,collaborative filtering,Overview,"The growth of the Internet has made it much more difficult to effectively extract useful information from all the available online information. The overwhelming amount of data necessitates mechanisms for efficient information filtering. Collaborative filtering is one of the techniques used for dealing with this problem.
",The growth of the Internet has made it much more difficult to effectively extract useful information from all the available online information. The overwhelming amount of data necessitates mechanisms for efficient information filtering.,"[' What has made it much more difficult to extract useful information from all the available online information?', ' The overwhelming amount of data necessitates mechanisms for what?']","['The growth of the Internet', 'efficient information filtering']"
2103,collaborative filtering,Overview,"The motivation for collaborative filtering comes from the idea that people often get the best recommendations from someone with tastes similar to themselves. Collaborative filtering encompasses techniques for matching people with similar interests and making recommendations on this basis.
",The motivation for collaborative filtering comes from the idea that people often get the best recommendations from someone with tastes similar to themselves. Collaborative filtering encompasses techniques for matching people with similar interests and making recommendations on this basis.,"[' What is the motivation for collaborative filtering?', ' What does collaborative filter entail?']","['people often get the best recommendations from someone with tastes similar to themselves', 'techniques for matching people with similar interests and making recommendations on this basis']"
2104,collaborative filtering,Overview,"A key problem of collaborative filtering is how to combine and weight the preferences of user neighbors. Sometimes, users can immediately rate the recommended items. As a result, the system gains an increasingly accurate representation of user preferences over time.
","A key problem of collaborative filtering is how to combine and weight the preferences of user neighbors. Sometimes, users can immediately rate the recommended items.","[' What is a key problem of collaborative filtering?', ' How can users combine and weight the preferences of user neighbors?']","['how to combine and weight the preferences of user neighbors', 'collaborative filtering']"
2105,collaborative filtering,Methodology,"This falls under the category of user-based collaborative filtering. A specific application of this is the user-based Nearest Neighbor algorithm.
",This falls under the category of user-based collaborative filtering. A specific application of this is the user-based Nearest Neighbor algorithm.,[' What is a specific application of the Nearest Neighbor algorithm?'],['user-based collaborative filtering']
2106,collaborative filtering,Methodology,"Another form of collaborative filtering can be based on implicit observations of normal user behavior (as opposed to the artificial behavior imposed by a rating task). These systems observe what a user has done together with what all users have done (what music they have listened to, what items they have bought) and use that data to predict the user's behavior in the future, or to predict how a user might like to behave given the chance. These predictions then have to be filtered through business logic to determine how they might affect the actions of a business system. For example, it is not useful to offer to sell somebody a particular album of music if they already have demonstrated that they own that music.
","Another form of collaborative filtering can be based on implicit observations of normal user behavior (as opposed to the artificial behavior imposed by a rating task). These systems observe what a user has done together with what all users have done (what music they have listened to, what items they have bought) and use that data to predict the user's behavior in the future, or to predict how a user might like to behave given the chance.","[' What kind of filtering can be based on implicit observations of normal user behavior?', ' What is the artificial behavior imposed by a rating task?', "" What is used to predict a user's behavior in the future?""]","['collaborative', 'implicit observations of normal user behavior', 'implicit observations of normal user behavior (as opposed to the artificial behavior imposed by a rating task). These systems observe what a user has done together with what all users have done (what music they have listened to, what items they have bought) and use that data']"
2107,collaborative filtering,Methodology,"Relying on a scoring or rating system which is averaged across all users ignores specific demands of a user, and is particularly poor in tasks where there is large variation in interest (as in the recommendation of music). However, there are other methods to combat information explosion, such as web search and data clustering.
","Relying on a scoring or rating system which is averaged across all users ignores specific demands of a user, and is particularly poor in tasks where there is large variation in interest (as in the recommendation of music). However, there are other methods to combat information explosion, such as web search and data clustering.","[' What system ignores specific demands of a user?', ' What is particularly poor in tasks where there is large variation in interest?', ' What are two methods to combat information explosion?']","['Relying on a scoring or rating system', 'Relying on a scoring or rating system', 'web search and data clustering']"
2108,collaborative filtering,Context-aware collaborative filtering,"Many recommender systems simply ignore other contextual information existing alongside user's rating in providing item recommendation. However, by pervasive availability of contextual information such as time, location, social information, and type of the device that user is using, it is becoming more important than ever for a successful recommender system to provide a context-sensitive recommendation. According to Charu Aggrawal, ""Context-sensitive recommender systems tailor their recommendations to additional information that defines the specific situation under which recommendations are made. This additional information is referred to as the context.""","Many recommender systems simply ignore other contextual information existing alongside user's rating in providing item recommendation. However, by pervasive availability of contextual information such as time, location, social information, and type of the device that user is using, it is becoming more important than ever for a successful recommender system to provide a context-sensitive recommendation.","[' What do many recommender systems simply ignore in providing item recommendation?', ' What is becoming more important than ever for a successful recommender system?', ' Is it more important than ever for a successful recommender system to provide context-sensitive recommendations?']","['other contextual information', 'pervasive availability of contextual information', 'it is becoming more important than ever']"
2109,collaborative filtering,Context-aware collaborative filtering,"Taking contextual information into consideration, we will have additional dimension to the existing user-item rating matrix. As an instance, assume a music recommender system which provide different recommendations in corresponding to time of the day. In this case, it is possible a user have different preferences for a music in different time of a day. Thus, instead of using user-item matrix, we may use tensor of order 3 (or higher for considering other contexts) to represent context-sensitive users' preferences.","Taking contextual information into consideration, we will have additional dimension to the existing user-item rating matrix. As an instance, assume a music recommender system which provide different recommendations in corresponding to time of the day.","[' What will be added to the existing user-item rating matrix?', ' What would a music recommender system provide in relation to time of day?']","['dimension', 'different recommendations']"
2110,collaborative filtering,Context-aware collaborative filtering,"In order to take advantage of collaborative filtering and particularly neighborhood-based methods, approaches can be extended from a two-dimensional rating matrix into a tensor of higher order. For this purpose, the approach is to find the most similar/like-minded users to a target user; one can extract and compute similarity of slices (e.g. item-time matrix) corresponding to each user. Unlike the context-insensitive case for which similarity of two rating vectors are calculated, in the context-aware approaches, the similarity of rating matrices corresponding to each user is calculated by using Pearson coefficients. After the most like-minded users are found, their corresponding ratings are aggregated to identify the set of items to be recommended to the target user.
","In order to take advantage of collaborative filtering and particularly neighborhood-based methods, approaches can be extended from a two-dimensional rating matrix into a tensor of higher order. For this purpose, the approach is to find the most similar/like-minded users to a target user; one can extract and compute similarity of slices (e.g.","[' What can be extended from a two-dimensional rating matrix into a tensor of higher order?', ' What is the approach to find the most similar/like-minded users to a target user?', ' How can one extract and compute similarity of slices?']","['approaches', 'collaborative filtering and particularly neighborhood-based methods, approaches can be extended from a two-dimensional rating matrix into a tensor of higher order', 'to find the most similar/like-minded users to a target user']"
2111,collaborative filtering,Context-aware collaborative filtering,"The most important disadvantage of taking context into recommendation model is to be able to deal with larger dataset that contains much more missing values in comparison to user-item rating matrix. Therefore, similar to matrix factorization methods, tensor factorization techniques can be used to reduce dimensionality of original data before using any neighborhood-based methods.
","The most important disadvantage of taking context into recommendation model is to be able to deal with larger dataset that contains much more missing values in comparison to user-item rating matrix. Therefore, similar to matrix factorization methods, tensor factorization techniques can be used to reduce dimensionality of original data before using any neighborhood-based methods.","[' What is the most important disadvantage of taking context into recommendation model?', ' What can be used to reduce dimensionality of original data before tensor factorization techniques?', ' What can be used to reduce the dimensionality of original data before using any neighborhood-based methods?']","['to be able to deal with larger dataset', 'neighborhood-based methods', 'tensor factorization techniques']"
2112,collaborative filtering,Application on social web,"Unlike the traditional model of mainstream media, in which there are few editors who set guidelines, collaboratively filtered social media can have a very large number of editors, and content improves as the number of participants increases. Services like Reddit, YouTube, and Last.fm are typical examples of collaborative filtering based media.","Unlike the traditional model of mainstream media, in which there are few editors who set guidelines, collaboratively filtered social media can have a very large number of editors, and content improves as the number of participants increases. Services like Reddit, YouTube, and Last.fm are typical examples of collaborative filtering based media.","[' What is a typical example of collaborative filtering based on social media?', ' How many editors do collaboratively filtered social media have?', ' What can happen as the number of participants increases?', ' What are two examples of collaborative filtering based media?']","['Reddit, YouTube, and Last.fm', 'very large number', 'content improves', 'Reddit, YouTube, and Last.fm']"
2113,collaborative filtering,Application on social web,"One scenario of collaborative filtering application is to recommend interesting or popular information as judged by the community. As a typical example, stories appear in the front page of Reddit as they are ""voted up"" (rated positively) by the community. As the community becomes larger and more diverse, the promoted stories can better reflect the average interest of the community members.
","One scenario of collaborative filtering application is to recommend interesting or popular information as judged by the community. As a typical example, stories appear in the front page of Reddit as they are ""voted up"" (rated positively) by the community.","[' What is one scenario of a collaborative filtering application?', ' What is voted up by the community?', ' Where do stories appear in the front page of Reddit?']","['to recommend interesting or popular information as judged by the community', 'stories', 'as they are ""voted up"" (rated positively) by the community']"
2114,collaborative filtering,Application on social web,Wikipedia is another application of collaborative filtering. Volunteers contribute to the encyclopedia by filtering out facts from falsehoods.,Wikipedia is another application of collaborative filtering. Volunteers contribute to the encyclopedia by filtering out facts from falsehoods.,"[' What is another application of collaborative filtering?', ' Volunteers contribute to the encyclopedia by filtering out facts from falsehoods?']","['Wikipedia', 'Wikipedia is another application of collaborative filtering']"
2115,collaborative filtering,Application on social web,"Another aspect of collaborative filtering systems is the ability to generate more personalized recommendations by analyzing information from the past activity of a specific user, or the history of other users deemed to be of similar taste to a given user. These resources are used as user profiling and helps the site recommend content on a user-by-user basis. The more a given user makes use of the system, the better the recommendations become, as the system gains data to improve its model of that user.
","Another aspect of collaborative filtering systems is the ability to generate more personalized recommendations by analyzing information from the past activity of a specific user, or the history of other users deemed to be of similar taste to a given user. These resources are used as user profiling and helps the site recommend content on a user-by-user basis.","[' What is another benefit of collaborative filtering systems?', ' What is the use of the information from past activity of a specific user?', ' What is used to help the site recommend content on a user by user basis?']","['the ability to generate more personalized recommendations', 'user profiling', 'user profiling']"
2116,collaborative filtering,Auxiliary information,"User-item matrix is a basic foundation of traditional collaborative filtering techniques, and it suffers from data sparsity problem (i.e. cold start). As a consequence, except for user-item matrix, researchers are trying to gather more auxiliary information to help boost recommendation performance and develop personalized recommender systems. Generally, there are two popular auxiliary information: attribute information and interaction information. Attribute information describes a user's or an item's properties. For example, user attribute might include general profile (e.g. gender and age) and social contacts (e.g. followers or friends in social networks); Item attribute means properties like category, brand or content. In addition, interaction information refers to the implicit data showing how users interplay with the item. Widely used interaction information contains tags, comments or reviews and browsing history etc. Auxiliary information plays a significant role in a variety of aspects. Explicit social links, as a reliable representative of trust or friendship, is always employed in similarity calculation to find similar persons who share interest with the target user. The interaction-associated information - tags - is taken as a third dimension (in addition to user and item) in advanced collaborative filtering to construct a 3-dimensional tensor structure for exploration of recommendation.","User-item matrix is a basic foundation of traditional collaborative filtering techniques, and it suffers from data sparsity problem (i.e. cold start).","[' What is a basic foundation of traditional collaborative filtering techniques?', ' What problem does user-item matrix suffer from?']","['User-item matrix', 'data sparsity problem']"
2117,recommender systems,Summary,"Recommender systems are used in a variety of areas, with commonly recognised examples taking the form of playlist generators for video and music services, product recommenders for online stores, or content recommenders for social media platforms and open web content recommenders. These systems can operate using a single input, like music, or multiple inputs within and across platforms like news, books, and search queries. There are also popular recommender systems for specific topics like restaurants and online dating. Recommender systems have also been developed to explore research articles and experts, collaborators, and financial services.","Recommender systems are used in a variety of areas, with commonly recognised examples taking the form of playlist generators for video and music services, product recommenders for online stores, or content recommenders for social media platforms and open web content recommenders. These systems can operate using a single input, like music, or multiple inputs within and across platforms like news, books, and search queries.","[' Recommender systems are used in a variety of areas, such as what?', ' What is a common example of a recommendation system?', ' How can a recommender system operate?', ' What type of system can operate using a single input, like music?', ' What types of systems can operate with multiple inputs within and across platforms?']","['playlist generators for video and music services', 'playlist generators', 'using a single input, like music, or multiple inputs within and across platforms like news, books, and search queries', 'Recommender', 'Recommender systems']"
2118,recommender systems,Overview,"Recommender systems usually make use of either or both collaborative filtering and content-based filtering (also known as the personality-based approach), as well as other systems such as knowledge-based systems. Collaborative filtering approaches build a model from a user's past behavior (items previously purchased or selected and/or numerical ratings given to those items) as well as similar decisions made by other users. This model is then used to predict items (or ratings for items) that the user may have an interest in. Content-based filtering approaches utilize a series of discrete, pre-tagged characteristics of an item in order to recommend additional items with similar properties.","Recommender systems usually make use of either or both collaborative filtering and content-based filtering (also known as the personality-based approach), as well as other systems such as knowledge-based systems. Collaborative filtering approaches build a model from a user's past behavior (items previously purchased or selected and/or numerical ratings given to those items) as well as similar decisions made by other users.","[' What is another name for content-based filtering?', "" What does collaborative filtering build a model from a user's past behavior?"", ' What are items previously purchased or selected and/or numerical ratings given to those items?', "" What are other users' decisions?""]","['personality-based approach', 'items previously purchased or selected and/or numerical ratings given to those items', 'past behavior', 'similar']"
2119,recommender systems,Overview,"Each type of system has its strengths and weaknesses. In the above example, Last.fm requires a large amount of information about a user to make accurate recommendations. This is an example of the cold start problem, and is common in collaborative filtering systems. Whereas Pandora needs very little information to start, it is far more limited in scope (for example, it can only make recommendations that are similar to the original seed).
","Each type of system has its strengths and weaknesses. In the above example, Last.fm requires a large amount of information about a user to make accurate recommendations.","[' What type of system has its strengths and weaknesses?', ' What system requires a large amount of information about a user to make accurate recommendations?']","['Last.fm', 'Last.fm']"
2120,recommender systems,Overview,"Recommender systems are a useful alternative to search algorithms since they help users discover items they might not have found otherwise. Of note, recommender systems are often implemented using search engines indexing non-traditional data.
","Recommender systems are a useful alternative to search algorithms since they help users discover items they might not have found otherwise. Of note, recommender systems are often implemented using search engines indexing non-traditional data.","[' Recommender systems are a useful alternative to what?', ' Recommendation systems help users discover what they might not have found otherwise?', ' How are recommender systems implemented?']","['search algorithms', 'search algorithms', 'using search engines indexing non-traditional data']"
2121,recommender systems,Overview,"Montaner provided the first overview of recommender systems from an intelligent agent perspective. Adomavicius provided a new, alternate overview of recommender systems.  Herlocker provides an additional overview of evaluation techniques for recommender systems, and Beel et al. discussed the problems of offline evaluations. Beel et al. have also provided literature surveys on available research paper recommender systems and existing challenges.","Montaner provided the first overview of recommender systems from an intelligent agent perspective. Adomavicius provided a new, alternate overview of recommender systems.","[' Montaner provided the first overview of recommender systems from what perspective?', ' Adomavicius provided a new, alternate overview of what?']","['intelligent agent', 'recommender systems']"
2122,recommender systems,The Netflix Prize,"One of the events that energized research in recommender systems was the Netflix Prize. From 2006 to 2009, Netflix sponsored a competition, offering a grand prize of $1,000,000 to the team that could take an offered dataset of over 100 million movie ratings and return recommendations that were 10% more accurate than those offered by the company's existing recommender system. This competition energized the search for new and more accurate algorithms. On 21 September 2009, the grand prize of US$1,000,000 was given to the BellKor's Pragmatic Chaos team using tiebreaking rules.","One of the events that energized research in recommender systems was the Netflix Prize. From 2006 to 2009, Netflix sponsored a competition, offering a grand prize of $1,000,000 to the team that could take an offered dataset of over 100 million movie ratings and return recommendations that were 10% more accurate than those offered by the company's existing recommender system.","[' What was one of the events that energized research in recommender systems?', ' When did Netflix sponsor a competition?', ' What was the grand prize offered to the team that took an offered dataset of over 100 million movie ratings and returned recommendations that were 10% more?', "" How many movie ratings and return recommendations were 10% more accurate than the company's existing recommender system?""]","['the Netflix Prize', 'From 2006 to 2009', '$1,000,000', 'over 100 million']"
2123,recommender systems,The Netflix Prize,"The most accurate algorithm in 2007 used an ensemble method of 107 different algorithmic approaches, blended into a single prediction. As stated by the winners, Bell et al.:","The most accurate algorithm in 2007 used an ensemble method of 107 different algorithmic approaches, blended into a single prediction. As stated by the winners, Bell et al.","[' What was the most accurate algorithm in 2007?', ' How many different algorithmic approaches were used in the 2007 algorithm?']","['an ensemble method of 107 different algorithmic approaches, blended into a single prediction', '107']"
2124,recommender systems,The Netflix Prize,"
Predictive accuracy is substantially improved when blending multiple predictors. Our experience is that most efforts should be concentrated in deriving substantially different approaches, rather than refining a single technique.  Consequently, our solution is an ensemble of many methods.","
Predictive accuracy is substantially improved when blending multiple predictors. Our experience is that most efforts should be concentrated in deriving substantially different approaches, rather than refining a single technique.","[' What is substantially improved when blending multiple predictors?', ' What should most efforts be concentrated on rather than refining a single technique?']","['Predictive accuracy', 'deriving substantially different approaches']"
2125,recommender systems,The Netflix Prize,"Many benefits accrued to the web due to the Netflix project. Some teams have taken their technology and applied it to other markets. Some members from the team that finished second place founded Gravity R&D, a recommendation engine that's active in the RecSys community. 4-Tell, Inc. created a Netflix project–derived solution for ecommerce websites.
",Many benefits accrued to the web due to the Netflix project. Some teams have taken their technology and applied it to other markets.,"[' What project has brought many benefits to the web?', ' What has some teams taken their technology and applied it to other markets?']","['Netflix', 'Netflix']"
2126,recommender systems,The Netflix Prize,"A number of privacy issues arose around the dataset offered by Netflix for the Netflix Prize competition. Although the data sets were anonymized in order to preserve customer privacy, in 2007 two researchers from the University of Texas were able to identify individual users by matching the data sets with film ratings on the Internet Movie Database. As a result, in December 2009, an anonymous Netflix user sued Netflix in Doe v. Netflix, alleging that Netflix had violated United States fair trade laws and the Video Privacy Protection Act by releasing the datasets. This, as well as concerns from the Federal Trade Commission, led to the cancellation of a second Netflix Prize competition in 2010.","A number of privacy issues arose around the dataset offered by Netflix for the Netflix Prize competition. Although the data sets were anonymized in order to preserve customer privacy, in 2007 two researchers from the University of Texas were able to identify individual users by matching the data sets with film ratings on the Internet Movie Database.","[' What did Netflix offer for the Netflix Prize competition?', ' Why were the data sets anonymized?', ' What university was able to identify individual users by matching the data with the data?', ' To identify individual users by matching the data sets with what?']","['dataset', 'to preserve customer privacy', 'University of Texas', 'film ratings']"
2127,recommender systems,Performance measures,"Evaluation is important in assessing the effectiveness of recommendation algorithms. To measure the effectiveness of recommender systems, and compare different approaches, three types of evaluations are available: user studies, online evaluations (A/B tests), and offline evaluations.","Evaluation is important in assessing the effectiveness of recommendation algorithms. To measure the effectiveness of recommender systems, and compare different approaches, three types of evaluations are available: user studies, online evaluations (A/B tests), and offline evaluations.","[' What is important in assessing the effectiveness of recommendation algorithms?', ' How many types of evaluations are available?', ' What are user studies, online evaluations, and offline evaluations?']","['Evaluation', 'three', 'A/B tests']"
2128,recommender systems,Performance measures,"The commonly used metrics are the mean squared error and root mean squared error, the latter having been used in the Netflix Prize. The information retrieval metrics such as precision and recall or DCG are useful to assess the quality of a recommendation method. Diversity, novelty, and coverage are also considered as important aspects in evaluation. However, many of the classic evaluation measures are highly criticized.","The commonly used metrics are the mean squared error and root mean squared error, the latter having been used in the Netflix Prize. The information retrieval metrics such as precision and recall or DCG are useful to assess the quality of a recommendation method.","[' What are two commonly used metrics?', ' What has been used in the Netflix Prize?', ' The information retrieval metrics are useful to assess what?']","['mean squared error and root mean squared error', 'mean squared error and root mean squared error', 'the quality of a recommendation method']"
2129,recommender systems,Performance measures,"Evaluating the performance of a recommendation algorithm on a fixed test dataset will always be extremely challenging as it is impossible to accurately predict the reactions of real users to the recommendations. Hence any metric that computes the effectiveness of an algorithm in offline data will be imprecise.  
",Evaluating the performance of a recommendation algorithm on a fixed test dataset will always be extremely challenging as it is impossible to accurately predict the reactions of real users to the recommendations. Hence any metric that computes the effectiveness of an algorithm in offline data will be imprecise.,"[' What will always be extremely challenging?', ' What is impossible to accurately predict the reactions of real users to the recommendations?', ' Any metric that computes the effectiveness of an algorithm in offline data will be what?']","['Evaluating the performance of a recommendation algorithm on a fixed test dataset', 'recommendation algorithm', 'imprecise']"
2130,recommender systems,Performance measures,"User studies are rather a small scale. A few dozens or hundreds of users are presented recommendations created by different recommendation approaches, and then the users judge which recommendations are best. 
","User studies are rather a small scale. A few dozens or hundreds of users are presented recommendations created by different recommendation approaches, and then the users judge which recommendations are best.","[' What is the scale of user studies?', ' How many users are presented with recommendations?', ' What do users judge?']","['small scale', 'A few dozens or hundreds', 'which recommendations are best']"
2131,recommender systems,Performance measures,"In A/B tests, recommendations are shown to typically thousands of users of a real product, and the recommender system randomly picks at least two different recommendation approaches to generate recommendations. The effectiveness is measured with implicit measures of effectiveness such as conversion rate or click-through rate. 
","In A/B tests, recommendations are shown to typically thousands of users of a real product, and the recommender system randomly picks at least two different recommendation approaches to generate recommendations. The effectiveness is measured with implicit measures of effectiveness such as conversion rate or click-through rate.","[' In A/B tests, how many users of a real product are shown recommendations to?', ' How many different recommendations approaches does the recommender system randomly pick to generate recommendations?', ' What measures of effectiveness are implicit measures of?']","['thousands', 'two', 'conversion rate or click-through rate']"
2132,recommender systems,Performance measures,"Offline evaluations are based on historic data, e.g. a dataset that contains information about how users previously rated movies.","Offline evaluations are based on historic data, e.g. a dataset that contains information about how users previously rated movies.","[' What are offline evaluations based on?', ' What is a dataset that contains information about how users previously rated movies?']","['historic data', 'historic data']"
2133,recommender systems,Performance measures,"The effectiveness of recommendation approaches is then measured based on how well a recommendation approach can predict the users' ratings in the dataset. While a rating is an explicit expression of whether a user liked a movie, such information is not available in all domains. For instance, in the domain of citation recommender systems, users typically do not rate a citation or recommended article. In such cases, offline evaluations may use implicit measures of effectiveness. For instance, it may be assumed that a recommender system is effective that is able to recommend as many articles as possible that are contained in a research article's reference list. However, this kind of offline evaluations is seen critical by many researchers. For instance, it has been shown that results of offline evaluations have low correlation with results from user studies or A/B tests. A dataset popular for offline evaluation has been shown to contain duplicate data and thus to lead to wrong conclusions in the evaluation of algorithms. Often, results of so-called offline evaluations do not correlate with actually assessed user-satisfaction. This is probably because offline training is highly biased toward the highly reachable items, and offline testing data is highly influenced by the outputs of the online recommendation module. Researchers have concluded that the results of offline evaluations should be viewed critically.","The effectiveness of recommendation approaches is then measured based on how well a recommendation approach can predict the users' ratings in the dataset. While a rating is an explicit expression of whether a user liked a movie, such information is not available in all domains.","[' How is the effectiveness of recommendation approaches measured?', ' What is an explicit expression of whether a user liked a movie?']","[""based on how well a recommendation approach can predict the users' ratings in the dataset"", 'a rating']"
2134,web services,Summary,"In practice, a web service commonly provides an object-oriented Web-based interface to a database server, utilized for example by another Web server, or by a mobile app, that provides a user interface to the end-user. Many organizations that provide data in formatted HTML pages will also provide that data on their server as XML or JSON, often through a Web service to allow syndication, for example, Wikipedia's Export. Another application offered to the end-user may be a mashup, where a Web server consumes several Web services at different machines and compiles the content into one user interface.
","In practice, a web service commonly provides an object-oriented Web-based interface to a database server, utilized for example by another Web server, or by a mobile app, that provides a user interface to the end-user. Many organizations that provide data in formatted HTML pages will also provide that data on their server as XML or JSON, often through a Web service to allow syndication, for example, Wikipedia's Export.","[' What provides an object-oriented Web-based interface to a database server?', ' What provides a user interface to the end-user?', ' Many organizations that provide data in formatted HTML pages will also provide what?', ' In formatted HTML pages, what will also provide data on their server as XML or JSON?', "" What does Wikipedia's Export do?""]","['a web service', 'mobile app', 'XML or JSON', 'Many organizations', 'syndication']"
2135,web services,W3C Web services,"A web service is a software system designed to support interoperable machine-to-machine interaction over a network. It has an interface described in a machine-processable format (specifically WSDL). Other systems interact with the web service in a manner prescribed by its description using SOAP-messages, typically conveyed using HTTP with an XML serialization in conjunction with other web-related standards.",A web service is a software system designed to support interoperable machine-to-machine interaction over a network. It has an interface described in a machine-processable format (specifically WSDL).,"[' What is a web service designed to support?', ' What is the name of the interface described in WSDL?']","['interoperable machine-to-machine interaction over a network', 'machine-processable format']"
2136,web services,W3C Web services,"W3C Web Services may use SOAP over HTTP protocol, allowing less costly (more efficient) interactions over the Internet than via proprietary solutions like EDI/B2B. Besides SOAP over HTTP, Web services can also be implemented on other reliable transport mechanisms like FTP. In a 2002 document, the Web Services Architecture Working Group defined a Web services architecture, requiring a standardized implementation of a ""Web service.""
","W3C Web Services may use SOAP over HTTP protocol, allowing less costly (more efficient) interactions over the Internet than via proprietary solutions like EDI/B2B. Besides SOAP over HTTP, Web services can also be implemented on other reliable transport mechanisms like FTP.","[' W3C Web Services may use what protocol over HTTP protocol?', ' What does SOAP over HTTP allow less costly interactions over the internet than via?']","['SOAP', 'proprietary solutions like EDI/B2B']"
2137,principal component analysis,Summary,"The principal components of a collection of points in a real coordinate space are a sequence of 



p


{\displaystyle p}
 unit vectors, where the 



i


{\displaystyle i}
-th vector is the direction of a line that best fits the data while being orthogonal to the first 



i
−
1


{\displaystyle i-1}
 vectors. Here, a best-fitting line is defined as one that minimizes the average squared distance from the points to the line. These directions constitute an orthonormal basis in which different individual dimensions of the data are linearly uncorrelated. Principal component analysis (PCA) is the process of computing the principal components and using them to perform a change of basis on the data, sometimes using only the first few principal components and ignoring the rest.
","The principal components of a collection of points in a real coordinate space are a sequence of 



p


{\displaystyle p}
 unit vectors, where the 



i


{\displaystyle i}
-th vector is the direction of a line that best fits the data while being orthogonal to the first 



i
−
1


{\displaystyle i-1}
 vectors. Here, a best-fitting line is defined as one that minimizes the average squared distance from the points to the line.","[' What is the direction of a line that best fits the data while being orthogonal to the first i <unk> 1 <unk>displaystyle i-1<unk> vectors?', ' What is a best-fitting line defined as?', ' How is the average squared distance from the points to the line defined?']","['i\n\n\n{\\displaystyle i}\n-th vector', 'one that minimizes the average squared distance from the points to the line', 'minimizes']"
2138,principal component analysis,Summary,"PCA is used in exploratory data analysis and for making predictive models. It is commonly used for dimensionality reduction by projecting each data point onto only the first few principal components to obtain lower-dimensional data while preserving as much of the data's variation as possible. The first principal component can equivalently be defined as a direction that maximizes the variance of the projected data. The 



i


{\displaystyle i}
-th principal component can be taken as a direction orthogonal to the first 



i
−
1


{\displaystyle i-1}
 principal components that maximizes the variance of the projected data.
",PCA is used in exploratory data analysis and for making predictive models. It is commonly used for dimensionality reduction by projecting each data point onto only the first few principal components to obtain lower-dimensional data while preserving as much of the data's variation as possible.,"[' What is used in exploratory data analysis and making predictive models?', ' What is PCA commonly used for?', ' How is each data point projected onto only the first few principal components?']","['PCA', 'dimensionality reduction', 'PCA']"
2139,principal component analysis,Summary,"For either objective, it can be shown that the principal components are eigenvectors of the data's covariance matrix. Thus, the principal components are often computed by eigendecomposition of the data covariance matrix or singular value decomposition of the data matrix. PCA is the simplest of the true eigenvector-based multivariate analyses and is closely related to factor analysis. Factor analysis typically incorporates more domain specific assumptions about the underlying structure and solves eigenvectors of a slightly different matrix. PCA is also related to canonical correlation analysis (CCA). CCA defines coordinate systems that optimally describe the cross-covariance between two datasets while PCA defines a new orthogonal coordinate system that optimally describes variance in a single dataset. Robust and L1-norm-based variants of standard PCA have also been proposed.","For either objective, it can be shown that the principal components are eigenvectors of the data's covariance matrix. Thus, the principal components are often computed by eigendecomposition of the data covariance matrix or singular value decomposition of the data matrix.","["" What can be shown that the principal components are eigenvectors of the data's covariance matrix?"", ' The principal components of a data matrix are often computed by what?']","['either objective', 'eigendecomposition of the data covariance matrix or singular value decomposition of the data matrix']"
2140,principal component analysis,History,"PCA was invented in 1901 by Karl Pearson, as an analogue of the principal axis theorem in mechanics; it was later independently developed and named by Harold Hotelling in the 1930s. Depending on the field of application, it is also named the discrete Karhunen–Loève transform (KLT) in signal processing, the Hotelling transform in multivariate quality control, proper orthogonal decomposition (POD) in mechanical engineering, singular value decomposition (SVD) of X (invented in the last quarter of the 19th century), eigenvalue decomposition (EVD) of XTX in linear algebra, factor analysis (for a discussion of the differences between PCA and factor analysis see Ch. 7 of Jolliffe's Principal Component Analysis), Eckart–Young theorem (Harman, 1960), or empirical orthogonal functions (EOF) in meteorological science, empirical eigenfunction decomposition (Sirovich, 1987), empirical component analysis (Lorenz, 1956), quasiharmonic modes (Brooks et al., 1988), spectral decomposition in noise and vibration, and empirical modal analysis in structural dynamics.
","PCA was invented in 1901 by Karl Pearson, as an analogue of the principal axis theorem in mechanics; it was later independently developed and named by Harold Hotelling in the 1930s. Depending on the field of application, it is also named the discrete Karhunen–Loève transform (KLT) in signal processing, the Hotelling transform in multivariate quality control, proper orthogonal decomposition (POD) in mechanical engineering, singular value decomposition (SVD) of X (invented in the last quarter of the 19th century), eigenvalue decomposition (EVD) of XTX in linear algebra, factor analysis (for a discussion of the differences between PCA and factor analysis see Ch.","[' Who invented PCA?', ' When was PCA invented?', ' Who developed PCA in the 1930s?', ' What is the discrete Karhunen-Loève transform?', ' What is the discrete Karhunen-Loève transform in signal processing?', ' The Hotelling transform in multivariate quality control is what?', ' When was the singular value decomposition of X invented?', ' What is EVD?', ' What is the EVD of XTX in linear algebra, factor analysis?']","['Karl Pearson', '1901', 'Harold Hotelling', 'KLT', 'KLT', 'Karhunen–Loève transform', 'last quarter of the 19th century', 'eigenvalue decomposition', 'EVD']"
2141,principal component analysis,Intuition,"PCA can be thought of as fitting a p-dimensional ellipsoid to the data, where each axis of the ellipsoid represents a principal component. If some axis of the ellipsoid is small, then the variance along that axis is also small.
","PCA can be thought of as fitting a p-dimensional ellipsoid to the data, where each axis of the ellipsoid represents a principal component. If some axis of the ellipsoid is small, then the variance along that axis is also small.","[' What can PCA be thought of as fitting to the data?', ' Where each axis of the ellipsoid represents a principal component?']","['a p-dimensional ellipsoid', 'p-dimensional ellipsoid']"
2142,principal component analysis,Intuition,"To find the axes of the ellipsoid, we must first center the values of each variable in the dataset on 0 by subtracting the mean of the variable's observed values from each of those values. These transformed values are used instead of the original observed values for each of the variables. Then, we compute the covariance matrix of the data and calculate the eigenvalues and corresponding eigenvectors of this covariance matrix. Then we must normalize each of the orthogonal eigenvectors to turn them into unit vectors. Once this is done, each of the mutually orthogonal unit eigenvectors can be interpreted as an axis of the ellipsoid fitted to the data. This choice of basis will transform our covariance matrix into a diagonalised form with the diagonal elements representing the variance of each axis. The proportion of the variance that each eigenvector represents can be calculated by dividing the eigenvalue corresponding to that eigenvector by the sum of all eigenvalues.
","To find the axes of the ellipsoid, we must first center the values of each variable in the dataset on 0 by subtracting the mean of the variable's observed values from each of those values. These transformed values are used instead of the original observed values for each of the variables.","[' How do we find the axes of the ellipsoid?', ' What must we first center the values of each variable in the dataset on?', ' How are transformed values used instead of the original observed values?', ' Instead of the original observed values for each of the variables, what did the variables use instead of the observed values?']","[""we must first center the values of each variable in the dataset on 0 by subtracting the mean of the variable's observed values from each of those values"", '0', ""subtracting the mean of the variable's observed values from each of those values"", 'transformed values']"
2143,principal component analysis,Further considerations,"Given a set of points in Euclidean space, the first principal component corresponds to a line that passes through the multidimensional mean and minimizes the sum of squares of the distances of the points from the line. The second principal component corresponds to the same concept after all correlation with the first principal component has been subtracted from the points. The singular values (in Σ) are the square roots of the eigenvalues of the matrix XTX. Each eigenvalue is proportional to the portion of the ""variance"" (more correctly of the sum of the squared distances of the points from their multidimensional mean) that is associated with each eigenvector. The sum of all the eigenvalues is equal to the sum of the squared distances of the points from their multidimensional mean. PCA essentially rotates the set of points around their mean in order to align with the principal components. This moves as much of the variance as possible (using an orthogonal transformation) into the first few dimensions. The values in the remaining dimensions, therefore, tend to be small and may be dropped with minimal loss of information (see below). PCA is often used in this manner for dimensionality reduction. PCA has the distinction of being the optimal orthogonal transformation for keeping the subspace that has largest ""variance"" (as defined above). This advantage, however, comes at the price of greater computational requirements if compared, for example, and when applicable, to the discrete cosine transform, and in particular to the DCT-II which is simply known as the ""DCT"". Nonlinear dimensionality reduction techniques tend to be more computationally demanding than PCA.
","Given a set of points in Euclidean space, the first principal component corresponds to a line that passes through the multidimensional mean and minimizes the sum of squares of the distances of the points from the line. The second principal component corresponds to the same concept after all correlation with the first principal component has been subtracted from the points.","[' What is the first principal component of a set of points in Euclidean space?', ' What is a line that passes through the multidimensional mean and minimizes the sum of squares of the distances from the line?', ' What is subtracted from the points?', ' What is the first principal component?']","['a line that passes through the multidimensional mean and minimizes the sum of squares of the distances of the points from the line', 'the first principal component', 'correlation with the first principal component', 'a line that passes through the multidimensional mean and minimizes the sum of squares of the distances of the points from the line']"
2144,principal component analysis,Further considerations,"PCA is sensitive to the scaling of the variables. If we have just two variables and they have the same sample variance and are positively correlated, then the PCA will entail a rotation by 45° and the ""weights"" (they are the cosines of rotation) for the two variables with respect to the principal component will be equal. But if we multiply all values of the first variable by 100, then the first principal component will be almost the same as that variable, with a small contribution from the other variable, whereas the second component will be almost aligned with the second original variable. This means that whenever the different variables have different units (like temperature and mass), PCA is a somewhat arbitrary method of analysis. (Different results would be obtained if one used Fahrenheit rather than Celsius for example.) Pearson's original paper was entitled ""On Lines and Planes of Closest Fit to Systems of Points in Space"" – ""in space"" implies physical Euclidean space where such concerns do not arise. One way of making the PCA less arbitrary is to use variables scaled so as to have unit variance, by standardizing the data and hence use the autocorrelation matrix instead of the autocovariance matrix as a basis for PCA. However, this compresses (or expands) the fluctuations in all dimensions of the signal space to unit variance.
","PCA is sensitive to the scaling of the variables. If we have just two variables and they have the same sample variance and are positively correlated, then the PCA will entail a rotation by 45° and the ""weights"" (they are the cosines of rotation) for the two variables with respect to the principal component will be equal.","[' What is sensitive to the scaling of the variables?', ' If we have just two variables and they have the same sample variance and are positively correlated, what will the PCA entail?', ' What are the ""weights""?', ' The cosines of rotation) for the two variables with respect to the principal component will be equal what?']","['PCA', 'a rotation by 45°', 'they are the cosines of rotation', 'weights']"
2145,principal component analysis,Further considerations,"Mean subtraction (a.k.a. ""mean centering"") is necessary for performing classical PCA to ensure that the first principal component describes the direction of maximum variance. If mean subtraction is not performed, the first principal component might instead correspond more or less to the mean of the data. A mean of zero is needed for finding a basis that minimizes the mean square error of the approximation of the data.","Mean subtraction (a.k.a. ""mean centering"") is necessary for performing classical PCA to ensure that the first principal component describes the direction of maximum variance.","[' What is mean subtraction also known as?', ' What is the purpose of means subtraction in PCA?']","['mean centering', 'to ensure that the first principal component describes the direction of maximum variance']"
2146,principal component analysis,Further considerations,"Mean-centering is unnecessary if performing a principal components analysis on a correlation matrix, as the data are already centered after calculating correlations. Correlations are derived from the cross-product of two standard scores (Z-scores) or statistical moments (hence the name: Pearson Product-Moment Correlation). Also see the article by Kromrey & Foster-Johnson (1998) on ""Mean-centering in Moderated Regression: Much Ado About Nothing"".
","Mean-centering is unnecessary if performing a principal components analysis on a correlation matrix, as the data are already centered after calculating correlations. Correlations are derived from the cross-product of two standard scores (Z-scores) or statistical moments (hence the name: Pearson Product-Moment Correlation).","[' What is unnecessary if performing a principal components analysis on a correlation matrix?', ' What is the name for the cross-product of two standard scores?']","['Mean-centering', 'Pearson Product-Moment Correlation']"
2147,principal component analysis,Further considerations,"PCA is a popular primary technique in pattern recognition. It is not, however, optimized for class separability. However, it has been used to quantify the distance between two or more classes by calculating center of mass for each class in principal component space and reporting Euclidean distance between center of mass of two or more classes. The linear discriminant analysis is an alternative which is optimized for class separability.
","PCA is a popular primary technique in pattern recognition. It is not, however, optimized for class separability.","[' What is a popular primary technique in pattern recognition?', ' What is PCA not optimized for?']","['PCA', 'class separability']"
2148,principal component analysis,Computing PCA using the covariance method,"Mean subtraction is an integral part of the solution towards finding a principal component basis that minimizes the mean square error of approximating the data. Hence we proceed by centering the data as follows:
",Mean subtraction is an integral part of the solution towards finding a principal component basis that minimizes the mean square error of approximating the data. Hence we proceed by centering the data as follows:,[' What is an integral part of the solution towards finding a principal component basis that minimizes the mean square error of approximating the data?'],['Mean subtraction']
2149,principal component analysis,Computing PCA using the covariance method,"In some applications, each variable (column of B) may also be scaled to have a variance equal to 1 (see Z-score).  This step affects the calculated principal components, but makes them independent of the units used to measure the different variables.
","In some applications, each variable (column of B) may also be scaled to have a variance equal to 1 (see Z-score). This step affects the calculated principal components, but makes them independent of the units used to measure the different variables.","[' What may be scaled to have a variance equal to 1?', ' What affects the calculated principal components but makes them independent of the units used to measure the different variables?']","['each variable (column of B)', 'each variable (column of B) may also be scaled to have a variance equal to 1']"
2150,principal component analysis,Derivation of PCA using the covariance method,"Let X be a d-dimensional random vector expressed as column vector. Without loss of generality, assume X has zero mean.
","Let X be a d-dimensional random vector expressed as column vector. Without loss of generality, assume X has zero mean.","[' Let X be a d-dimensional random vector expressed as what?', ' Without loss of generality, assume X has zero mean?']","['column vector', 'Let X be a d-dimensional random vector expressed as column vector']"
2151,principal component analysis,Covariance-free computation,"In practical implementations, especially with high dimensional data (large p), the naive covariance method is rarely used because it is not efficient due to high computational and memory costs of explicitly determining the covariance matrix. The covariance-free approach avoids the np2 operations of explicitly calculating and storing the covariance matrix XTX, instead utilizing one of matrix-free methods, for example, based on the function evaluating the product XT(X r) at the cost of 2np operations.
","In practical implementations, especially with high dimensional data (large p), the naive covariance method is rarely used because it is not efficient due to high computational and memory costs of explicitly determining the covariance matrix. The covariance-free approach avoids the np2 operations of explicitly calculating and storing the covariance matrix XTX, instead utilizing one of matrix-free methods, for example, based on the function evaluating the product XT(X r) at the cost of 2np operations.","[' Why is the naive covariance method rarely used in practical implementations?', ' What is the reason for the high computational and memory costs of explicitly determining the covariant matrix?', ' The covariancy-free approach avoids what operations of np2?', ' What does np2 operations of explicitly calculating and storing the covariance matrix XTX use instead of using one of matrix-free methods?', ' What is the cost of 2np operations?']","['it is not efficient', 'it is not efficient', 'explicitly calculating and storing the covariance matrix XTX', 'based on the function evaluating the product XT(X r) at the cost of 2np operations', 'XT(X r)']"
2152,principal component analysis,PCA and qualitative variables,"In PCA, it is common that we want to introduce qualitative variables as supplementary elements. For example, many quantitative variables have been measured on plants. For these plants, some qualitative variables are available as, for example, the species to which the plant belongs. These data were subjected to PCA for quantitative variables. When analyzing the results, it is natural to connect the principal components to the qualitative variable species.
For this, the following results are produced.
","In PCA, it is common that we want to introduce qualitative variables as supplementary elements. For example, many quantitative variables have been measured on plants.","[' What type of variables are often introduced as supplementary elements in PCA?', ' What has been measured on plants?']","['qualitative', 'quantitative variables']"
2153,principal component analysis,PCA and qualitative variables,"These results are what is called introducing a qualitative variable as supplementary element. This procedure is detailed in and Husson, Lê & Pagès 2009 and Pagès 2013.
Few software offer this option in an ""automatic"" way. This is the case of SPAD that historically, following the work of Ludovic Lebart, was the first to propose this option, and the R package FactoMineR.
","These results are what is called introducing a qualitative variable as supplementary element. This procedure is detailed in and Husson, Lê & Pagès 2009 and Pagès 2013.","[' What is introducing a qualitative variable as supplementary element called?', ' Husson, Lê & Pagès 2009 and what year?']","['These results', '2013']"
2154,cryptanalysis,Summary,"Cryptanalysis (from the Greek kryptós, ""hidden"", and analýein, ""to analyze"") refers to the process of analyzing information systems in order to understand hidden aspects of the systems.  Cryptanalysis is used to breach cryptographic security systems and gain access to the contents of encrypted messages, even if the cryptographic key is unknown.
","Cryptanalysis (from the Greek kryptós, ""hidden"", and analýein, ""to analyze"") refers to the process of analyzing information systems in order to understand hidden aspects of the systems. Cryptanalysis is used to breach cryptographic security systems and gain access to the contents of encrypted messages, even if the cryptographic key is unknown.","[' What does kryptós mean in Greek?', ' What does anal<unk>ein mean in English?', ' How is the process of analyzing information systems used?', ' What are the contents of encrypted messages even if the cryptographic key is unknown?']","['hidden', 'to analyze', 'in order to understand hidden aspects of the systems', 'Cryptanalysis']"
2155,cryptanalysis,Summary,"Even though the goal has been the same, the methods and techniques of cryptanalysis have changed drastically through the history of cryptography, adapting to increasing cryptographic complexity, ranging from the pen-and-paper methods of the past, through machines like the British Bombes and Colossus computers at Bletchley Park in World War II, to the mathematically advanced computerized schemes of the present. Methods for breaking modern cryptosystems often involve solving carefully constructed problems in pure mathematics, the best-known being integer factorization.
","Even though the goal has been the same, the methods and techniques of cryptanalysis have changed drastically through the history of cryptography, adapting to increasing cryptographic complexity, ranging from the pen-and-paper methods of the past, through machines like the British Bombes and Colossus computers at Bletchley Park in World War II, to the mathematically advanced computerized schemes of the present. Methods for breaking modern cryptosystems often involve solving carefully constructed problems in pure mathematics, the best-known being integer factorization.","[' What have the methods and techniques of cryptanalysis changed drastically through the history of cryptography?', ' What was the goal of the British Bombes and Colossus computers?', ' In what park were Bombs and Colossus computers located?', ' What is the best known method for breaking modern cryptosystems?']","['adapting to increasing cryptographic complexity', 'the same, the methods and techniques of cryptanalysis', 'Bletchley Park', 'integer factorization']"
2156,cryptanalysis,Overview,"Given some encrypted data (""ciphertext""), the goal of the cryptanalyst is to gain as much information as possible about the original, unencrypted data (""plaintext""). Cryptographic attacks can be characterized in a number of ways:
","Given some encrypted data (""ciphertext""), the goal of the cryptanalyst is to gain as much information as possible about the original, unencrypted data (""plaintext""). Cryptographic attacks can be characterized in a number of ways:","[' What is the goal of a cryptanalyst?', ' What can be characterized in a number of ways?']","['to gain as much information as possible', 'Cryptographic attacks']"
2157,cryptanalysis,History,"Cryptanalysis has coevolved together with cryptography, and the contest can be traced through the history of cryptography—new ciphers being designed to replace old broken designs, and new cryptanalytic techniques invented to crack the improved schemes. In practice, they are viewed as two sides of the same coin: secure cryptography requires design against possible cryptanalysis.","Cryptanalysis has coevolved together with cryptography, and the contest can be traced through the history of cryptography—new ciphers being designed to replace old broken designs, and new cryptanalytic techniques invented to crack the improved schemes. In practice, they are viewed as two sides of the same coin: secure cryptography requires design against possible cryptanalysis.","[' What has coevolved together with cryptography?', ' What are new ciphers being designed to replace?', ' New cryptanalytic techniques invented to crack the improved schemes?', ' What requires design against possible cryptanalysis?', ' How many sides of the same coin are there?']","['Cryptanalysis', 'old broken designs', '—', 'secure cryptography', 'two']"
2158,cryptanalysis,Asymmetric ciphers,"Asymmetric cryptography (or public-key cryptography) is cryptography that relies on using two (mathematically related) keys; one private, and one public. Such ciphers invariably rely on ""hard"" mathematical problems as the basis of their security, so an obvious point of attack is to develop methods for solving the problem. The security of two-key cryptography depends on mathematical questions in a way that single-key cryptography generally does not, and conversely links cryptanalysis to wider mathematical research in a new way.","Asymmetric cryptography (or public-key cryptography) is cryptography that relies on using two (mathematically related) keys; one private, and one public. Such ciphers invariably rely on ""hard"" mathematical problems as the basis of their security, so an obvious point of attack is to develop methods for solving the problem.","[' Asymmetric cryptography relies on using how many keys?', ' Asymmetric ciphers rely on what as the basis of their security?', ' What is an obvious point of attack?']","['two', 'hard"" mathematical problems', 'to develop methods for solving the problem']"
2159,cryptanalysis,Asymmetric ciphers,"Asymmetric schemes are designed around the (conjectured) difficulty of solving various mathematical problems. If an improved algorithm can be found to solve the problem, then the system is weakened. For example, the security of the Diffie–Hellman key exchange scheme depends on the difficulty of calculating the discrete logarithm. In 1983, Don Coppersmith found a faster way to find discrete logarithms (in certain groups), and thereby requiring cryptographers to use larger groups (or different types of groups). RSA's security depends (in part) upon the difficulty of integer factorization – a breakthrough in factoring would impact the security of RSA.","Asymmetric schemes are designed around the (conjectured) difficulty of solving various mathematical problems. If an improved algorithm can be found to solve the problem, then the system is weakened.","[' Asymmetric schemes are designed around the difficulty of solving what?', ' If an improved algorithm can be found to solve the problem, what is the system weakened?']","['various mathematical problems', 'Asymmetric schemes']"
2160,cryptanalysis,Asymmetric ciphers,"In 1980, one could factor a difficult 50-digit number at an expense of 1012 elementary computer operations. By 1984 the state of the art in factoring algorithms had advanced to a point where a 75-digit number could be factored in 1012 operations. Advances in computing technology also meant that the operations could be performed much faster, too. Moore's law predicts that computer speeds will continue to increase. Factoring techniques may continue to do so as well, but will most likely depend on mathematical insight and creativity, neither of which has ever been successfully predictable. 150-digit numbers of the kind once used in RSA have been factored. The effort was greater than above, but was not unreasonable on fast modern computers. By the start of the 21st century, 150-digit numbers were no longer considered a large enough key size for RSA. Numbers with several hundred digits were still considered too hard to factor in 2005, though methods will probably continue to improve over time, requiring key size to keep pace or other methods such as elliptic curve cryptography to be used.","In 1980, one could factor a difficult 50-digit number at an expense of 1012 elementary computer operations. By 1984 the state of the art in factoring algorithms had advanced to a point where a 75-digit number could be factored in 1012 operations.","[' What was the cost of factoring a difficult 50-digit number in 1980?', ' By 1984, the state of the art in factoring algorithms had advanced to a point where what could be factored in 1012 operations?']","['1012 elementary computer operations', 'a 75-digit number']"
2161,cryptanalysis,Quantum computing applications for cryptanalysis,"Quantum computers, which are still in the early phases of research, have potential use in cryptanalysis. For example, Shor's Algorithm could factor large numbers in polynomial time, in effect breaking some commonly used forms of public-key encryption.","Quantum computers, which are still in the early phases of research, have potential use in cryptanalysis. For example, Shor's Algorithm could factor large numbers in polynomial time, in effect breaking some commonly used forms of public-key encryption.","[' What type of computers have potential use in cryptanalysis?', "" What could Shor's Algorithm factor large numbers in polynomial time?""]","['Quantum computers', 'breaking some commonly used forms of public-key encryption']"
2162,cryptanalysis,Quantum computing applications for cryptanalysis,"By using Grover's algorithm on a quantum computer, brute-force key search can be made quadratically faster. However, this could be countered by doubling the key length.","By using Grover's algorithm on a quantum computer, brute-force key search can be made quadratically faster. However, this could be countered by doubling the key length.","["" What can be made quadratically faster by using Grover's algorithm on a quantum computer?"", ' What could be countered by doubling the key length?']","['brute-force key search', 'brute-force key search']"
2163,ontologies,Summary,"In computer science and information science, an ontology encompasses a representation, formal naming, and definition of the categories, properties, and relations between the concepts, data, and entities that substantiate one, many, or all domains of discourse. More simply, an ontology is a way of showing the properties of a subject area and how they are related, by defining a set of concepts and categories that represent the subject.
","In computer science and information science, an ontology encompasses a representation, formal naming, and definition of the categories, properties, and relations between the concepts, data, and entities that substantiate one, many, or all domains of discourse. More simply, an ontology is a way of showing the properties of a subject area and how they are related, by defining a set of concepts and categories that represent the subject.","[' What is an ontology in computer science and information science?', ' What are the categories, properties, and relations between concepts, data, and entities that substantiate one, many, or all domains of discourse?', ' What is a way of showing the properties of a subject area and how they are related?', ' What are concepts and categories that represent the subject area?']","['a representation, formal naming, and definition of the categories, properties, and relations between the concepts, data, and entities', 'an ontology', 'an ontology', 'an ontology']"
2164,ontologies,Summary,"Every academic discipline or field creates ontologies to limit complexity and organize data into information and knowledge. 
Each uses ontological assumptions to frame explicit theories, research and applications. New ontologies may improve problem solving within that domain. Translating research papers within every field is a problem made easier when experts from different countries maintain a controlled vocabulary of jargon between each of their languages.","Every academic discipline or field creates ontologies to limit complexity and organize data into information and knowledge. Each uses ontological assumptions to frame explicit theories, research and applications.","[' What does each academic discipline or field create to limit complexity and organize data into information and knowledge?', ' What uses ontological assumptions to frame explicit theories, research and applications?']","['ontologies', 'Every academic discipline or field creates ontologies']"
2165,ontologies,Summary,"For instance, the definition and ontology of economics is a primary concern in Marxist economics, but also in other subfields of economics. An example of economics relying on information science occurs in cases where a simulation or model is intended to enable economic decisions, such as determining what capital assets are at risk and by how much (see risk management). 
","For instance, the definition and ontology of economics is a primary concern in Marxist economics, but also in other subfields of economics. An example of economics relying on information science occurs in cases where a simulation or model is intended to enable economic decisions, such as determining what capital assets are at risk and by how much (see risk management).","[' What is a primary concern in Marxist economics?', ' What is an example of economics relying on information science?', ' To enable economic decisions, such as determining what is at risk and by how much?']","['the definition and ontology of economics', 'in cases where a simulation or model is intended to enable economic decisions', 'a simulation or model']"
2166,ontologies,Summary,"What ontologies in both information science and philosophy have in common is the attempt to represent entities, ideas and events, with all their interdependent properties and relations, according to a system of categories. In both fields, there is considerable work on problems of ontology engineering (e.g., Quine and Kripke in philosophy, Sowa and Guarino in computer science), and debates concerning to what extent normative ontology is possible (e.g., foundationalism and coherentism in philosophy, BFO and Cyc in artificial intelligence).
","What ontologies in both information science and philosophy have in common is the attempt to represent entities, ideas and events, with all their interdependent properties and relations, according to a system of categories. In both fields, there is considerable work on problems of ontology engineering (e.g., Quine and Kripke in philosophy, Sowa and Guarino in computer science), and debates concerning to what extent normative ontology is possible (e.g., foundationalism and coherentism in philosophy, BFO and Cyc in artificial intelligence).","[' Ontologies in information science and philosophy attempt to represent entities, ideas and events according to a system of what?', ' In both fields, there is considerable work on problems of ontology engineering.', ' What are two examples of problems of ontology engineering?', ' What is possible to what extent in philosophy?', ' BFO and Cyc are examples of what?']","['categories', 'Quine and Kripke in philosophy, Sowa and Guarino in computer science', 'Quine and Kripke in philosophy, Sowa and Guarino in computer science', 'normative ontology', 'artificial intelligence']"
2167,ontologies,Summary,"Applied ontology is considered a spiritual successor to prior work in philosophy, however many current efforts are more concerned with establishing controlled vocabularies of narrow domains than first principles, the existence of fixed essences or whether enduring objects (e.g., perdurantism and endurantism) may be ontologically more primary than processes. Artificial intelligence has retained the most attention regarding applied ontology in subfields like natural language processing within machine translation and knowledge representation, but ontology editors are being used often in a range of fields like education without the intent to contribute to AI.","Applied ontology is considered a spiritual successor to prior work in philosophy, however many current efforts are more concerned with establishing controlled vocabularies of narrow domains than first principles, the existence of fixed essences or whether enduring objects (e.g., perdurantism and endurantism) may be ontologically more primary than processes. Artificial intelligence has retained the most attention regarding applied ontology in subfields like natural language processing within machine translation and knowledge representation, but ontology editors are being used often in a range of fields like education without the intent to contribute to AI.","[' What is considered a spiritual successor to prior work in philosophy?', ' What are many current efforts more concerned with establishing controlled vocabularies of?', ' What may be ontological more primary than processes?', ' Artificial intelligence has retained the most attention regarding applied ontology in subfields like natural language processing?', ' Ontology editors are being used often in a range of fields without the intent to contribute what?', ' How many fields are there without the intent to contribute to AI?']","['Applied ontology', 'narrow domains', 'enduring objects', 'machine translation and knowledge representation', 'AI', 'education']"
2168,ontologies,Etymology,"The compound word ontology combines onto-, from the Greek ὄν, on (gen. ὄντος, ontos), i.e. ""being; that which is"", which is the present participle of the verb εἰμί, eimí, i.e. ""to be, I am"", and -λογία, -logia, i.e. ""logical discourse"", see classical compounds for this type of word formation.","The compound word ontology combines onto-, from the Greek ὄν, on (gen. ὄντος, ontos), i.e. ""being; that which is"", which is the present participle of the verb εἰμί, eimí, i.e.","[' The compound word ontology combines onto-, from what Greek word?', ' What is the present participle of the verb <unk>, eim<unk>?']","['ὄν', 'εἰμί']"
2169,ontologies,History,"Ontologies arise out of the branch of philosophy known as metaphysics, which deals with questions like ""what exists?"" and ""what is the nature of reality?"". One of five traditional branches of philosophy, metaphysics is concerned with exploring existence through properties, entities and relations such as those between particulars and universals, intrinsic and extrinsic properties, or essence and existence. Metaphysics has been an ongoing topic of discussion since recorded history.
","Ontologies arise out of the branch of philosophy known as metaphysics, which deals with questions like ""what exists?"" and ""what is the nature of reality?"".","[' What branch of philosophy does ontologies come from?', ' Metaphysics deals with questions like ""what exists?""']","['metaphysics', 'what exists?"" and ""what is the nature of reality']"
2170,ontologies,History,"Since the mid-1970s, researchers in the field of artificial intelligence (AI) have recognized that knowledge engineering is the key to building large and powerful AI systems. AI researchers argued that they could create new ontologies as computational models that enable certain kinds of automated reasoning, which was only marginally successful. In the 1980s, the AI community began to use the term ontology to refer to both a theory of a modeled world and a component of knowledge-based systems. In particular, David Powers introduced the word ontology to AI to refer to real world or robotic grounding, publishing in 1990 literature reviews emphasizing grounded ontology in association with the call for papers for a AAAI Summer Symposium Machine Learning of Natural Language and Ontology, with an expanded version published in SIGART Bulletin and included as a preface to the proceedings.  Some researchers, drawing inspiration from philosophical ontologies, viewed computational ontology as a kind of applied philosophy.","Since the mid-1970s, researchers in the field of artificial intelligence (AI) have recognized that knowledge engineering is the key to building large and powerful AI systems. AI researchers argued that they could create new ontologies as computational models that enable certain kinds of automated reasoning, which was only marginally successful.","[' Since when have researchers in the field of artificial intelligence recognized that knowledge engineering is the key to building large and powerful AI systems?', ' What did researchers argue they could create as computational models that enable certain kinds of automated reasoning?']","['mid-1970s', 'new ontologies']"
2171,ontologies,History,"
In 1993, the widely cited web page and paper ""Toward Principles for the Design of Ontologies Used for Knowledge Sharing"" by Tom Gruber used ontology as a technical term in computer science closely related to earlier idea of semantic networks and taxonomies. Gruber introduced the term as a specification of a conceptualization: ","
In 1993, the widely cited web page and paper ""Toward Principles for the Design of Ontologies Used for Knowledge Sharing"" by Tom Gruber used ontology as a technical term in computer science closely related to earlier idea of semantic networks and taxonomies. Gruber introduced the term as a specification of a conceptualization:","[' In what year was the paper ""Toward Principles for the Design of Ontologies Used for Knowledge Sharing"" published?', ' What was the technical term for ontology in computer science closely related to?', ' What did Gruber introduce the term as a specification of?']","['1993', 'earlier idea of semantic networks and taxonomies', 'a conceptualization']"
2172,ontologies,History,"An ontology is a description (like a formal specification of a program) of the concepts and relationships that can formally exist for an agent or a community of agents. This definition is consistent with the usage of ontology as set of concept definitions, but more general. And it is a different sense of the word than its use in philosophy.","An ontology is a description (like a formal specification of a program) of the concepts and relationships that can formally exist for an agent or a community of agents. This definition is consistent with the usage of ontology as set of concept definitions, but more general.","[' What is an ontology?', ' What is a description of concepts and relationships that can formally exist for an agent or a community of agents?']","['a description (like a formal specification of a program) of the concepts and relationships that can formally exist for an agent or a community of agents', 'An ontology']"
2173,ontologies,History,"Ontologies are often equated with taxonomic hierarchies of classes, class definitions, and the subsumption relation, but ontologies need not be limited to these forms. Ontologies are also not limited to conservative definitions — that is, definitions in the traditional logic sense that only introduce terminology and do not add any knowledge about the world. To specify a conceptualization, one needs to state axioms that do constrain the possible interpretations for the defined terms.","Ontologies are often equated with taxonomic hierarchies of classes, class definitions, and the subsumption relation, but ontologies need not be limited to these forms. Ontologies are also not limited to conservative definitions — that is, definitions in the traditional logic sense that only introduce terminology and do not add any knowledge about the world.","[' Ontologies are often equated with taxonomic hierarchies of classes, class definitions, and the subsumption relation, but they need not be limited to what forms?', ' Ontology definitions in the traditional logic sense introduce terminology and do not add what?', ' What do they introduce that only introduce terminology and do not add any knowledge about?']","['conservative definitions', 'knowledge about the world', 'conservative definitions']"
2174,ontologies,Components,"Contemporary ontologies share many structural similarities, regardless of the language in which they are expressed.  Most ontologies describe individuals (instances), classes (concepts), attributes and relations.  In this section each of these components is discussed in turn.
","Contemporary ontologies share many structural similarities, regardless of the language in which they are expressed. Most ontologies describe individuals (instances), classes (concepts), attributes and relations.","[' Contemporary ontologies share many structural similarities regardless of what language?', ' What describe individuals?']","['the language in which they are expressed', 'instances']"
2175,ontologies,Visualization,"A survey of ontology visualization methods is presented by Katifori et al. An updated survey of ontology visualization methods and tools was published by Dudás et al. The most established ontology visualization methods, namely indented tree and graph visualization are evaluated by Fu et al. A visual language for ontologies represented in OWL is specified by the Visual Notation for OWL Ontologies (VOWL).",A survey of ontology visualization methods is presented by Katifori et al. An updated survey of ontology visualization methods and tools was published by Dudás et al.,"[' Who presented a survey of ontology visualization methods and tools?', ' Who published an updated survey on ontological visualization tools and methods?']","['Katifori et al', 'Dudás et al']"
2176,ontologies,Engineering,"Ontology engineering (also called ontology building)  is a set of tasks related to the development of ontologies for a particular domain. It is a subfield of knowledge engineering that studies the ontology development process, the ontology life cycle, the methods and methodologies for building ontologies, and the tools and languages that support them.","Ontology engineering (also called ontology building)  is a set of tasks related to the development of ontologies for a particular domain. It is a subfield of knowledge engineering that studies the ontology development process, the ontology life cycle, the methods and methodologies for building ontologies, and the tools and languages that support them.","[' What is another name for ontology engineering?', ' What is a set of tasks related to the development of ontologies for a particular domain called?', ' What are methods and methodologies for building?', ' What are tools and languages that support ontologies?']","['ontology building', 'Ontology engineering', 'ontologies', 'methods and methodologies']"
2177,ontologies,Engineering,"Ontology engineering aims to make explicit the knowledge contained in software applications, and organizational procedures for a particular domain. Ontology engineering offers a direction for overcoming semantic obstacles, such as those related to the definitions of business terms and software classes. Known challenges with ontology engineering include:
","Ontology engineering aims to make explicit the knowledge contained in software applications, and organizational procedures for a particular domain. Ontology engineering offers a direction for overcoming semantic obstacles, such as those related to the definitions of business terms and software classes.","[' Ontology engineering aims to make explicit the knowledge contained in what?', ' What offers a direction for overcoming semantic obstacles?']","['software applications', 'Ontology engineering']"
2178,ontologies,Languages,"An ontology language is a formal language used to encode an ontology. There are a number of such languages for ontologies, both proprietary and standards-based:
","An ontology language is a formal language used to encode an ontology. There are a number of such languages for ontologies, both proprietary and standards-based:","[' What is a formal language used to encode an ontology?', ' What are there a number of for ontologies?']","['ontology language', 'ontology language']"
2179,parallel algorithm,Summary,"In computer science, a parallel algorithm, as opposed to a traditional serial algorithm, is an algorithm which can do multiple operations in a given time. It has been a tradition of computer science to describe serial algorithms in abstract machine models, often the one known as random-access machine. Similarly, many computer science researchers have used a so-called parallel random-access machine (PRAM) as a parallel abstract machine (shared-memory).","In computer science, a parallel algorithm, as opposed to a traditional serial algorithm, is an algorithm which can do multiple operations in a given time. It has been a tradition of computer science to describe serial algorithms in abstract machine models, often the one known as random-access machine.","[' What is a parallel algorithm?', ' What is an algorithm that can do multiple operations in a given time?', ' In computer science, serial algorithms are often described in what model?']","['an algorithm which can do multiple operations in a given time', 'parallel algorithm', 'random-access machine']"
2180,parallel algorithm,Summary,"Many parallel algorithms are executed concurrently – though in general concurrent algorithms are a distinct concept – and thus these concepts are often conflated, with which aspect of an algorithm is parallel and which is concurrent not being clearly distinguished. Further, non-parallel, non-concurrent algorithms are often referred to as ""sequential algorithms"", by contrast with concurrent algorithms.
","Many parallel algorithms are executed concurrently – though in general concurrent algorithms are a distinct concept – and thus these concepts are often conflated, with which aspect of an algorithm is parallel and which is concurrent not being clearly distinguished. Further, non-parallel, non-concurrent algorithms are often referred to as ""sequential algorithms"", by contrast with concurrent algorithms.","[' Many parallel algorithms are executed concurrently, but concurrent algorithms are a distinct concept?', ' What are non-parallel, non-concurrent algorithms referred to as?', ' What are non-parallel, non-concurrent algorithms often referred to as?', ' What are sequential algorithms compared to?']","['parallel', 'sequential algorithms', 'sequential algorithms', 'concurrent algorithms']"
2181,parallel algorithm,Parallelizability,"Algorithms vary significantly in how parallelizable they are, ranging from easily parallelizable to completely unparallelizable. Further, a given problem may accommodate different algorithms, which may be more or less parallelizable.
","Algorithms vary significantly in how parallelizable they are, ranging from easily parallelizable to completely unparallelizable. Further, a given problem may accommodate different algorithms, which may be more or less parallelizable.","[' Algorithms range from easily parallelizable to completely unparallelizable, what is the range?', ' A given problem may accommodate what?']","['how parallelizable', 'different algorithms']"
2182,parallel algorithm,Parallelizability,Some problems are easy to divide up into pieces in this way – these are called embarrassingly parallel problems. Examples include many algorithms to solve Rubik's Cubes and find values which result in a given hash.,Some problems are easy to divide up into pieces in this way – these are called embarrassingly parallel problems. Examples include many algorithms to solve Rubik's Cubes and find values which result in a given hash.,"[' What are problems that are easy to divide up into pieces called?', ' What are some examples of embarrassing parallel problems?']","['embarrassingly parallel problems', ""many algorithms to solve Rubik's Cubes and find values which result in a given hash""]"
2183,parallel algorithm,Parallelizability,"Some problems cannot be split up into parallel portions, as they require the results from a preceding step to effectively carry on with the next step – these are called inherently serial problems. Examples include iterative numerical methods, such as Newton's method, iterative solutions to the three-body problem, and most of the available algorithms to compute pi (π). Some sequential algorithms can be converted into parallel algorithms using automatic parallelization.","Some problems cannot be split up into parallel portions, as they require the results from a preceding step to effectively carry on with the next step – these are called inherently serial problems. Examples include iterative numerical methods, such as Newton's method, iterative solutions to the three-body problem, and most of the available algorithms to compute pi (π).","[' What are problems that require the results from a previous step to carry on with the next step called?', "" What is Newton's method an example of?"", "" Newton's method, iterative solutions to the three-body problem, and most of the available algorithms to compute pi (<unk>)?""]","['inherently serial problems', 'iterative numerical methods', 'π']"
2184,parallel algorithm,Motivation,"Parallel algorithms on individual devices have become more common since the early 2000s because of substantial improvements in multiprocessing systems and the rise of multi-core processors. Up until the end of 2004, single-core processor performance rapidly increased via frequency scaling, and thus it was easier to construct a computer with a single fast core than one with many slower cores with the same throughput, so multicore systems were of more limited use. Since 2004 however, frequency scaling hit a wall, and thus multicore systems have become more widespread, making parallel algorithms of more general use.
","Parallel algorithms on individual devices have become more common since the early 2000s because of substantial improvements in multiprocessing systems and the rise of multi-core processors. Up until the end of 2004, single-core processor performance rapidly increased via frequency scaling, and thus it was easier to construct a computer with a single fast core than one with many slower cores with the same throughput, so multicore systems were of more limited use.","[' Why have parallel algorithms become more common since the early 2000s?', ' Up until the end of 2004, how did single-core processor performance increase?', ' What was easier to construct a computer with a single fast core than one with many slower cores with the same throughput?', ' Multicore systems were of what use?']","['substantial improvements in multiprocessing systems and the rise of multi-core processors', 'frequency scaling', 'single-core processor performance rapidly increased via frequency scaling', 'limited']"
2185,robustness,Summary,"Robustness is the property of being strong and healthy in constitution. When it is transposed into a system, it refers to the ability of tolerating perturbations that might affect the system’s functional body. In the same line robustness can be defined as ""the ability of a system to resist change without adapting its initial stable configuration"". 
""Robustness in the small"" refers to situations wherein perturbations are small in magnitude, which considers that the ""small"" magnitude hypothesis can be difficult to verify because ""small"" or ""large"" depends on the specific problem. Conversely, ""Robustness in the large problem"" refers to situations wherein no assumptions can be made about the magnitude of perturbations, which can either be small or large.
It has been discussed that robustness has two dimensions: resistance and avoidance.","Robustness is the property of being strong and healthy in constitution. When it is transposed into a system, it refers to the ability of tolerating perturbations that might affect the system’s functional body.","[' What is the property of being strong and healthy in constitution?', ' When it is transposed into a system, it refers to the ability of tolerating what?']","['Robustness', 'perturbations']"
2186,resource allocation,Summary,"In economics, resource allocation is the assignment of available resources to various uses. In the context of an entire economy, resources can be allocated by various means, such as markets, or planning.
","In economics, resource allocation is the assignment of available resources to various uses. In the context of an entire economy, resources can be allocated by various means, such as markets, or planning.","[' In economics, what is the assignment of available resources to various uses?', ' In the context of an entire economy, how can resources be allocated?']","['resource allocation', 'by various means']"
2187,resource allocation,Economics,"In economics, the field of public finance deals with three broad areas: macroeconomic stabilization, the distribution of income and wealth, and the allocation of resources. Much of the study of the allocation of resources is devoted to finding the conditions under which particular mechanisms of resource allocation lead to Pareto efficient outcomes, in which no party's situation can be improved without hurting that of another party.
","In economics, the field of public finance deals with three broad areas: macroeconomic stabilization, the distribution of income and wealth, and the allocation of resources. Much of the study of the allocation of resources is devoted to finding the conditions under which particular mechanisms of resource allocation lead to Pareto efficient outcomes, in which no party's situation can be improved without hurting that of another party.","[' How many broad areas does the field of public finance deal with?', ' What is the focus of much of the study of the allocation of resources?', ' How are the conditions under which particular mechanisms of resource allocation lead to Pareto?', ' Under which mechanisms of resource allocation lead to Pareto efficient outcomes?', "" No party's situation can be improved without hurting that of another party?""]","['three', 'finding the conditions under which particular mechanisms of resource allocation lead to Pareto efficient outcomes', 'efficient outcomes', 'conditions', 'no party']"
2188,resource allocation,Strategic planning,"In strategic planning, resource allocation is a plan for using available resources,  for example human resources, especially in the near term, to achieve goals for the future. It is the process of allocating scarce resources among the various projects or business units.","In strategic planning, resource allocation is a plan for using available resources,  for example human resources, especially in the near term, to achieve goals for the future. It is the process of allocating scarce resources among the various projects or business units.","[' In strategic planning, what is a plan for using available resources?', ' What is the process of allocating scarce resources among the various projects or business units?']","['resource allocation', 'resource allocation']"
2189,resource allocation,Strategic planning,"There are a number of approaches to solving resource allocation problems e.g. resources can be allocated using a manual approach, an algorithmic approach (see below), or a combination of both.","There are a number of approaches to solving resource allocation problems e.g. resources can be allocated using a manual approach, an algorithmic approach (see below), or a combination of both.","[' How can resources be allocated using a manual approach or an algorithmic approach?', ' What can be used to solve resource allocation problems?']","['manual', 'manual approach']"
2190,resource allocation,Algorithms,"This is especially common in electronic devices dedicated to routing and communication. For example, channel allocation in wireless communication may be decided by a base transceiver station using an appropriate algorithm.","This is especially common in electronic devices dedicated to routing and communication. For example, channel allocation in wireless communication may be decided by a base transceiver station using an appropriate algorithm.","[' What kind of devices are used for routing and communication?', ' What may be decided by a base transceiver station using an appropriate algorithm?']","['electronic', 'channel allocation in wireless communication']"
2191,combinatorial optimization,Summary,"Combinatorial optimization is a subfield of mathematical optimization   that consists of finding an optimal object from a finite set of objects, where the set of feasible solutions is discrete or can be reduced to a discrete set. Typical combinatorial optimization problems are the travelling salesman problem (""TSP""), the minimum spanning tree problem (""MST""), and the knapsack problem. In many such problems, such as the ones previously mentioned, exhaustive search is not tractable, and so specialized algorithms that quickly rule out large parts of the search space or approximation algorithms must be resorted to instead.
","Combinatorial optimization is a subfield of mathematical optimization   that consists of finding an optimal object from a finite set of objects, where the set of feasible solutions is discrete or can be reduced to a discrete set. Typical combinatorial optimization problems are the travelling salesman problem (""TSP""), the minimum spanning tree problem (""MST""), and the knapsack problem.","[' What is a subfield of mathematical optimization?', ' Combinatorial optimization consists of finding an optimal object from a finite set of objects?', ' What is the travelling salesman problem?', ' What is the travel salesman problem?', ' What is MST?', ' The knapsack problem is what?']","['Combinatorial optimization', 'Combinatorial optimization', 'TSP', 'TSP', 'minimum spanning tree problem', 'combinatorial optimization problems']"
2192,combinatorial optimization,Summary,"Combinatorial optimization is related to operations research, algorithm theory, and computational complexity theory. It has important applications in several fields, including artificial intelligence, machine learning, auction theory, software engineering, applied mathematics and theoretical computer science.
","Combinatorial optimization is related to operations research, algorithm theory, and computational complexity theory. It has important applications in several fields, including artificial intelligence, machine learning, auction theory, software engineering, applied mathematics and theoretical computer science.","[' Combinatorial optimization is related to operations research, algorithm theory, and what other theory?', ' In addition to artificial intelligence, machine learning, auction theory, software engineering, applied mathematics, and theoretical computer science, what other field does combinatorial optimizing have important applications in?']","['computational complexity', 'computational complexity theory']"
2193,combinatorial optimization,Summary,"Some research literature considers discrete optimization to consist of integer programming together with combinatorial optimization (which in turn is composed of optimization problems dealing with graph structures), although all of these topics have closely intertwined research literature. It often involves determining the way to efficiently allocate resources used to find solutions to mathematical problems.","Some research literature considers discrete optimization to consist of integer programming together with combinatorial optimization (which in turn is composed of optimization problems dealing with graph structures), although all of these topics have closely intertwined research literature. It often involves determining the way to efficiently allocate resources used to find solutions to mathematical problems.","[' What does some research literature consider discrete optimization to consist of?', ' Combinatorial optimization is composed of optimization problems dealing with what?', ' What often involves determining the way to efficiently allocate resources used to find?', ' determining the way to efficiently allocate resources used to find solutions to what?']","['integer programming', 'graph structures', 'combinatorial optimization', 'mathematical problems']"
2194,combinatorial optimization,Methods,"There is a large amount of literature on polynomial-time algorithms for certain special classes of discrete optimization. A considerable amount of it is unified by the theory of linear programming. Some examples of combinatorial optimization problems that are covered by this framework are shortest paths and shortest-path trees, flows and circulations, spanning trees, matching, and matroid problems.
",There is a large amount of literature on polynomial-time algorithms for certain special classes of discrete optimization. A considerable amount of it is unified by the theory of linear programming.,"[' What is a large amount of literature on?', ' What is the theory of linear programming unified by?']","['polynomial-time algorithms', 'polynomial-time algorithms']"
2195,combinatorial optimization,Methods,"Combinatorial optimization problems can be viewed as searching for the best element of some set of discrete items; therefore, in principle, any sort of search algorithm or metaheuristic can be used to solve them. Perhaps the most universally applicable approaches are branch-and-bound (an exact algorithm which can be stopped at any point in time to serve as heuristic), branch-and-cut (uses linear optimisation to generate bounds), dynamic programming (a recursive solution construction with limited search window) and tabu search (a greedy-type swapping algorithm). However, generic search algorithms are not guaranteed to find an optimal solution first, nor are they guaranteed to run quickly (in polynomial time). Since some discrete optimization problems are NP-complete, such as the traveling salesman (decision) problem, this is expected unless P=NP.
","Combinatorial optimization problems can be viewed as searching for the best element of some set of discrete items; therefore, in principle, any sort of search algorithm or metaheuristic can be used to solve them. Perhaps the most universally applicable approaches are branch-and-bound (an exact algorithm which can be stopped at any point in time to serve as heuristic), branch-and-cut (uses linear optimisation to generate bounds), dynamic programming (a recursive solution construction with limited search window) and tabu search (a greedy-type swapping algorithm).","[' What can be viewed as searching for the best element of some set of discrete items?', ' What is one of the most universally applicable approaches?', ' Branch-and-bound is an exact algorithm which can be stopped at what point?', ' What type of algorithm is branch-and-bound?', ' What kind of algorithm uses linear optimisation to generate bounds?']","['Combinatorial optimization problems', 'branch-and-bound', 'any point in time', 'an exact', 'branch-and-cut']"
2196,combinatorial optimization,Formal definition,"For each combinatorial optimization problem, there is a corresponding decision problem that asks whether there is a feasible solution for some particular measure 




m

0




{\displaystyle m_{0}}
. For example, if there is a graph 



G


{\displaystyle G}
 which contains vertices 



u


{\displaystyle u}
 and 



v


{\displaystyle v}
, an optimization problem might be ""find a path from 



u


{\displaystyle u}
 to 



v


{\displaystyle v}
 that uses the fewest edges"". This problem might have an answer of, say, 4. A corresponding decision problem would be ""is there a path from 



u


{\displaystyle u}
 to 



v


{\displaystyle v}
 that uses 10 or fewer edges?"" This problem can be answered with a simple 'yes' or 'no'.
","For each combinatorial optimization problem, there is a corresponding decision problem that asks whether there is a feasible solution for some particular measure 




m

0




{\displaystyle m_{0}}
. For example, if there is a graph 



G


{\displaystyle G}
 which contains vertices 



u


{\displaystyle u}
 and 



v


{\displaystyle v}
, an optimization problem might be ""find a path from 



u


{\displaystyle u}
 to 



v


{\displaystyle v}
 that uses the fewest edges"".","[' What is the corresponding decision problem for each combinatorial optimization problem?', ' What might an optimization problem be called?', ' v <unk>displaystyle v<unk> is an optimization problem?']","['m\n\n0\n\n\n\n\n{\\displaystyle m_{0}}', 'find a path from \n\n\n\nu\n\n\n{\\displaystyle u}\n to \n\n\n\nv\n\n\n{\\displaystyle v}\n that uses the fewest edges"".', 'v']"
2197,combinatorial optimization,Formal definition,"The field of approximation algorithms deals with algorithms to find near-optimal solutions to hard problems. The usual decision version is then an inadequate definition of the problem since it only specifies acceptable solutions. Even though we could introduce suitable decision problems, the problem is then more naturally characterized as an optimization problem.",The field of approximation algorithms deals with algorithms to find near-optimal solutions to hard problems. The usual decision version is then an inadequate definition of the problem since it only specifies acceptable solutions.,"[' What field deals with algorithms to find near-optimal solutions to hard problems?', ' What is the usual decision version of a problem?']","['approximation algorithms', 'an inadequate definition']"
2198,combinatorial optimization,NP optimization problem,"An NP-optimization problem (NPO) is a combinatorial optimization problem with the following additional conditions. Note that the below referred polynomials are functions of the size of the respective functions' inputs, not the size of some implicit set of input instances.
","An NP-optimization problem (NPO) is a combinatorial optimization problem with the following additional conditions. Note that the below referred polynomials are functions of the size of the respective functions' inputs, not the size of some implicit set of input instances.","[' What is an NP-optimization problem?', ' What are the below referred polynomials?']","['combinatorial optimization problem with the following additional conditions', ""functions of the size of the respective functions' inputs""]"
2199,combinatorial optimization,NP optimization problem,"This implies that the corresponding decision problem is in NP. In computer science, interesting optimization problems usually have the above properties and are therefore NPO problems. A problem is additionally called a P-optimization (PO) problem, if there exists an algorithm which finds optimal solutions in polynomial time. Often, when dealing with the class NPO, one is interested in optimization problems for which the decision versions are NP-complete. Note that hardness relations are always with respect to some reduction. Due to the connection between approximation algorithms and computational optimization problems, reductions which preserve approximation in some respect are for this subject preferred than the usual Turing and Karp reductions. An example of such a reduction would be L-reduction. For this reason, optimization problems with NP-complete decision versions are not necessarily called NPO-complete.","This implies that the corresponding decision problem is in NP. In computer science, interesting optimization problems usually have the above properties and are therefore NPO problems.","[' What implies that the corresponding decision problem is in NP?', ' What are interesting optimization problems usually have?']","['This', 'the above properties']"
2200,combinatorial optimization,NP optimization problem,"An NPO problem is called polynomially bounded (PB) if, for every instance 



x


{\displaystyle x}
 and for every solution 



y
∈
f
(
x
)


{\displaystyle y\in f(x)}
, the measure 



m
(
x
,
y
)


{\displaystyle m(x,y)}
is bounded by a polynomial function of the size of 



x


{\displaystyle x}
. The class NPOPB is the class of NPO problems that are polynomially-bounded.
","An NPO problem is called polynomially bounded (PB) if, for every instance 



x


{\displaystyle x}
 and for every solution 



y
∈
f
(
x
)


{\displaystyle y\in f(x)}
, the measure 



m
(
x
,
y
)


{\displaystyle m(x,y)}
is bounded by a polynomial function of the size of 



x


{\displaystyle x}
. The class NPOPB is the class of NPO problems that are polynomially-bounded.","[' What is an NPO problem called?', ' What is the class of NPO problems that are polynomially bounded?']","['polynomially bounded', 'NPOPB']"
2201,competitive ratio,Summary,"Competitive analysis is a method invented for analyzing online algorithms, in which the performance of an online algorithm (which must satisfy an unpredictable sequence of requests, completing each request without being able to see the future) is compared to the performance of an optimal offline algorithm that can view the sequence of requests in advance.  An algorithm is competitive if its competitive ratio—the ratio between its performance and the offline algorithm's performance—is bounded.  Unlike traditional worst-case analysis, where the performance of an algorithm is measured only for ""hard"" inputs, competitive analysis requires that an algorithm perform well both on hard and easy inputs, where ""hard"" and ""easy"" are defined by the performance of the optimal offline algorithm.
","Competitive analysis is a method invented for analyzing online algorithms, in which the performance of an online algorithm (which must satisfy an unpredictable sequence of requests, completing each request without being able to see the future) is compared to the performance of an optimal offline algorithm that can view the sequence of requests in advance. An algorithm is competitive if its competitive ratio—the ratio between its performance and the offline algorithm's performance—is bounded.","[' Competitive analysis is a method invented for analyzing what?', ' Competitive analysis compares the performance of an online algorithm to that of an optimal offline algorithm?', "" What is the competitive ratio between an algorithm's performance and the offline algorithm?"", ' What is an algorithm competitive if its competitive ratio is bounded?']","['online algorithms', 'Competitive analysis', 'bounded', ""performance and the offline algorithm's performance""]"
2202,competitive ratio,Summary,"For many algorithms, performance is dependent not only on the size of the inputs, but also on their values.  For example, sorting an array of elements varies in difficulty depending on the initial order.  Such data-dependent algorithms are analysed for average-case and worst-case data.  Competitive analysis is a way of doing worst case analysis for on-line and randomized algorithms, which are typically data dependent.
","For many algorithms, performance is dependent not only on the size of the inputs, but also on their values. For example, sorting an array of elements varies in difficulty depending on the initial order.","[' For many algorithms, performance is dependent not only on the size of what?', ' For example, sorting an array of elements varies in difficulty depending on the initial order?']","['inputs', 'performance']"
2203,competitive ratio,Summary,"In competitive analysis, one imagines an ""adversary"" which deliberately chooses difficult data, to maximize the ratio of the cost of the algorithm being studied and some optimal algorithm.  When considering a randomized algorithm, one must further distinguish between an oblivious adversary, which has no knowledge of the random choices made by the algorithm pitted against it, and an adaptive adversary which has full knowledge of the algorithm's internal state at any point during its execution.  (For a deterministic algorithm, there is no difference; either adversary can simply compute what state that algorithm must have at any time in the future, and choose difficult data accordingly.)
","In competitive analysis, one imagines an ""adversary"" which deliberately chooses difficult data, to maximize the ratio of the cost of the algorithm being studied and some optimal algorithm. When considering a randomized algorithm, one must further distinguish between an oblivious adversary, which has no knowledge of the random choices made by the algorithm pitted against it, and an adaptive adversary which has full knowledge of the algorithm's internal state at any point during its execution.","[' What is an adversary that deliberately chooses difficult data to maximize the ratio of the cost of the algorithm being studied and some optimal algorithm?', ' When considering a randomized algorithm, what must one distinguish between an oblivious adversary which has no knowledge of the random choices made?', ' What is an adversary that has no knowledge of the random choices made by the algorithm pitted against it?', ' What does an adaptive adversary have knowledge of at any point during its execution?']","['oblivious adversary', 'the algorithm pitted against it', 'oblivious', ""the algorithm's internal state""]"
2204,competitive ratio,Summary,"For example, the quicksort algorithm chooses one element, called the ""pivot"", that is, on average, not too far from the center value of the data being sorted. Quicksort then separates the data into two piles, one of which contains all elements with value less than the value of the pivot, and the other containing the rest of the elements.  If quicksort chooses the pivot in some deterministic fashion (for instance, always choosing the first element in the list), then it is easy for an adversary to arrange the data beforehand so that quicksort will perform in worst-case time.  If, however, quicksort chooses some random element to be the pivot, then an adversary without knowledge of what random numbers are coming up cannot arrange the data to guarantee worst-case execution time for quicksort.
","For example, the quicksort algorithm chooses one element, called the ""pivot"", that is, on average, not too far from the center value of the data being sorted. Quicksort then separates the data into two piles, one of which contains all elements with value less than the value of the pivot, and the other containing the rest of the elements.","[' What is the name of the element that the quicksort algorithm chooses?', ' How many piles of data does quicksort split data into?', ' What does one of the piles contain?', ' What element has value less than the value of the pivot?', ' What element contains the rest of the elements?']","['pivot', 'two', 'all elements with value less than the value of the pivot', 'all elements', 'the other']"
2205,competitive ratio,Summary,"The classic on-line problem first analysed with competitive analysis (Sleator & Tarjan 1985) is the list update problem: Given a list of items and a sequence of requests for the various items, minimize the cost of accessing the list where the elements closer to the front of the list cost less to access.  (Typically, the cost of accessing an item is equal to its position in the list.)  After an access, the list may be rearranged.  Most rearrangements have a cost.  The Move-To-Front algorithm simply moves the requested item to the front after the access, at no cost.  The Transpose algorithm swaps the accessed item with the item immediately before it, also at no cost.  Classical methods of analysis showed that Transpose is optimal in certain contexts.  In practice, Move-To-Front performed much better.  Competitive analysis was used to show that an adversary can make Transpose perform arbitrarily badly compared to an optimal algorithm, whereas Move-To-Front can never be made to incur more than twice the cost of an optimal algorithm.
","The classic on-line problem first analysed with competitive analysis (Sleator & Tarjan 1985) is the list update problem: Given a list of items and a sequence of requests for the various items, minimize the cost of accessing the list where the elements closer to the front of the list cost less to access. (Typically, the cost of accessing an item is equal to its position in the list.)","[' What is the classic on-line problem first analysed with competitive analysis?', ' What was the list update problem?', ' The elements closer to the front of the list cost less to access what?', ' The cost of accessing an item is typically equal to its position in the list?']","['the list update problem', 'Given a list of items and a sequence of requests for the various items, minimize the cost of accessing the list where the elements closer to the front of the list cost less to access', 'the list', 'list update problem']"
2206,competitive ratio,Summary,"In the case of online requests from a server, competitive algorithms are used to overcome uncertainties about the future. That is, the algorithm does not ""know"" the future, while the imaginary adversary (the ""competitor"") ""knows"". Similarly, competitive algorithms were developed for distributed systems, where the algorithm has to react to a request arriving at one location, without ""knowing"" what has just happened in a remote location. This setting was presented in (Awerbuch, Kutten & Peleg 1992).
","In the case of online requests from a server, competitive algorithms are used to overcome uncertainties about the future. That is, the algorithm does not ""know"" the future, while the imaginary adversary (the ""competitor"") ""knows"".","[' What are competitive algorithms used to overcome?', ' What does the algorithm not know?', ' Who is the imaginary adversary?']","['uncertainties about the future', 'the future', 'the ""competitor']"
2207,function symbol,Summary,"In formal logic and related branches of mathematics, a functional predicate, or function symbol, is a logical symbol that may be applied to an object term to produce another object term.
Functional predicates are also sometimes called mappings, but that term has additional meanings in mathematics.
In a model, a function symbol will be modelled by a function.
","In formal logic and related branches of mathematics, a functional predicate, or function symbol, is a logical symbol that may be applied to an object term to produce another object term. Functional predicates are also sometimes called mappings, but that term has additional meanings in mathematics.","[' What is a functional predicate?', ' What is another term for a function symbol?', ' Functional predicates are sometimes called what?']","['a logical symbol that may be applied to an object term to produce another object term', 'functional predicate', 'mappings']"
2208,function symbol,Summary,"Specifically, the symbol F in a formal language is a functional symbol if, given any symbol X representing an object in the language, F(X) is again a symbol representing an object in that language.
In typed logic, F is a functional symbol with domain type T and codomain type U if, given any symbol X representing an object of type T, F(X) is a symbol representing an object of type U.
One can similarly define function symbols of more than one variable, analogous to functions of more than one variable; a function symbol in zero variables is simply a constant symbol.
","Specifically, the symbol F in a formal language is a functional symbol if, given any symbol X representing an object in the language, F(X) is again a symbol representing an object in that language. In typed logic, F is a functional symbol with domain type T and codomain type U if, given any symbol X representing an object of type T, F(X) is a symbol representing an object of type U.","[' What is a functional symbol with domain type T and codomain type U in typed logic?', ' What is the symbol F in a formal language?', ' What is a functional symbol with domain type T and codomain type U?', ' What is F(X) a symbol representing?']","['F', 'a functional symbol', 'F', 'an object of type U']"
2209,function symbol,Summary,"Now consider a model of the formal language, with the types T and U modelled by sets [T] and [U] and each symbol X of type T modelled by an element [X] in [T].
Then F can be modelled by the set
","Now consider a model of the formal language, with the types T and U modelled by sets [T] and [U] and each symbol X of type T modelled by an element [X] in [T]. Then F can be modelled by the set","[' What can be modelled by the set [T] and [U]?', ' What can each symbol of type T be modeled by?']","['F', 'an element [X] in [T].']"
2210,function symbol,Summary,"which is simply a function with domain [T] and codomain [U].
It is a requirement of a consistent model that [F(X)] = [F(Y)] whenever [X] = [Y].
",which is simply a function with domain [T] and codomain [U]. It is a requirement of a consistent model that [F(X)] = [F(Y)] whenever [X] = [Y].,"[' What is simply a function with domain [T] and codomain [U]?', ' What is a requirement of a consistent model?']","['which is simply a function with domain [T] and codomain [U]. It is a requirement of a consistent model that [F(X)] = [F(Y)] whenever [X] = [Y].', '[F(X)] = [F(Y)] whenever [X] = [Y].']"
2211,function symbol,Introducing new function symbols,"In a treatment of predicate logic that allows one to introduce new predicate symbols, one will also want to be able to introduce new function symbols. Given the function symbols F and G, one can introduce a new function symbol F ∘ G, the composition of F and G, satisfying (F ∘ G)(X) = F(G(X)), for all X.
Of course, the right side of this equation doesn't make sense in typed logic unless the domain type of F matches the codomain type of G, so this is required for the composition to be defined.
","In a treatment of predicate logic that allows one to introduce new predicate symbols, one will also want to be able to introduce new function symbols. Given the function symbols F and G, one can introduce a new function symbol F ∘ G, the composition of F and G, satisfying (F ∘ G)(X) = F(G(X)), for all X.","[' What will one want to be able to introduce in a treatment of predicate logic that allows one to introduce new function symbols?', ' What can one introduce a new function symbol F <unk> G?', ' What is the composition of F and G?', ' For all X, what is satisfying (F <unk> G) = F(G(X)?']","['function symbol', 'the composition of F and G, satisfying (F ∘ G)(X) = F(G(X)), for all X', 'F ∘ G', 'function symbols']"
2212,function symbol,Introducing new function symbols,"One also gets certain function symbols automatically.
In untyped logic, there is an identity predicate id that satisfies id(X) = X for all X.
In typed logic, given any type T, there is an identity predicate idT with domain and codomain type T; it satisfies idT(X) = X for all X of type T.
Similarly, if T is a subtype of U, then there is an inclusion predicate of domain type T and codomain type U that satisfies the same equation; there are additional function symbols associated with other ways of constructing new types out of old ones.
","One also gets certain function symbols automatically. In untyped logic, there is an identity predicate id that satisfies id(X) = X for all X.","[' In untyped logic, what does id(X) = X satisfy for all X?']",['identity predicate id']
2213,function symbol,Introducing new function symbols,"Additionally, one can define functional predicates after proving an appropriate theorem.
(If you're working in a formal system that doesn't allow you to introduce new symbols after proving theorems, then you will have to use relation symbols to get around this, as in the next section.)
Specifically, if you can prove that for every X (or every X of a certain type), there exists a unique Y satisfying some condition P, then you can introduce a function symbol F to indicate this.
Note that P will itself be a relational predicate involving both X and Y.
So if there is such a predicate P and a theorem:
","Additionally, one can define functional predicates after proving an appropriate theorem. (If you're working in a formal system that doesn't allow you to introduce new symbols after proving theorems, then you will have to use relation symbols to get around this, as in the next section.)","[' How can one define functional predicates after proving an appropriate theorem?', "" What do you have to use if you're working in a formal system that doesn't allow you to introduce new symbols?""]","['relation symbols', 'relation symbols']"
2214,function symbol,Doing without functional predicates,"Many treatments of predicate logic don't allow functional predicates, only relational predicates.
This is useful, for example, in the context of proving metalogical theorems (such as Gödel's incompleteness theorems), where one doesn't want to allow the introduction of new functional symbols (nor any other new symbols, for that matter).
But there is a method of replacing functional symbols with relational symbols wherever the former may occur; furthermore, this is algorithmic and thus suitable for applying most metalogical theorems to the result.
","Many treatments of predicate logic don't allow functional predicates, only relational predicates. This is useful, for example, in the context of proving metalogical theorems (such as Gödel's incompleteness theorems), where one doesn't want to allow the introduction of new functional symbols (nor any other new symbols, for that matter).","["" What do many treatments of predicate logic don't allow?"", "" What does Gödel's incompleteness theorems prove?""]","['functional predicates', 'metalogical theorems']"
2215,function symbol,Doing without functional predicates,"Specifically, if F has domain type T and codomain type U, then it can be replaced with a predicate P of type (T,U).
Intuitively, P(X,Y) means F(X) = Y.
Then whenever F(X) would appear in a statement, you can replace it with a new symbol Y of type U and include another statement P(X,Y).
To be able to make the same deductions, you need an additional proposition:
","Specifically, if F has domain type T and codomain type U, then it can be replaced with a predicate P of type (T,U). Intuitively, P(X,Y) means F(X) = Y.","[' What can be replaced with a predicate of type (T,U)?', ' Intuitively, P(X,Y) means F(X) = Y?']","['P', 'F has domain type T and codomain type U']"
2216,function symbol,Doing without functional predicates,"Because the elimination of functional predicates is both convenient for some purposes and possible, many treatments of formal logic do not deal explicitly with function symbols but instead use only relation symbols; another way to think of this is that a functional predicate is a special kind of predicate, specifically one that satisfies the proposition above.
This may seem to be a problem if you wish to specify a proposition schema that applies only to functional predicates F; how do you know ahead of time whether it satisfies that condition?
To get an equivalent formulation of the schema, first replace anything of the form F(X) with a new variable Y.
Then universally quantify over each Y immediately after the corresponding X is introduced (that is, after X is quantified over, or at the beginning of the statement if X is free), and guard the quantification with P(X,Y).
Finally, make the entire statement a material consequence of the uniqueness condition for a functional predicate above.
","Because the elimination of functional predicates is both convenient for some purposes and possible, many treatments of formal logic do not deal explicitly with function symbols but instead use only relation symbols; another way to think of this is that a functional predicate is a special kind of predicate, specifically one that satisfies the proposition above. This may seem to be a problem if you wish to specify a proposition schema that applies only to functional predicates F; how do you know ahead of time whether it satisfies that condition?","[' What is a special kind of predicate?', ' What is convenient for some purposes and possible?', ' A functional predicate is a special kind of what?']","['functional predicate', 'elimination of functional predicates', 'predicate']"
2217,function symbol,Doing without functional predicates,"Let us take as an example the axiom schema of replacement in Zermelo–Fraenkel set theory.
(This example uses mathematical symbols.)
This schema states (in one form), for any functional predicate F in one variable:
",Let us take as an example the axiom schema of replacement in Zermelo–Fraenkel set theory. (This example uses mathematical symbols.),[' What is the axiom schema of replacement in Zermelo-Fraenkel set theory?'],['mathematical symbols']
2218,function symbol,Doing without functional predicates,"This version of the axiom schema of replacement is now suitable for use in a formal language that doesn't allow the introduction of new function symbols. Alternatively, one may interpret the original statement as a statement in such a formal language; it was merely an abbreviation for the statement produced at the end.
","This version of the axiom schema of replacement is now suitable for use in a formal language that doesn't allow the introduction of new function symbols. Alternatively, one may interpret the original statement as a statement in such a formal language; it was merely an abbreviation for the statement produced at the end.","[' What language does the axiom schema of replacement not allow the introduction of new function symbols?', ' What is the abbreviation for the statement produced?', ' What was the abbreviation for the statement produced at the end?']","['formal', 'at the end', 'axiom schema']"
2219,cognitive radio,Summary,"
A cognitive radio (CR) is a radio that can be programmed and configured dynamically to use the best wireless channels in its vicinity to avoid user interference and congestion. Such a radio automatically detects available channels in wireless spectrum, then accordingly changes its transmission or reception parameters to allow more concurrent wireless communications in a given spectrum band at one location. This process is a form of dynamic spectrum management.
","
A cognitive radio (CR) is a radio that can be programmed and configured dynamically to use the best wireless channels in its vicinity to avoid user interference and congestion. Such a radio automatically detects available channels in wireless spectrum, then accordingly changes its transmission or reception parameters to allow more concurrent wireless communications in a given spectrum band at one location.","[' What type of radio can be programmed and configured dynamically to avoid user interference and congestion?', ' A cognitive radio automatically detects available channels in what spectrum?', ' How many concurrent wireless communications are possible in a given spectrum band at one location?', ' What is the purpose of changing transmission or reception parameters?']","['cognitive radio', 'wireless', 'cognitive radio', 'to allow more concurrent wireless communications in a given spectrum band at one location']"
2220,cognitive radio,Description,"In response to the operator's commands, the cognitive engine is capable of configuring radio-system parameters. These parameters include ""waveform, protocol, operating frequency, and networking"". This functions as an autonomous unit in the communications environment, exchanging information about the environment with the networks it accesses and other cognitive radios (CRs). A CR ""monitors its own performance continuously"", in addition to ""reading the radio's outputs""; it then uses this information to ""determine the RF environment, channel conditions, link performance, etc."", and adjusts the ""radio's settings to deliver the required quality of service subject to an appropriate combination of user requirements, operational limitations, and regulatory constraints"".
","In response to the operator's commands, the cognitive engine is capable of configuring radio-system parameters. These parameters include ""waveform, protocol, operating frequency, and networking"".","[' What is the cognitive engine capable of configuring?', ' What are some radio-system parameters?']","['radio-system parameters', 'waveform, protocol, operating frequency, and networking']"
2221,cognitive radio,History,"The concept of cognitive radio was first proposed by Joseph Mitola III in a seminar at KTH Royal Institute of Technology in Stockholm in 1998 and published in an article by Mitola and Gerald Q. Maguire, Jr. in 1999. It was a novel approach in wireless communications, which Mitola later described as:
","The concept of cognitive radio was first proposed by Joseph Mitola III in a seminar at KTH Royal Institute of Technology in Stockholm in 1998 and published in an article by Mitola and Gerald Q. Maguire, Jr. in 1999. It was a novel approach in wireless communications, which Mitola later described as:","[' Who first proposed the concept of cognitive radio?', ' What was the name of the seminar that Joseph Mitola III attended in 1998?', ' When was cognitive radio first published?', ' Who published an article on cognitive radio in 1999?', ' What did Mitola describe as a novel approach in wireless communications?', ' What was the name of the new approach?']","['Joseph Mitola III', 'KTH Royal Institute of Technology', '1999', 'Mitola and Gerald Q. Maguire, Jr.', 'cognitive radio', 'cognitive radio']"
2222,cognitive radio,History,"Traditional regulatory structures have been built for an analog model and are not optimized for cognitive radio. Regulatory bodies in the world (including the Federal Communications Commission in the United States and Ofcom in the United Kingdom) as well as different independent measurement campaigns found that most radio frequency spectrum was inefficiently utilized. Cellular network bands are overloaded in most parts of the world, but other frequency bands (such as military, amateur radio and paging frequencies) are insufficiently utilized. Independent studies performed in some countries confirmed that observation, and concluded that spectrum utilization depends on time and place. Moreover, fixed spectrum allocation prevents rarely used frequencies (those assigned to specific services) from being used, even when any unlicensed users would not cause noticeable interference to the assigned service. Regulatory bodies in the world have been considering whether to allow unlicensed users in licensed bands if they would not cause any interference to licensed users. These initiatives have focused cognitive-radio research on dynamic spectrum access.
",Traditional regulatory structures have been built for an analog model and are not optimized for cognitive radio. Regulatory bodies in the world (including the Federal Communications Commission in the United States and Ofcom in the United Kingdom) as well as different independent measurement campaigns found that most radio frequency spectrum was inefficiently utilized.,"[' What have traditional regulatory structures been built for?', ' Traditional regulatory structures are not optimized for what?', ' Which regulatory bodies in the world found that most radio frequency spectrum spectrum?', ' What was found to be inefficiently utilized by different measurement campaigns?']","['an analog model', 'cognitive radio', 'Federal Communications Commission in the United States and Ofcom in the United Kingdom', 'radio frequency spectrum']"
2223,cognitive radio,History,"The first cognitive radio wireless regional area network standard, IEEE 802.22, was developed by IEEE 802 LAN/MAN Standard Committee (LMSC) and published in 2011.  This standard uses geolocation and spectrum sensing for spectral awareness.  Geolocation combines with a database of licensed transmitters in the area to identify available channels for use by the cognitive radio network.  Spectrum sensing observes the spectrum and identifies occupied channels.  IEEE 802.22 was designed to utilize the unused frequencies or fragments of time in a location.  This white space is unused television channels in the geolocated areas.  However, cognitive radio cannot occupy the same unused space all the time.  As spectrum availability changes, the network adapts to prevent interference with licensed transmissions.","The first cognitive radio wireless regional area network standard, IEEE 802.22, was developed by IEEE 802 LAN/MAN Standard Committee (LMSC) and published in 2011. This standard uses geolocation and spectrum sensing for spectral awareness.","[' What is the name of the first cognitive radio wireless regional area network standard?', ' Who developed IEEE 802.22?', ' When was the first Cognitive Radio Wireless Regional Area Network standard published?', ' What does the IEEE 802.22 use for spectral awareness?']","['IEEE 802.22', 'IEEE 802 LAN/MAN Standard Committee (LMSC)', '2011', 'geolocation and spectrum sensing']"
2224,cognitive radio,Technology,"Although cognitive radio was initially thought of as a software-defined radio extension (full cognitive radio), most research work focuses on spectrum-sensing cognitive radio (particularly in the TV bands). The chief problem in spectrum-sensing cognitive radio is designing high-quality spectrum-sensing devices and algorithms for exchanging spectrum-sensing data between nodes. It has been shown that a simple energy detector cannot guarantee the accurate detection of signal presence, calling for more sophisticated spectrum sensing techniques and requiring information about spectrum sensing to be regularly exchanged between nodes. Increasing the number of cooperating sensing nodes decreases the probability of false detection.","Although cognitive radio was initially thought of as a software-defined radio extension (full cognitive radio), most research work focuses on spectrum-sensing cognitive radio (particularly in the TV bands). The chief problem in spectrum-sensing cognitive radio is designing high-quality spectrum-sensing devices and algorithms for exchanging spectrum-sensing data between nodes.","[' What was cognitive radio originally thought of as?', ' What is the chief problem in spectrum-sensing cognitive radio?']","['software-defined radio extension', 'designing high-quality spectrum-sensing devices and algorithms for exchanging spectrum-sensing data between nodes']"
2225,cognitive radio,Technology,"Filling free RF bands adaptively, using OFDMA, is a possible approach. Timo A. Weiss and Friedrich K. Jondral of the University of Karlsruhe proposed a spectrum pooling system, in which free bands (sensed by nodes) were immediately filled by OFDMA subbands. Applications of spectrum-sensing cognitive radio include emergency-network and WLAN higher throughput and transmission-distance extensions. The evolution of cognitive radio toward cognitive networks is underway; the concept of cognitive networks is to intelligently organize a network of cognitive radios.
","Filling free RF bands adaptively, using OFDMA, is a possible approach. Timo A. Weiss and Friedrich K. Jondral of the University of Karlsruhe proposed a spectrum pooling system, in which free bands (sensed by nodes) were immediately filled by OFDMA subbands.","[' What is a possible approach to filling free RF bands?', ' What did Timo A. Weiss and Friedrich K. Jondral propose?']","['adaptively, using OFDMA', 'a spectrum pooling system']"
2226,cognitive radio,Applications,"Cognitive Radio (CR) can sense its environment and, without the intervention of the user, can adapt to the user's communications needs while conforming to FCC rules in the United States. In theory, the amount of spectrum is infinite; practically, for propagation and other reasons it is finite because of the desirability of certain spectrum portions. Assigned spectrum is far from being fully utilized, and efficient spectrum use is a growing concern; CR offers a solution to this problem. A CR can intelligently detect whether any portion of the spectrum is in use, and can temporarily use it without interfering with the transmissions of other users. According to Bruce Fette, ""Some of the radio's other cognitive abilities include determining its location, sensing spectrum use by neighboring devices, changing frequency, adjusting output power or even altering transmission parameters and characteristics. All of these capabilities, and others yet to be realized, will provide wireless spectrum users with the ability to adapt to real-time spectrum conditions, offering regulators, licenses and the general public flexible, efficient and comprehensive use of the spectrum"".
","Cognitive Radio (CR) can sense its environment and, without the intervention of the user, can adapt to the user's communications needs while conforming to FCC rules in the United States. In theory, the amount of spectrum is infinite; practically, for propagation and other reasons it is finite because of the desirability of certain spectrum portions.","[' What is Cognitive Radio?', "" Cognitive Radio can sense its environment and adapt to the user's communications needs while conforming to FCC rules in the United States?"", ' What is the amount of spectrum in theory?', ' Why is propagation finite?', ' Why are certain spectrum portions desirable?']","['can sense its environment', 'Cognitive Radio', 'infinite', 'desirability of certain spectrum portions', 'desirability']"
2227,cognitive radio,Simulation of CR networks,"At present, modeling & simulation is the only paradigm which allows the simulation of complex behavior in a given environment's cognitive radio networks. Network simulators like OPNET, NetSim, MATLAB and ns2 can be used to simulate a cognitive radio network. CogNS  is an open-source NS2-based  simulation framework for cognitive radio networks. Areas of research using network simulators include:
","At present, modeling & simulation is the only paradigm which allows the simulation of complex behavior in a given environment's cognitive radio networks. Network simulators like OPNET, NetSim, MATLAB and ns2 can be used to simulate a cognitive radio network.","["" What is the only paradigm that allows the simulation of complex behavior in a given environment's cognitive radio networks?"", ' Network simulators like OPNET, NetSim, MATLAB and ns2 can be used to simulate what?']","['modeling & simulation', 'a cognitive radio network']"
2228,cognitive radio,Simulation of CR networks,Network Simulator 3 (ns-3) is also a viable option for simulating CR. ns-3 can be also used to emulate and experiment CR networks with the aid from commodity hardware like Atheros WiFi devices.,Network Simulator 3 (ns-3) is also a viable option for simulating CR. ns-3 can be also used to emulate and experiment CR networks with the aid from commodity hardware like Atheros WiFi devices.,"[' What is Network Simulator 3?', ' What can ns-3 emulate and experiment with?']","['simulating CR', 'CR networks']"
2229,cognitive radio,Future plans,"The success of the unlicensed band in accommodating a range of wireless devices and services has led the FCC to consider opening further bands for unlicensed use. In contrast, the licensed bands are underutilized due to static frequency allocation. Realizing that CR technology has the potential to exploit the inefficiently utilized licensed bands without causing interference to incumbent users, the FCC released a Notice of Proposed Rule Making which would allow unlicensed radios to operate in the TV-broadcast bands. The IEEE 802.22 working group, formed in November 2004, is tasked with defining the air-interface standard for wireless regional area networks (based on CR sensing) for the operation of unlicensed devices in the spectrum allocated to TV service. To comply with later FCC regulations on unlicensed utilization of TV spectrum, the IEEE 802.22 has defined interfaces to the mandatory TV White Space Database in order to avoid interference to incumbent services.","The success of the unlicensed band in accommodating a range of wireless devices and services has led the FCC to consider opening further bands for unlicensed use. In contrast, the licensed bands are underutilized due to static frequency allocation.","[' What has led the FCC to consider opening further bands for unlicensed use?', ' What is underutilized due to static frequency allocation?']","['The success of the unlicensed band in accommodating a range of wireless devices and services', 'licensed bands']"
2230,information extraction,Summary,"Information extraction (IE) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents and other electronically represented sources. In most of the cases this activity concerns processing human language texts by means of natural language processing (NLP). Recent activities in multimedia document processing like automatic annotation and content extraction out of images/audio/video/documents could be seen as information extraction
",Information extraction (IE) is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents and other electronically represented sources. In most of the cases this activity concerns processing human language texts by means of natural language processing (NLP).,"[' What is the task of automatically extracting structured information from unstructured and/or semi-structured machine-readable documents and other electronically represented sources?', ' In most cases this activity concerns processing human language texts by means of what?']","['Information extraction (IE)', 'natural language processing']"
2231,information extraction,Summary,"Due to the difficulty of the problem, current approaches to IE focus on narrowly restricted domains.  An example is the extraction from newswire reports of corporate mergers, such as denoted by the formal relation: 
","Due to the difficulty of the problem, current approaches to IE focus on narrowly restricted domains. An example is the extraction from newswire reports of corporate mergers, such as denoted by the formal relation:","[' Why do current approaches to IE focus on narrowly restricted domains?', ' What is an example of extraction from newswire reports of corporate mergers denoted by the formal relation?']","['the difficulty of the problem', 'narrowly restricted domains']"
2232,information extraction,Summary,"A broad goal of IE is to allow computation to be done on the previously unstructured data.  A more specific goal is to allow logical reasoning to draw inferences based on the logical content of the input data.  Structured data is semantically well-defined data from a chosen target domain, interpreted with respect to category and context.
",A broad goal of IE is to allow computation to be done on the previously unstructured data. A more specific goal is to allow logical reasoning to draw inferences based on the logical content of the input data.,"[' What is the goal of IE?', "" What is IE's goal?"", ' How can logical reasoning draw inferences?']","['to allow computation to be done on the previously unstructured data', 'to allow computation to be done on the previously unstructured data', 'based on the logical content of the input data']"
2233,information extraction,Summary,"Information extraction is the part of a greater puzzle which deals with the problem of devising automatic methods for text management, beyond its transmission, storage and display. The discipline of information retrieval (IR) has developed automatic methods, typically of a statistical flavor, for indexing large document collections and classifying documents. Another complementary approach is that of natural language processing (NLP) which has solved the problem of modelling human language processing with considerable success when taking into account the magnitude of the task. In terms of both difficulty and emphasis,  IE deals with tasks in between both IR and NLP. In terms of input, IE assumes the existence of a set of documents in which each document follows a template, i.e. describes one or more entities or events  in a manner that is similar to those in other documents but differing in the details. An example, consider a group of newswire articles on Latin American terrorism with each article presumed to be based upon one or more terroristic acts. We also define for any given IE task a template, which is a(or a set of) case frame(s) to hold the information contained in a single document. For the terrorism example, a template would have slots corresponding to the perpetrator, victim, and weapon of the terroristic act, and the date on which the event happened. An IE system for this problem is required to “understand” an attack article only enough to find data corresponding to the slots in this template.
","Information extraction is the part of a greater puzzle which deals with the problem of devising automatic methods for text management, beyond its transmission, storage and display. The discipline of information retrieval (IR) has developed automatic methods, typically of a statistical flavor, for indexing large document collections and classifying documents.",[' What is the part of a larger puzzle that deals with the problem of devising automatic methods for text management?'],['Information extraction']
2234,information extraction,History,Information extraction dates back to the late 1970s in the early days of NLP.  An early commercial system from the mid-1980s was JASPER built for Reuters by the Carnegie Group Inc with the aim of providing real-time financial news to financial traders.,Information extraction dates back to the late 1970s in the early days of NLP. An early commercial system from the mid-1980s was JASPER built for Reuters by the Carnegie Group Inc with the aim of providing real-time financial news to financial traders.,"[' When did information extraction date back to?', ' What was JASPER built for?']","['late 1970s', 'Reuters']"
2235,information extraction,History,"Beginning in 1987, IE was spurred by a series of Message Understanding Conferences. MUC is a competition-based conference that focused on the following domains: 
","Beginning in 1987, IE was spurred by a series of Message Understanding Conferences. MUC is a competition-based conference that focused on the following domains:","[' When did the Message Understanding Conferences begin?', ' What is a competition-based conference?']","['1987', 'MUC']"
2236,information extraction,Present significance,"The present significance of IE pertains to the growing amount of information available in unstructured form. Tim Berners-Lee, inventor of the world wide web, refers to the existing Internet as the web of documents  and advocates that more of the content be made available as a web of data.  Until this transpires, the web largely consists of unstructured documents lacking semantic metadata.  Knowledge contained within these documents can be made more accessible for machine processing by means of transformation into relational form, or by marking-up with XML tags.  An intelligent agent monitoring a news data feed requires IE to transform unstructured data into something that can be reasoned with.  A typical application of IE is to scan a set of documents written in a natural language and populate a database with the information extracted.","The present significance of IE pertains to the growing amount of information available in unstructured form. Tim Berners-Lee, inventor of the world wide web, refers to the existing Internet as the web of documents  and advocates that more of the content be made available as a web of data.","[' What is the current significance of IE?', ' Who invented the world wide web?', ' What does Tim Berners-Lee refer to the internet as?']","['the growing amount of information available in unstructured form', 'Tim Berners-Lee', 'the web of documents']"
2237,information extraction,Tasks and subtasks,"Applying information extraction to text is linked to the problem of text simplification in order to create a structured view of the information present in free text. The overall goal being to create a more easily machine-readable text to process the sentences. Typical IE tasks and subtasks include:
",Applying information extraction to text is linked to the problem of text simplification in order to create a structured view of the information present in free text. The overall goal being to create a more easily machine-readable text to process the sentences.,"[' Application of information extraction to text is linked to what problem?', ' The goal of text simplification is to create a structured view of the information present in free text?']","['text simplification', 'Applying information extraction to text is linked to the problem of text simplification in order to create a structured view of the information present in free text. The overall goal being to create a more easily machine-readable text to process the sentences']"
2238,information extraction,Tasks and subtasks,"Note that this list is not exhaustive and that the exact meaning of IE activities is not commonly accepted and that many approaches combine multiple sub-tasks of IE in order to achieve a wider goal. Machine learning, statistical analysis and/or natural language processing are often used in IE.
","Note that this list is not exhaustive and that the exact meaning of IE activities is not commonly accepted and that many approaches combine multiple sub-tasks of IE in order to achieve a wider goal. Machine learning, statistical analysis and/or natural language processing are often used in IE.","[' What is not commonly accepted?', ' What are often used in IE?', ' Machine learning, statistical analysis and/or natural language processing are used what?']","['the exact meaning of IE activities', 'Machine learning, statistical analysis and/or natural language processing', 'IE']"
2239,information extraction,Tasks and subtasks,"IE on non-text documents is becoming an increasingly interesting topic in research, and information extracted from multimedia documents can now be expressed in a high level structure as it is done on text. This naturally leads to the fusion of extracted information from multiple kinds of documents and sources.
","IE on non-text documents is becoming an increasingly interesting topic in research, and information extracted from multimedia documents can now be expressed in a high level structure as it is done on text. This naturally leads to the fusion of extracted information from multiple kinds of documents and sources.","[' What is becoming an increasingly interesting topic in research?', ' Information extracted from multimedia documents can now be expressed in a high level structure as it is done on what?']","['IE on non-text documents', 'text']"
2240,information extraction,World Wide Web applications,"IE has been the focus of the MUC conferences. The proliferation of the Web, however, intensified the need for developing IE systems that help people to cope with the enormous amount of data that is available online. Systems that perform IE from online text should meet the requirements of low cost, flexibility in development and easy adaptation to new domains. MUC systems fail to meet those criteria. Moreover, linguistic analysis performed for unstructured text does not exploit the HTML/XML tags and the layout formats that are available in online texts. As a result, less linguistically intensive approaches have been developed for IE on the Web using wrappers, which are sets of highly accurate rules that extract a particular page's content. Manually developing wrappers has proved to be a time-consuming task, requiring a high level of expertise. Machine learning techniques, either supervised or unsupervised, have been used to induce such rules automatically.
","IE has been the focus of the MUC conferences. The proliferation of the Web, however, intensified the need for developing IE systems that help people to cope with the enormous amount of data that is available online.","[' What has been the focus of the MUC conferences?', ' What has intensified the need for developing IE systems?']","['IE', 'The proliferation of the Web']"
2241,information extraction,World Wide Web applications,"Wrappers typically handle highly structured collections of web pages, such as product catalogs and telephone directories. They fail, however, when the text type is less structured, which is also common on the Web. Recent effort on adaptive information extraction motivates the development of IE systems that can handle different types of text, from well-structured to almost free text -where common wrappers fail- including mixed types. Such systems can exploit shallow natural language knowledge and thus can be also applied to less structured texts.
","Wrappers typically handle highly structured collections of web pages, such as product catalogs and telephone directories. They fail, however, when the text type is less structured, which is also common on the Web.","[' What are examples of highly structured collections of web pages?', ' What type of text type is also common on the Web?']","['product catalogs and telephone directories', 'less structured']"
2242,information extraction,World Wide Web applications,"A recent development is Visual Information Extraction, that relies on rendering a webpage in a browser and creating rules based on the proximity of regions in the rendered web page. This helps in extracting entities from complex web pages that may exhibit a visual pattern, but lack a discernible pattern in the HTML source code.
","A recent development is Visual Information Extraction, that relies on rendering a webpage in a browser and creating rules based on the proximity of regions in the rendered web page. This helps in extracting entities from complex web pages that may exhibit a visual pattern, but lack a discernible pattern in the HTML source code.","[' What does Visual Information Extraction do?', ' What is the name of the recent development that relies on rendering a webpage in a browser and creating rules?', ' What may exhibit a visual pattern, but lack a discernible pattern in the HTML source code?']","['helps in extracting entities from complex web pages', 'Visual Information Extraction', 'complex web pages']"
2243,regular expression,Summary,"A regular expression (shortened as regex or regexp; also referred to as rational expression) is a sequence of characters that specifies a search pattern in text. Usually such patterns are used by string-searching algorithms for ""find"" or ""find and replace"" operations on strings, or for input validation. It is a technique developed in theoretical computer science and formal language theory.
","A regular expression (shortened as regex or regexp; also referred to as rational expression) is a sequence of characters that specifies a search pattern in text. Usually such patterns are used by string-searching algorithms for ""find"" or ""find and replace"" operations on strings, or for input validation.","[' What is a regular expression also referred to as?', ' What is the sequence of characters that specifies a search pattern in text?']","['rational expression', 'A regular expression']"
2244,regular expression,Summary,"The concept of regular expressions began in the 1950s, when the American mathematician Stephen Cole Kleene formalized the description of a regular language. They came into common use with Unix text-processing utilities. Different syntaxes for writing regular expressions have existed since the 1980s, one being the POSIX standard and another, widely used, being the Perl syntax.
","The concept of regular expressions began in the 1950s, when the American mathematician Stephen Cole Kleene formalized the description of a regular language. They came into common use with Unix text-processing utilities.","[' When did the concept of regular expressions begin?', ' Who formalized the description of a regular language?']","['1950s', 'Stephen Cole Kleene']"
2245,regular expression,Summary,"Regular expressions are used in search engines, search and replace dialogs of word processors and text editors, in text processing utilities such as sed and AWK and in lexical analysis. Many programming languages provide regex capabilities either built-in or via libraries, as it has uses in many situations.
","Regular expressions are used in search engines, search and replace dialogs of word processors and text editors, in text processing utilities such as sed and AWK and in lexical analysis. Many programming languages provide regex capabilities either built-in or via libraries, as it has uses in many situations.","[' What are used in search engines?', ' What are text processing utilities such as sed and AWK used for?']","['Regular expressions', 'lexical analysis']"
2246,regular expression,History,"Regular expressions originated in 1951, when mathematician Stephen Cole Kleene described regular languages using his mathematical notation called regular events. These arose in theoretical computer science, in the subfields of automata theory (models of computation) and the description and classification of formal languages. Other early implementations of pattern matching include the SNOBOL language, which did not use regular expressions, but instead its own pattern matching constructs.
","Regular expressions originated in 1951, when mathematician Stephen Cole Kleene described regular languages using his mathematical notation called regular events. These arose in theoretical computer science, in the subfields of automata theory (models of computation) and the description and classification of formal languages.","[' When did regular expressions originate?', ' What mathematician described regular languages using his mathematical notation called regular events?']","['1951', 'Stephen Cole Kleene']"
2247,regular expression,History,"Regular expressions entered popular use from 1968 in two uses: pattern matching in a text editor and lexical analysis in a compiler. Among the first appearances of regular expressions in program form was when Ken Thompson built Kleene's notation into the editor QED as a means to match patterns in text files. For speed, Thompson implemented regular expression matching by just-in-time compilation (JIT) to IBM 7094 code on the Compatible Time-Sharing System, an important early example of JIT compilation. He later added this capability to the Unix editor ed, which eventually led to the popular search tool grep's use of regular expressions (""grep"" is a word derived from the command for regular expression searching in the ed editor: g/re/p meaning ""Global search for Regular Expression and Print matching lines""). Around the same time when Thompson developed QED, a group of researchers including Douglas T. Ross implemented a tool based on regular expressions that is used for lexical analysis in compiler design.",Regular expressions entered popular use from 1968 in two uses: pattern matching in a text editor and lexical analysis in a compiler. Among the first appearances of regular expressions in program form was when Ken Thompson built Kleene's notation into the editor QED as a means to match patterns in text files.,"[' When did regular expressions enter popular use?', ' What was one of the first appearances of regular expression in program form?', "" Who built Kleene's notation into the editor QED?"", ' What editor is used to match patterns in text files?']","['1968', ""Ken Thompson built Kleene's notation"", 'Ken Thompson', 'QED']"
2248,regular expression,History,"Many variations of these original forms of regular expressions were used in Unix programs at Bell Labs in the 1970s, including vi, lex, sed, AWK, and expr, and in other programs such as Emacs. Regexes were subsequently adopted by a wide range of programs, with these early forms standardized in the POSIX.2 standard in 1992.
","Many variations of these original forms of regular expressions were used in Unix programs at Bell Labs in the 1970s, including vi, lex, sed, AWK, and expr, and in other programs such as Emacs. Regexes were subsequently adopted by a wide range of programs, with these early forms standardized in the POSIX.2 standard in 1992.","[' In what year were many variations of regular expressions used in Unix programs at Bell Labs?', ' What was one of the programs that used variations of the original forms of regexes in the 1970s called?', ' In what program was the original form of regex adopted by a wide range of programs?', ' When was the POSIX.2 standard introduced?']","['1970s', 'Emacs', 'Emacs', '1992']"
2249,regular expression,History,"In the 1980s the more complicated regexes arose in Perl, which originally derived from a regex library written by Henry Spencer (1986), who later wrote an implementation of Advanced Regular Expressions for Tcl. The Tcl library is a hybrid NFA/DFA implementation with improved performance characteristics. Software projects that have adopted Spencer's Tcl regular expression implementation include PostgreSQL. Perl later expanded on Spencer's original library to add many new features. Part of the effort in the design of Raku (formerly named Perl 6) is to improve Perl's regex integration, and to increase their scope and capabilities to allow the definition of parsing expression grammars. The result is a mini-language called Raku rules, which are used to define Raku grammar as well as provide a tool to programmers in the language. These rules maintain existing features of Perl 5.x regexes, but also allow BNF-style definition of a recursive descent parser via sub-rules.
","In the 1980s the more complicated regexes arose in Perl, which originally derived from a regex library written by Henry Spencer (1986), who later wrote an implementation of Advanced Regular Expressions for Tcl. The Tcl library is a hybrid NFA/DFA implementation with improved performance characteristics.","[' When did the more complicated regexes arose in Perl?', ' Who wrote the regex library in 1986?', ' What is the Tcl library a hybrid NFA/DFA implementation with?']","['1980s', 'Henry Spencer', 'improved performance characteristics']"
2250,regular expression,History,"The use of regexes in structured information standards for document and database modeling started in the 1960s and expanded in the 1980s when industry standards like ISO SGML (precursored by ANSI ""GCA 101-1983"") consolidated. The kernel of the structure specification language standards consists of regexes. Its use is evident in the DTD element group syntax. Prior to the use of regular expressions, many search languages allowed simple wildcards, for example ""*"" to match any sequence of characters, and ""?"" to match a single character. Relics of this can be found today in the glob syntax for filenames, and in the SQL LIKE operator.
","The use of regexes in structured information standards for document and database modeling started in the 1960s and expanded in the 1980s when industry standards like ISO SGML (precursored by ANSI ""GCA 101-1983"") consolidated. The kernel of the structure specification language standards consists of regexes.","[' When did the use of regexes in structured information standards for document and database modeling start?', ' When did industry standards like ISO SGML expand?', ' What is the kernel of the structure specification language standards?']","['1960s', '1980s', 'regexes']"
2251,regular expression,History,"Today, regexes are widely supported in programming languages, text processing programs (particularly lexers), advanced text editors, and some other programs. Regex support is part of the standard library of many programming languages, including Java and Python, and is built into the syntax of others, including Perl and ECMAScript. Implementations of regex functionality is often called a regex engine, and a number of libraries are available for reuse. In the late 2010s, several companies started to offer hardware, FPGA, GPU implementations of PCRE compatible regex engines that are faster compared to CPU implementations.
","Today, regexes are widely supported in programming languages, text processing programs (particularly lexers), advanced text editors, and some other programs. Regex support is part of the standard library of many programming languages, including Java and Python, and is built into the syntax of others, including Perl and ECMAScript.","[' Regexes are widely supported in what programming languages?', ' Regex support is part of the standard library of many programming languages, including Java and what other programming language?']","['Java and Python', 'Python']"
2252,regular expression,Patterns,"The phrase regular expressions, or regexes, is often used to mean the specific, standard textual syntax for representing patterns for matching text, as distinct from the mathematical notation described below. Each character in a regular expression (that is, each character in the string describing its pattern) is either a metacharacter, having a special meaning, or a regular character that has a literal meaning. For example, in the regex b., 'b' is a literal character that matches just 'b', while '.' is a metacharacter that matches every character except a newline. Therefore, this regex matches, for example, 'b%', or 'bx', or 'b5'. Together, metacharacters and literal characters can be used to identify text of a given pattern or process a number of instances of it. Pattern matches may vary from a precise equality to a very general similarity, as controlled by the metacharacters. For example, . is a very general pattern, [a-z] (match all lower case letters from 'a' to 'z') is less general and b is a precise pattern (matches just 'b'). The metacharacter syntax is designed specifically to represent prescribed targets in a concise and flexible way to direct the automation of text processing of a variety of input data, in a form easy to type using a standard ASCII keyboard.
","The phrase regular expressions, or regexes, is often used to mean the specific, standard textual syntax for representing patterns for matching text, as distinct from the mathematical notation described below. Each character in a regular expression (that is, each character in the string describing its pattern) is either a metacharacter, having a special meaning, or a regular character that has a literal meaning.","[' What does the term regexes mean?', ' What is the term used to mean the specific, standard textual syntax for representing patterns?', ' What is a metacharacter having a special meaning?', ' A regular character that has a literal meaning is what?']","['the specific, standard textual syntax for representing patterns for matching text', 'regular expressions', 'Each character in a regular expression', 'metacharacter']"
2253,regular expression,Patterns,"A very simple case of a regular expression in this syntax is to locate a word spelled two different ways in a text editor, the regular expression seriali[sz]e matches both ""serialise"" and ""serialize"". Wildcard characters also achieve this, but are more limited in what they can pattern, as they have fewer metacharacters and a simple language-base.
","A very simple case of a regular expression in this syntax is to locate a word spelled two different ways in a text editor, the regular expression seriali[sz]e matches both ""serialise"" and ""serialize"". Wildcard characters also achieve this, but are more limited in what they can pattern, as they have fewer metacharacters and a simple language-base.","[' What is a very simple case of a regular expression in this syntax?', ' The regular expression seriali[sz]e matches both ""serialise"" and what other word?', ' Wildcard characters are more limited in what they can pattern as they have what?', ' What are more limited in what they can pattern?', ' What do metacharacters have?']","['to locate a word spelled two different ways in a text editor', 'serialize', 'fewer metacharacters and a simple language-base', 'Wildcard characters', 'simple language-base']"
2254,regular expression,Patterns,"The usual context of wildcard characters is in globbing similar names in a list of files, whereas regexes are usually employed in applications that pattern-match text strings in general. For example, the regex ^[ \t]+|[ \t]+$ matches excess whitespace at the beginning or end of a line. An advanced regular expression that matches any numeral is [+-]?(\d+(\.\d*)?|\.\d+)([eE][+-]?\d+)?.
","The usual context of wildcard characters is in globbing similar names in a list of files, whereas regexes are usually employed in applications that pattern-match text strings in general. For example, the regex ^[ \t]+|[ \t]+$ matches excess whitespace at the beginning or end of a line.","[' What is the usual context of wildcard characters?', ' What are usually employed in applications that pattern-match text strings in general?']","['globbing similar names in a list of files', 'regexes']"
2255,regular expression,Patterns,"A regex processor translates a regular expression in the above syntax into an internal representation that can be executed and matched against a string representing the text being searched in. One possible approach is the Thompson's construction algorithm to construct a nondeterministic finite automaton (NFA), which is then made deterministic and the resulting deterministic finite automaton (DFA) is run on the target text string to recognize substrings that match the regular expression.
The picture shows the NFA scheme N(s*) obtained from the regular expression s*, where s denotes a simpler regular expression in turn, which has already been recursively translated to the NFA N(s).
","A regex processor translates a regular expression in the above syntax into an internal representation that can be executed and matched against a string representing the text being searched in. One possible approach is the Thompson's construction algorithm to construct a nondeterministic finite automaton (NFA), which is then made deterministic and the resulting deterministic finite automaton (DFA) is run on the target text string to recognize substrings that match the regular expression.","[' What does a regex processor translate into an internal representation that can be executed and matched against a string representing the text being searched in?', ' What is one possible approach to construct a nondeterministic finite automaton?', ' What is the name of the nondeterministic finite automaton?', ' What is DFA?', ' How is the DFA run on the target text string?']","['a regular expression in the above syntax', ""Thompson's construction algorithm"", 'NFA', 'deterministic finite automaton', 'to recognize substrings that match the regular expression']"
2256,regular expression,Basic concepts,"A regular expression, often called a pattern, specifies a set of strings required for a particular purpose. A simple way to specify a finite set of strings is to list its elements or members. However, there are often more concise ways: for example, the set containing the three strings ""Handel"", ""Händel"", and ""Haendel"" can be specified by the pattern H(ä|ae?)ndel; we say that this pattern matches each of the three strings. However, there can be many ways to write a regular expression for the same set of strings: for example, (Hän|Han|Haen)del also specifies the same set of three strings in this example.
","A regular expression, often called a pattern, specifies a set of strings required for a particular purpose. A simple way to specify a finite set of strings is to list its elements or members.","[' What is a regular expression often called?', ' What specifies a set of strings required for a particular purpose?']","['a pattern', 'A regular expression']"
2257,regular expression,Basic concepts,"The wildcard . matches any character. For example, a.b matches any string that contains an ""a"", and then any character and then ""b""; and a.*b matches any string that contains an ""a"", and then the character ""b"" at some later point.
",The wildcard . matches any character.,[' Which wildcard matches any character?'],['.']
2258,regular expression,Basic concepts,"These constructions can be combined to form arbitrarily complex expressions, much like one can construct arithmetical expressions from numbers and the operations +, −, ×, and ÷. For example, H(ae?|ä)ndel and H(a|ae|ä)ndel are both valid patterns which match the same strings as the earlier example, H(ä|ae?)ndel.
","These constructions can be combined to form arbitrarily complex expressions, much like one can construct arithmetical expressions from numbers and the operations +, −, ×, and ÷. For example, H(ae?|ä)ndel and H(a|ae|ä)ndel are both valid patterns which match the same strings as the earlier example, H(ä|ae?","[' What can be combined to form arbitrarily complex expressions?', ' How can one construct arithmetical expressions from numbers and the operations +, <unk>, and <unk>?', ' What are both valid patterns which match the same strings as the earlier example?']","['constructions', 'combined to form arbitrarily complex expressions', 'H(ae?|ä)ndel and H(a|ae|ä)ndel']"
2259,regular expression,Formal language theory,"Regular expressions describe regular languages in formal language theory. They have the same expressive power as regular grammars.
",Regular expressions describe regular languages in formal language theory. They have the same expressive power as regular grammars.,"[' What describe regular languages in formal language theory?', ' Regular expressions have the same expressive power as what?']","['Regular expressions', 'regular grammars']"
2260,regular expression,Syntax,"A regex pattern matches a target string. The pattern is composed of a sequence of atoms. An atom is a single point within the regex pattern which it tries to match to the target string. The simplest atom is a literal, but grouping parts of the pattern to match an atom will require using ( ) as metacharacters. Metacharacters help form: atoms; quantifiers telling how many atoms (and whether it is a greedy quantifier or not); a logical OR character, which offers a set of alternatives, and a logical NOT character, which negates an atom's existence; and backreferences to refer to previous atoms of a completing pattern of atoms. A match is made, not when all the atoms of the string are matched, but rather when all the pattern atoms in the regex have matched. The idea is to make a small pattern of characters stand for a large number of possible strings, rather than compiling a large list of all the literal possibilities.
",A regex pattern matches a target string. The pattern is composed of a sequence of atoms.,"[' What pattern matches a target string?', ' What is the pattern composed of?']","['regex', 'a sequence of atoms']"
2261,regular expression,Syntax,"Depending on the regex processor there are about fourteen metacharacters, characters that may or may not have their literal character meaning, depending on context, or whether they are ""escaped"", i.e. preceded by an escape sequence, in this case, the backslash \. Modern and POSIX extended regexes use metacharacters more often than their literal meaning, so to avoid ""backslash-osis"" or leaning toothpick syndrome it makes sense to have a metacharacter escape to a literal mode; but starting out, it makes more sense to have the four bracketing metacharacters ( ) and { } be primarily literal, and ""escape"" this usual meaning to become metacharacters. Common standards implement both. The usual metacharacters are  {}[]()^$.|*+? and \. The usual characters that become metacharacters when escaped are dswDSW and N.
","Depending on the regex processor there are about fourteen metacharacters, characters that may or may not have their literal character meaning, depending on context, or whether they are ""escaped"", i.e. preceded by an escape sequence, in this case, the backslash \.","[' How many metacharacters are there depending on the regex processor?', ' What is preceded by an escape sequence?']","['fourteen', 'metacharacters']"
2262,regular expression,Patterns for non-regular languages,"Many features found in virtually all modern regular expression libraries provide an expressive power that exceeds the regular languages. For example, many implementations allow grouping subexpressions with parentheses and recalling the value they match in the same expression (backreferences). This means that, among other things, a pattern can match strings of repeated words like ""papa"" or ""WikiWiki"", called squares in formal language theory. The pattern for these strings is (.+)\1.
","Many features found in virtually all modern regular expression libraries provide an expressive power that exceeds the regular languages. For example, many implementations allow grouping subexpressions with parentheses and recalling the value they match in the same expression (backreferences).","[' What do many features in almost all modern regular expression libraries provide an expressive power that exceeds the regular languages?', ' Many implementations allow grouping subexpressions with what?']","['many implementations allow grouping subexpressions with parentheses and recalling the value they match in the same expression', 'parentheses']"
2263,regular expression,Patterns for non-regular languages,"The language of squares is not regular, nor is it context-free, due to the pumping lemma. However, pattern matching with an unbounded number of backreferences, as supported by numerous modern tools, is still context sensitive. The general problem of matching any number of backreferences is NP-complete, growing exponentially by the number of backref groups used.","The language of squares is not regular, nor is it context-free, due to the pumping lemma. However, pattern matching with an unbounded number of backreferences, as supported by numerous modern tools, is still context sensitive.","[' Why is the language of squares not regular or context-free?', ' What is still context sensitive?']","['pumping lemma', 'pattern matching']"
2264,regular expression,Patterns for non-regular languages,"However, many tools, libraries, and engines that provide such constructions still use the term regular expression for their patterns. This has led to a nomenclature where the term regular expression has different meanings in formal language theory and pattern matching. For this reason, some people have taken to using the term regex, regexp, or simply pattern to describe the latter. Larry Wall, author of the Perl programming language, writes in an essay about the design of Raku:
","However, many tools, libraries, and engines that provide such constructions still use the term regular expression for their patterns. This has led to a nomenclature where the term regular expression has different meanings in formal language theory and pattern matching.","[' What term do many tools, libraries, and engines still use for their patterns?', ' What has led to a nomenclature where the term regular expression has different meanings?']","['regular expression', 'many tools, libraries, and engines that provide such constructions still use the term regular expression for their patterns']"
2265,regular expression,Patterns for non-regular languages,"""Regular expressions"" […] are only marginally related to real regular expressions. Nevertheless, the term has grown with the capabilities of our pattern matching engines, so I'm not going to try to fight linguistic necessity here. I will, however, generally call them ""regexes"" (or ""regexen"", when I'm in an Anglo-Saxon mood).","""Regular expressions"" […] are only marginally related to real regular expressions. Nevertheless, the term has grown with the capabilities of our pattern matching engines, so I'm not going to try to fight linguistic necessity here.","[' What are ""Regular expressions"" only marginally related to?', ' What has grown with the capabilities of our pattern matching engines?', "" I'm not going to try to fight what here?""]","['real regular expressions', 'Regular expressions', 'linguistic necessity']"
2266,regular expression,Patterns for non-regular languages,"Other features not found in describing regular languages include assertions. These include the ubiquitous ^ and $, as well as some more sophisticated extensions like lookaround. They define the surrounding of a match and don't spill into the match itself, a feature only relevant for the use case of string searching. Some of them can be simulated in a regular language by treating the surroundings as a part of the language as well.","Other features not found in describing regular languages include assertions. These include the ubiquitous ^ and $, as well as some more sophisticated extensions like lookaround.","[' What are some other features not found in describing regular languages?', ' Other than assertions, what are some more sophisticated extensions?']","['assertions', 'lookaround']"
2267,regular expression,Implementations and running times,"The oldest and fastest relies on a result in formal language theory that allows every nondeterministic finite automaton (NFA) to be transformed into a deterministic finite automaton (DFA). The DFA can be constructed explicitly and then run on the resulting input string one symbol at a time. Constructing the DFA for a regular expression of size m has the time and memory cost of O(2m), but it can be run on a string of size n in time O(n).  Note that the size of the expression is the size after abbreviations, such as numeric quantifiers, have been expanded.
",The oldest and fastest relies on a result in formal language theory that allows every nondeterministic finite automaton (NFA) to be transformed into a deterministic finite automaton (DFA). The DFA can be constructed explicitly and then run on the resulting input string one symbol at a time.,"[' The oldest and fastest relies on a result in formal language theory that allows every nondeterministic finite automaton to be transformed into what?', ' What can be constructed explicitly and then run on the resulting input string one symbol at a time?']","['a deterministic finite automaton', 'The DFA']"
2268,regular expression,Implementations and running times,"An alternative approach is to simulate the NFA directly, essentially building each DFA state on demand and then discarding it at the next step. This keeps the DFA implicit and avoids the exponential construction cost, but running cost rises to O(mn). The explicit approach is called the DFA algorithm and the implicit approach the NFA algorithm. Adding caching to the NFA algorithm is often called the ""lazy DFA"" algorithm, or just the DFA algorithm without making a distinction. These algorithms are fast, but using them for recalling grouped subexpressions, lazy quantification, and similar features is tricky. Modern implementations include the re1-re2-sregex family based on Cox's code.
","An alternative approach is to simulate the NFA directly, essentially building each DFA state on demand and then discarding it at the next step. This keeps the DFA implicit and avoids the exponential construction cost, but running cost rises to O(mn).","[' What is an alternative approach to simulate the NFA?', ' What does this approach avoid?', ' How does the running cost rise?']","['directly', 'exponential construction cost', 'O(mn).']"
2269,regular expression,Implementations and running times,"The third algorithm is to match the pattern against the input string by backtracking. This algorithm is commonly called NFA, but this terminology can be confusing. Its running time can be exponential, which simple implementations exhibit when matching against expressions like (a|aa)*b that contain both alternation and unbounded quantification and force the algorithm to consider an exponentially increasing number of sub-cases. This behavior can cause a security problem called Regular expression Denial of Service (ReDoS).
","The third algorithm is to match the pattern against the input string by backtracking. This algorithm is commonly called NFA, but this terminology can be confusing.","[' What is the third algorithm to match the pattern against the input string by?', ' What is NFA?']","['backtracking', 'The third algorithm is to match the pattern against the input string by backtracking']"
2270,regular expression,Implementations and running times,"Although backtracking implementations only give an exponential guarantee in the worst case, they provide much greater flexibility and expressive power. For example, any implementation which allows the use of backreferences, or implements the various extensions introduced by Perl, must include some kind of backtracking. Some implementations try to provide the best of both algorithms by first running a fast DFA algorithm, and revert to a potentially slower backtracking algorithm only when a backreference is encountered during the match. GNU grep (and the underlying gnulib DFA) uses such a strategy.","Although backtracking implementations only give an exponential guarantee in the worst case, they provide much greater flexibility and expressive power. For example, any implementation which allows the use of backreferences, or implements the various extensions introduced by Perl, must include some kind of backtracking.","[' What do backtracking implementations provide in the worst case?', ' What does a Perl implementation that allows the use of backreferences have to include?']","['an exponential guarantee', 'some kind of backtracking']"
2271,regular expression,Implementations and running times,"Sublinear runtime algorithms have been achieved using Boyer-Moore (BM) based algorithms and related DFA optimization techniques such as the reverse scan. GNU grep, which supports a wide variety of POSIX syntaxes and extensions, uses BM for a first-pass prefiltering, and then uses an implicit DFA. Wu agrep, which implements approximate matching, combines the prefiltering into the DFA in BDM (backward DAWG matching). NR-grep's BNDM extends the BDM technique with Shift-Or bit-level parallelism.","Sublinear runtime algorithms have been achieved using Boyer-Moore (BM) based algorithms and related DFA optimization techniques such as the reverse scan. GNU grep, which supports a wide variety of POSIX syntaxes and extensions, uses BM for a first-pass prefiltering, and then uses an implicit DFA.","[' What BM based algorithm has been used to achieve sublinear runtime algorithms?', ' What is another DFA optimization technique?', ' GNU grep uses what for a first-pass prefiltering?']","['Boyer-Moore', 'the reverse scan', 'Boyer-Moore']"
2272,regular expression,Implementations and running times,"A few theoretical alternatives to backtracking for backreferences exist, and their ""exponents"" are tamer in that they are only related to the number of backreferences, a fixed property of some regexp languages such as POSIX. One naive method that duplicates a non-backtracking NFA for each backreference note has a complexity of 





O


(

n

2
k
+
2


)


{\displaystyle {\mathrm {O} }(n^{2k+2})}
 time and 





O


(

n

2
k
+
1


)


{\displaystyle {\mathrm {O} }(n^{2k+1})}
 space for a haystack of length n and k backreferences in the RegExp. A very recent theoretical work based on memory automata gives a tighter bound based on ""active"" variable nodes used, and a polynomial possibility for some backreferenced regexps.","A few theoretical alternatives to backtracking for backreferences exist, and their ""exponents"" are tamer in that they are only related to the number of backreferences, a fixed property of some regexp languages such as POSIX. One naive method that duplicates a non-backtracking NFA for each backreference note has a complexity of 





O


(

n

2
k
+
2


)


{\displaystyle {\mathrm {O} }(n^{2k+2})}
 time and 





O


(

n

2
k
+
1


)


{\displaystyle {\mathrm {O} }(n^{2k+1})}
 space for a haystack of length n and k backreferences in the RegExp.","[' What is a fixed property of some regexp languages such as POSIX?', ' What does a method that duplicates a non-backtracking NFA for each backreference note have?', ' What is the complexity of a non-backtracking NFA for each backreference note?', ' What is a haystack of length n and k backrefers in the RegExp?']","['number of backreferences', 'O', 'O', 'space']"
2273,regular expression,Unicode,"In theoretical terms, any token set can be matched by regular expressions as long as it is pre-defined. In terms of historical implementations, regexes were originally written to use ASCII characters as their token set though regex libraries have supported numerous other character sets. Many modern regex engines offer at least some support for Unicode. In most respects it makes no difference what the character set is, but some issues do arise when extending regexes to support Unicode.
","In theoretical terms, any token set can be matched by regular expressions as long as it is pre-defined. In terms of historical implementations, regexes were originally written to use ASCII characters as their token set though regex libraries have supported numerous other character sets.","[' What can be matched by regular expressions as long as it is pre-defined?', ' What were regexes originally written to use?']","['any token set', 'ASCII characters']"
2274,regular expression,Uses,"Regexes are useful in a wide variety of text processing tasks, and more generally string processing, where the data need not be textual. Common applications include data validation, data scraping (especially web scraping), data wrangling, simple parsing, the production of syntax highlighting systems, and many other tasks.
","Regexes are useful in a wide variety of text processing tasks, and more generally string processing, where the data need not be textual. Common applications include data validation, data scraping (especially web scraping), data wrangling, simple parsing, the production of syntax highlighting systems, and many other tasks.","[' Regexes are useful in a wide variety of what?', ' What are common applications of regexes?', ' Data validation, data scraping, data wrangling, and syntax highlighting systems are examples of what type of processing?']","['text processing tasks', 'data validation, data scraping (especially web scraping), data wrangling, simple parsing', 'simple parsing']"
2275,regular expression,Uses,"While regexes would be useful on Internet search engines, processing them across the entire database could consume excessive computer resources depending on the complexity and design of the regex. Although in many cases system administrators can run regex-based queries internally, most search engines do not offer regex support to the public. Notable exceptions include Google Code Search and Exalead. However, Google Code Search was shut down in January 2012.","While regexes would be useful on Internet search engines, processing them across the entire database could consume excessive computer resources depending on the complexity and design of the regex. Although in many cases system administrators can run regex-based queries internally, most search engines do not offer regex support to the public.","[' What would be useful on Internet search engines?', ' What could consume excessive computer resources depending on the complexity and design of the regex?', ' Most search engines do not offer what?', ' What do most search engines not offer to the public?']","['regexes', 'processing them across the entire database', 'regex support to the public', 'regex support']"
2276,regular expression,Examples,"The specific syntax rules vary depending on the specific implementation, programming language, or library in use. Additionally, the functionality of regex implementations can vary between versions.
","The specific syntax rules vary depending on the specific implementation, programming language, or library in use. Additionally, the functionality of regex implementations can vary between versions.","[' The specific syntax rules vary depending on what?', ' The functionality of what can vary between versions?']","['the specific implementation, programming language, or library in use', 'regex implementations']"
2277,regular expression,Examples,"Because regexes can be difficult to both explain and understand without examples, interactive websites for testing regexes are a useful resource for learning regexes by experimentation.
This section provides a basic description of some of the properties of regexes by way of illustration.
","Because regexes can be difficult to both explain and understand without examples, interactive websites for testing regexes are a useful resource for learning regexes by experimentation. This section provides a basic description of some of the properties of regexes by way of illustration.","[' What can be difficult to explain and understand without examples?', ' What is a useful resource for learning regexes by experimentation?']","['regexes', 'interactive websites for testing regexes']"
2278,regular expression,Examples,"Also worth noting is that these regexes are all Perl-like syntax. Standard POSIX regular expressions are different.
",Also worth noting is that these regexes are all Perl-like syntax. Standard POSIX regular expressions are different.,"[' What are all the regexes all Perl-like syntax?', ' Standard POSIX regular expressions are different?']","['regexes', 'regexes are all Perl-like syntax']"
2279,regular expression,Examples,"Unless otherwise indicated, the following examples conform to the Perl programming language, release 5.8.8, January 31, 2006. This means that other implementations may lack support for some parts of the syntax shown here (e.g. basic vs. extended regex, \( \) vs. (), or lack of \d instead of POSIX [:digit:]).
","Unless otherwise indicated, the following examples conform to the Perl programming language, release 5.8.8, January 31, 2006. This means that other implementations may lack support for some parts of the syntax shown here (e.g.","[' What programming language does the following examples conform to?', ' When was Perl released?', ' What may other implementations lack support for?']","['Perl', 'January 31, 2006', 'some parts of the syntax']"
2280,regular expression,Induction,"Regular expressions can often be created (""induced"" or ""learned"") based on a set of example strings. This is known as the induction of regular languages, and is part of the general problem of grammar induction in computational learning theory. Formally, given examples of strings in a regular language, and perhaps also given examples of strings not in that regular language, it is possible to induce a grammar for the language, i.e., a regular expression that generates that language. Not all regular languages can be induced in this way (see language identification in the limit), but many can. For example, the set of examples {1, 10, 100}, and negative set (of counterexamples) {11, 1001, 101, 0} can be used to induce the regular expression 1⋅0* (1 followed by zero or more 0s).
","Regular expressions can often be created (""induced"" or ""learned"") based on a set of example strings. This is known as the induction of regular languages, and is part of the general problem of grammar induction in computational learning theory.","[' What can be created based on a set of example strings?', ' What is the induction of regular languages part of?']","['Regular expressions', 'the general problem of grammar induction in computational learning theory']"
2281,proof system,Overview,"Usually a given proof calculus encompasses more than a single particular formal system, since many proof calculi are under-determined and can be used for radically different logics. For example, a paradigmatic case is the sequent calculus, which can be used to express the consequence relations of both intuitionistic logic and relevance logic. Thus, loosely speaking, a proof calculus is a template or design pattern, characterized by a certain style of formal inference, that may be specialized to produce specific formal systems, namely by specifying the actual inference rules for such a system. There is no consensus among logicians on how best to define the term.
","Usually a given proof calculus encompasses more than a single particular formal system, since many proof calculi are under-determined and can be used for radically different logics. For example, a paradigmatic case is the sequent calculus, which can be used to express the consequence relations of both intuitionistic logic and relevance logic.","[' What is a paradigmatic case of the sequent calculus?', ' What can be used to express the consequence relations of intuitionistic logic?', ' To express the consequence relations of intuitionistic logic and relevance logic?']","['intuitionistic logic and relevance logic', 'sequent calculus', 'sequent calculus']"
2282,shared memory,Summary,"In computer science, shared memory is memory that may be simultaneously accessed by multiple programs with an intent to provide communication among them or avoid redundant copies. Shared memory is an efficient means of passing data between programs. Depending on context, programs may run on a single processor or on multiple separate processors.
","In computer science, shared memory is memory that may be simultaneously accessed by multiple programs with an intent to provide communication among them or avoid redundant copies. Shared memory is an efficient means of passing data between programs.","[' What is shared memory in computer science?', ' What is the purpose of shared memory?']","['memory that may be simultaneously accessed by multiple programs with an intent to provide communication among them or avoid redundant copies', 'an efficient means of passing data between programs']"
2283,shared memory,Summary,"Using memory for communication inside a single program, e.g. among its multiple threads, is also referred to as shared memory.
","Using memory for communication inside a single program, e.g. among its multiple threads, is also referred to as shared memory.",[' Using memory for communication inside a single program is also referred to as what?'],['shared memory']
2284,shared memory,In hardware,"A shared memory system is relatively easy to program since all processors share a single view of data and the communication between processors can be as fast as memory accesses to a same location. The issue with shared memory systems is that many CPUs need fast access to memory and will likely cache memory, which has two complications:
","A shared memory system is relatively easy to program since all processors share a single view of data and the communication between processors can be as fast as memory accesses to a same location. The issue with shared memory systems is that many CPUs need fast access to memory and will likely cache memory, which has two complications:","[' What is relatively easy to program in a shared memory system?', ' What is the issue with shared memory systems?', ' What do many CPUs need fast access to?', ' How many complications does cache memory have?']","['all processors share a single view of data', 'many CPUs need fast access to memory and will likely cache memory', 'memory', 'two']"
2285,shared memory,In software,"Since both processes can access the shared memory area like regular working memory, this is a very fast way of communication (as opposed to other mechanisms of IPC such as named pipes, Unix domain sockets or CORBA). On the other hand, it is less scalable, as for example the communicating processes must be running on the same machine (of other IPC methods, only Internet domain sockets—not Unix domain sockets—can use a computer network), and care must be taken to avoid issues if processes sharing memory are running on separate CPUs and the underlying architecture is not cache coherent.
","Since both processes can access the shared memory area like regular working memory, this is a very fast way of communication (as opposed to other mechanisms of IPC such as named pipes, Unix domain sockets or CORBA). On the other hand, it is less scalable, as for example the communicating processes must be running on the same machine (of other IPC methods, only Internet domain sockets—not Unix domain sockets—can use a computer network), and care must be taken to avoid issues if processes sharing memory are running on separate CPUs and the underlying architecture is not cache coherent.","[' What can both processes access like regular working memory?', ' What is a very fast way of communication?', ' What must be running on the same machine of other IPC methods?', ' What is the only IPC method that can use a computer network?', ' What happens if processes sharing memory are running on separate CPUs and the underlying architecture is not cache coherent?']","['shared memory area', 'shared memory area like regular working memory', 'communicating processes', 'Internet domain sockets', 'care must be taken to avoid issues']"
2286,swarm intelligence,Summary,"Swarm intelligence (SI) is the collective behavior of decentralized, self-organized systems, natural or artificial. The concept is employed in work on artificial intelligence. The expression was introduced by Gerardo Beni and Jing Wang in 1989, in the context of cellular robotic systems.","Swarm intelligence (SI) is the collective behavior of decentralized, self-organized systems, natural or artificial. The concept is employed in work on artificial intelligence.","[' What is Swarm intelligence?', ' What is the collective behavior of decentralized, self-organized systems?']","['the collective behavior of decentralized, self-organized systems', 'Swarm intelligence']"
2287,swarm intelligence,Summary,"SI systems consist typically of a population of simple agents or boids interacting locally with one another and with their environment. The inspiration often comes from nature, especially biological systems. The agents follow very simple rules, and although there is no centralized control structure dictating how individual agents should behave, local, and to a certain degree random, interactions between such agents lead to the emergence of ""intelligent"" global behavior, unknown to the individual agents. Examples of swarm intelligence in natural systems include ant colonies, bee colonies, bird flocking, hawks hunting, animal herding, bacterial growth, fish schooling and microbial intelligence.
","SI systems consist typically of a population of simple agents or boids interacting locally with one another and with their environment. The inspiration often comes from nature, especially biological systems.","[' What are SI systems called?', ' Where does inspiration for SI systems come from?']","['simple agents or boids', 'nature']"
2288,swarm intelligence,Summary,The application of swarm principles to robots is called swarm robotics while swarm intelligence refers to the more general set of algorithms. Swarm prediction has been used in the context of forecasting problems. Similar approaches to those proposed for swarm robotics are considered for genetically modified organisms in synthetic collective intelligence.,The application of swarm principles to robots is called swarm robotics while swarm intelligence refers to the more general set of algorithms. Swarm prediction has been used in the context of forecasting problems.,"[' What is the term for the application of swarm principles to robots?', ' Swarm prediction has been used in the context of what?']","['swarm robotics', 'forecasting problems']"
2289,swarm intelligence,Metaheuristics,"Evolutionary algorithms (EA), particle swarm optimization (PSO), differential evolution (DE), ant colony optimization (ACO) and their variants dominate the field of nature-inspired metaheuristics. This list includes algorithms published up to circa the year 2000. A large number of more recent metaphor-inspired metaheuristics have started to attract criticism in the research community for hiding their lack of novelty behind an elaborate metaphor. For algorithms published since that time, see List of metaphor-based metaheuristics.
","Evolutionary algorithms (EA), particle swarm optimization (PSO), differential evolution (DE), ant colony optimization (ACO) and their variants dominate the field of nature-inspired metaheuristics. This list includes algorithms published up to circa the year 2000.","[' What is the name of the field of metaheuristics dominated by evolutionary algorithms?', ' What is another name for particle swarm optimization?']","['nature-inspired metaheuristics', 'PSO']"
2290,swarm intelligence,Metaheuristics,"Metaheuristics lack a confidence in a solution. When appropriate parameters are determined, and when sufficient convergence stage is achieved, they often find a solution that is optimal, or near close to optimum – nevertheless, if one does not know optimal solution in advance, a quality of a solution is not known. In spite of this obvious drawback it has been shown that these types of algorithms work well in practice, and have been extensively researched, and developed.  On the other hand, it is possible to avoid this drawback by calculating solution quality for a special case where such calculation is possible, and after such run it is known that every solution that is at least as good as the solution a special case had, has at least a solution confidence a special case had. One such instance is Ant inspired Monte Carlo algorithm for Minimum Feedback Arc Set where this has been achieved probabilistically via hybridization of Monte Carlo algorithm with Ant Colony Optimization technique.","Metaheuristics lack a confidence in a solution. When appropriate parameters are determined, and when sufficient convergence stage is achieved, they often find a solution that is optimal, or near close to optimum – nevertheless, if one does not know optimal solution in advance, a quality of a solution is not known.","[' Metaheuristics lack a confidence in what?', ' When appropriate parameters are determined, and when sufficient convergence stage is achieved, what often finds a solution that is optimal?', ' If one does not know optimal solution in advance, what is not a quality of solution?', ' What is not known about a solution in advance?']","['a solution', 'Metaheuristics', 'Metaheuristics lack a confidence in a solution. When appropriate parameters are determined, and when sufficient convergence stage is achieved, they often find a solution that is optimal, or near close to optimum – nevertheless, if one does not know optimal solution in advance, a quality of a solution is not known', 'a quality']"
2291,swarm intelligence,Applications,Swarm Intelligence-based techniques can be used in a number of applications.  The U.S. military is investigating swarm techniques for controlling unmanned vehicles. The European Space Agency is thinking about an orbital swarm for self-assembly and interferometry. NASA is investigating the use of swarm technology for planetary mapping.  A 1992 paper by M. Anthony Lewis and George A. Bekey discusses the possibility of using swarm intelligence to control nanobots within the body for the purpose of killing cancer tumors. Conversely al-Rifaie and Aber have used stochastic diffusion search to help locate tumours. Swarm intelligence has also been applied for data mining and cluster analysis. Ant based models are further subject of modern management theory.,Swarm Intelligence-based techniques can be used in a number of applications. The U.S. military is investigating swarm techniques for controlling unmanned vehicles.,"[' What can be used in a number of applications?', ' What is the U.S. military investigating for controlling unmanned vehicles?']","['Swarm Intelligence-based techniques', 'swarm techniques']"
2292,interoperability,Summary,"Interoperability is a characteristic of a product or system to work with other products or systems. While the term was initially defined for information technology or systems engineering services to allow for information exchange, a broader definition takes into account social, political, and organizational factors that impact system-to-system performance.","Interoperability is a characteristic of a product or system to work with other products or systems. While the term was initially defined for information technology or systems engineering services to allow for information exchange, a broader definition takes into account social, political, and organizational factors that impact system-to-system performance.","[' Interoperability is a characteristic of a product or system to work with what?', ' What was originally defined for information technology or systems engineering services?', ' A broader definition takes into account what factors that impact system-to-system performance?']","['other products or systems', 'Interoperability', 'social, political, and organizational']"
2293,interoperability,Types,"If two or more systems use common data formats and communication protocols and are capable of communicating with each other, they exhibit syntactic interoperability. XML and SQL are examples of common data formats and protocols. Lower-level data formats also contribute to syntactic interoperability, ensuring that alphabetical characters are stored in the same ASCII or a Unicode format in all the communicating systems.
","If two or more systems use common data formats and communication protocols and are capable of communicating with each other, they exhibit syntactic interoperability. XML and SQL are examples of common data formats and protocols.",[' XML and SQL are examples of common data formats and what?'],['protocols']
2294,interoperability,Types,"Beyond the ability of two or more computer systems to exchange information, semantic interoperability is the ability to automatically interpret the information exchanged meaningfully and accurately in order to produce useful results as defined by the end users of both systems. To achieve semantic interoperability, both sides must refer to a common information exchange reference model. The content of the information exchange requests are unambiguously defined: what is sent is the same as what is understood. The possibility of promoting this result by user-driven convergence of disparate interpretations of the same information has been the object of study by research prototypes such as S3DB.
","Beyond the ability of two or more computer systems to exchange information, semantic interoperability is the ability to automatically interpret the information exchanged meaningfully and accurately in order to produce useful results as defined by the end users of both systems. To achieve semantic interoperability, both sides must refer to a common information exchange reference model.","[' What is semantic interoperability?', ' What is the ability to automatically interpret the information exchanged?', ' What must both sides refer to in order to achieve semantic interoperability?']","['the ability to automatically interpret the information exchanged meaningfully and accurately', 'semantic interoperability', 'a common information exchange reference model']"
2295,interoperability,Interoperability and open standards,"Interoperability implies exchanges between a range of products, or similar products from several different vendors, or even between past and future revisions of the same product. Interoperability may be developed post-facto, as a special measure between two products, while excluding the rest, by using Open standards. When a vendor is forced to adapt its system to a dominant system that is not based on Open standards, it is compatibility, not interoperability.
","Interoperability implies exchanges between a range of products, or similar products from several different vendors, or even between past and future revisions of the same product. Interoperability may be developed post-facto, as a special measure between two products, while excluding the rest, by using Open standards.","[' What implies exchanges between a range of products?', ' What may be developed post-facto, as a special measure between two products while excluding the rest?']","['Interoperability', 'Interoperability']"
2296,interoperability,Organizations dedicated to interoperability,"Many organizations are dedicated to interoperability. All have in common that they want to push the development of the World Wide Web towards the semantic web. Some concentrate on eGovernment, eBusiness or data exchange in general. Internationally, Network Centric Operations Industry Consortium facilitates global interoperability across borders, language and technical barriers. In Europe, for instance, the European Commission and its IDABC program issue the European Interoperability Framework. IDABC was succeeded by the ISA program. They also initiated the Semantic Interoperability Centre Europe (SEMIC.EU). A European Land Information Service (EULIS) was established in 2006, as a consortium of European National Land Registers. The aim of the service is to establish a single portal through which customers are provided with access to information about individual properties, about land and property registration services, and about the associated legal environment. In the United States, the government's CORE.gov service provides a collaboration environment for component development, sharing, registration, and reuse and related to this is the National Information Exchange Model (NIEM) work and component repository. The National Institute of Standards and Technology serves as an agency for measurement standards.
",Many organizations are dedicated to interoperability. All have in common that they want to push the development of the World Wide Web towards the semantic web.,"[' What are many organizations dedicated to?', ' What do they want to push the development of the World Wide Web towards?']","['interoperability', 'the semantic web']"
2297,algorithm,Summary,"In mathematics and computer science, an algorithm ( (listen)) is a finite sequence of well-defined instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. 
By making use of Artificial Intelligence, algorithms can perform automated deductions (referred to as automated reasoning) and use mathematical and logical tests to divert the code through various routes (referred to as automated decision-making). Using human characteristics as descriptors of machines in metaphorical ways was already practiced by Alan Turing with terms such as ""memory"", ""search"" and ""stimulus"".","In mathematics and computer science, an algorithm ( (listen)) is a finite sequence of well-defined instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing.","[' What is a finite sequence of well-defined instructions used for in mathematics and computer science?', ' What are algorithms used as specifications for performing?']","['an algorithm', 'calculations and data processing']"
2298,algorithm,Summary,"As an effective method, an algorithm can be expressed within a finite amount of space and time, and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing ""output"" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.","As an effective method, an algorithm can be expressed within a finite amount of space and time, and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing ""output"" and terminating at a final ending state.","[' What can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function?', ' Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through what?', ' What is a computation that, when executed, proceeds through a finite number of well-defined successive states?']","['an algorithm', 'a finite number of well-defined successive states', 'the instructions']"
2299,algorithm,History,"The concept of algorithm has existed since antiquity. Arithmetic algorithms, such as a division algorithm, were used by ancient Babylonian mathematicians c. 2500 BC and Egyptian mathematicians c. 1550 BC. Greek mathematicians later used algorithms in 240 BC in the sieve of Eratosthenes for finding prime numbers, and the Euclidean algorithm for finding the greatest common divisor of two numbers. Arabic mathematicians such as al-Kindi in the 9th century used cryptographic algorithms for code-breaking, based on frequency analysis.","The concept of algorithm has existed since antiquity. Arithmetic algorithms, such as a division algorithm, were used by ancient Babylonian mathematicians c. 2500 BC and Egyptian mathematicians c. 1550 BC.","[' The concept of what has existed since antiquity?', ' What was used by ancient Babylonian mathematicians c. 2500 BC?', ' Who used Arithmetic algorithms?']","['algorithm', 'Arithmetic algorithms', 'ancient Babylonian mathematicians c. 2500 BC and Egyptian mathematicians c. 1550 BC']"
2300,algorithm,History,"The word algorithm is derived from the name of the 9th-century Persian mathematician Muḥammad ibn Mūsā al-Khwārizmī, whose nisba (identifying him as from Khwarazm) was Latinized as Algoritmi (Arabized Persian الخوارزمی c. 780–850).Muḥammad ibn Mūsā al-Khwārizmī was a mathematician, astronomer, geographer, and scholar in the House of Wisdom in Baghdad, whose name means 'the native of Khwarazm', a region that was part of Greater Iran and is now in Uzbekistan. About 825, al-Khwarizmi wrote an Arabic language treatise on the Hindu–Arabic numeral system, which was translated into Latin during the 12th century. The manuscript starts with the phrase Dixit Algorizmi ('Thus spake Al-Khwarizmi'), where ""Algorizmi"" was the translator's Latinization of Al-Khwarizmi's name. Al-Khwarizmi was the most widely read mathematician in Europe in the late Middle Ages, primarily through another of his books, the Algebra. In late medieval Latin, algorismus, English 'algorism', the corruption of his name, simply meant the ""decimal number system"". In the 15th century, under the influence of the Greek word ἀριθμός (arithmos), 'number' (cf. 'arithmetic'), the Latin word was altered to algorithmus, and the corresponding English term 'algorithm' is first attested in the 17th century; the modern sense was introduced in the 19th century.","The word algorithm is derived from the name of the 9th-century Persian mathematician Muḥammad ibn Mūsā al-Khwārizmī, whose nisba (identifying him as from Khwarazm) was Latinized as Algoritmi (Arabized Persian الخوارزمی c. 780–850).Muḥammad ibn Mūsā al-Khwārizmī was a mathematician, astronomer, geographer, and scholar in the House of Wisdom in Baghdad, whose name means 'the native of Khwarazm', a region that was part of Greater Iran and is now in Uzbekistan. About 825, al-Khwarizmi wrote an Arabic language treatise on the Hindu–Arabic numeral system, which was translated into Latin during the 12th century.","[' What is the name of the 9th-century Persian mathematician?', "" What was Mu<unk>ammad ibn M<unk>s<unk> al-Khw<unk>rizm<unk>'s nisba?"", ' When was Algoritmi (Arabized Persian <unk> c. 780-850) Latinized?', ' Where is the House of Wisdom located?', ' What does the name of the house of wisdom mean?', ' When did al-Khwarizmi write a treatise on the Hindu-Arabic numeral system?', ' In what century was the Hindu-Arabic numeral system translated into Latin?', ' What is the name of the system of numerals that was translated to Latin during the 12th century?']","['Muḥammad ibn Mūsā al-Khwārizmī', 'Algoritmi', 'nisba', 'Baghdad', 'the native of Khwarazm', 'About 825', '12th', 'Hindu–Arabic']"
2301,algorithm,History,"Indian mathematics was predominantly algorithmic.
Algorithms that are representative of the Indian mathematical tradition range from the ancient Śulbasūtrās to the medieval texts of the Kerala School.",Indian mathematics was predominantly algorithmic. Algorithms that are representative of the Indian mathematical tradition range from the ancient Śulbasūtrās to the medieval texts of the Kerala School.,"[' Indian mathematics was predominantly what?', ' Algorithms that are representative of the Indian mathematical tradition range from the ancient <unk>ulbas<unk>tr<unk>s to what texts?']","['algorithmic', 'the medieval texts of the Kerala School']"
2302,algorithm,History,"In English, the word algorithm was first used in about 1230 and then by Chaucer in 1391. English adopted the French term, but it was not until the late 19th century that ""algorithm"" took on the meaning that it has in modern English.","In English, the word algorithm was first used in about 1230 and then by Chaucer in 1391. English adopted the French term, but it was not until the late 19th century that ""algorithm"" took on the meaning that it has in modern English.","[' When was the word algorithm first used in English?', ' Who first used the term algorithm in English in 1391?', ' When did the French term algorithm take on the meaning of algorithm?', ' What century did ""algorithm"" take over the meaning that it has in modern english?']","['1230', 'Chaucer', 'late 19th century', '19th']"
2303,algorithm,History,"Another early use of the word is from 1240, in a manual titled Carmen de Algorismo composed by Alexandre de Villedieu. It begins with:
","Another early use of the word is from 1240, in a manual titled Carmen de Algorismo composed by Alexandre de Villedieu. It begins with:","[' In what year was Carmen de Algorismo first used?', "" Who wrote the manual 'Carmen de Algurismo'?"", ' What is the name of the author who wrote the first use of the word Carmen?']","['1240', 'Alexandre de Villedieu', 'Alexandre de Villedieu']"
2304,algorithm,History,"A partial formalization of the modern concept of algorithm began with attempts to solve the Entscheidungsproblem  (decision problem) posed by David Hilbert in 1928. Later formalizations were framed as attempts to define ""effective calculability"" or ""effective method"". Those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936–37 and 1939.
","A partial formalization of the modern concept of algorithm began with attempts to solve the Entscheidungsproblem  (decision problem) posed by David Hilbert in 1928. Later formalizations were framed as attempts to define ""effective calculability"" or ""effective method"".","[' When did David Hilbert attempt to solve the Entscheidungsproblem?', ' What were formalizations later framed as?']","['1928', 'attempts to define ""effective calculability"" or ""effective method"".']"
2305,algorithm,Informal definition,"No human being can write fast enough, or long enough, or small enough† ( †""smaller and smaller without limit ... you'd be trying to write on molecules, on atoms, on electrons"") to list all members of an enumerably infinite set by writing out their names, one after another, in some notation. But humans can do something equally useful, in the case of certain enumerably infinite sets: They can give explicit instructions for determining the nth member of the set, for arbitrary finite n. Such instructions are to be given quite explicitly, in a form in which they could be followed by a computing machine, or by a human who is capable of carrying out only very elementary operations on symbols.","No human being can write fast enough, or long enough, or small enough† ( †""smaller and smaller without limit ... you'd be trying to write on molecules, on atoms, on electrons"") to list all members of an enumerably infinite set by writing out their names, one after another, in some notation. But humans can do something equally useful, in the case of certain enumerably infinite sets: They can give explicit instructions for determining the nth member of the set, for arbitrary finite n. Such instructions are to be given quite explicitly, in a form in which they could be followed by a computing machine, or by a human who is capable of carrying out only very elementary operations on symbols.","[' No human being can write fast enough, long enough, or small enough to list all members of an enumerably infinite set by what?', ' What can humans do in the case of certain enumerably infinite sets?', ' How can humans give explicit instructions for determining the nth member of a set?', ' What kind of instructions are given quite explicitly in a form in which they could be followed by a computing machine?', ' What is a human capable of carrying out only very elementary operations on symbols?']","['writing out their names, one after another, in some notation', 'They can give explicit instructions for determining the nth member of the set', 'in a form in which they could be followed by a computing machine', 'enumerably infinite sets', 'humans can do something equally useful, in the case of certain enumerably infinite sets: They can give explicit instructions for determining the nth member of the set, for arbitrary finite n. Such instructions are to be given quite explicitly, in a form in which they could be followed by a computing machine']"
2306,algorithm,Informal definition,"An ""enumerably infinite set"" is one whose elements can be put into one-to-one correspondence with the integers. Thus Boolos and Jeffrey are saying that an algorithm implies instructions for a process that ""creates"" output integers from an arbitrary ""input"" integer or integers that, in theory, can be arbitrarily large. For example, an algorithm can be an algebraic equation such as y = m + n (i.e., two arbitrary ""input variables"" m and n that produce an output y), but various authors' attempts to define the notion indicate that the word implies much more than this, something on the order of (for the addition example):
","An ""enumerably infinite set"" is one whose elements can be put into one-to-one correspondence with the integers. Thus Boolos and Jeffrey are saying that an algorithm implies instructions for a process that ""creates"" output integers from an arbitrary ""input"" integer or integers that, in theory, can be arbitrarily large.","[' What is an ""enumerably infinite set""?', ' Boolos and Jeffrey are saying that an algorithm implies instructions for a process that creates output integers from what?']","['one whose elements can be put into one-to-one correspondence with the integers', 'an arbitrary ""input"" integer']"
2307,algorithm,Informal definition,"The concept of algorithm is also used to define the notion of decidability—a notion that is central for explaining how formal systems come into being starting from a small set of axioms and rules. In logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related to the customary physical dimension. From such uncertainties, that characterize ongoing work, stems the unavailability of a definition of algorithm that suits both concrete (in some sense) and abstract usage of the term.
","The concept of algorithm is also used to define the notion of decidability—a notion that is central for explaining how formal systems come into being starting from a small set of axioms and rules. In logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related to the customary physical dimension.","[' What concept is used to define the notion of decidability?', ' What is central for explaining how formal systems come into being starting from a small set of axioms and rules?', ' In logic, the time that an algorithm requires to complete cannot be measured as it is what?', ' What does algorithm requires to complete not be measured?', ' What is not apparently related to the customary physical dimension?']","['algorithm', 'decidability', 'it is not apparently related to the customary physical dimension', 'time', 'the time that an algorithm requires to complete']"
2308,algorithm,Informal definition,"Most algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.
","Most algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.","[' Most algorithms are intended to be implemented as what?', ' Algorithms are implemented by what other means?']","['computer programs', 'in an electrical circuit, or in a mechanical device']"
2309,algorithm,Formalization,"Algorithms are essential to the way computers process data. Many computer programs contain algorithms that detail the specific instructions a computer should perform—in a specific order—to carry out a specified task, such as calculating employees' paychecks or printing students' report cards. Thus, an algorithm can be considered to be any sequence of operations that can be simulated by a Turing-complete system. Authors who assert this thesis include Minsky (1967), Savage (1987) and Gurevich (2000):
","Algorithms are essential to the way computers process data. Many computer programs contain algorithms that detail the specific instructions a computer should perform—in a specific order—to carry out a specified task, such as calculating employees' paychecks or printing students' report cards.","[' Algorithms are essential to the way computers process what?', ' Many computer programs contain what kind of instructions?']","['data', 'algorithms']"
2310,algorithm,Formalization," Minsky: ""But we will also maintain, with Turing ... that any procedure which could ""naturally"" be called effective, can, in fact, be realized by a (simple) machine. Although this may seem extreme, the arguments ... in its favor are hard to refute"".
 Gurevich: ""… Turing's informal argument in favor of his thesis justifies a stronger thesis: every algorithm can be simulated by a Turing machine … according to Savage [1987], an algorithm is a computational process defined by a Turing machine""."," Minsky: ""But we will also maintain, with Turing ... that any procedure which could ""naturally"" be called effective, can, in fact, be realized by a (simple) machine. Although this may seem extreme, the arguments ... in its favor are hard to refute"".","[' Minsky maintains that any procedure that could ""naturally"" be called effective can, in fact, be realized by what?']",['a (simple) machine']
2311,algorithm,Formalization,"Turing machines can define computational processes that do not terminate. The informal definitions of algorithms generally require that the algorithm always terminates. This requirement renders the task of deciding whether a formal procedure is an algorithm impossible in the general case—due to a major theorem of computability theory known as the halting problem.
",Turing machines can define computational processes that do not terminate. The informal definitions of algorithms generally require that the algorithm always terminates.,"[' What kind of machines can define computational processes that do not terminate?', ' The informal definitions of what require that the algorithm always terminates?']","['Turing machines', 'algorithms']"
2312,algorithm,Formalization,"Typically, when an algorithm is associated with processing information, data can be read from an input source, written to an output device and stored for further processing. Stored data are regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more data structures.
","Typically, when an algorithm is associated with processing information, data can be read from an input source, written to an output device and stored for further processing. Stored data are regarded as part of the internal state of the entity performing the algorithm.","[' When an algorithm is associated with processing information, data can be read from an input source, written to an output device and stored for what purpose?', ' Stored data are regarded as part of what state of the entity performing the algorithm?']","['further processing', 'internal']"
2313,algorithm,Formalization,"For some of these computational processes, the algorithm must be rigorously defined: specified in the way it applies in all possible circumstances that could arise. This means that any conditional steps must be systematically dealt with, case-by-case; the criteria for each case must be clear (and computable).
","For some of these computational processes, the algorithm must be rigorously defined: specified in the way it applies in all possible circumstances that could arise. This means that any conditional steps must be systematically dealt with, case-by-case; the criteria for each case must be clear (and computable).","[' How must the algorithm be defined for some computational processes?', ' What must be systematically dealt with, case-by-case?', ' The criteria for each case must be what?']","['rigorously', 'any conditional steps', 'clear']"
2314,algorithm,Formalization,"Because an algorithm is a precise list of precise steps, the order of computation is always crucial to the functioning of the algorithm. Instructions are usually assumed to be listed explicitly, and are described as starting ""from the top"" and going ""down to the bottom""—an idea that is described more formally by flow of control.
","Because an algorithm is a precise list of precise steps, the order of computation is always crucial to the functioning of the algorithm. Instructions are usually assumed to be listed explicitly, and are described as starting ""from the top"" and going ""down to the bottom""—an idea that is described more formally by flow of control.","[' What is always crucial to the functioning of an algorithm?', ' Instructions are usually assumed to be listed explicitly and are described as starting from what?', ' What is going ""down to the bottom"" described more formally by?']","['the order of computation', 'the top', 'flow of control']"
2315,algorithm,Formalization,"So far, the discussion on the formalization of an algorithm has assumed the premises of imperative programming. This is the most common conception—one which attempts to describe a task in discrete, ""mechanical"" means. Unique to this conception of formalized algorithms is the assignment operation, which sets the value of a variable. It derives from the intuition of ""memory"" as a scratchpad. An example of such an assignment can be found below.
","So far, the discussion on the formalization of an algorithm has assumed the premises of imperative programming. This is the most common conception—one which attempts to describe a task in discrete, ""mechanical"" means.","[' What is the most common conception of imperative programming?', ' What does imperative programming attempt to describe?']","['—one which attempts to describe a task in discrete, ""mechanical"" means', 'a task']"
2316,algorithm,Expressing algorithms,"Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in the statements based on natural language. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer, but are also often used as a way to define or document algorithms.
","Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms.","[' Algorithms can be expressed in what types of notation?', ' Natural language expressions of algorithms tend to be verbose and ambiguous and are rarely used for what?']","['natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters).', 'complex or technical algorithms']"
2317,algorithm,Design,"Algorithm design refers to a method or a mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories of operation research, such as dynamic programming and divide-and-conquer. Techniques for designing and implementing algorithm designs are also called algorithm design patterns, with examples including the template method pattern and the decorator pattern.
","Algorithm design refers to a method or a mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories of operation research, such as dynamic programming and divide-and-conquer.","[' Algorithm design refers to a method or a mathematical process for what?', ' The design of algorithms is part of what solution theories of operation research?', ' Dynamic programming and divide-and-conquer are examples of what type of theory?']","['problem-solving and engineering algorithms', 'dynamic programming and divide-and-conquer', 'solution theories of operation research']"
2318,algorithm,Design,"One of the most important aspects of algorithm design is resource (run-time, memory usage) efficiency; the big O notation is used to describe e.g. an algorithm's run-time growth as the size of its input increases.
","One of the most important aspects of algorithm design is resource (run-time, memory usage) efficiency; the big O notation is used to describe e.g. an algorithm's run-time growth as the size of its input increases.","[' What is one of the most important aspects of algorithm design?', "" What is used to describe an algorithm's run-time growth?""]","['resource (run-time, memory usage) efficiency', 'big O notation']"
2319,algorithm,Computer algorithms,"Algorithm versus function computable by an algorithm: For a given function multiple algorithms may exist. This is true, even without expanding the available instruction set available to the programmer. Rogers observes that ""It is ... important to distinguish between the notion of algorithm, i.e. procedure and the notion of function computable by algorithm, i.e. mapping yielded by procedure. The same function may have several different algorithms"".","Algorithm versus function computable by an algorithm: For a given function multiple algorithms may exist. This is true, even without expanding the available instruction set available to the programmer.","[' Algorithm versus function computable by an algorithm?', ' For a given function, how many algorithms may exist?']","['expanding the available instruction set available to the programmer', 'multiple']"
2320,algorithm,Computer algorithms,"Unfortunately, there may be a tradeoff between goodness (speed) and elegance (compactness)—an elegant program may take more steps to complete a computation than one less elegant. An example that uses Euclid's algorithm appears below.
","Unfortunately, there may be a tradeoff between goodness (speed) and elegance (compactness)—an elegant program may take more steps to complete a computation than one less elegant. An example that uses Euclid's algorithm appears below.","[' What is a tradeoff between goodness and elegance?', ' An elegant program may take more steps to complete a computation than what?']","['speed', 'one less elegant']"
2321,algorithm,Computer algorithms,"Computers (and computors), models of computation: A computer (or human ""computor"") is a restricted type of machine, a ""discrete deterministic mechanical device"" that blindly follows its instructions. Melzak's and Lambek's primitive models reduced this notion to four elements: (i) discrete, distinguishable locations, (ii) discrete, indistinguishable counters (iii) an agent, and (iv) a list of instructions that are effective relative to the capability of the agent.","Computers (and computors), models of computation: A computer (or human ""computor"") is a restricted type of machine, a ""discrete deterministic mechanical device"" that blindly follows its instructions. Melzak's and Lambek's primitive models reduced this notion to four elements: (i) discrete, distinguishable locations, (ii) discrete, indistinguishable counters (iii) an agent, and (iv) a list of instructions that are effective relative to the capability of the agent.","[' What are computer models?', ' What is a computer a restricted type of?', "" How many elements did Melzak and Lambek's primitive models reduce this notion to?"", ' What is a list of instructions that are effective relative to the capability of the agent?']","['models of computation', 'machine, a ""discrete deterministic mechanical device"" that blindly follows its instructions', 'four', 'iv']"
2322,algorithm,Computer algorithms,"Minsky describes a more congenial variation of Lambek's ""abacus"" model in his ""Very Simple Bases for Computability"". Minsky's machine proceeds sequentially through its five (or six, depending on how one counts) instructions unless either a conditional IF-THEN GOTO or an unconditional GOTO changes program flow out of sequence. Besides HALT, Minsky's machine includes three assignment (replacement, substitution) operations: ZERO (e.g. the contents of location replaced by 0: L ← 0), SUCCESSOR (e.g. L ← L+1), and DECREMENT (e.g. L ← L − 1). Rarely must a programmer write ""code"" with such a limited instruction set. But Minsky shows (as do Melzak and Lambek) that his machine is Turing complete with only four general types of instructions: conditional GOTO, unconditional GOTO, assignment/replacement/substitution, and HALT.  However, a few different assignment instructions (e.g. DECREMENT, INCREMENT, and ZERO/CLEAR/EMPTY for a Minsky machine) are also required for Turing-completeness; their exact specification is somewhat up to the designer. The unconditional GOTO is a convenience; it can be constructed by initializing a dedicated location to zero e.g. the instruction "" Z ← 0 ""; thereafter the instruction IF Z=0 THEN GOTO xxx is unconditional.
","Minsky describes a more congenial variation of Lambek's ""abacus"" model in his ""Very Simple Bases for Computability"". Minsky's machine proceeds sequentially through its five (or six, depending on how one counts) instructions unless either a conditional IF-THEN GOTO or an unconditional GOTO changes program flow out of sequence.","[' Minsky describes a more congenial variation of Lambek\'s ""abacus"" model in what book?', "" Minsky's machine proceeds sequentially through how many instructions?""]","['Very Simple Bases for Computability', 'five']"
2323,algorithm,Computer algorithms,"Simulation of an algorithm: computer (computor) language: Knuth advises the reader that ""the best way to learn an algorithm is to try it . . . immediately take pen and paper and work through an example"". But what about a simulation or execution of the real thing? The programmer must translate the algorithm into a language that the simulator/computer/computor can effectively execute. Stone gives an example of this: when computing the roots of a quadratic equation the computor must know how to take a square root. If they don't, then the algorithm, to be effective, must provide a set of rules for extracting a square root.","Simulation of an algorithm: computer (computor) language: Knuth advises the reader that ""the best way to learn an algorithm is to try it . .","[' What does computer language mean?', ' What is the best way to learn an algorithm?']","['computor', 'to try it']"
2324,algorithm,Computer algorithms,"But what model should be used for the simulation? Van Emde Boas observes ""even if we base complexity theory on abstract instead of concrete machines, arbitrariness of the choice of a model remains. It is at this point that the notion of simulation enters"". When speed is being measured, the instruction set matters. For example, the subprogram in Euclid's algorithm to compute the remainder would execute much faster if the programmer had a ""modulus"" instruction available rather than just subtraction (or worse: just Minsky's ""decrement"").
","But what model should be used for the simulation? Van Emde Boas observes ""even if we base complexity theory on abstract instead of concrete machines, arbitrariness of the choice of a model remains.","[' What model should be used for simulation?', ' Van Emde Boas observes that even if we base complexity theory on abstract instead of concrete machines, what remains?']","['concrete machines', 'arbitrariness of the choice of a model']"
2325,algorithm,Computer algorithms,"Structured programming, canonical structures: Per the Church–Turing thesis, any algorithm can be computed by a model known to be Turing complete, and per Minsky's demonstrations, Turing completeness requires only four instruction types—conditional GOTO, unconditional GOTO, assignment, HALT. Kemeny and Kurtz observe that, while ""undisciplined"" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in ""spaghetti code"", a programmer can write structured programs using only these instructions; on the other hand ""it is also possible, and not too hard, to write badly structured programs in a structured language"". Tausworthe augments the three Böhm-Jacopini canonical structures: SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE. An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.","Structured programming, canonical structures: Per the Church–Turing thesis, any algorithm can be computed by a model known to be Turing complete, and per Minsky's demonstrations, Turing completeness requires only four instruction types—conditional GOTO, unconditional GOTO, assignment, HALT. Kemeny and Kurtz observe that, while ""undisciplined"" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in ""spaghetti code"", a programmer can write structured programs using only these instructions; on the other hand ""it is also possible, and not too hard, to write badly structured programs in a structured language"".","[' What can be computed by a model known as Turing complete?', ' What requires only four instruction types?', ' Kemeny and Kurtz observe that ""undisciplined"" use of unconditional GOTOs and what else?', ' What can result in ""spaghetti code""?', ' What can a programmer write using only these instructions?', ' How is it possible to write badly structured programs in a structured language?']","['any algorithm', 'Turing completeness', 'conditional IF-THEN GOTOs', 'undisciplined"" use of unconditional GOTOs and conditional IF-THEN GOTOs', 'structured programs', 'it is also possible, and not too hard']"
2326,algorithm,Computer algorithms,"Canonical flowchart symbols: The graphical aide called a flowchart, offers a way to describe and document an algorithm (and a computer program of one). Like the program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down. Its primary symbols are only four: the directed arrow showing program flow, the rectangle (SEQUENCE, GOTO), the diamond (IF-THEN-ELSE), and the dot (OR-tie). The Böhm–Jacopini canonical structures are made of these primitive shapes. Sub-structures can ""nest"" in rectangles, but only if a single exit occurs from the superstructure. The symbols, and their use to build the canonical structures are shown in the diagram.
","Canonical flowchart symbols: The graphical aide called a flowchart, offers a way to describe and document an algorithm (and a computer program of one). Like the program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down.","[' What is a graphical aide called?', ' What does a flowchart offer a way to describe and document?']","['flowchart', 'an algorithm']"
2327,algorithm,Algorithmic analysis,"It is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, an algorithm which adds up the elements of a list of n numbers would have a time requirement of O(n), using big O notation. At all times the algorithm only needs to remember two values: the sum of all the elements so far, and its current position in the input list. Therefore, it is said to have a space requirement of O(1), if the space required to store the input numbers is not counted, or O(n) if it is counted.
","It is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, an algorithm which adds up the elements of a list of n numbers would have a time requirement of O(n), using big O notation.","[' What is often important to know how much of a particular resource is theoretically required for an algorithm?', ' Methods have been developed for the analysis of algorithms to obtain what?', ' What would an algorithm that adds up the elements of a list of n numbers have a time requirement of?']","['time or storage', 'quantitative answers (estimates', 'O(n']"
2328,algorithm,Algorithmic analysis,"Different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost O(log n)) outperforms a sequential search (cost O(n) ) when used for table lookups on sorted lists or arrays.
","Different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost O(log n)) outperforms a sequential search (cost O(n) ) when used for table lookups on sorted lists or arrays.","[' What may different algorithms complete the same task with a different set of instructions in less or more time, space, or effort than others?', ' What outperforms a sequential search?']","['binary search algorithm', 'a binary search algorithm']"
2329,algorithm,Legal issues,"Algorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute ""processes"" (USPTO 2006), and hence algorithms are not patentable (as in Gottschalk v. Benson). However practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is highly controversial, and there are highly criticized patents involving algorithms, especially data compression algorithms, such as Unisys' LZW patent.
","Algorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute ""processes"" (USPTO 2006), and hence algorithms are not patentable (as in Gottschalk v. Benson).","[' What are algorithms not usually patentable?', ' What does a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals not constitute in the US?']","['by themselves', 'processes']"
2330,blockchain,Summary,"A blockchain is a growing list of records, called blocks, that are linked together using cryptography. Each block contains a cryptographic hash of the previous block, a timestamp, and transaction data (generally represented as a Merkle tree). The timestamp proves that the transaction data existed when the block was published in order to get into its hash. As blocks each contain information about the block previous to it, they form a chain, with each additional block reinforcing the ones before it. Therefore, blockchains are resistant to modification of their data because once recorded, the data in any given block cannot be altered retroactively without altering all subsequent blocks.
","A blockchain is a growing list of records, called blocks, that are linked together using cryptography. Each block contains a cryptographic hash of the previous block, a timestamp, and transaction data (generally represented as a Merkle tree).","[' What is a growing list of records called?', ' What are the records called that are linked together using cryptography?', ' A block contains a cryptographic hash of what?']","['A blockchain', 'blocks', 'the previous block']"
2331,blockchain,Summary,"Blockchains are typically managed by a peer-to-peer network for use as a publicly distributed ledger, where nodes collectively adhere to a protocol to communicate and validate new blocks. Although blockchain records are not unalterable as forks are possible, blockchains may be considered secure by design and exemplify a distributed computing system with high Byzantine fault tolerance.","Blockchains are typically managed by a peer-to-peer network for use as a publicly distributed ledger, where nodes collectively adhere to a protocol to communicate and validate new blocks. Although blockchain records are not unalterable as forks are possible, blockchains may be considered secure by design and exemplify a distributed computing system with high Byzantine fault tolerance.","[' What type of network typically manages blockchains?', ' What is used as a publicly distributed ledger?', ' Nodes collectively adhere to what to communicate and validate new blocks?', ' What is a distributed computing system with high Byzantine fault tolerance?']","['peer-to-peer', 'Blockchains', 'a protocol', 'Blockchains']"
2332,blockchain,Summary,"The blockchain was popularized by a person (or group of people) using the name Satoshi Nakamoto in 2008 to serve as the public transaction ledger of the cryptocurrency bitcoin, based on work by Stuart Haber, W. Scott Stornetta, and Dave Bayer. The identity of Satoshi Nakamoto remains unknown to date. The implementation of the blockchain within bitcoin made it the first digital currency to solve the double-spending problem without the need of a trusted authority or central server. The bitcoin design has inspired other applications and blockchains that are readable by the public and are widely used by cryptocurrencies. The blockchain is considered a type of payment rail.","The blockchain was popularized by a person (or group of people) using the name Satoshi Nakamoto in 2008 to serve as the public transaction ledger of the cryptocurrency bitcoin, based on work by Stuart Haber, W. Scott Stornetta, and Dave Bayer. The identity of Satoshi Nakamoto remains unknown to date.","[' What was popularized by a person using the name Satoshi Nakamoto in 2008?', ' What was the public transaction ledger of the cryptocurrency bitcoin based on work by Stuart Haber, W. Scott Stornetta, and Dave Bayer?']","['The blockchain', 'The blockchain']"
2333,blockchain,Summary,"Private blockchains have been proposed for business use. Computerworld called the marketing of such privatized blockchains without a proper security model ""snake oil""; however, others have argued that permissioned blockchains, if carefully designed, may be more decentralized and therefore more secure in practice than permissionless ones.","Private blockchains have been proposed for business use. Computerworld called the marketing of such privatized blockchains without a proper security model ""snake oil""; however, others have argued that permissioned blockchains, if carefully designed, may be more decentralized and therefore more secure in practice than permissionless ones.","[' What type of blockchains have been proposed for business use?', ' Computerworld called the marketing of privatized blockchains without a proper security model what?']","['Private blockchains', 'snake oil']"
2334,blockchain,History,"Cryptographer David Chaum first proposed a blockchain-like protocol in his 1982 dissertation ""Computer Systems Established, Maintained, and Trusted by Mutually Suspicious Groups."" Further work on a cryptographically secured chain of blocks was described in 1991 by Stuart Haber and W. Scott Stornetta. They wanted to implement a system wherein document timestamps could not be tampered with. In 1992, Haber, Stornetta, and Dave Bayer incorporated Merkle trees to the design, which improved its efficiency by allowing several document certificates to be collected into one block. Under their company Surety, their document certificate hashes have been published in The New York Times every week since 1995.","Cryptographer David Chaum first proposed a blockchain-like protocol in his 1982 dissertation ""Computer Systems Established, Maintained, and Trusted by Mutually Suspicious Groups."" Further work on a cryptographically secured chain of blocks was described in 1991 by Stuart Haber and W. Scott Stornetta.","[' Who first proposed a blockchain-like protocol?', "" What was Chaum's 1982 dissertation called?"", ' Who described a cryptographically secured chain of blocks in 1991?']","['David Chaum', 'Computer Systems Established, Maintained, and Trusted by Mutually Suspicious Groups.""', 'Stuart Haber and W. Scott Stornetta']"
2335,blockchain,History,"The first decentralized blockchain was conceptualized by a person (or group of people) known as Satoshi Nakamoto in 2008. Nakamoto improved the design in an important way using a Hashcash-like method to timestamp blocks without requiring them to be signed by a trusted party and introducing a difficulty parameter to stabilize the rate at which blocks are added to the chain. The design was implemented the following year by Nakamoto as a core component of the cryptocurrency bitcoin, where it serves as the public ledger for all transactions on the network.",The first decentralized blockchain was conceptualized by a person (or group of people) known as Satoshi Nakamoto in 2008. Nakamoto improved the design in an important way using a Hashcash-like method to timestamp blocks without requiring them to be signed by a trusted party and introducing a difficulty parameter to stabilize the rate at which blocks are added to the chain.,"[' When was the first decentralized blockchain conceptualized?', ' What was the name of the person or group of people who conceptualized the first blockchain?', ' Who improved the design of the blockchain by using a Hashcash-like method to timestamp blocks?', ' What is introduced to stabilize the rate at which blocks are added to the chain?']","['2008', 'Satoshi Nakamoto', 'Satoshi Nakamoto', 'a difficulty parameter']"
2336,blockchain,History,"In August 2014, the bitcoin blockchain file size, containing records of all transactions that have occurred on the network, reached 20 GB (gigabytes). In January 2015, the size had grown to almost 30 GB, and from January 2016 to January 2017, the bitcoin blockchain grew from 50 GB to 100 GB in size.  The ledger size had exceeded 200 GB by early 2020.","In August 2014, the bitcoin blockchain file size, containing records of all transactions that have occurred on the network, reached 20 GB (gigabytes). In January 2015, the size had grown to almost 30 GB, and from January 2016 to January 2017, the bitcoin blockchain grew from 50 GB to 100 GB in size.","[' When did the bitcoin blockchain file size reach 20 GB?', ' What was the size of the bitcoin file in January 2015?', ' From January 2016 to January 2017, how much did the blockchain size grow from 50 to 100 GB in size?']","['August 2014', '30\xa0GB', 'almost 30\xa0GB']"
2337,blockchain,History,"According to Accenture, an application of the diffusion of innovations theory suggests that blockchains attained a 13.5% adoption rate within financial services in 2016, therefore reaching the early adopters phase. Industry trade groups joined to create the Global Blockchain Forum in 2016, an initiative of the Chamber of Digital Commerce.
","According to Accenture, an application of the diffusion of innovations theory suggests that blockchains attained a 13.5% adoption rate within financial services in 2016, therefore reaching the early adopters phase. Industry trade groups joined to create the Global Blockchain Forum in 2016, an initiative of the Chamber of Digital Commerce.","[' According to Accenture, what was the adoption rate of blockchains in financial services in 2016?', ' In what year did industry trade groups join together to create the Global Blockchain Forum?']","['13.5%', '2016']"
2338,blockchain,History,"In May 2018, Gartner found that only 1% of CIOs indicated any kind of blockchain adoption within their organisations, and only 8% of CIOs were in the short-term ""planning or [looking at] active experimentation with blockchain"". For the year 2019 Gartner reported 5% of CIOs believed blockchain technology was a 'game-changer' for their business.","In May 2018, Gartner found that only 1% of CIOs indicated any kind of blockchain adoption within their organisations, and only 8% of CIOs were in the short-term ""planning or [looking at] active experimentation with blockchain"". For the year 2019 Gartner reported 5% of CIOs believed blockchain technology was a 'game-changer' for their business.","[' What percent of CIOs indicated any kind of blockchain adoption within their organizations in May 2018?', ' What percent were in the short-term ""planning or looking at active experimentation with blockchain""?', ' Gartner found that what percent believed blockchain technology was a good idea for their organization in 2019?', "" What percentage of CIOs believed blockchain technology was a 'game-changer' for their business?""]","['1%', '8%', '5%', '5%']"
2339,blockchain,Structure,"A blockchain is a decentralized, distributed, and oftentimes public, digital ledger consisting of records called blocks that is used to record transactions across many computers so that any involved block cannot be altered retroactively, without the alteration of all subsequent blocks. This allows the participants to verify and audit transactions independently and relatively inexpensively. A blockchain database is managed autonomously using a peer-to-peer network and a distributed timestamping server. They are authenticated by mass collaboration powered by collective self-interests. Such a design facilitates robust workflow where participants' uncertainty regarding data security is marginal. The use of a blockchain removes the characteristic of infinite reproducibility from a digital asset. It confirms that each unit of value was transferred only once, solving the long-standing problem of double spending. A blockchain has been described as a value-exchange protocol. A blockchain can maintain title rights because, when properly set up to detail the exchange agreement, it provides a record that compels offer and acceptance.
","A blockchain is a decentralized, distributed, and oftentimes public, digital ledger consisting of records called blocks that is used to record transactions across many computers so that any involved block cannot be altered retroactively, without the alteration of all subsequent blocks. This allows the participants to verify and audit transactions independently and relatively inexpensively.","[' What is a decentralized, distributed, and oftentimes public, digital ledger consisting of records called?', ' What is used to record transactions across many computers?', ' How can any involved block be altered retroactively without the alteration of all subsequent blocks?', ' What allows the participants to verify and audit transactions independently and relatively inexpensive?']","['A blockchain', 'A blockchain', 'blockchain', 'A blockchain']"
2340,blockchain,Uses,"Blockchain technology can be integrated into multiple areas. The primary use of blockchains is as a distributed ledger for cryptocurrencies such as bitcoin; there were also a few other operational products which had matured from proof of concept by late 2016. As of 2016, some businesses have been testing the technology and conducting low-level implementation to gauge blockchain's effects on organizational efficiency in their back office.",Blockchain technology can be integrated into multiple areas. The primary use of blockchains is as a distributed ledger for cryptocurrencies such as bitcoin; there were also a few other operational products which had matured from proof of concept by late 2016.,"[' How can blockchain technology be integrated into multiple areas?', ' What is the primary use of blockchains?', ' When did a few other operational products mature from proof of concept?']","['distributed ledger', 'a distributed ledger for cryptocurrencies', 'late 2016']"
2341,blockchain,Uses,"In 2019, it was estimated that around $2.9 billion were invested in blockchain technology, which represents an 89% increase from the year prior. Additionally, the International Data Corp has estimated that corporate investment into blockchain technology will reach $12.4 billion by 2022. Furthermore, According to PricewaterhouseCoopers (PwC), the second-largest professional services network in the world, blockchain technology has the potential to generate an annual business value of more than $3 trillion by 2030. PwC's estimate is further augmented by a 2018 study that they have conducted, in which PwC surveyed 600 business executives and determined that 84% have at least some exposure to utilizing blockchain technology, which indicts a significant demand and interest in blockchain technology.","In 2019, it was estimated that around $2.9 billion were invested in blockchain technology, which represents an 89% increase from the year prior. Additionally, the International Data Corp has estimated that corporate investment into blockchain technology will reach $12.4 billion by 2022.","[' How much money was invested in blockchain technology in 2019?', ' How much more money did the International Data Corp expect to invest in blockchain by 2022?']","['$2.9 billion', '$12.4 billion']"
2342,blockchain,Uses,"Individual use of blockchain technology has also greatly increased since 2016. According to statistics in 2020, there were more than 40 million blockchain wallets in 2020 in comparison to around 10 million blockchain wallets in 2016.","Individual use of blockchain technology has also greatly increased since 2016. According to statistics in 2020, there were more than 40 million blockchain wallets in 2020 in comparison to around 10 million blockchain wallets in 2016.",[' How many blockchain wallets were there in 2020 compared to 10 million in 2016?'],['more than 40 million']
2343,blockchain,Blockchain interoperability,"With the increasing number of blockchain systems appearing, even only those that support cryptocurrencies, blockchain interoperability is becoming a topic of major importance. The objective is to support transferring assets from one blockchain system to another blockchain system. Wegner stated that ""interoperability is the ability of two or more software components to cooperate despite differences in language, interface, and execution platform"". The objective of blockchain interoperability is therefore to support such cooperation among blockchain systems, despite those kinds of differences.
","With the increasing number of blockchain systems appearing, even only those that support cryptocurrencies, blockchain interoperability is becoming a topic of major importance. The objective is to support transferring assets from one blockchain system to another blockchain system.","[' What is becoming a topic of major importance?', ' What is the objective of blockchain interoperability?']","['blockchain interoperability', 'to support transferring assets from one blockchain system to another blockchain system']"
2344,blockchain,Blockchain interoperability,"There are already several blockchain interoperability solutions available. They can be classified in three categories: cryptocurrency interoperability approaches, blockchain engines, and blockchain connectors.
","There are already several blockchain interoperability solutions available. They can be classified in three categories: cryptocurrency interoperability approaches, blockchain engines, and blockchain connectors.","[' How many categories can blockchain interoperability solutions be classified into?', ' What are the three categories of blockchain solutions?', ' Blockchain engines and blockchain connectors can be classified in what categories?']","['three', 'cryptocurrency interoperability approaches, blockchain engines, and blockchain connectors', 'three']"
2345,blockchain,Energy consumption concerns,"Blockchain mining — the peer-to-peer computer computations by which transactions are validated and verified — requires a significant amount of energy. In June 2018 the Bank for International Settlements criticized the use of public proof-of-work blockchains due to their high energy consumption. In 2021, a study conducted by Cambridge University determined that Bitcoin (at 121.36 terawatt-hours per year) uses more electricity annually than Argentina (at 121 TWh) and the Netherlands (at 108.8 TWh). According to Digiconomist, one bitcoin transaction requires about 707.6 kilowatt-hours of electrical energy, the amount of energy the average U.S. household consumes in 24 days.",Blockchain mining — the peer-to-peer computer computations by which transactions are validated and verified — requires a significant amount of energy. In June 2018 the Bank for International Settlements criticized the use of public proof-of-work blockchains due to their high energy consumption.,"[' What is the name of the peer-to-peer computer computations by which transactions are validated and verified?', ' When did the Bank for International Settlements criticize the use of public proof-of-work blockchains?']","['Blockchain mining', 'June 2018']"
2346,blockchain,Energy consumption concerns,"In February 2021 U.S. Treasury Secretary Janet Yellen called Bitcoin ""an extremely inefficient way to conduct transactions"", saying ""the amount of energy consumed in processing those transactions is staggering."" In March 2021 Bill Gates stated that ""Bitcoin uses more electricity per transaction than any other method known to mankind"", ""It's not a great climate thing.""","In February 2021 U.S. Treasury Secretary Janet Yellen called Bitcoin ""an extremely inefficient way to conduct transactions"", saying ""the amount of energy consumed in processing those transactions is staggering."" In March 2021 Bill Gates stated that ""Bitcoin uses more electricity per transaction than any other method known to mankind"", ""It's not a great climate thing.""","[' What did Janet Yellen call Bitcoin in February 2021?', ' What did Bill Gates say Bitcoin uses more electricity per transaction than any other method known to mankind?', ' How many transactions per transaction is there than any other method known to mankind?']","['an extremely inefficient way to conduct transactions', '2021', 'more electricity']"
2347,blockchain,Energy consumption concerns,"Nicholas Weaver, of the International Computer Science Institute at the University of California, Berkeley, examined blockchain's online security, and the energy efficiency of proof-of-work public blockchains, and in both cases found it grossly inadequate. The 31–45 TWh of electricity used for bitcoin in 2018 produced 17–22.9 MtCO2.","Nicholas Weaver, of the International Computer Science Institute at the University of California, Berkeley, examined blockchain's online security, and the energy efficiency of proof-of-work public blockchains, and in both cases found it grossly inadequate. The 31–45 TWh of electricity used for bitcoin in 2018 produced 17–22.9 MtCO2.","["" Nicholas Weaver of the International Computer Science Institute at what university examined blockchain's online security?"", ' What did Nicholas Wever find in both cases of public blockchains?', ' How much electricity did the 31-45 TWh of electricity used for bitcoin produce?']","['University of California, Berkeley', 'it grossly inadequate', '17–22.9 MtCO2']"
2348,blockchain,Academic research,"In October 2014, the MIT Bitcoin Club, with funding from MIT alumni, provided undergraduate students at the Massachusetts Institute of Technology access to $100 of bitcoin. The adoption rates, as studied by Catalini and Tucker (2016), revealed that when people who typically adopt technologies early are given delayed access, they tend to reject the technology. Many universities have founded departments focusing on crypto and blockchain, including MIT, in 2017. In the same year, Edinburgh became ""one of the first big European universities to launch a blockchain course"", according to the Financial Times.","In October 2014, the MIT Bitcoin Club, with funding from MIT alumni, provided undergraduate students at the Massachusetts Institute of Technology access to $100 of bitcoin. The adoption rates, as studied by Catalini and Tucker (2016), revealed that when people who typically adopt technologies early are given delayed access, they tend to reject the technology.","[' What did the MIT Bitcoin Club provide undergraduate students at the Massachusetts Institute of Technology with in October 2014?', ' What did Catalini and Tucker study about adoption rates of bitcoin?', ' When people who typically adopt technologies early are given delayed access, what happens?', ' Who typically adopts technologies early is given delayed access?', ' Who tends to reject the technology?']","['$100 of bitcoin', 'when people who typically adopt technologies early are given delayed access, they tend to reject the technology', 'they tend to reject the technology', 'they tend to reject the technology', 'people who typically adopt technologies early']"
2349,image registration,Summary,"Image registration is the process of transforming different sets of data into one coordinate system. Data may be multiple photographs, data from different sensors, times, depths, or viewpoints. It is used in computer vision, medical imaging, military automatic target recognition, and compiling and analyzing images and data from satellites. Registration is necessary in order to be able to compare or integrate the data obtained from these different measurements.
","Image registration is the process of transforming different sets of data into one coordinate system. Data may be multiple photographs, data from different sensors, times, depths, or viewpoints.","[' What is the process of transforming different sets of data into one coordinate system?', ' Data may be multiple photographs, data from different sensors, times, depths, or what else?']","['Image registration', 'viewpoints']"
2350,image registration,Uncertainty,"There is a level of uncertainty associated with registering images that have any spatio-temporal differences.  A confident registration with a measure of uncertainty is critical for many change detection applications such as medical diagnostics.
",There is a level of uncertainty associated with registering images that have any spatio-temporal differences. A confident registration with a measure of uncertainty is critical for many change detection applications such as medical diagnostics.,"[' What is the level of uncertainty associated with registering images that have any spatio-temporal differences?', ' What is critical for many change detection applications such as medical diagnostics?']","['There is a level of uncertainty associated with registering images that have any spatio-temporal differences. A confident registration with a measure of uncertainty', 'A confident registration with a measure of uncertainty']"
2351,image registration,Uncertainty,"In remote sensing applications where a digital image pixel may represent several kilometers of spatial distance (such as NASA's LANDSAT imagery), an uncertain image registration can mean that a solution could be several kilometers from ground truth.  Several notable papers have attempted to quantify uncertainty in image registration in order to compare results. However, many approaches to quantifying uncertainty or estimating deformations are computationally intensive or are only applicable to limited sets of spatial transformations.
","In remote sensing applications where a digital image pixel may represent several kilometers of spatial distance (such as NASA's LANDSAT imagery), an uncertain image registration can mean that a solution could be several kilometers from ground truth. Several notable papers have attempted to quantify uncertainty in image registration in order to compare results.","["" What is NASA's remote sensing application?"", ' What can be several kilometers from ground truth?', ' How have papers attempted to quantify uncertainty in image registration?', ' Have you tried to quantify uncertainty in image registration in order to compare results?']","['LANDSAT imagery', 'an uncertain image registration', 'to compare results', 'Several notable papers']"
2352,image registration,Applications,"Image registration has applications in remote sensing (cartography updating), and computer vision.  Due to the vast range of applications to which image registration can be applied, it is impossible to develop a general method that is optimized for all uses.
","Image registration has applications in remote sensing (cartography updating), and computer vision. Due to the vast range of applications to which image registration can be applied, it is impossible to develop a general method that is optimized for all uses.",[' Image registration has applications in remote sensing (cartography updating) and what other field?'],['computer vision']
2353,image registration,Applications,"Medical image registration (for data of the same patient taken at different points in time such as change detection or tumor monitoring) often additionally involves elastic (also known as nonrigid) registration to cope with deformation of the subject (due to breathing, anatomical changes, and so forth). Nonrigid registration of medical images can also be used to register a patient's data to an anatomical atlas, such as the Talairach atlas for neuroimaging.
","Medical image registration (for data of the same patient taken at different points in time such as change detection or tumor monitoring) often additionally involves elastic (also known as nonrigid) registration to cope with deformation of the subject (due to breathing, anatomical changes, and so forth). Nonrigid registration of medical images can also be used to register a patient's data to an anatomical atlas, such as the Talairach atlas for neuroimaging.","[' What type of registration is used for data of the same patient taken at different points in time?', ' What is elastic registration also known as?', "" What can be used to register a patient's data to an anatomical atlas?""]","['Medical image', 'nonrigid', 'Nonrigid registration of medical images']"
2354,image registration,Applications,"In astrophotography image alignment and stacking are often used to increase the signal to noise ratio for faint objects. Without stacking it may be used to produce a timelapse of events such as a planet's rotation of a transit across the Sun.  Using control points (automatically or manually entered), the computer performs transformations on one image to make major features align with a second or multiple images. This technique may also be used for images of different sizes, to allow images taken through different telescopes or lenses to be combined.
",In astrophotography image alignment and stacking are often used to increase the signal to noise ratio for faint objects. Without stacking it may be used to produce a timelapse of events such as a planet's rotation of a transit across the Sun.,"[' Image alignment and stacking are often used to increase the signal to noise ratio for faint objects.', "" Without stacking it may be used to produce a timelapse of events such as a planet's rotation of what?""]","['astrophotography', 'a transit across the Sun']"
2355,image registration,Applications,"In cryo-TEM instability causes specimen drift and many fast acquisitions with accurate image registration is required to preserve high resolution and obtain high signal to noise images. For low SNR data, the best image registration is achieved by cross-correlating all permutations of images in an image stack.","In cryo-TEM instability causes specimen drift and many fast acquisitions with accurate image registration is required to preserve high resolution and obtain high signal to noise images. For low SNR data, the best image registration is achieved by cross-correlating all permutations of images in an image stack.","[' What causes specimen drift in cryo-TEM?', ' What is required to preserve high resolution and obtain high signal to noise images?', ' For low SNR data, what is the best image registration?']","['instability', 'accurate image registration', 'cross-correlating all permutations of images in an image stack']"
2356,image registration,Applications,"Image registration is an essential part of panoramic image creation. There are many different techniques that can be implemented in real time and run on embedded devices like cameras and camera-phones.
",Image registration is an essential part of panoramic image creation. There are many different techniques that can be implemented in real time and run on embedded devices like cameras and camera-phones.,"[' What is an essential part of panoramic image creation?', ' What can be implemented in real time and run on embedded devices?']","['Image registration', 'Image registration is an essential part of panoramic image creation. There are many different techniques']"
2357,graph transformation,Summary,"In computer science, graph transformation, or graph rewriting, concerns the technique of creating a new graph out of an original graph algorithmically. It has numerous applications, ranging from software engineering (software construction and also software verification) to layout algorithms and picture generation.
","In computer science, graph transformation, or graph rewriting, concerns the technique of creating a new graph out of an original graph algorithmically. It has numerous applications, ranging from software engineering (software construction and also software verification) to layout algorithms and picture generation.","[' What is another name for graph rewriting?', ' What is a common application of graph transformation?']","['graph transformation', 'software engineering']"
2358,graph transformation,Summary,"Graph transformations can be used as a computation abstraction. The basic idea is that if the state of a computation can be represented as a graph, further steps in that computation can then be represented as transformation rules on that graph. Such rules consist of an original graph, which is to be matched to a subgraph in the complete state, and a replacing graph, which will replace the matched subgraph.
","Graph transformations can be used as a computation abstraction. The basic idea is that if the state of a computation can be represented as a graph, further steps in that computation can then be represented as transformation rules on that graph.","[' What can be used as a computation abstraction?', ' What is the basic idea?']","['Graph transformations', 'if the state of a computation can be represented as a graph, further steps in that computation can then be represented as transformation rules on that graph']"
2359,graph transformation,Summary,"Formally, a graph rewriting system usually consists of a set of graph rewrite rules of the form 



L
→
R


{\displaystyle L\rightarrow R}
, with 



L


{\displaystyle L}
 being called pattern graph (or left-hand side) and 



R


{\displaystyle R}
 being called replacement graph (or right-hand side of the rule). A graph rewrite rule is applied to the host graph by searching for an occurrence of the pattern graph (pattern matching, thus solving the subgraph isomorphism problem) and by replacing the found occurrence by an instance of the replacement graph. Rewrite rules can be further regulated in the case of labeled graphs, such as in string-regulated graph grammars.
","Formally, a graph rewriting system usually consists of a set of graph rewrite rules of the form 



L
→
R


{\displaystyle L\rightarrow R}
, with 



L


{\displaystyle L}
 being called pattern graph (or left-hand side) and 



R


{\displaystyle R}
 being called replacement graph (or right-hand side of the rule). A graph rewrite rule is applied to the host graph by searching for an occurrence of the pattern graph (pattern matching, thus solving the subgraph isomorphism problem) and by replacing the found occurrence by an instance of the replacement graph.","[' What is the form of a graph rewriting system?', ' What is L <unk> R <unk>displaystyle L<unk>rightarrow R<unk>?', ' What is applied to the host graph by searching for an occurrence of the pattern graph?', ' What solves the subgraph isomorphism problem?']","['L\n→\nR', 'L\n→\nR', 'graph rewrite rule', 'A graph rewrite rule is applied to the host graph by searching for an occurrence of the pattern graph (pattern matching']"
2360,graph transformation,Summary,"Sometimes graph grammar is used as a synonym for graph rewriting system, especially in the context of formal languages; the different wording is used to emphasize the goal of constructions, like the enumeration of all graphs from some starting graph, i.e. the generation of a graph language – instead of simply transforming a given state (host graph) into a new state.
","Sometimes graph grammar is used as a synonym for graph rewriting system, especially in the context of formal languages; the different wording is used to emphasize the goal of constructions, like the enumeration of all graphs from some starting graph, i.e. the generation of a graph language – instead of simply transforming a given state (host graph) into a new state.","[' What is sometimes used as a synonym for graph rewriting system?', ' What is used to emphasize the goal of constructions?', ' What does the generation of a graph language do?', ' What is a host graph?']","['graph grammar', 'different wording', 'instead of simply transforming a given state (host graph) into a new state', 'state']"
2361,graph transformation,Classes of graph grammar and graph rewriting system,"Graph rewriting systems naturally group into classes according to the kind of representation of graphs that are used and how the rewrites are expressed.  The term graph grammar, otherwise equivalent to graph rewriting system or graph replacement system, is most often used in classifications.  Some common types are:
","Graph rewriting systems naturally group into classes according to the kind of representation of graphs that are used and how the rewrites are expressed. The term graph grammar, otherwise equivalent to graph rewriting system or graph replacement system, is most often used in classifications.","[' Graph rewriting systems naturally group into classes according to what?', ' What term is most often used in classifications?']","['the kind of representation of graphs that are used and how the rewrites are expressed', 'graph grammar']"
2362,graph transformation,Implementations and applications,"Graphs are an expressive, visual and mathematically precise formalism for modelling of objects (entities) linked by relations; objects are represented by nodes and relations between them by edges. Nodes and edges are commonly typed and attributed. Computations are described in this model by changes in the relations between the entities or by attribute changes of the graph elements. They are encoded in graph rewrite/graph transformation rules and executed by graph rewrite systems/graph transformation tools.
","Graphs are an expressive, visual and mathematically precise formalism for modelling of objects (entities) linked by relations; objects are represented by nodes and relations between them by edges. Nodes and edges are commonly typed and attributed.","[' What is an expressive, visual and mathematically precise formalism for modelling of objects linked by relations?', ' What are objects represented by?', ' Nodes and relations between objects are represented by what?']","['Graphs', 'nodes and relations between them by edges', 'edges']"
2363,human-computer interaction,Summary,"Human–computer interaction (HCI) is research in the design and the use of computer technology, which focuses on the interfaces between people (users) and computers. HCI researchers observe the ways humans interact with computers and design technologies that allow humans to interact with computers in novel ways.
","Human–computer interaction (HCI) is research in the design and the use of computer technology, which focuses on the interfaces between people (users) and computers. HCI researchers observe the ways humans interact with computers and design technologies that allow humans to interact with computers in novel ways.","[' What is HCI?', ' What does HCI focus on?', ' How do HCI researchers observe the ways humans interact with computers?']","['research in the design and the use of computer technology', 'interfaces between people (users) and computers', 'design technologies that allow humans to interact with computers in novel ways']"
2364,human-computer interaction,Summary,"As a field of research, human–computer interaction is situated at the intersection of computer science, behavioral sciences, design, media studies, and several other fields of study. The term was popularized by Stuart K. Card, Allen Newell, and Thomas P. Moran in their 1983 book, The Psychology of Human–Computer Interaction, although the authors first used the term in 1980, and the first known use was in 1975. The term is intended to convey that, unlike other tools with specific and limited uses, computers have many uses which often involve an open-ended dialogue between the user and the computer. The notion of dialogue likens human–computer interaction to human-to-human interaction: an analogy that is crucial to theoretical considerations in the field.","As a field of research, human–computer interaction is situated at the intersection of computer science, behavioral sciences, design, media studies, and several other fields of study. The term was popularized by Stuart K. Card, Allen Newell, and Thomas P. Moran in their 1983 book, The Psychology of Human–Computer Interaction, although the authors first used the term in 1980, and the first known use was in 1975.","[' Who popularized the term human-computer interaction in 1983?', ' What was the name of the 1983 book that popularized human–computer interaction?', ' In what year was The Psychology of Human-Computer Interaction published?', ' When was the first known use of the term Human-computer interaction?']","['Stuart K. Card, Allen Newell, and Thomas P. Moran', 'The Psychology of Human–Computer Interaction', '1983', '1975']"
2365,human-computer interaction,Introduction,"Humans interact with computers in many ways, and the interface between the two is crucial to facilitating this interaction. HCI is also sometimes termed human–machine interaction (HMI), man-machine interaction (MMI) or computer-human interaction (CHI). Desktop applications, internet browsers, handheld computers, and computer kiosks make use of the prevalent graphical user interfaces (GUI) of today. Voice user interfaces (VUI) are used for speech recognition and synthesizing systems, and the emerging multi-modal and Graphical user interfaces (GUI) allow humans to engage with embodied character agents in a way that cannot be achieved with other interface paradigms. The growth in human–computer interaction field has led to an increase in the quality of interaction, and resulted in many new areas of research beyond. Instead of designing regular interfaces, the different research branches focus on the concepts of multimodality over unimodality, intelligent adaptive interfaces over command/action based ones, and active interfaces over passive interfaces.","Humans interact with computers in many ways, and the interface between the two is crucial to facilitating this interaction. HCI is also sometimes termed human–machine interaction (HMI), man-machine interaction (MMI) or computer-human interaction (CHI).","[' How do humans interact with computers?', ' What is HCI also called?']","['in many ways', 'human–machine interaction']"
2366,human-computer interaction,Introduction,"The Association for Computing Machinery (ACM) defines human–computer interaction as ""a discipline that is concerned with the design, evaluation, and implementation of interactive computing systems for human use and with the study of major phenomena surrounding them"". An important facet of HCI is user satisfaction (or End-User Computing Satisfaction). It goes on to say:
","The Association for Computing Machinery (ACM) defines human–computer interaction as ""a discipline that is concerned with the design, evaluation, and implementation of interactive computing systems for human use and with the study of major phenomena surrounding them"". An important facet of HCI is user satisfaction (or End-User Computing Satisfaction).","[' What does ACM stand for?', ' What is an important aspect of HCI?']","['Association for Computing Machinery', 'user satisfaction']"
2367,human-computer interaction,Introduction,"""Because human–computer interaction studies a human and a machine in communication, it draws from supporting knowledge on both the machine and the human side. On the machine side, techniques in computer graphics, operating systems, programming languages, and development environments are relevant. On the human side, communication theory, graphic and industrial design disciplines, linguistics, social sciences, cognitive psychology, social psychology, and human factors such as computer user satisfaction are relevant. And, of course, engineering and design methods are relevant.""","""Because human–computer interaction studies a human and a machine in communication, it draws from supporting knowledge on both the machine and the human side. On the machine side, techniques in computer graphics, operating systems, programming languages, and development environments are relevant.","[' What studies a human and a machine in communication?', ' What is relevant on the machine side?']","['human–computer interaction', 'techniques in computer graphics, operating systems, programming languages, and development environments']"
2368,human-computer interaction,Introduction,"Poorly designed human-machine interfaces can lead to many unexpected problems. A classic example is the Three Mile Island accident, a nuclear meltdown accident, where investigations concluded that the design of the human-machine interface was at least partly responsible for the disaster. Similarly, accidents in aviation have resulted from manufacturers' decisions to use non-standard flight instruments or throttle quadrant layouts: even though the new designs were proposed to be superior in basic human-machine interaction, pilots had already ingrained the ""standard"" layout. Thus, the conceptually good idea had unintended results.
","Poorly designed human-machine interfaces can lead to many unexpected problems. A classic example is the Three Mile Island accident, a nuclear meltdown accident, where investigations concluded that the design of the human-machine interface was at least partly responsible for the disaster.","[' Poorly designed human-machine interfaces can lead to what?', ' What is a classic example of a nuclear meltdown accident?', ' Investigations concluded that the design of what was at least partly responsible for the disaster?']","['many unexpected problems', 'Three Mile Island accident', 'human-machine interface']"
2369,human-computer interaction,Human–computer interface,"The human–computer interface can be described as the point of communication between the human user and the computer. The flow of information between the human and computer is defined as the loop of interaction. The loop of interaction has several aspects to it, including:
",The human–computer interface can be described as the point of communication between the human user and the computer. The flow of information between the human and computer is defined as the loop of interaction.,"[' What can be described as the point of communication between human user and computer?', ' The flow of information between human and computer is defined as what?']","['human–computer interface', 'the loop of interaction']"
2370,human-computer interaction,Goals for computers,"Human–computer interaction studies the ways in which humans make—or do not make—use of computational artifacts, systems, and infrastructures. Much of the research in this field seeks to improve the human–computer interaction by improving the usability of computer interfaces. How usability is to be precisely understood, how it relates to other social and cultural values, and when it is, and when it may not be a desirable property of computer interfaces is increasingly debated.","Human–computer interaction studies the ways in which humans make—or do not make—use of computational artifacts, systems, and infrastructures. Much of the research in this field seeks to improve the human–computer interaction by improving the usability of computer interfaces.","[' What studies the ways in which humans make use of computational artifacts, systems, and infrastructures?', ' Much of the research in this field seeks to improve the human-computer interaction by improving what?']","['Human–computer interaction', 'usability of computer interfaces']"
2371,human-computer interaction,Goals for computers,"Visions of what researchers in the field seek to achieve might vary. When pursuing a cognitivist perspective, researchers of HCI may seek to align computer interfaces with the mental model that humans have of their activities. When pursuing a post-cognitivist perspective, researchers of HCI may seek to align computer interfaces with existing social practices or existing sociocultural values.
","Visions of what researchers in the field seek to achieve might vary. When pursuing a cognitivist perspective, researchers of HCI may seek to align computer interfaces with the mental model that humans have of their activities.",[' What might researchers in HCI seek to align computer interfaces with the mental model that humans have of their activities?'],['cognitivist perspective']
2372,human-computer interaction,Display designs,"Displays are human-made artifacts designed to support the perception of relevant system variables and facilitate further processing of that information. Before a display is designed, the task that the display is intended to support must be defined (e.g., navigating, controlling, decision making, learning, entertaining, etc.). A user or operator must be able to process whatever information a system generates and displays; therefore, the information must be displayed according to principles to support perception, situation awareness, and understanding.
","Displays are human-made artifacts designed to support the perception of relevant system variables and facilitate further processing of that information. Before a display is designed, the task that the display is intended to support must be defined (e.g., navigating, controlling, decision making, learning, entertaining, etc.).","[' What are human-made artifacts designed to support?', ' What must be defined before a display is designed?']","['perception of relevant system variables', 'the task that the display is intended to support']"
2373,human-computer interaction,Factors of change,"Traditionally, computer use was modeled as a human–computer dyad in which the two were connected by a narrow explicit communication channel, such as text-based terminals. Much work has been done to make the interaction between a computing system and a human more reflective of the multidimensional nature of everyday communication. Because of potential issues, human–computer interaction shifted focus beyond the interface to respond to observations as articulated by D. Engelbart: ""If ease of use were the only valid criterion, people would stick to tricycles and never try bicycles.""","Traditionally, computer use was modeled as a human–computer dyad in which the two were connected by a narrow explicit communication channel, such as text-based terminals. Much work has been done to make the interaction between a computing system and a human more reflective of the multidimensional nature of everyday communication.","[' What was traditionally modeled as a human-computer dyad?', ' What were the two connected by?']","['computer use', 'a narrow explicit communication channel']"
2374,human-computer interaction,Factors of change,"How humans interact with computers continues to evolve rapidly. Human–computer interaction is affected by developments in computing. These forces include:
",How humans interact with computers continues to evolve rapidly. Human–computer interaction is affected by developments in computing.,"[' How humans interact with computers continues to evolve rapidly?', ' What is affected by developments in computing?']","['Human–computer interaction is affected by developments in computing.', 'Human–computer interaction']"
2375,human-computer interaction,Scientific conferences,"One of the main conferences for new research in human–computer interaction is the annually held Association for Computing Machinery's (ACM) Conference on Human Factors in Computing Systems, usually referred to by its short name CHI (pronounced kai, or Khai). CHI is organized by ACM Special Interest Group on Computer-Human Interaction (SIGCHI). CHI is a large conference, with thousands of attendants, and is quite broad in scope. It is attended by academics, practitioners, and industry people, with company sponsors such as Google, Microsoft, and PayPal.
","One of the main conferences for new research in human–computer interaction is the annually held Association for Computing Machinery's (ACM) Conference on Human Factors in Computing Systems, usually referred to by its short name CHI (pronounced kai, or Khai). CHI is organized by ACM Special Interest Group on Computer-Human Interaction (SIGCHI).","[' What is one of the main conferences for new research in human-computer interaction?', ' What is the short name of the ACM Conference on Human Factors in Computing Systems?', ' Who organizes CHI?', ' What group organizes the ACM Special Interest Group on Computer-Human Interaction?']","[""Association for Computing Machinery's (ACM) Conference on Human Factors in Computing Systems"", 'CHI', 'ACM Special Interest Group on Computer-Human Interaction', 'Association for Computing Machinery']"
2376,process algebra,Summary,"In computer science, the process calculi (or process algebras) are a diverse family of related approaches for formally modelling concurrent systems. Process calculi provide a tool for the high-level description of interactions, communications, and synchronizations between a collection of independent agents or processes. They also provide algebraic laws that allow process descriptions to be manipulated and analyzed, and permit formal reasoning about equivalences between processes (e.g., using bisimulation). Leading examples of process calculi include  CSP, CCS, ACP, and LOTOS. More recent additions to the family include the π-calculus, the ambient calculus, PEPA, the fusion calculus and the join-calculus.
","In computer science, the process calculi (or process algebras) are a diverse family of related approaches for formally modelling concurrent systems. Process calculi provide a tool for the high-level description of interactions, communications, and synchronizations between a collection of independent agents or processes.","[' What are process calculi in computer science?', ' What is a tool for the high-level description of interactions, communications, and synchronizations?']","['a diverse family of related approaches for formally modelling concurrent systems', 'Process calculi']"
2377,process algebra,Mathematics of processes,"To define a process calculus, one starts with a set of names (or channels) whose purpose is to provide means of communication.  In many implementations, channels have rich internal structure to improve efficiency, but this is abstracted away in  most theoretic models.  In addition to names, one needs a means to form new processes from old ones. The basic operators, always present in some form or other, allow:","To define a process calculus, one starts with a set of names (or channels) whose purpose is to provide means of communication. In many implementations, channels have rich internal structure to improve efficiency, but this is abstracted away in  most theoretic models.","[' To define a process calculus, one starts with a set of names whose purpose is to provide means of what?', ' In many implementations, channels have rich internal structure to improve efficiency, but what is abstracted away in most theoretic models?']","['communication', 'process calculus']"
2378,process algebra,History,"In the first half of the 20th century, various formalisms were proposed to capture the informal concept of a computable function, with μ-recursive functions, Turing machines  and the lambda calculus possibly being the best-known  examples today.  The surprising fact that they are essentially equivalent, in the sense that they are all encodable into each other, supports the Church-Turing thesis.  Another shared feature is more rarely commented on: they all are most readily understood as models of sequential computation. The subsequent consolidation of computer science required a more subtle formulation of the notion of computation, in particular explicit representations of concurrency and communication. Models of concurrency such as the process calculi, Petri nets in 1962, and the actor model in 1973 emerged from this line of inquiry.
","In the first half of the 20th century, various formalisms were proposed to capture the informal concept of a computable function, with μ-recursive functions, Turing machines  and the lambda calculus possibly being the best-known  examples today. The surprising fact that they are essentially equivalent, in the sense that they are all encodable into each other, supports the Church-Turing thesis.","[' What was proposed to capture the informal concept of a computable function in the first half of the 20th century?', ' What are some of the best-known examples of formalisms today?', ' What thesis does the Church-Turing thesis support?']","['various formalisms', 'μ-recursive functions, Turing machines  and the lambda calculus', 'The surprising fact that they are essentially equivalent, in the sense that they are all encodable into each other']"
2379,process algebra,History,"Research on process calculi began in earnest with Robin Milner's seminal work on the Calculus of Communicating Systems (CCS) during the period from 1973 to 1980. C.A.R. Hoare's Communicating Sequential Processes (CSP) first appeared in 1978, and was subsequently developed into a full-fledged process calculus during the early 1980s. There was much cross-fertilization of ideas between CCS and CSP as they developed. In 1982 Jan Bergstra and Jan Willem Klop began work on what came to be known as the Algebra of Communicating Processes (ACP), and introduced the term process algebra to describe their work. CCS, CSP, and ACP constitute the three major branches of the process calculi family: the majority of the other process calculi can trace their roots to one of these three calculi.
",Research on process calculi began in earnest with Robin Milner's seminal work on the Calculus of Communicating Systems (CCS) during the period from 1973 to 1980. C.A.R.,"[' When did research on process calculi begin?', ' Who wrote the Calculus of Communicating Systems?']","['1973 to 1980', 'Robin Milner']"
2380,process algebra,Current research,"Various process calculi have been studied and not all of them fit the paradigm sketched here. The most prominent example may be the ambient calculus. This is to be expected as process calculi are an active field of study. Currently research on process calculi focuses on the following problems.
",Various process calculi have been studied and not all of them fit the paradigm sketched here. The most prominent example may be the ambient calculus.,"[' Various process calculi have been studied and not all of them fit what?', ' What is the most prominent example of ambient calculus?']","['the paradigm sketched here', 'process calculi']"
2381,process algebra,Relationship to other models of concurrency,"The history monoid is the free object that is generically able to represent the histories of individual communicating processes. A process calculus is then a formal language imposed on a history monoid in a consistent fashion. That is, a history monoid can only record a sequence of events, with synchronization, but does not specify the allowed state transitions. Thus, a process calculus is to a history monoid what a formal language is to a free monoid (a formal language is a subset of the set of all possible finite-length strings of an alphabet generated by the Kleene star).
",The history monoid is the free object that is generically able to represent the histories of individual communicating processes. A process calculus is then a formal language imposed on a history monoid in a consistent fashion.,"[' What is the free object that is generically able to represent the histories of individual communicating processes?', ' What is then a formal language imposed on a history monoid in a consistent fashion?']","['history monoid', 'A process calculus']"
2382,process algebra,Relationship to other models of concurrency,"The use of channels for communication is one of the features distinguishing the process calculi from other models of concurrency, such as Petri nets and the actor model (see Actor model and process calculi). One of the fundamental motivations for including channels in the process calculi was to enable certain algebraic techniques, thereby making it easier to reason about processes algebraically.
","The use of channels for communication is one of the features distinguishing the process calculi from other models of concurrency, such as Petri nets and the actor model (see Actor model and process calculi). One of the fundamental motivations for including channels in the process calculi was to enable certain algebraic techniques, thereby making it easier to reason about processes algebraically.","[' What is one of the features that distinguishes the process calculi from other models of concurrency?', ' What is another model that is similar to the actor model?', ' The use of channels for communication was one of what motivations for including channels in processes calculi?', ' What was included in the process calculi to enable certain algebraic techniques?', ' What made it easier to reason about processes algebraically?']","['The use of channels for communication', 'Petri nets', 'to enable certain algebraic techniques', 'channels', 'algebraic techniques']"
2383,voronoi diagram,Summary,"In mathematics, a Voronoi diagram is a partition of a plane into regions close to each of a given set of objects. In the simplest case, these objects are just finitely many points in the plane (called seeds, sites, or generators). For each seed there is a corresponding region, called a Voronoi cell, consisting of all points of the plane closer to that seed than to any other. The Voronoi diagram of a set of points is dual to its Delaunay triangulation.
","In mathematics, a Voronoi diagram is a partition of a plane into regions close to each of a given set of objects. In the simplest case, these objects are just finitely many points in the plane (called seeds, sites, or generators).","[' What is a Voronoi diagram?', ' What are seeds, sites, and generators?']","['a partition of a plane into regions close to each of a given set of objects', 'finitely many points in the plane']"
2384,voronoi diagram,Summary,"The Voronoi diagram is named after Georgy Voronoy, and is also called a Voronoi tessellation, a Voronoi decomposition, a Voronoi partition, or a Dirichlet tessellation (after Peter Gustav Lejeune Dirichlet). Voronoi cells are also known as Thiessen polygons. Voronoi diagrams have practical and theoretical applications in many fields, mainly in science and technology, but also in visual art.","The Voronoi diagram is named after Georgy Voronoy, and is also called a Voronoi tessellation, a Voronoi decomposition, a Voronoi partition, or a Dirichlet tessellation (after Peter Gustav Lejeune Dirichlet). Voronoi cells are also known as Thiessen polygons.","[' Who is the Voronoi diagram named after?', ' What is another name for a Voronoy diagram?', ' Who was the Dirichlet tessellation named for?']","['Georgy Voronoy', 'Voronoi tessellation', 'Peter Gustav Lejeune Dirichlet']"
2385,voronoi diagram,The simplest case,"In the simplest case, shown in the first picture, we are given a finite set of points {p1, ..., pn} in the Euclidean plane. In this case each site pk is simply a point, and its corresponding Voronoi cell Rk consists of every point in the Euclidean plane whose distance to pk is less than or equal to its distance to any other pk. Each such cell is obtained from the intersection of half-spaces, and hence it is a (convex) polyhedron. The line segments of the Voronoi diagram are all the points in the plane that are equidistant to the two nearest sites.  The Voronoi vertices (nodes) are the points equidistant to three (or more) sites.
","In the simplest case, shown in the first picture, we are given a finite set of points {p1, ..., pn} in the Euclidean plane. In this case each site pk is simply a point, and its corresponding Voronoi cell Rk consists of every point in the Euclidean plane whose distance to pk is less than or equal to its distance to any other pk.","[' What is the simplest case of a finite set of points in the Euclidean plane?', ' What is each site pk simply a point?', ' How many points are in the Voronoi cell Rk?', ' What is the name of every point in the Euclidean plane whose distance is less than or equal to its distance to any other pk?']","['shown in the first picture', 'Euclidean plane', 'every point in the Euclidean plane whose distance to pk is less than or equal to its distance to any other pk', 'Voronoi cell Rk']"
2386,voronoi diagram,Formal definition,"Let 



X


{\textstyle X}
 be a metric space with distance function 



d


{\textstyle d}
. Let 



K


{\textstyle K}
 be a set of indices and let 



(

P

k



)

k
∈
K




{\textstyle (P_{k})_{k\in K}}
 be a tuple (ordered collection) of nonempty subsets (the sites) in the space 



X


{\textstyle X}
. The Voronoi cell, or Voronoi region,  




R

k




{\textstyle R_{k}}
, associated with the site 




P

k




{\textstyle P_{k}}
 is the set of all points in 



X


{\textstyle X}
 whose distance to 




P

k




{\textstyle P_{k}}
 is not greater than their distance to the other sites 




P

j




{\textstyle P_{j}}
, where 



j


{\textstyle j}
 is any index different from 



k


{\textstyle k}
. In other words, if 



d
(
x
,

A
)
=
inf
{
d
(
x
,

a
)
∣
a
∈
A
}


{\textstyle d(x,\,A)=\inf\{d(x,\,a)\mid a\in A\}}
 denotes the distance between the point 



x


{\textstyle x}
 and the subset 



A


{\textstyle A}
, then
","Let 



X


{\textstyle X}
 be a metric space with distance function 



d


{\textstyle d}
. Let 



K


{\textstyle K}
 be a set of indices and let 



(

P

k



)

k
∈
K




{\textstyle (P_{k})_{k\in K}}
 be a tuple (ordered collection) of nonempty subsets (the sites) in the space 



X


{\textstyle X}
.","[' Let X <unk>textstyle X<unk> be a metric space with what function?', ' Let k <unk> textstyle K<unk> be an set of indices and let (P_<unk>k<unk>)_<unk>kain K <unk> be what?']","['distance', 'K']"
2387,voronoi diagram,Formal definition,"The Voronoi diagram is simply the tuple of cells 



(

R

k



)

k
∈
K




{\textstyle (R_{k})_{k\in K}}
. In principle, some of the sites can intersect and even coincide (an application is described below for sites representing shops), but usually they are assumed to be disjoint. In addition, infinitely many sites are allowed in the definition (this setting has applications in geometry of numbers and crystallography), but again, in many cases only finitely many sites are considered.
","The Voronoi diagram is simply the tuple of cells 



(

R

k



)

k
∈
K




{\textstyle (R_{k})_{k\in K}}
. In principle, some of the sites can intersect and even coincide (an application is described below for sites representing shops), but usually they are assumed to be disjoint.","[' What is the Voronoi diagram?', ' What can intersect and even coincide?']","['the tuple of cells \n\n\n\n(\n\nR\n\nk\n\n\n\n)\n\nk\n∈\nK\n\n\n\n\n{\\textstyle (R_{k})_{k\\in K}}', 'some of the sites']"
2388,voronoi diagram,Formal definition,"In the particular case where the space is a finite-dimensional Euclidean space, each site is a point, there are finitely many points and all of them are different, then the Voronoi cells are convex polytopes and they can be represented in a combinatorial way using their vertices, sides, two-dimensional faces, etc. Sometimes the induced combinatorial structure is referred to as the Voronoi diagram. In general however, the Voronoi cells may not be convex or even connected.
","In the particular case where the space is a finite-dimensional Euclidean space, each site is a point, there are finitely many points and all of them are different, then the Voronoi cells are convex polytopes and they can be represented in a combinatorial way using their vertices, sides, two-dimensional faces, etc. Sometimes the induced combinatorial structure is referred to as the Voronoi diagram.","[' How can the Voronoi cells be represented in a combinatorial way?', ' What is each site in the Euclidean space?', ' What is the induced combinatorial structure referred to as?']","['using their vertices, sides, two-dimensional faces', 'a point', 'Voronoi diagram']"
2389,voronoi diagram,Formal definition,"In the usual Euclidean space,  we can rewrite the formal definition in usual terms. Each Voronoi polygon 




R

k




{\textstyle R_{k}}
 is associated with a generator point  




P

k




{\textstyle P_{k}}
.
Let 



X


{\textstyle X}
 be the set of all points in the Euclidean space. Let 




P

1




{\textstyle P_{1}}
 be a point that generates its Voronoi region  




R

1




{\textstyle R_{1}}
, 




P

2




{\textstyle P_{2}}
 that generates  




R

2




{\textstyle R_{2}}
, and 




P

3




{\textstyle P_{3}}
 that generates  




R

3




{\textstyle R_{3}}
, and so on.  Then, as expressed by Tran et al, ""all locations in the Voronoi polygon are closer to the generator point of that polygon than any other generator point in the Voronoi diagram in Euclidean plane"".
","In the usual Euclidean space,  we can rewrite the formal definition in usual terms. Each Voronoi polygon 




R

k




{\textstyle R_{k}}
 is associated with a generator point  




P

k




{\textstyle P_{k}}
.","[' In what space can we rewrite the formal definition in usual terms?', ' Each Voronoi polygon is associated with what?']","['Euclidean', 'a generator point']"
2390,voronoi diagram,Illustration,"As a simple illustration, consider a group of shops in a city. Suppose we want to estimate the number of customers of a given shop. With all else being equal (price, products, quality of service, etc.), it is reasonable to assume that customers choose their preferred shop simply by distance considerations: they will go to the shop located nearest to them. In this case the Voronoi cell 




R

k




{\displaystyle R_{k}}
 of a given shop 




P

k




{\displaystyle P_{k}}
 can be used for giving a rough estimate on the number of potential customers going to this shop (which is modeled by a point in our city).
","As a simple illustration, consider a group of shops in a city. Suppose we want to estimate the number of customers of a given shop.",[' What is a simple example of a group of shops in a city?'],['estimate the number of customers of a given shop']
2391,voronoi diagram,History and research,"Informal use of Voronoi diagrams can be traced back to Descartes in 1644. Peter Gustav Lejeune Dirichlet used two-dimensional and three-dimensional Voronoi diagrams in his study of quadratic forms in 1850.
British physician John Snow used a Voronoi-like diagram in 1854 to illustrate how the majority of people who died in the Broad Street cholera outbreak lived closer to the infected Broad Street pump than to any other water pump.
",Informal use of Voronoi diagrams can be traced back to Descartes in 1644. Peter Gustav Lejeune Dirichlet used two-dimensional and three-dimensional Voronoi diagrams in his study of quadratic forms in 1850.,"[' Who first used Voronoi diagrams in 1644?', ' What year did Peter Gustav Lejeune Dirichlet study quadratic forms?']","['Descartes', '1850']"
2392,voronoi diagram,History and research,"Voronoi diagrams are named after Georgy Feodosievych Voronoy who defined and studied the general n-dimensional case in 1908. Voronoi diagrams that are used in geophysics and meteorology to analyse spatially distributed data (such as rainfall measurements) are called Thiessen polygons after American meteorologist Alfred H. Thiessen. Other equivalent names for this concept (or particular important cases of it): Voronoi polyhedra, Voronoi polygons, domain(s) of influence, Voronoi decomposition, Voronoi tessellation(s), Dirichlet tessellation(s).
",Voronoi diagrams are named after Georgy Feodosievych Voronoy who defined and studied the general n-dimensional case in 1908. Voronoi diagrams that are used in geophysics and meteorology to analyse spatially distributed data (such as rainfall measurements) are called Thiessen polygons after American meteorologist Alfred H. Thiessen.,"[' What is the name of Georgy Feodosievych Voronoy?', ' Who defined and studied the general n-dimensional case in 1908?', ' What are Voronoi diagrams used in geophysics and meteorology called?']","['Voronoi diagrams', 'Georgy Feodosievych Voronoy', 'Thiessen polygons']"
2393,voronoi diagram,Higher-order Voronoi diagrams,"Although a normal Voronoi cell is defined as the set of points closest to a single point in S, an nth-order Voronoi cell is defined as the set of points having a particular set of n points in S as its n nearest neighbors. Higher-order Voronoi diagrams also subdivide space.
","Although a normal Voronoi cell is defined as the set of points closest to a single point in S, an nth-order Voronoi cell is defined as the set of points having a particular set of n points in S as its n nearest neighbors. Higher-order Voronoi diagrams also subdivide space.","[' What is defined as the set of points closest to a single point in S?', ' An nth-order Voronoi cell has a particular set of how many points in S as its n nearest neighbors?', ' Higher-order voronoi diagrams also subdivide space.']","['normal Voronoi cell', 'n points', 'Voronoi cell']"
2394,voronoi diagram,Higher-order Voronoi diagrams,"Higher-order Voronoi diagrams can be generated recursively.  To generate the nth-order Voronoi diagram from set S, start with the (n − 1)th-order diagram and replace each cell generated by X = {x1, x2, ..., xn−1} with a Voronoi diagram generated on the set S − X.
","Higher-order Voronoi diagrams can be generated recursively. To generate the nth-order Voronoi diagram from set S, start with the (n − 1)th-order diagram and replace each cell generated by X = {x1, x2, ..., xn−1} with a Voronoi diagram generated on the set S − X.","[' How can higher-order Voronoi diagrams be generated?', ' What can be generated recursively?', ' To generate the nth-order voronoi from set S, start with the (n <unk> 1)th order diagram and replace each cell generated by X = <unk>x1, x2,..., xn<unk>1<unk> with a Vornoi diagram generated on what set?']","['recursively', 'Higher-order Voronoi diagrams', 'S\xa0−\xa0X']"
2395,voronoi diagram,Generalizations and variations,"As implied by the definition, Voronoi cells can be defined for metrics other than Euclidean, such as the Mahalanobis distance or Manhattan distance. However, in these cases the boundaries of the Voronoi cells may be more complicated than in the Euclidean case, since the equidistant locus for two points may fail to be subspace of codimension 1, even in the two-dimensional case.
","As implied by the definition, Voronoi cells can be defined for metrics other than Euclidean, such as the Mahalanobis distance or Manhattan distance. However, in these cases the boundaries of the Voronoi cells may be more complicated than in the Euclidean case, since the equidistant locus for two points may fail to be subspace of codimension 1, even in the two-dimensional case.","[' What can be defined for metrics other than Euclidean?', ' What may the boundaries of the Voronoi cells be more complicated than in the Euclidan case?', ' What is the Euclidean case?', ' The equidistant locus for two points may fail to be subspace of what?']","['Voronoi cells', 'the equidistant locus for two points may fail to be subspace of codimension 1', 'two-dimensional case', 'codimension 1']"
2396,voronoi diagram,Generalizations and variations,"A weighted Voronoi diagram is the one in which the function of a pair of points to define a Voronoi cell is a distance function modified by multiplicative or additive weights assigned to generator points. In contrast to the case of Voronoi cells defined using a distance which is a metric, in this case some of the Voronoi cells may be empty. A power diagram is a type of Voronoi diagram defined from a set of circles using the power distance; it can also be thought of as a weighted Voronoi diagram in which a weight defined from the radius of each circle is added to the squared Euclidean distance from the circle's center.","A weighted Voronoi diagram is the one in which the function of a pair of points to define a Voronoi cell is a distance function modified by multiplicative or additive weights assigned to generator points. In contrast to the case of Voronoi cells defined using a distance which is a metric, in this case some of the Voronoi cells may be empty.","[' What is a weighted Voronoi diagram?', ' What is the function of a pair of points to define a Vornoi cell a distance function modified by?', ' What are Voronoi cells defined using a distance which is a metric?']","['the function of a pair of points to define a Voronoi cell is a distance function modified by multiplicative or additive weights assigned to generator points', 'multiplicative or additive weights assigned to generator points', 'some of the Voronoi cells may be empty']"
2397,voronoi diagram,Generalizations and variations,"The Voronoi diagram of 



n


{\displaystyle n}
 points in 



d


{\displaystyle d}
-dimensional space can have 



O
(

n

⌈
d

/

2
⌉


)


{\textstyle O(n^{\lceil d/2\rceil })}
 vertices, requiring the same bound for the amount of memory needed to store an explicit description of it. Therefore, Voronoi diagrams are often not feasible for moderate or high dimensions. A more space-efficient alternative is to use approximate Voronoi diagrams.","The Voronoi diagram of 



n


{\displaystyle n}
 points in 



d


{\displaystyle d}
-dimensional space can have 



O
(

n

⌈
d

/

2
⌉


)


{\textstyle O(n^{\lceil d/2\rceil })}
 vertices, requiring the same bound for the amount of memory needed to store an explicit description of it. Therefore, Voronoi diagrams are often not feasible for moderate or high dimensions.","[' What can a Voronoi diagram of n <unk>displaystyle n<unk> points have?', ' What is required for the amount of memory needed to store an explicit description of a diagram?', ' Vornoi diagrams are often not feasible for what dimensions?']","['O\n(\n\nn\n\n⌈\nd\n\n/\n\n2\n⌉\n\n\n)\n\n\n{\\textstyle O(n^{\\lceil d/2\\rceil })}\n vertices', 'same bound', 'moderate or high']"
2398,voronoi diagram,Generalizations and variations,"Voronoi diagrams are also related to other geometric structures such as the medial axis (which has found applications in image segmentation, optical character recognition, and other computational applications), straight skeleton, and zone diagrams. Besides points, such diagrams use lines and polygons as seeds. By augmenting the diagram with line segments that connect to nearest points on the seeds, a planar subdivision of the environment is obtained. This structure can be used as a navigation mesh for path-finding through large spaces. The navigation mesh has been generalized to support 3D multi-layered environments, such as an airport or a multi-storey building.","Voronoi diagrams are also related to other geometric structures such as the medial axis (which has found applications in image segmentation, optical character recognition, and other computational applications), straight skeleton, and zone diagrams. Besides points, such diagrams use lines and polygons as seeds.","[' Voronoi diagrams are related to what other geometric structure?', ' What has found applications in image segmentation, optical character recognition, and other computational applications?', ' Lines and polygons are used as what?']","['the medial axis', 'the medial axis', 'seeds']"
2399,voronoi diagram,Algorithms,"Several efficient algorithms are known for constructing Voronoi diagrams, either directly (as the diagram itself) or indirectly by starting with a Delaunay triangulation and then obtaining its dual.
Direct algorithms include Fortune's algorithm, an O(n log(n)) algorithm for generating a Voronoi diagram from a set of points in a plane.
Bowyer–Watson algorithm, an O(n log(n)) to O(n2) algorithm for generating a Delaunay triangulation in any number of dimensions, can be used in an indirect algorithm for the Voronoi diagram. The Jump Flooding Algorithm can generate approximate Voronoi diagrams in constant time and is suited for use on commodity graphics hardware. ","Several efficient algorithms are known for constructing Voronoi diagrams, either directly (as the diagram itself) or indirectly by starting with a Delaunay triangulation and then obtaining its dual. Direct algorithms include Fortune's algorithm, an O(n log(n)) algorithm for generating a Voronoi diagram from a set of points in a plane.","[' How many efficient algorithms are known for constructing Voronoi diagrams?', ' What are some direct algorithms?', "" Fortune's algorithm is an O(n log(n)) algorithm for what?""]","['Several', ""Fortune's algorithm"", 'generating a Voronoi diagram from a set of points in a plane']"
2400,voronoi diagram,Algorithms,"Lloyd's algorithm and its generalization via the Linde–Buzo–Gray algorithm (aka k-means clustering), use the construction of Voronoi diagrams as a subroutine.
These methods alternate between steps in which one constructs the Voronoi diagram for a set of seed points, and steps in which the seed points are moved to new locations that are more central within their cells. These methods can be used in spaces of arbitrary dimension to iteratively converge towards a specialized form of the Voronoi diagram, called a Centroidal Voronoi tessellation, where the sites have been moved to points that are also the geometric centers of their cells.
","Lloyd's algorithm and its generalization via the Linde–Buzo–Gray algorithm (aka k-means clustering), use the construction of Voronoi diagrams as a subroutine. These methods alternate between steps in which one constructs the Voronoi diagram for a set of seed points, and steps in which the seed points are moved to new locations that are more central within their cells.","["" What is Lloyd's algorithm's generalization called?"", ' What is the Linde-Buzo-Gray algorithm?', ' How are the Voronoi diagrams constructed?', ' How are seed points moved?', ' What are the steps in which seed points are moved to new locations?']","['Linde–Buzo–Gray algorithm', 'k-means clustering', 'as a subroutine', 'to new locations that are more central within their cells', 'more central within their cells']"
2401,multiobjective optimization,Summary,"Multi-objective optimization (also known as multi-objective programming, vector optimization, multicriteria optimization, multiattribute optimization or Pareto optimization) is an area of multiple criteria decision making that is concerned with mathematical optimization problems involving more than one objective function to be optimized simultaneously. Multi-objective optimization has been applied in many fields of science, including engineering, economics and logistics where optimal decisions need to be taken in the presence of trade-offs between two or more conflicting objectives. Minimizing cost while maximizing comfort while buying a car, and maximizing performance whilst minimizing fuel consumption and emission of pollutants of a vehicle are examples of multi-objective optimization problems involving two and three objectives, respectively. In practical problems, there can be more than three objectives.
","Multi-objective optimization (also known as multi-objective programming, vector optimization, multicriteria optimization, multiattribute optimization or Pareto optimization) is an area of multiple criteria decision making that is concerned with mathematical optimization problems involving more than one objective function to be optimized simultaneously. Multi-objective optimization has been applied in many fields of science, including engineering, economics and logistics where optimal decisions need to be taken in the presence of trade-offs between two or more conflicting objectives.","[' What is multi-objective optimization also known as?', ' What is the area of multiple criteria decision making concerned with?', ' How many objective functions are to be optimized simultaneously?', ' Multi-objective optimization has been applied in many fields of science, including engineering, economics and logistics where?']","['multi-objective programming', 'Multi-objective optimization', 'more than one', 'optimal decisions need to be taken in the presence of trade-offs between two or more conflicting objectives']"
2402,multiobjective optimization,Summary,"For a nontrivial multi-objective optimization problem, no single solution exists that simultaneously optimizes each objective. In that case, the objective functions are said to be conflicting. A solution is called nondominated, Pareto optimal, Pareto efficient or noninferior, if none of the objective functions can be improved in value without degrading some of the other objective values. Without additional subjective preference information, there may exist a (possibly infinite) number of Pareto optimal solutions, all of which are considered equally good. Researchers study multi-objective optimization problems from different viewpoints and, thus, there exist different solution philosophies and goals when setting and solving them. The goal may be to find a representative set of Pareto optimal solutions, and/or quantify the trade-offs in satisfying the different objectives, and/or finding a single solution that satisfies the subjective preferences of a human decision maker (DM).
","For a nontrivial multi-objective optimization problem, no single solution exists that simultaneously optimizes each objective. In that case, the objective functions are said to be conflicting.","[' In a nontrivial multi-objective optimization problem, what does no single solution exist that simultaneously optimizes each objective?', ' The objective functions are said to be conflicting in what case?']","['the objective functions are said to be conflicting', 'nontrivial multi-objective optimization problem']"
2403,multiobjective optimization,Introduction,"A multi-objective optimization problem is an optimization problem that involves multiple objective functions. In mathematical terms, a multi-objective optimization problem can be formulated as
","A multi-objective optimization problem is an optimization problem that involves multiple objective functions. In mathematical terms, a multi-objective optimization problem can be formulated as","[' What is an optimization problem that involves multiple objective functions?', ' What can a multi-objective optimization problem be formulated as?']","['multi-objective optimization problem', 'In mathematical terms']"
2404,multiobjective optimization,Introduction,"where the integer 



k
≥
2


{\displaystyle k\geq 2}
 is the number of objectives and the set 



X


{\displaystyle X}
 is the feasible set of decision vectors, which is typically 



X
⊆


R


n




{\displaystyle X\subseteq \mathbb {R} ^{n}}
 but it depends on the 



n


{\displaystyle n}
-dimensional application domain. The feasible set is typically defined by some constraint functions. In addition, the vector-valued objective function is often defined as
","where the integer 



k
≥
2


{\displaystyle k\geq 2}
 is the number of objectives and the set 



X


{\displaystyle X}
 is the feasible set of decision vectors, which is typically 



X
⊆


R


n




{\displaystyle X\subseteq \mathbb {R} ^{n}}
 but it depends on the 



n


{\displaystyle n}
-dimensional application domain. The feasible set is typically defined by some constraint functions.","[' Where the integer k <unk> 2 <unk>displaystyle k<unk>geq 2<unk> is the number of objectives?', ' What is the feasible set of decision vectors?']","['k\n≥\n2', 'X']"
2405,model transformation,Summary,"A model transformation, in model-driven engineering, is an automated way of modifying and creating models. An example use of model transformation is ensuring that a family of models is consistent, in a precise sense which the software engineer can define. The aim of using a model transformation is to save effort and reduce errors by automating the building and modification of models where possible.
","A model transformation, in model-driven engineering, is an automated way of modifying and creating models. An example use of model transformation is ensuring that a family of models is consistent, in a precise sense which the software engineer can define.","[' What is a model transformation in model-driven engineering?', ' What is an example use of model transformation?', ' How can a software engineer define a family of models?']","['an automated way of modifying and creating models', 'ensuring that a family of models is consistent', 'precise sense']"
2406,model transformation,Overview,"Model transformations can be thought of as programs that take models as input. There is a wide variety of kinds of model transformation and uses of them, which differ in their inputs and outputs and also in the way they are expressed.
","Model transformations can be thought of as programs that take models as input. There is a wide variety of kinds of model transformation and uses of them, which differ in their inputs and outputs and also in the way they are expressed.","[' Model transformations can be thought of as programs that take what as input?', ' There is a wide variety of kinds of what?']","['models', 'Model transformations']"
2407,model transformation,Classification of model transformations,"Model transformations and languages for them have been classified in many ways.
Some of the more common distinctions drawn are:
",Model transformations and languages for them have been classified in many ways. Some of the more common distinctions drawn are:,"[' What has been classified in many ways?', ' What are some of the more common distinctions drawn?']","['Model transformations and languages', 'Model transformations and languages']"
2408,model transformation,Languages for model transformations,"A model transformation may be written in a general purpose programming language, but specialised model transformation languages are also available. Bidirectional transformations, in particular, are best written in a language that ensures the directions are appropriately related. The OMG-standardised model transformation languages are collectively known as QVT.
","A model transformation may be written in a general purpose programming language, but specialised model transformation languages are also available. Bidirectional transformations, in particular, are best written in a language that ensures the directions are appropriately related.","[' In what type of programming language can a model transformation be written?', ' What type of transformations are best written in a language that ensures directions are appropriately related?']","['general purpose', 'Bidirectional transformations']"
2409,model transformation,Languages for model transformations,"In some model transformation languages, for example the QVT languages, a model transformation is itself a model, that is, it conforms to a metamodel which is part of the model transformation language's definition. This facilitates the definition of Higher Order Transformations (HOTs), i.e. transformations which have other transformations as input and/or output.
","In some model transformation languages, for example the QVT languages, a model transformation is itself a model, that is, it conforms to a metamodel which is part of the model transformation language's definition. This facilitates the definition of Higher Order Transformations (HOTs), i.e.","[' What does a model transformation conform to in some model transformation languages?', ' What facilitates the definition of Higher Order Transformations?']","['a metamodel', 'metamodel']"
2410,query expansion,Summary,"Query expansion (QE) is the process of reformulating a given query to improve retrieval performance in information retrieval operations, particularly in the context of query understanding.
In the context of search engines, query expansion involves evaluating a user's input (what words were typed into the search query area, and sometimes other types of data) and expanding the search query to match additional documents.  Query expansion involves techniques such as:
","Query expansion (QE) is the process of reformulating a given query to improve retrieval performance in information retrieval operations, particularly in the context of query understanding. In the context of search engines, query expansion involves evaluating a user's input (what words were typed into the search query area, and sometimes other types of data) and expanding the search query to match additional documents.","[' What is the process of reformulating a given query to improve retrieval performance in information retrieval operations?', "" In the context of search engines, what involves evaluating a user's input?"", ' What is QE?', ' What was typed into the search query area and sometimes other types of data?', ' What was expanded to match additional documents?']","['Query expansion (QE)', 'query expansion', 'Query expansion (QE) is the process of reformulating a given query to improve retrieval performance in information retrieval operations', 'what words', 'search query']"
2411,query expansion,Precision and recall trade-offs,"Search engines invoke query expansion to increase the quality of user search results.  It is assumed that users do not always formulate search queries using the best terms. Best in this case may be because the database does not contain the user entered terms.
",Search engines invoke query expansion to increase the quality of user search results. It is assumed that users do not always formulate search queries using the best terms.,"[' Search engines invoke query expansion to increase what?', ' What is assumed that users do not always formulate search queries using the best terms?']","['quality of user search results', 'Search engines']"
2412,query expansion,Precision and recall trade-offs,"By stemming a user-entered term, more documents are matched, as the alternate word forms for a user entered term are matched as well, increasing the total recall. This comes at the expense of reducing the precision.  By expanding a search query to search for the synonyms of a user entered term, the recall is also increased at the expense of precision.  This is due to the nature of the equation of how precision is calculated, in that a larger recall implicitly causes a decrease in precision, given that factors of recall are part of the denominator. It is also inferred that a larger recall negatively impacts overall search result quality, given that many users do not want more results to comb through, regardless of the precision.
","By stemming a user-entered term, more documents are matched, as the alternate word forms for a user entered term are matched as well, increasing the total recall. This comes at the expense of reducing the precision.","[' How are more documents matched by stemming a user-entered term?', ' How are the alternate word forms for a term matched?']","['alternate word forms for a user entered term are matched as well, increasing the total recall', 'increasing the total recall']"
2413,query expansion,Precision and recall trade-offs,"The goal of query expansion in this regard is by increasing recall, precision can potentially increase (rather than decrease as mathematically equated), by including in the result set pages which are more relevant (of higher quality), or at least equally relevant. Pages which would not be included in the result set, which have the potential to be more relevant to the user's desired query, are included, and without query expansion would not have, regardless of relevance.  At the same time, many of the current commercial search engines use word frequency (tf-idf) to assist in ranking.  By ranking the occurrences of both the user entered words and synonyms and alternate morphological forms, documents with a higher density (high frequency and close proximity) tend to migrate higher up in the search results, leading to a higher quality of the search results near the top of the results, despite the larger recall.
","The goal of query expansion in this regard is by increasing recall, precision can potentially increase (rather than decrease as mathematically equated), by including in the result set pages which are more relevant (of higher quality), or at least equally relevant. Pages which would not be included in the result set, which have the potential to be more relevant to the user's desired query, are included, and without query expansion would not have, regardless of relevance.","[' What is the goal of query expansion in this regard?', ' What can possibly increase (rather than decrease as mathematical equated?', "" What are pages that would not be included in the result set that have the potential to be more relevant to the user's desired query?""]","['by increasing recall', 'precision', 'included']"
2414,query expansion,Query expansion methods,"Automatic methods for query expansion were proposed in 1960 by Maron and Kuhns. Modern query expansion methods either imply document collection analysis (global or local)  or are dictionary- or ontology-based. The global analysis of the document collection is applied for searching for relations between terms. The local analysis refers to the relevance feedback introduced by Rocchio. Rocchio proposed to judge manually some of the retrieved documents and use this feedback information to expand the query. Since collecting users' judgment can be challenging, only the first top retrieved documents are considered as relevant. This is so called pseudo-relevance feedback (PRF). Pseudo-relevance feedback is efficient in average but can damage results for some queries, especially difficult ones since the top retrieved documents are probably non-relevant. Pseudo-relevant documents are used to find expansion candidate terms that co-occur with many query terms. This idea was further developed within the relevance language model formalism in positional relevance  and proximity relevance models  which consider the distance to query terms in the pseudo-relevant documents. Another direction in query expansion is the application of word embeddings.",Automatic methods for query expansion were proposed in 1960 by Maron and Kuhns. Modern query expansion methods either imply document collection analysis (global or local)  or are dictionary- or ontology-based.,"[' When were automatic methods for query expansion proposed?', ' What are modern query expansion methods imply?']","['1960', 'document collection analysis']"
2415,algorithms,Summary,"In mathematics and computer science, an algorithm ( (listen)) is a finite sequence of well-defined instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. 
By making use of Artificial Intelligence, algorithms can perform automated deductions (referred to as automated reasoning) and use mathematical and logical tests to divert the code through various routes (referred to as automated decision-making). Using human characteristics as descriptors of machines in metaphorical ways was already practiced by Alan Turing with terms such as ""memory"", ""search"" and ""stimulus"".","In mathematics and computer science, an algorithm ( (listen)) is a finite sequence of well-defined instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing.","[' What is a finite sequence of well-defined instructions used for in mathematics and computer science?', ' What are algorithms used as specifications for performing?']","['an algorithm', 'calculations and data processing']"
2416,algorithms,Summary,"As an effective method, an algorithm can be expressed within a finite amount of space and time, and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing ""output"" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.","As an effective method, an algorithm can be expressed within a finite amount of space and time, and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing ""output"" and terminating at a final ending state.","[' What can be expressed within a finite amount of space and time and in a well-defined formal language for calculating a function?', ' Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through what?', ' What is a computation that, when executed, proceeds through a finite number of well-defined successive states?']","['an algorithm', 'a finite number of well-defined successive states', 'the instructions']"
2417,algorithms,History,"The concept of algorithm has existed since antiquity. Arithmetic algorithms, such as a division algorithm, were used by ancient Babylonian mathematicians c. 2500 BC and Egyptian mathematicians c. 1550 BC. Greek mathematicians later used algorithms in 240 BC in the sieve of Eratosthenes for finding prime numbers, and the Euclidean algorithm for finding the greatest common divisor of two numbers. Arabic mathematicians such as al-Kindi in the 9th century used cryptographic algorithms for code-breaking, based on frequency analysis.","The concept of algorithm has existed since antiquity. Arithmetic algorithms, such as a division algorithm, were used by ancient Babylonian mathematicians c. 2500 BC and Egyptian mathematicians c. 1550 BC.","[' The concept of what has existed since antiquity?', ' What was used by ancient Babylonian mathematicians c. 2500 BC?', ' Who used Arithmetic algorithms?']","['algorithm', 'Arithmetic algorithms', 'ancient Babylonian mathematicians c. 2500 BC and Egyptian mathematicians c. 1550 BC']"
2418,algorithms,History,"The word algorithm is derived from the name of the 9th-century Persian mathematician Muḥammad ibn Mūsā al-Khwārizmī, whose nisba (identifying him as from Khwarazm) was Latinized as Algoritmi (Arabized Persian الخوارزمی c. 780–850).Muḥammad ibn Mūsā al-Khwārizmī was a mathematician, astronomer, geographer, and scholar in the House of Wisdom in Baghdad, whose name means 'the native of Khwarazm', a region that was part of Greater Iran and is now in Uzbekistan. About 825, al-Khwarizmi wrote an Arabic language treatise on the Hindu–Arabic numeral system, which was translated into Latin during the 12th century. The manuscript starts with the phrase Dixit Algorizmi ('Thus spake Al-Khwarizmi'), where ""Algorizmi"" was the translator's Latinization of Al-Khwarizmi's name. Al-Khwarizmi was the most widely read mathematician in Europe in the late Middle Ages, primarily through another of his books, the Algebra. In late medieval Latin, algorismus, English 'algorism', the corruption of his name, simply meant the ""decimal number system"". In the 15th century, under the influence of the Greek word ἀριθμός (arithmos), 'number' (cf. 'arithmetic'), the Latin word was altered to algorithmus, and the corresponding English term 'algorithm' is first attested in the 17th century; the modern sense was introduced in the 19th century.","The word algorithm is derived from the name of the 9th-century Persian mathematician Muḥammad ibn Mūsā al-Khwārizmī, whose nisba (identifying him as from Khwarazm) was Latinized as Algoritmi (Arabized Persian الخوارزمی c. 780–850).Muḥammad ibn Mūsā al-Khwārizmī was a mathematician, astronomer, geographer, and scholar in the House of Wisdom in Baghdad, whose name means 'the native of Khwarazm', a region that was part of Greater Iran and is now in Uzbekistan. About 825, al-Khwarizmi wrote an Arabic language treatise on the Hindu–Arabic numeral system, which was translated into Latin during the 12th century.","[' What is the name of the 9th-century Persian mathematician?', "" What was Mu<unk>ammad ibn M<unk>s<unk> al-Khw<unk>rizm<unk>'s nisba?"", ' When was Algoritmi (Arabized Persian <unk> c. 780-850) Latinized?', ' Where is the House of Wisdom located?', ' What does the name of the house of wisdom mean?', ' When did al-Khwarizmi write a treatise on the Hindu-Arabic numeral system?', ' In what century was the Hindu-Arabic numeral system translated into Latin?', ' What is the name of the system of numerals that was translated to Latin during the 12th century?']","['Muḥammad ibn Mūsā al-Khwārizmī', 'Algoritmi', 'nisba', 'Baghdad', 'the native of Khwarazm', 'About 825', '12th', 'Hindu–Arabic']"
2419,algorithms,History,"Indian mathematics was predominantly algorithmic.
Algorithms that are representative of the Indian mathematical tradition range from the ancient Śulbasūtrās to the medieval texts of the Kerala School.",Indian mathematics was predominantly algorithmic. Algorithms that are representative of the Indian mathematical tradition range from the ancient Śulbasūtrās to the medieval texts of the Kerala School.,"[' Indian mathematics was predominantly what?', ' Algorithms that are representative of the Indian mathematical tradition range from the ancient <unk>ulbas<unk>tr<unk>s to what texts?']","['algorithmic', 'the medieval texts of the Kerala School']"
2420,algorithms,History,"In English, the word algorithm was first used in about 1230 and then by Chaucer in 1391. English adopted the French term, but it was not until the late 19th century that ""algorithm"" took on the meaning that it has in modern English.","In English, the word algorithm was first used in about 1230 and then by Chaucer in 1391. English adopted the French term, but it was not until the late 19th century that ""algorithm"" took on the meaning that it has in modern English.","[' When was the word algorithm first used in English?', ' Who first used the term algorithm in English in 1391?', ' When did the French term algorithm take on the meaning of algorithm?', ' What century did ""algorithm"" take over the meaning that it has in modern english?']","['1230', 'Chaucer', 'late 19th century', '19th']"
2421,algorithms,History,"Another early use of the word is from 1240, in a manual titled Carmen de Algorismo composed by Alexandre de Villedieu. It begins with:
","Another early use of the word is from 1240, in a manual titled Carmen de Algorismo composed by Alexandre de Villedieu. It begins with:","[' In what year was Carmen de Algorismo first used?', "" Who wrote the manual 'Carmen de Algurismo'?"", ' What is the name of the author who wrote the first use of the word Carmen?']","['1240', 'Alexandre de Villedieu', 'Alexandre de Villedieu']"
2422,algorithms,History,"A partial formalization of the modern concept of algorithm began with attempts to solve the Entscheidungsproblem  (decision problem) posed by David Hilbert in 1928. Later formalizations were framed as attempts to define ""effective calculability"" or ""effective method"". Those formalizations included the Gödel–Herbrand–Kleene recursive functions of 1930, 1934 and 1935, Alonzo Church's lambda calculus of 1936, Emil Post's Formulation 1 of 1936, and Alan Turing's Turing machines of 1936–37 and 1939.
","A partial formalization of the modern concept of algorithm began with attempts to solve the Entscheidungsproblem  (decision problem) posed by David Hilbert in 1928. Later formalizations were framed as attempts to define ""effective calculability"" or ""effective method"".","[' When did David Hilbert attempt to solve the Entscheidungsproblem?', ' What were formalizations later framed as?']","['1928', 'attempts to define ""effective calculability"" or ""effective method"".']"
2423,algorithms,Informal definition,"No human being can write fast enough, or long enough, or small enough† ( †""smaller and smaller without limit ... you'd be trying to write on molecules, on atoms, on electrons"") to list all members of an enumerably infinite set by writing out their names, one after another, in some notation. But humans can do something equally useful, in the case of certain enumerably infinite sets: They can give explicit instructions for determining the nth member of the set, for arbitrary finite n. Such instructions are to be given quite explicitly, in a form in which they could be followed by a computing machine, or by a human who is capable of carrying out only very elementary operations on symbols.","No human being can write fast enough, or long enough, or small enough† ( †""smaller and smaller without limit ... you'd be trying to write on molecules, on atoms, on electrons"") to list all members of an enumerably infinite set by writing out their names, one after another, in some notation. But humans can do something equally useful, in the case of certain enumerably infinite sets: They can give explicit instructions for determining the nth member of the set, for arbitrary finite n. Such instructions are to be given quite explicitly, in a form in which they could be followed by a computing machine, or by a human who is capable of carrying out only very elementary operations on symbols.","[' No human being can write fast enough, long enough, or small enough to list all members of an enumerably infinite set by what?', ' What can humans do in the case of certain enumerably infinite sets?', ' How can humans give explicit instructions for determining the nth member of a set?', ' What kind of instructions are given quite explicitly in a form in which they could be followed by a computing machine?', ' What is a human capable of carrying out only very elementary operations on symbols?']","['writing out their names, one after another, in some notation', 'They can give explicit instructions for determining the nth member of the set', 'in a form in which they could be followed by a computing machine', 'enumerably infinite sets', 'humans can do something equally useful, in the case of certain enumerably infinite sets: They can give explicit instructions for determining the nth member of the set, for arbitrary finite n. Such instructions are to be given quite explicitly, in a form in which they could be followed by a computing machine']"
2424,algorithms,Informal definition,"An ""enumerably infinite set"" is one whose elements can be put into one-to-one correspondence with the integers. Thus Boolos and Jeffrey are saying that an algorithm implies instructions for a process that ""creates"" output integers from an arbitrary ""input"" integer or integers that, in theory, can be arbitrarily large. For example, an algorithm can be an algebraic equation such as y = m + n (i.e., two arbitrary ""input variables"" m and n that produce an output y), but various authors' attempts to define the notion indicate that the word implies much more than this, something on the order of (for the addition example):
","An ""enumerably infinite set"" is one whose elements can be put into one-to-one correspondence with the integers. Thus Boolos and Jeffrey are saying that an algorithm implies instructions for a process that ""creates"" output integers from an arbitrary ""input"" integer or integers that, in theory, can be arbitrarily large.","[' What is an ""enumerably infinite set""?', ' Boolos and Jeffrey are saying that an algorithm implies instructions for a process that creates output integers from what?']","['one whose elements can be put into one-to-one correspondence with the integers', 'an arbitrary ""input"" integer']"
2425,algorithms,Informal definition,"The concept of algorithm is also used to define the notion of decidability—a notion that is central for explaining how formal systems come into being starting from a small set of axioms and rules. In logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related to the customary physical dimension. From such uncertainties, that characterize ongoing work, stems the unavailability of a definition of algorithm that suits both concrete (in some sense) and abstract usage of the term.
","The concept of algorithm is also used to define the notion of decidability—a notion that is central for explaining how formal systems come into being starting from a small set of axioms and rules. In logic, the time that an algorithm requires to complete cannot be measured, as it is not apparently related to the customary physical dimension.","[' What concept is used to define the notion of decidability?', ' What is central for explaining how formal systems come into being starting from a small set of axioms and rules?', ' In logic, the time that an algorithm requires to complete cannot be measured as it is what?', ' What does algorithm requires to complete not be measured?', ' What is not apparently related to the customary physical dimension?']","['algorithm', 'decidability', 'it is not apparently related to the customary physical dimension', 'time', 'the time that an algorithm requires to complete']"
2426,algorithms,Informal definition,"Most algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.
","Most algorithms are intended to be implemented as computer programs. However, algorithms are also implemented by other means, such as in a biological neural network (for example, the human brain implementing arithmetic or an insect looking for food), in an electrical circuit, or in a mechanical device.","[' Most algorithms are intended to be implemented as what?', ' Algorithms are implemented by what other means?']","['computer programs', 'in an electrical circuit, or in a mechanical device']"
2427,algorithms,Formalization,"Algorithms are essential to the way computers process data. Many computer programs contain algorithms that detail the specific instructions a computer should perform—in a specific order—to carry out a specified task, such as calculating employees' paychecks or printing students' report cards. Thus, an algorithm can be considered to be any sequence of operations that can be simulated by a Turing-complete system. Authors who assert this thesis include Minsky (1967), Savage (1987) and Gurevich (2000):
","Algorithms are essential to the way computers process data. Many computer programs contain algorithms that detail the specific instructions a computer should perform—in a specific order—to carry out a specified task, such as calculating employees' paychecks or printing students' report cards.","[' Algorithms are essential to the way computers process what?', ' Many computer programs contain what kind of instructions?']","['data', 'algorithms']"
2428,algorithms,Formalization," Minsky: ""But we will also maintain, with Turing ... that any procedure which could ""naturally"" be called effective, can, in fact, be realized by a (simple) machine. Although this may seem extreme, the arguments ... in its favor are hard to refute"".
 Gurevich: ""… Turing's informal argument in favor of his thesis justifies a stronger thesis: every algorithm can be simulated by a Turing machine … according to Savage [1987], an algorithm is a computational process defined by a Turing machine""."," Minsky: ""But we will also maintain, with Turing ... that any procedure which could ""naturally"" be called effective, can, in fact, be realized by a (simple) machine. Although this may seem extreme, the arguments ... in its favor are hard to refute"".","[' Minsky maintains that any procedure that could ""naturally"" be called effective can, in fact, be realized by what?']",['a (simple) machine']
2429,algorithms,Formalization,"Turing machines can define computational processes that do not terminate. The informal definitions of algorithms generally require that the algorithm always terminates. This requirement renders the task of deciding whether a formal procedure is an algorithm impossible in the general case—due to a major theorem of computability theory known as the halting problem.
",Turing machines can define computational processes that do not terminate. The informal definitions of algorithms generally require that the algorithm always terminates.,"[' What kind of machines can define computational processes that do not terminate?', ' The informal definitions of what require that the algorithm always terminates?']","['Turing machines', 'algorithms']"
2430,algorithms,Formalization,"Typically, when an algorithm is associated with processing information, data can be read from an input source, written to an output device and stored for further processing. Stored data are regarded as part of the internal state of the entity performing the algorithm. In practice, the state is stored in one or more data structures.
","Typically, when an algorithm is associated with processing information, data can be read from an input source, written to an output device and stored for further processing. Stored data are regarded as part of the internal state of the entity performing the algorithm.","[' When an algorithm is associated with processing information, data can be read from an input source, written to an output device and stored for what purpose?', ' Stored data are regarded as part of what state of the entity performing the algorithm?']","['further processing', 'internal']"
2431,algorithms,Formalization,"For some of these computational processes, the algorithm must be rigorously defined: specified in the way it applies in all possible circumstances that could arise. This means that any conditional steps must be systematically dealt with, case-by-case; the criteria for each case must be clear (and computable).
","For some of these computational processes, the algorithm must be rigorously defined: specified in the way it applies in all possible circumstances that could arise. This means that any conditional steps must be systematically dealt with, case-by-case; the criteria for each case must be clear (and computable).","[' How must the algorithm be defined for some computational processes?', ' What must be systematically dealt with, case-by-case?', ' The criteria for each case must be what?']","['rigorously', 'any conditional steps', 'clear']"
2432,algorithms,Formalization,"Because an algorithm is a precise list of precise steps, the order of computation is always crucial to the functioning of the algorithm. Instructions are usually assumed to be listed explicitly, and are described as starting ""from the top"" and going ""down to the bottom""—an idea that is described more formally by flow of control.
","Because an algorithm is a precise list of precise steps, the order of computation is always crucial to the functioning of the algorithm. Instructions are usually assumed to be listed explicitly, and are described as starting ""from the top"" and going ""down to the bottom""—an idea that is described more formally by flow of control.","[' What is always crucial to the functioning of an algorithm?', ' Instructions are usually assumed to be listed explicitly and are described as starting from what?', ' What is going ""down to the bottom"" described more formally by?']","['the order of computation', 'the top', 'flow of control']"
2433,algorithms,Formalization,"So far, the discussion on the formalization of an algorithm has assumed the premises of imperative programming. This is the most common conception—one which attempts to describe a task in discrete, ""mechanical"" means. Unique to this conception of formalized algorithms is the assignment operation, which sets the value of a variable. It derives from the intuition of ""memory"" as a scratchpad. An example of such an assignment can be found below.
","So far, the discussion on the formalization of an algorithm has assumed the premises of imperative programming. This is the most common conception—one which attempts to describe a task in discrete, ""mechanical"" means.","[' What is the most common conception of imperative programming?', ' What does imperative programming attempt to describe?']","['—one which attempts to describe a task in discrete, ""mechanical"" means', 'a task']"
2434,algorithms,Expressing algorithms,"Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms. Pseudocode, flowcharts, drakon-charts and control tables are structured ways to express algorithms that avoid many of the ambiguities common in the statements based on natural language. Programming languages are primarily intended for expressing algorithms in a form that can be executed by a computer, but are also often used as a way to define or document algorithms.
","Algorithms can be expressed in many kinds of notation, including natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters). Natural language expressions of algorithms tend to be verbose and ambiguous, and are rarely used for complex or technical algorithms.","[' Algorithms can be expressed in what types of notation?', ' Natural language expressions of algorithms tend to be verbose and ambiguous and are rarely used for what?']","['natural languages, pseudocode, flowcharts, drakon-charts, programming languages or control tables (processed by interpreters).', 'complex or technical algorithms']"
2435,algorithms,Design,"Algorithm design refers to a method or a mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories of operation research, such as dynamic programming and divide-and-conquer. Techniques for designing and implementing algorithm designs are also called algorithm design patterns, with examples including the template method pattern and the decorator pattern.
","Algorithm design refers to a method or a mathematical process for problem-solving and engineering algorithms. The design of algorithms is part of many solution theories of operation research, such as dynamic programming and divide-and-conquer.","[' Algorithm design refers to a method or a mathematical process for what?', ' The design of algorithms is part of what solution theories of operation research?', ' Dynamic programming and divide-and-conquer are examples of what type of theory?']","['problem-solving and engineering algorithms', 'dynamic programming and divide-and-conquer', 'solution theories of operation research']"
2436,algorithms,Design,"One of the most important aspects of algorithm design is resource (run-time, memory usage) efficiency; the big O notation is used to describe e.g. an algorithm's run-time growth as the size of its input increases.
","One of the most important aspects of algorithm design is resource (run-time, memory usage) efficiency; the big O notation is used to describe e.g. an algorithm's run-time growth as the size of its input increases.","[' What is one of the most important aspects of algorithm design?', "" What is used to describe an algorithm's run-time growth?""]","['resource (run-time, memory usage) efficiency', 'big O notation']"
2437,algorithms,Computer algorithms,"Algorithm versus function computable by an algorithm: For a given function multiple algorithms may exist. This is true, even without expanding the available instruction set available to the programmer. Rogers observes that ""It is ... important to distinguish between the notion of algorithm, i.e. procedure and the notion of function computable by algorithm, i.e. mapping yielded by procedure. The same function may have several different algorithms"".","Algorithm versus function computable by an algorithm: For a given function multiple algorithms may exist. This is true, even without expanding the available instruction set available to the programmer.","[' Algorithm versus function computable by an algorithm?', ' For a given function, how many algorithms may exist?']","['expanding the available instruction set available to the programmer', 'multiple']"
2438,algorithms,Computer algorithms,"Unfortunately, there may be a tradeoff between goodness (speed) and elegance (compactness)—an elegant program may take more steps to complete a computation than one less elegant. An example that uses Euclid's algorithm appears below.
","Unfortunately, there may be a tradeoff between goodness (speed) and elegance (compactness)—an elegant program may take more steps to complete a computation than one less elegant. An example that uses Euclid's algorithm appears below.","[' What is a tradeoff between goodness and elegance?', ' An elegant program may take more steps to complete a computation than what?']","['speed', 'one less elegant']"
2439,algorithms,Computer algorithms,"Computers (and computors), models of computation: A computer (or human ""computor"") is a restricted type of machine, a ""discrete deterministic mechanical device"" that blindly follows its instructions. Melzak's and Lambek's primitive models reduced this notion to four elements: (i) discrete, distinguishable locations, (ii) discrete, indistinguishable counters (iii) an agent, and (iv) a list of instructions that are effective relative to the capability of the agent.","Computers (and computors), models of computation: A computer (or human ""computor"") is a restricted type of machine, a ""discrete deterministic mechanical device"" that blindly follows its instructions. Melzak's and Lambek's primitive models reduced this notion to four elements: (i) discrete, distinguishable locations, (ii) discrete, indistinguishable counters (iii) an agent, and (iv) a list of instructions that are effective relative to the capability of the agent.","[' What are computer models?', ' What is a computer a restricted type of?', "" How many elements did Melzak and Lambek's primitive models reduce this notion to?"", ' What is a list of instructions that are effective relative to the capability of the agent?']","['models of computation', 'machine, a ""discrete deterministic mechanical device"" that blindly follows its instructions', 'four', 'iv']"
2440,algorithms,Computer algorithms,"Minsky describes a more congenial variation of Lambek's ""abacus"" model in his ""Very Simple Bases for Computability"". Minsky's machine proceeds sequentially through its five (or six, depending on how one counts) instructions unless either a conditional IF-THEN GOTO or an unconditional GOTO changes program flow out of sequence. Besides HALT, Minsky's machine includes three assignment (replacement, substitution) operations: ZERO (e.g. the contents of location replaced by 0: L ← 0), SUCCESSOR (e.g. L ← L+1), and DECREMENT (e.g. L ← L − 1). Rarely must a programmer write ""code"" with such a limited instruction set. But Minsky shows (as do Melzak and Lambek) that his machine is Turing complete with only four general types of instructions: conditional GOTO, unconditional GOTO, assignment/replacement/substitution, and HALT.  However, a few different assignment instructions (e.g. DECREMENT, INCREMENT, and ZERO/CLEAR/EMPTY for a Minsky machine) are also required for Turing-completeness; their exact specification is somewhat up to the designer. The unconditional GOTO is a convenience; it can be constructed by initializing a dedicated location to zero e.g. the instruction "" Z ← 0 ""; thereafter the instruction IF Z=0 THEN GOTO xxx is unconditional.
","Minsky describes a more congenial variation of Lambek's ""abacus"" model in his ""Very Simple Bases for Computability"". Minsky's machine proceeds sequentially through its five (or six, depending on how one counts) instructions unless either a conditional IF-THEN GOTO or an unconditional GOTO changes program flow out of sequence.","[' Minsky describes a more congenial variation of Lambek\'s ""abacus"" model in what book?', "" Minsky's machine proceeds sequentially through how many instructions?""]","['Very Simple Bases for Computability', 'five']"
2441,algorithms,Computer algorithms,"Simulation of an algorithm: computer (computor) language: Knuth advises the reader that ""the best way to learn an algorithm is to try it . . . immediately take pen and paper and work through an example"". But what about a simulation or execution of the real thing? The programmer must translate the algorithm into a language that the simulator/computer/computor can effectively execute. Stone gives an example of this: when computing the roots of a quadratic equation the computor must know how to take a square root. If they don't, then the algorithm, to be effective, must provide a set of rules for extracting a square root.","Simulation of an algorithm: computer (computor) language: Knuth advises the reader that ""the best way to learn an algorithm is to try it . .","[' What does computer language mean?', ' What is the best way to learn an algorithm?']","['computor', 'to try it']"
2442,algorithms,Computer algorithms,"But what model should be used for the simulation? Van Emde Boas observes ""even if we base complexity theory on abstract instead of concrete machines, arbitrariness of the choice of a model remains. It is at this point that the notion of simulation enters"". When speed is being measured, the instruction set matters. For example, the subprogram in Euclid's algorithm to compute the remainder would execute much faster if the programmer had a ""modulus"" instruction available rather than just subtraction (or worse: just Minsky's ""decrement"").
","But what model should be used for the simulation? Van Emde Boas observes ""even if we base complexity theory on abstract instead of concrete machines, arbitrariness of the choice of a model remains.","[' What model should be used for simulation?', ' Van Emde Boas observes that even if we base complexity theory on abstract instead of concrete machines, what remains?']","['concrete machines', 'arbitrariness of the choice of a model']"
2443,algorithms,Computer algorithms,"Structured programming, canonical structures: Per the Church–Turing thesis, any algorithm can be computed by a model known to be Turing complete, and per Minsky's demonstrations, Turing completeness requires only four instruction types—conditional GOTO, unconditional GOTO, assignment, HALT. Kemeny and Kurtz observe that, while ""undisciplined"" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in ""spaghetti code"", a programmer can write structured programs using only these instructions; on the other hand ""it is also possible, and not too hard, to write badly structured programs in a structured language"". Tausworthe augments the three Böhm-Jacopini canonical structures: SEQUENCE, IF-THEN-ELSE, and WHILE-DO, with two more: DO-WHILE and CASE. An additional benefit of a structured program is that it lends itself to proofs of correctness using mathematical induction.","Structured programming, canonical structures: Per the Church–Turing thesis, any algorithm can be computed by a model known to be Turing complete, and per Minsky's demonstrations, Turing completeness requires only four instruction types—conditional GOTO, unconditional GOTO, assignment, HALT. Kemeny and Kurtz observe that, while ""undisciplined"" use of unconditional GOTOs and conditional IF-THEN GOTOs can result in ""spaghetti code"", a programmer can write structured programs using only these instructions; on the other hand ""it is also possible, and not too hard, to write badly structured programs in a structured language"".","[' What can be computed by a model known as Turing complete?', ' What requires only four instruction types?', ' Kemeny and Kurtz observe that ""undisciplined"" use of unconditional GOTOs and what else?', ' What can result in ""spaghetti code""?', ' What can a programmer write using only these instructions?', ' How is it possible to write badly structured programs in a structured language?']","['any algorithm', 'Turing completeness', 'conditional IF-THEN GOTOs', 'undisciplined"" use of unconditional GOTOs and conditional IF-THEN GOTOs', 'structured programs', 'it is also possible, and not too hard']"
2444,algorithms,Computer algorithms,"Canonical flowchart symbols: The graphical aide called a flowchart, offers a way to describe and document an algorithm (and a computer program of one). Like the program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down. Its primary symbols are only four: the directed arrow showing program flow, the rectangle (SEQUENCE, GOTO), the diamond (IF-THEN-ELSE), and the dot (OR-tie). The Böhm–Jacopini canonical structures are made of these primitive shapes. Sub-structures can ""nest"" in rectangles, but only if a single exit occurs from the superstructure. The symbols, and their use to build the canonical structures are shown in the diagram.
","Canonical flowchart symbols: The graphical aide called a flowchart, offers a way to describe and document an algorithm (and a computer program of one). Like the program flow of a Minsky machine, a flowchart always starts at the top of a page and proceeds down.","[' What is a graphical aide called?', ' What does a flowchart offer a way to describe and document?']","['flowchart', 'an algorithm']"
2445,algorithms,Algorithmic analysis,"It is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, an algorithm which adds up the elements of a list of n numbers would have a time requirement of O(n), using big O notation. At all times the algorithm only needs to remember two values: the sum of all the elements so far, and its current position in the input list. Therefore, it is said to have a space requirement of O(1), if the space required to store the input numbers is not counted, or O(n) if it is counted.
","It is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); for example, an algorithm which adds up the elements of a list of n numbers would have a time requirement of O(n), using big O notation.","[' What is often important to know how much of a particular resource is theoretically required for an algorithm?', ' Methods have been developed for the analysis of algorithms to obtain what?', ' What would an algorithm that adds up the elements of a list of n numbers have a time requirement of?']","['time or storage', 'quantitative answers (estimates', 'O(n']"
2446,algorithms,Algorithmic analysis,"Different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost O(log n)) outperforms a sequential search (cost O(n) ) when used for table lookups on sorted lists or arrays.
","Different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. For example, a binary search algorithm (with cost O(log n)) outperforms a sequential search (cost O(n) ) when used for table lookups on sorted lists or arrays.","[' What may different algorithms complete the same task with a different set of instructions in less or more time, space, or effort than others?', ' What outperforms a sequential search?']","['binary search algorithm', 'a binary search algorithm']"
2447,algorithms,Legal issues,"Algorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute ""processes"" (USPTO 2006), and hence algorithms are not patentable (as in Gottschalk v. Benson). However practical applications of algorithms are sometimes patentable. For example, in Diamond v. Diehr, the application of a simple feedback algorithm to aid in the curing of synthetic rubber was deemed patentable. The patenting of software is highly controversial, and there are highly criticized patents involving algorithms, especially data compression algorithms, such as Unisys' LZW patent.
","Algorithms, by themselves, are not usually patentable. In the United States, a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals does not constitute ""processes"" (USPTO 2006), and hence algorithms are not patentable (as in Gottschalk v. Benson).","[' What are algorithms not usually patentable?', ' What does a claim consisting solely of simple manipulations of abstract concepts, numbers, or signals not constitute in the US?']","['by themselves', 'processes']"
2448,transfer learning,Summary,"Transfer learning (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks.  This area of research bears some relation to the long history of psychological literature on transfer of learning, although practical ties between the two fields are limited. From the practical standpoint, reusing or transferring information from previously learned tasks for the learning of new tasks has the potential to significantly improve the sample efficiency of a reinforcement learning agent.","Transfer learning (TL) is a research problem in machine learning (ML) that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem. For example, knowledge gained while learning to recognize cars could apply when trying to recognize trucks.","[' Transfer learning is a research problem in what?', ' Transfer learning focuses on storing knowledge gained while solving one problem and applying it to a different but related problem?', ' Knowledge gained while learning to recognize cars could apply when trying to recognize trucks?']","['machine learning', 'machine learning', 'Transfer learning']"
2449,transfer learning,History,"In 1976, Stevo Bozinovski and Ante Fulgosi published a paper explicitly addressing transfer learning in neural networks training. The paper gives a mathematical and geometrical model of transfer learning. In 1981, a report was given on the application of transfer learning in training a neural network on a dataset of images representing letters of computer terminals. Both positive and negative transfer learning was experimentally demonstrated.","In 1976, Stevo Bozinovski and Ante Fulgosi published a paper explicitly addressing transfer learning in neural networks training. The paper gives a mathematical and geometrical model of transfer learning.","[' In what year did Bozinovski and Fulgosi publish a paper addressing transfer learning in neural networks training?', ' What did the paper give a mathematical and geometrical model of?']","['1976', 'transfer learning']"
2450,transfer learning,History,"In 1997, Pratt and Sebastian Thrun guest edited a special issue of Machine Learning  devoted to transfer learning, and by 1998, the field had advanced to include multi-task learning,  along with a more formal analysis of its theoretical foundations. Learning to Learn, edited by Thrun and Pratt, is a 1998 review of the subject.
","In 1997, Pratt and Sebastian Thrun guest edited a special issue of Machine Learning  devoted to transfer learning, and by 1998, the field had advanced to include multi-task learning,  along with a more formal analysis of its theoretical foundations. Learning to Learn, edited by Thrun and Pratt, is a 1998 review of the subject.","[' In what year did Pratt and Sebastian Thrun guest edit a special issue of Machine Learning devoted to transfer learning?', ' By 1998, the field had advanced to include what?', ' What is the title of the edited volume of Learning to Learn?', ' Who edited Learning to Learn?', ' What was the title of the 1998 review of learning to learn?']","['1997', 'multi-task learning', 'a 1998 review of the subject', 'Thrun and Pratt', 'Learning to Learn']"
2451,transfer learning,Definition,"The definition of transfer learning is given in terms of domains and tasks. A domain 





D




{\displaystyle {\mathcal {D}}}
 consists of: a feature space 





X




{\displaystyle {\mathcal {X}}}
 and a marginal probability distribution 



P
(
X
)


{\displaystyle P(X)}
, where 



X
=
{

x

1


,
.
.
.
,

x

n


}
∈


X




{\displaystyle X=\{x_{1},...,x_{n}\}\in {\mathcal {X}}}
. Given a specific domain, 





D


=
{


X


,
P
(
X
)
}


{\displaystyle {\mathcal {D}}=\{{\mathcal {X}},P(X)\}}
, a task consists of two components: a label space 





Y




{\displaystyle {\mathcal {Y}}}
 and an objective predictive function 



f
:


X


→


Y




{\displaystyle f:{\mathcal {X}}\rightarrow {\mathcal {Y}}}
. The function 



f


{\displaystyle f}
 is used to predict the corresponding label 



f
(
x
)


{\displaystyle f(x)}
 of a new instance 



x


{\displaystyle x}
. This task, denoted by 





T


=
{


Y


,
f
(
x
)
}


{\displaystyle {\mathcal {T}}=\{{\mathcal {Y}},f(x)\}}
, is learned from the training data consisting of pairs 



{

x

i


,

y

i


}


{\displaystyle \{x_{i},y_{i}\}}
, where 




x

i


∈
X


{\displaystyle x_{i}\in X}
 and 




y

i


∈


Y




{\displaystyle y_{i}\in {\mathcal {Y}}}
.","The definition of transfer learning is given in terms of domains and tasks. A domain 





D




{\displaystyle {\mathcal {D}}}
 consists of: a feature space 





X




{\displaystyle {\mathcal {X}}}
 and a marginal probability distribution 



P
(
X
)


{\displaystyle P(X)}
, where 



X
=
{

x

1


,
.","[' The definition of transfer learning is given in terms of what?', ' A domain D consists of: a feature space X <unk>displaystyle <unk>mathcal <unk>D<unk> and a marginal probability distribution P ( X) <unk>Displaystyle P(X)<unk>?']","['domains and tasks', 'P\n(\nX\n)\n\n\n{\\displaystyle P(X)}\n, where \n\n\n\nX']"
2452,transfer learning,Applications,"Algorithms are available for transfer learning in Markov logic networks and Bayesian networks. Transfer learning has also been applied to cancer subtype discovery, building utilization, general game playing, text classification, digit recognition, medical imaging and spam filtering.","Algorithms are available for transfer learning in Markov logic networks and Bayesian networks. Transfer learning has also been applied to cancer subtype discovery, building utilization, general game playing, text classification, digit recognition, medical imaging and spam filtering.","[' Algorithms are available for transfer learning in what networks?', ' Transfer learning has been applied to what?']","['Markov logic networks and Bayesian networks', 'cancer subtype discovery, building utilization, general game playing, text classification, digit recognition, medical imaging and spam filtering']"
2453,transfer learning,Applications,"In 2020 it was discovered that, due to their similar physical natures, transfer learning is possible between Electromyographic (EMG) signals from the muscles when classifying the behaviors of Electroencephalographic (EEG) brainwaves from the gesture recognition domain to the mental state recognition domain. It was also noted that this relationship worked vice versa, showing that EEG can likewise be used to classify EMG in addition. The experiments noted that the accuracy of neural networks and convolutional neural networks were improved through transfer learning both at the first epoch (prior to any learning, ie. compared to standard random weight distribution) and at the asymptote (the end of the learning process). That is, algorithms are improved by exposure to another domain. Moreover, the end-user of a pre-trained model can change the structure of fully-connected layers to achieve superior performance.","In 2020 it was discovered that, due to their similar physical natures, transfer learning is possible between Electromyographic (EMG) signals from the muscles when classifying the behaviors of Electroencephalographic (EEG) brainwaves from the gesture recognition domain to the mental state recognition domain. It was also noted that this relationship worked vice versa, showing that EEG can likewise be used to classify EMG in addition.","[' When was transfer learning discovered between Electromyographic signals from the muscles?', ' What is EEG?', ' When was it discovered that transfer learning is possible between EMG signals?', ' What can be used to classify EMG in addition to EEG?']","['2020', 'Electroencephalographic', '2020', 'EEG']"
2454,intrusion detection system,Summary,An intrusion detection system (IDS; also intrusion prevention system or IPS) is a device or software application that monitors a network or systems for malicious activity or policy violations. Any intrusion activity or violation is typically reported either to an administrator or collected centrally using a security information and event management (SIEM) system. A SIEM system combines outputs from multiple sources and uses alarm filtering techniques to distinguish malicious activity from false alarms.,An intrusion detection system (IDS; also intrusion prevention system or IPS) is a device or software application that monitors a network or systems for malicious activity or policy violations. Any intrusion activity or violation is typically reported either to an administrator or collected centrally using a security information and event management (SIEM) system.,"[' What is a device or software application that monitors a network or systems for malicious activity or policy violations?', ' What is an intrusion detection system also called?', ' What does SIEM stand for?', ' What is a security information and event management system?']","['An intrusion detection system', 'IDS', 'security information and event management', 'SIEM']"
2455,intrusion detection system,Summary,"IDS types range in scope from single computers to large networks. The most common classifications are network intrusion detection systems (NIDS) and host-based intrusion detection systems (HIDS). A system that monitors important operating system files is an example of an HIDS, while a system that analyzes incoming network traffic is an example of an NIDS. It is also possible to classify IDS by detection approach. The most well-known variants are signature-based detection (recognizing bad patterns, such as malware) and anomaly-based detection (detecting deviations from a model of ""good"" traffic, which often relies on machine learning). Another common variant is reputation-based detection (recognizing the potential threat according to the reputation scores). Some IDS products have the ability to respond to detected intrusions. Systems with response capabilities are typically referred to as an intrusion prevention system. Intrusion detection systems can also serve specific purposes by augmenting them with custom tools, such as using a honeypot to attract and characterize malicious traffic.",IDS types range in scope from single computers to large networks. The most common classifications are network intrusion detection systems (NIDS) and host-based intrusion detection systems (HIDS).,"[' What are the most common classifications for network intrusion detection systems?', ' What is HIDS?']","['NIDS) and host-based intrusion detection systems (HIDS).', 'host-based intrusion detection systems']"
2456,intrusion detection system,Comparison with firewalls,"Although they both relate to network security, an IDS differs from a firewall in that a traditional network firewall (distinct from a Next-Generation Firewall) uses a static set of rules to permit or deny network connections. It implicitly prevents intrusions, assuming an appropriate set of rules have been defined. Essentially, firewalls limit access between networks to prevent intrusion and do not signal an attack from inside the network. An IDS describes a suspected intrusion once it has taken place and signals an alarm. An IDS also watches for attacks that originate from within a system. This is traditionally achieved by examining network communications, identifying heuristics and patterns (often known as signatures) of common computer attacks, and taking action to alert operators.  A system that terminates connections is called an intrusion prevention system, and performs access control like an application layer firewall.","Although they both relate to network security, an IDS differs from a firewall in that a traditional network firewall (distinct from a Next-Generation Firewall) uses a static set of rules to permit or deny network connections. It implicitly prevents intrusions, assuming an appropriate set of rules have been defined.","[' What does an IDS differ from a firewall in that it uses a static set of rules to permit or deny network connections?', ' An IDS implicitly prevents intrusions, assuming what has been defined?']","['traditional network firewall', 'an appropriate set of rules']"
2457,intrusion detection system,Intrusion prevention,"Some systems may attempt to stop an intrusion attempt but this is neither required nor expected of a monitoring system. Intrusion detection and prevention systems (IDPS) are primarily focused on identifying possible incidents, logging information about them, and reporting attempts.  In addition, organizations use IDPS for other purposes, such as identifying problems with security policies, documenting existing threats and deterring individuals from violating security policies. IDPS have become a necessary addition to the security infrastructure of nearly every organization.","Some systems may attempt to stop an intrusion attempt but this is neither required nor expected of a monitoring system. Intrusion detection and prevention systems (IDPS) are primarily focused on identifying possible incidents, logging information about them, and reporting attempts.","[' What are intrusion detection and prevention systems primarily focused on?', ' What does IDPS stand for?']","['identifying possible incidents, logging information about them, and reporting attempts', 'Intrusion detection and prevention systems']"
2458,intrusion detection system,Intrusion prevention,"IDPS typically record information related to observed events, notify security administrators of important observed events and produce reports. Many IDPS can also respond to a detected threat by attempting to prevent it from succeeding. They use several response techniques, which involve the IDPS stopping the attack itself, changing the security environment (e.g. reconfiguring a firewall) or changing the attack's content.","IDPS typically record information related to observed events, notify security administrators of important observed events and produce reports. Many IDPS can also respond to a detected threat by attempting to prevent it from succeeding.","[' IDPS typically record information related to what?', ' IDPS notify security administrators of important observed events and produce what kind of reports?']","['observed events', 'reports']"
2459,intrusion detection system,Intrusion prevention,"Intrusion prevention systems (IPS), also known as intrusion detection and prevention systems (IDPS), are network security appliances that monitor network or system activities for malicious activity. The main functions of intrusion prevention systems are to identify malicious activity, log information about this activity, report it and attempt to block or stop it..
","Intrusion prevention systems (IPS), also known as intrusion detection and prevention systems (IDPS), are network security appliances that monitor network or system activities for malicious activity. The main functions of intrusion prevention systems are to identify malicious activity, log information about this activity, report it and attempt to block or stop it..","[' What is IPS also known as?', ' What are intrusion prevention systems used for?', ' Report the activity to the police and try to stop it.']","['Intrusion prevention systems', 'to identify malicious activity, log information about this activity, report it and attempt to block or stop it', 'Intrusion prevention systems']"
2460,intrusion detection system,Intrusion prevention,"Intrusion prevention systems are considered extensions of intrusion detection systems because they both monitor network traffic and/or system activities for malicious activity. The main differences are, unlike intrusion detection systems, intrusion prevention systems are placed in-line and are able to actively prevent or block intrusions that are detected.: 273 : 289  IPS can take such actions as sending an alarm, dropping detected malicious packets, resetting a connection or blocking traffic from the offending IP address. An IPS also can correct cyclic redundancy check (CRC) errors, defragment packet streams, mitigate TCP sequencing issues, and clean up unwanted transport and network layer options.: 278 .
","Intrusion prevention systems are considered extensions of intrusion detection systems because they both monitor network traffic and/or system activities for malicious activity. The main differences are, unlike intrusion detection systems, intrusion prevention systems are placed in-line and are able to actively prevent or block intrusions that are detected.","[' What are intrusion prevention systems considered extensions of?', ' What do intrusion detection systems monitor for malicious activity?', ' In contrast to a detection system, what is the main difference?']","['intrusion detection systems', 'network traffic and/or system activities', 'intrusion prevention systems are placed in-line and are able to actively prevent or block intrusions that are detected']"
2461,intrusion detection system,Placement,"The correct placement of intrusion detection systems is critical and varies depending on the network. The most common placement is behind the firewall, on the edge of a network. This practice provides the IDS with high visibility of traffic entering your network and will not receive any traffic between users on the network. The edge of the network is the point in which a network connects to the extranet. Another practice that can be accomplished if more resources are available is a strategy where a technician will place their first IDS at the point of highest visibility and depending on resource availability will place another at the next highest point, continuing that process until all points of the network are covered.","The correct placement of intrusion detection systems is critical and varies depending on the network. The most common placement is behind the firewall, on the edge of a network.",[' What is the most common location for intrusion detection systems?'],['behind the firewall']
2462,intrusion detection system,Placement,"If an IDS is placed beyond a network's firewall, its main purpose would be to defend against noise from the internet but, more importantly, defend against common attacks, such as port scans and network mapper. An IDS in this position would monitor layers 4 through 7 of the OSI model and would be signature-based. This is a very useful practice, because rather than showing actual breaches into the network that made it through the firewall, attempted breaches will be shown which reduces the amount of false positives. The IDS in this position also assists in decreasing the amount of time it takes to discover successful attacks against a network.","If an IDS is placed beyond a network's firewall, its main purpose would be to defend against noise from the internet but, more importantly, defend against common attacks, such as port scans and network mapper. An IDS in this position would monitor layers 4 through 7 of the OSI model and would be signature-based.","["" What is the main purpose of an IDS if it is placed beyond a network's firewall?"", ' What are two common attacks that IDSs protect against?', ' An IDS in this position would monitor layers 4-7 of what model?', ' What would monitor layers 4 through 7 of the OSI model?', ' What would be signature-based?']","['to defend against noise from the internet', 'port scans and network mapper', 'OSI', 'An IDS', 'An IDS in this position would monitor layers 4 through 7 of the OSI model']"
2463,intrusion detection system,Placement,Sometimes an IDS with more advanced features will be integrated with a firewall in order to be able to intercept sophisticated attacks entering the network. Examples of advanced features would include multiple security contexts in the routing level and bridging mode. All of this in turn potentially reduces cost and operational complexity.,Sometimes an IDS with more advanced features will be integrated with a firewall in order to be able to intercept sophisticated attacks entering the network. Examples of advanced features would include multiple security contexts in the routing level and bridging mode.,"[' What is an IDS with more advanced features sometimes integrated with?', ' What is one example of advanced features?']","['a firewall', 'multiple security contexts in the routing level and bridging mode']"
2464,intrusion detection system,Placement,"Another option for IDS placement is within the actual network. These will reveal attacks or suspicious activity within the network. Ignoring the security within a network can cause many problems, it will either allow users to bring about security risks or allow an attacker who has already broken into the network to roam around freely. Intense intranet security makes it difficult for even those hackers within the network to maneuver around and escalate their privileges.",Another option for IDS placement is within the actual network. These will reveal attacks or suspicious activity within the network.,"[' What is another option for IDS placement?', ' What will reveal attacks or suspicious activity within the network?']","['within the actual network', 'within the actual network']"
2465,intrusion detection system,Development,"The earliest preliminary IDS concept was delineated in 1980 by James Anderson at the National Security Agency and consisted of a set of tools intended to help administrators review audit trails. User access logs, file access logs, and system event logs are examples of audit trails.
","The earliest preliminary IDS concept was delineated in 1980 by James Anderson at the National Security Agency and consisted of a set of tools intended to help administrators review audit trails. User access logs, file access logs, and system event logs are examples of audit trails.","[' When was the earliest preliminary IDS concept delineated?', ' What did James Anderson at the National Security Agency delineate?']","['1980', 'IDS concept']"
2466,intrusion detection system,Development,"Dorothy E. Denning, assisted by Peter G. Neumann, published a model of an IDS in 1986 that formed the basis for many systems today.  Her model used statistics for anomaly detection, and resulted in an early IDS at SRI International named the Intrusion Detection Expert System (IDES), which ran on Sun workstations and could consider both user and network level data.  IDES had a dual approach with a rule-based Expert System to detect known types of intrusions plus a statistical anomaly detection component based on profiles of users, host systems, and target systems. The author of ""IDES: An Intelligent System for Detecting Intruders,"" Teresa F. Lunt, proposed adding an Artificial neural network as a third component.  She said all three components could then report to a resolver.  SRI followed IDES in 1993 with the Next-generation Intrusion Detection Expert System (NIDES).","Dorothy E. Denning, assisted by Peter G. Neumann, published a model of an IDS in 1986 that formed the basis for many systems today. Her model used statistics for anomaly detection, and resulted in an early IDS at SRI International named the Intrusion Detection Expert System (IDES), which ran on Sun workstations and could consider both user and network level data.","[' Who published a model of an IDS in 1986?', "" What did Dorothy Denning's model use for anomaly detection?"", ' At what SRI International organization was the IDS named?', ' What was the Intrusion Detection Expert System (IDES) named?', ' On what workstations did the IDES run?', ' What could IDES consider user and network data?']","['Dorothy E. Denning', 'statistics', 'Intrusion Detection Expert System', 'SRI International', 'Sun workstations', 'both user and network level']"
2467,intrusion detection system,Development,"The Multics intrusion detection and alerting system (MIDAS), an expert system using P-BEST and Lisp, was developed in 1988 based on the work of Denning and Neumann.  Haystack was also developed in that year using statistics to reduce audit trails.","The Multics intrusion detection and alerting system (MIDAS), an expert system using P-BEST and Lisp, was developed in 1988 based on the work of Denning and Neumann. Haystack was also developed in that year using statistics to reduce audit trails.","[' What is MIDAS?', ' When was the Multics intrusion detection and alerting system developed?', ' What was developed in 1988 based on the work of Denning and Neumann?']","['Multics intrusion detection and alerting system', '1988', 'The Multics intrusion detection and alerting system']"
2468,intrusion detection system,Development,"In 1986 the National Security Agency started an IDS research transfer program under Rebecca Bace. Bace later published the seminal text on the subject, Intrusion Detection, in 2000.","In 1986 the National Security Agency started an IDS research transfer program under Rebecca Bace. Bace later published the seminal text on the subject, Intrusion Detection, in 2000.","[' In what year did the National Security Agency start an IDS research transfer program?', ' What was the name of the program that was started by Rebecca Bace?', ' Who published the seminal text on intrusion detection in 2000?']","['1986', 'IDS research transfer program', 'Rebecca Bace']"
2469,intrusion detection system,Development,"Wisdom & Sense (W&S) was a statistics-based anomaly detector developed in 1989 at the Los Alamos National Laboratory.  W&S created rules based on statistical analysis, and then used those rules for anomaly detection.
","Wisdom & Sense (W&S) was a statistics-based anomaly detector developed in 1989 at the Los Alamos National Laboratory. W&S created rules based on statistical analysis, and then used those rules for anomaly detection.","[' What was the name of the statistics-based anomaly detector developed in 1989 at the Los Alamos National Laboratory?', ' What did Wisdom & Sense use to create rules for anomaly detection?']","['Wisdom & Sense', 'statistical analysis']"
2470,intrusion detection system,Development,"In 1990, the Time-based Inductive Machine (TIM) did anomaly detection using inductive learning of sequential user patterns in Common Lisp on a VAX 3500 computer.  The Network Security Monitor (NSM) performed masking on access matrices for anomaly detection on a Sun-3/50 workstation.  The Information Security Officer's Assistant (ISOA) was a 1990 prototype that considered a variety of strategies including statistics, a profile checker, and an expert system.  ComputerWatch at AT&T Bell Labs used statistics and rules for audit data reduction and intrusion detection.","In 1990, the Time-based Inductive Machine (TIM) did anomaly detection using inductive learning of sequential user patterns in Common Lisp on a VAX 3500 computer. The Network Security Monitor (NSM) performed masking on access matrices for anomaly detection on a Sun-3/50 workstation.","[' In what year did the Time-based Inductive Machine do anomaly detection?', ' What did the TIM do on a VAX 3500 computer?', ' The Network Security Monitor performed masking on access matrices for what purpose?']","['1990', 'anomaly detection', 'anomaly detection']"
2471,intrusion detection system,Development,"Then, in 1991, researchers at the University of California, Davis created a prototype Distributed Intrusion Detection System (DIDS), which was also an expert system.  The Network Anomaly Detection and Intrusion Reporter (NADIR), also in 1991, was a prototype IDS developed at the Los Alamos National Laboratory's Integrated Computing Network (ICN), and was heavily influenced by the work of Denning and Lunt.  NADIR used a statistics-based anomaly detector and an expert system.
","Then, in 1991, researchers at the University of California, Davis created a prototype Distributed Intrusion Detection System (DIDS), which was also an expert system. The Network Anomaly Detection and Intrusion Reporter (NADIR), also in 1991, was a prototype IDS developed at the Los Alamos National Laboratory's Integrated Computing Network (ICN), and was heavily influenced by the work of Denning and Lunt.","[' In what year did researchers at the University of California, Davis create a prototype Distributed Intrusion Detection System (DIDS)?', "" What was the prototype of the Network Anomaly Detect and Intrusion Reporter (NADIR) developed at the Los Alamos National Laboratory's Integrated Computing Network?"", ' What was the name of the Integrated Computing Network at the Los Alamos National Laboratory?']","['1991', 'IDS', 'ICN']"
2472,intrusion detection system,Development,"The Lawrence Berkeley National Laboratory announced Bro in 1998, which used its own rule language for packet analysis from libpcap data.  Network Flight Recorder (NFR) in 1999 also used libpcap.","The Lawrence Berkeley National Laboratory announced Bro in 1998, which used its own rule language for packet analysis from libpcap data. Network Flight Recorder (NFR) in 1999 also used libpcap.","[' When did the Lawrence Berkeley National Laboratory announce Bro?', ' What did Bro use for packet analysis?', ' When did NFR use libpcap?']","['1998', 'rule language', '1999']"
2473,intrusion detection system,Development,"APE was developed as a packet sniffer, also using libpcap, in November, 1998, and was renamed Snort one month later. Snort has since become the world's largest used IDS/IPS system with over 300,000 active users. It can monitor both local systems, and remote capture points using the TZSP protocol.
","APE was developed as a packet sniffer, also using libpcap, in November, 1998, and was renamed Snort one month later. Snort has since become the world's largest used IDS/IPS system with over 300,000 active users.","[' When was APE developed?', ' What was the original name of APE?', "" Snort became the world's largest used IDS/IPS system with how many users?""]","['November, 1998', 'Snort', '300,000']"
2474,intrusion detection system,Development,"The Audit Data Analysis and Mining (ADAM) IDS in 2001 used tcpdump to build profiles of rules for classifications. In 2003, Yongguang Zhang and Wenke Lee argue for the importance of IDS in networks with mobile nodes.","The Audit Data Analysis and Mining (ADAM) IDS in 2001 used tcpdump to build profiles of rules for classifications. In 2003, Yongguang Zhang and Wenke Lee argue for the importance of IDS in networks with mobile nodes.","[' When did the Audit Data Analysis and Mining (ADAM) IDS use tcpdump?', ' Who argue for the importance of IDS in networks with mobile nodes?']","['2001', 'Yongguang Zhang and Wenke Lee']"
2475,intrusion detection system,Development,"In 2015, Viegas and his colleagues  proposed an anomaly-based intrusion detection engine, aiming System-on-Chip (SoC) for applications in Internet of Things (IoT), for instance. The proposal applies machine learning for anomaly detection, providing energy-efficiency to a Decision Tree, Naive-Bayes, and k-Nearest Neighbors classifiers implementation in an Atom CPU and its hardware-friendly implementation in a FPGA. In the literature, this was the first work that implement each classifier equivalently in software and hardware and measures its energy consumption on both. Additionally, it was the first time that was measured the energy consumption for extracting each features used to make the network packet classification, implemented in software and hardware.","In 2015, Viegas and his colleagues  proposed an anomaly-based intrusion detection engine, aiming System-on-Chip (SoC) for applications in Internet of Things (IoT), for instance. The proposal applies machine learning for anomaly detection, providing energy-efficiency to a Decision Tree, Naive-Bayes, and k-Nearest Neighbors classifiers implementation in an Atom CPU and its hardware-friendly implementation in a FPGA.","[' In what year did Viegas and his colleagues propose an anomaly-based intrusion detection engine?', ' What was the SoC engine aimed at?', ' How does SoC apply machine learning for anomaly detection?', ' What is the name of the classifier implemented in an Atom CPU?', ' Where is the k-Nearest Neighbors implementation implemented?']","['2015', 'Internet of Things', 'providing energy-efficiency to a Decision Tree, Naive-Bayes, and k-Nearest Neighbors classifiers implementation in an Atom CPU and its hardware-friendly implementation in a FPGA', 'k-Nearest Neighbors', 'Atom CPU']"
2476,grid computing,Summary,"Grid computing is the use of widely distributed computer resources to reach a common goal. A computing grid can be thought of as a distributed system with non-interactive workloads that involve many files. Grid computing is distinguished from conventional high-performance computing systems such as cluster computing in that grid computers have each node set to perform a different task/application. Grid computers also tend to be more heterogeneous and geographically dispersed (thus not physically coupled) than cluster computers. Although a single grid can be dedicated to a particular application, commonly a grid is used for a variety of purposes. Grids are often constructed with general-purpose grid middleware software libraries. Grid sizes can be quite large.",Grid computing is the use of widely distributed computer resources to reach a common goal. A computing grid can be thought of as a distributed system with non-interactive workloads that involve many files.,"[' What is the use of widely distributed computer resources to reach a common goal?', ' What can be thought of as a distributed system with non-interactive workloads?']","['Grid computing', 'A computing grid']"
2477,grid computing,Summary,"Grids are a form of distributed computing whereby a ""super virtual computer"" is composed of many networked loosely coupled computers acting together to perform large tasks. For certain applications, distributed or grid computing can be seen as a special type of parallel computing that relies on complete computers (with onboard CPUs, storage, power supplies, network interfaces, etc.) connected to a computer network (private or public) by a conventional network interface, such as Ethernet. This is in contrast to the traditional notion of a supercomputer, which has many processors connected by a local high-speed computer bus. This technology has been applied to computationally intensive scientific, mathematical, and academic problems through volunteer computing, and it is used in commercial enterprises for such diverse applications as drug discovery, economic forecasting, seismic analysis, and back office data processing in support for e-commerce and Web services.
","Grids are a form of distributed computing whereby a ""super virtual computer"" is composed of many networked loosely coupled computers acting together to perform large tasks. For certain applications, distributed or grid computing can be seen as a special type of parallel computing that relies on complete computers (with onboard CPUs, storage, power supplies, network interfaces, etc.)","[' What is a form of distributed computing called?', ' What are grids?', ' How many computers are in a grid?', ' What type of computing relies on complete computers?', ' What are onboard CPUs, storage, power supplies, network interfaces, etc.?']","['Grids', 'a form of distributed computing', 'many networked loosely coupled computers', 'distributed or grid computing', 'complete computers']"
2478,grid computing,Summary,"Grid computing combines computers from multiple administrative domains to reach a common goal, to solve a single task, and may then disappear just as quickly. The size of a grid may vary from small—confined to a network of computer workstations within a corporation, for example—to large, public collaborations across many companies and networks. ""The notion of a confined grid may also be known as an intra-nodes cooperation whereas the notion of a larger, wider grid may thus refer to an inter-nodes cooperation"".","Grid computing combines computers from multiple administrative domains to reach a common goal, to solve a single task, and may then disappear just as quickly. The size of a grid may vary from small—confined to a network of computer workstations within a corporation, for example—to large, public collaborations across many companies and networks.","[' What type of computing combines computers from multiple administrative domains to reach a common goal, to solve a single task, and may disappear just as quickly?', ' The size of a grid may vary from small, confined to what?', ' What is an example of a large, public collaboration across many companies and networks?']","['Grid computing', 'a network of computer workstations within a corporation', 'Grid computing']"
2479,grid computing,Summary,"Coordinating applications on Grids can be a complex task, especially when coordinating the flow of information across distributed computing resources. Grid workflow systems have been developed as a specialized form of a workflow management system designed specifically to compose and execute a series of computational or data manipulation steps, or a workflow, in the grid context.
","Coordinating applications on Grids can be a complex task, especially when coordinating the flow of information across distributed computing resources. Grid workflow systems have been developed as a specialized form of a workflow management system designed specifically to compose and execute a series of computational or data manipulation steps, or a workflow, in the grid context.","[' What can be a complex task when coordinating the flow of information across distributed computing resources?', ' What has been developed as a specialized form of a workflow management system?', ' Execute a series of computational or data manipulation steps in the grid context?']","['Coordinating applications on Grids', 'Grid workflow systems', 'Grid workflow systems']"
2480,grid computing,Comparison of grids and conventional supercomputers,"“Distributed” or “grid” computing in general is a special type of parallel computing that relies on complete computers (with onboard CPUs, storage, power supplies, network interfaces, etc.) connected to a network (private, public or the Internet) by a conventional network interface producing commodity hardware, compared to the lower efficiency of designing and constructing a small number of custom supercomputers. The primary performance disadvantage is that the various processors and local storage areas do not have high-speed connections. This arrangement is thus well-suited to applications in which multiple parallel computations can take place independently, without the need to communicate intermediate results between processors. The high-end scalability of geographically dispersed grids is generally favorable, due to the low need for connectivity between nodes relative to the capacity of the public Internet.","“Distributed” or “grid” computing in general is a special type of parallel computing that relies on complete computers (with onboard CPUs, storage, power supplies, network interfaces, etc.) connected to a network (private, public or the Internet) by a conventional network interface producing commodity hardware, compared to the lower efficiency of designing and constructing a small number of custom supercomputers.","[' What type of computing relies on complete computers with onboard CPUs, storage, power supplies, network interfaces, etc. connected to a network by a conventional network interface producing commodity hardware?', ' What is the lower efficiency of designing and constructing a small number of custom supercomputers?']","['Distributed', 'grid” computing']"
2481,grid computing,Comparison of grids and conventional supercomputers,"There are also some differences in programming and MC. It can be costly and difficult to write programs that can run in the environment of a supercomputer, which may have a custom operating system, or require the program to address concurrency issues. If a problem can be adequately parallelized, a “thin” layer of “grid” infrastructure can allow conventional, standalone programs, given a different part of the same problem, to run on multiple machines. This makes it possible to write and debug on a single conventional machine and eliminates complications due to multiple instances of the same program running in the same shared memory and storage space at the same time.
","There are also some differences in programming and MC. It can be costly and difficult to write programs that can run in the environment of a supercomputer, which may have a custom operating system, or require the program to address concurrency issues.","[' What are some differences between programming and MC?', ' What can be costly and difficult to write programs that can run in the environment of a supercomputer?']","['It can be costly and difficult to write programs that can run in the environment of a supercomputer, which may have a custom operating system, or require the program to address concurrency issues', 'programming and MC']"
2482,grid computing,Design considerations and variations,"One feature of distributed grids is that they can be formed from computing resources belonging to one or more multiple individuals or organizations (known as multiple administrative domains). This can facilitate commercial transactions, as in utility computing, or make it easier to assemble volunteer computing networks.
","One feature of distributed grids is that they can be formed from computing resources belonging to one or more multiple individuals or organizations (known as multiple administrative domains). This can facilitate commercial transactions, as in utility computing, or make it easier to assemble volunteer computing networks.","[' Distributed grids can be formed from computing resources belonging to who?', ' What is one feature of distributed grids?', ' Multiple administrative domains are also known as what?']","['one or more multiple individuals or organizations', 'they can be formed from computing resources belonging to one or more multiple individuals or organizations', 'distributed grids is that they can be formed from computing resources belonging to one or more multiple individuals or organizations']"
2483,grid computing,Design considerations and variations,"One disadvantage of this feature is that the computers which are actually performing the calculations might not be entirely trustworthy. The designers of the system must thus introduce measures to prevent malfunctions or malicious participants from producing false, misleading, or erroneous results, and from using the system as an attack vector. This often involves assigning work randomly to different nodes (presumably with different owners) and checking that at least two different nodes report the same answer for a given work unit. Discrepancies would identify malfunctioning and malicious nodes. However, due to the lack of central control over the hardware, there is no way to guarantee that nodes will not drop out of the network at random times. Some nodes (like laptops or dial-up Internet customers) may also be available for computation but not network communications for unpredictable periods. These variations can be accommodated by assigning large work units (thus reducing the need for continuous network connectivity) and reassigning work units when a given node fails to report its results in expected time.
","One disadvantage of this feature is that the computers which are actually performing the calculations might not be entirely trustworthy. The designers of the system must thus introduce measures to prevent malfunctions or malicious participants from producing false, misleading, or erroneous results, and from using the system as an attack vector.","[' What might not be entirely trustworthy?', ' Who must introduce measures to prevent malfunctions or malicious participants from producing false, misleading, or erroneous results?', ' What does the system use as an attack vector?', ' What is one of the reasons for erroneous results?']","['the computers which are actually performing the calculations', 'The designers of the system', 'system', 'malfunctions or malicious participants']"
2484,grid computing,Design considerations and variations,"The impacts of trust and availability on performance and development difficulty can influence the choice of whether to deploy onto a dedicated cluster, to idle machines internal to the developing organization, or to an open external network of volunteers or contractors. In many cases, the participating nodes must trust the central system not to abuse the access that is being granted, by interfering with the operation of other programs, mangling stored information, transmitting private data, or creating new security holes. Other systems employ measures to reduce the amount of trust “client” nodes must place in the central system such as placing applications in virtual machines.
","The impacts of trust and availability on performance and development difficulty can influence the choice of whether to deploy onto a dedicated cluster, to idle machines internal to the developing organization, or to an open external network of volunteers or contractors. In many cases, the participating nodes must trust the central system not to abuse the access that is being granted, by interfering with the operation of other programs, mangling stored information, transmitting private data, or creating new security holes.","[' The impacts of trust and availability on performance and development difficulty can influence what?', ' What can influence the choice of whether to deploy onto a dedicated cluster or to idle machines internal to the developing organization?', ' Who must trust the central system not to abuse the access that is being granted?', ' What must the participating nodes trust not to do?']","['the choice of whether to deploy onto a dedicated cluster, to idle machines internal to the developing organization, or to an open external network of volunteers or contractors', 'The impacts of trust and availability on performance and development difficulty', 'the participating nodes', 'abuse the access that is being granted']"
2485,grid computing,Design considerations and variations,"Public systems or those crossing administrative domains (including different departments in the same organization) often result in the need to run on heterogeneous systems, using different operating systems and hardware architectures. With many languages, there is a trade-off between investment in software development and the number of platforms that can be supported (and thus the size of the resulting network). Cross-platform languages can reduce the need to make this tradeoff, though potentially at the expense of high performance on any given node (due to run-time interpretation or lack of optimization for the particular platform). Various middleware projects have created generic infrastructure to allow diverse scientific and commercial projects to harness a particular associated grid or for the purpose of setting up new grids. BOINC is a common one for various academic projects seeking public volunteers; more are listed at the end of the article.
","Public systems or those crossing administrative domains (including different departments in the same organization) often result in the need to run on heterogeneous systems, using different operating systems and hardware architectures. With many languages, there is a trade-off between investment in software development and the number of platforms that can be supported (and thus the size of the resulting network).","[' Public systems or those crossing administrative domains often result in the need to run on what?', ' What is the trade-off between investment in software development and the number of platforms that can be used?', ' How many platforms can be supported?', ' What is the size of the resulting network?']","['heterogeneous systems', 'With many languages', 'number', 'number of platforms that can be supported']"
2486,grid computing,Design considerations and variations,"In fact, the middleware can be seen as a layer between the hardware and the software. On top of the middleware, a number of technical areas have to be considered, and these may or may not be middleware independent. Example areas include SLA management, Trust, and Security, Virtual organization management, License Management, Portals and Data Management. These technical areas may be taken care of in a commercial solution, though the cutting edge of each area is often found within specific research projects examining the field.
","In fact, the middleware can be seen as a layer between the hardware and the software. On top of the middleware, a number of technical areas have to be considered, and these may or may not be middleware independent.","[' What can be seen as a layer between the hardware and the software?', ' What technical areas have to be considered on top of the middleware?']","['the middleware', 'a number of technical areas have to be considered, and these may or may not be middleware independent']"
2487,grid computing,CPU scavenging,"CPU-scavenging, cycle-scavenging, or shared computing creates a “grid” from the idle resources in a network of participants (whether worldwide or internal to an organization). Typically, this technique exploits the 'spare' instruction cycles resulting from the intermittent inactivity that typically occurs at night, during lunch breaks, or even during the (comparatively minuscule, though numerous) moments of idle waiting that modern desktop CPU's experience throughout the day (when the computer is waiting on IO from the user, network, or storage). In practice, participating computers also donate some supporting amount of disk storage space, RAM, and network bandwidth, in addition to raw CPU power.","CPU-scavenging, cycle-scavenging, or shared computing creates a “grid” from the idle resources in a network of participants (whether worldwide or internal to an organization). Typically, this technique exploits the 'spare' instruction cycles resulting from the intermittent inactivity that typically occurs at night, during lunch breaks, or even during the (comparatively minuscule, though numerous) moments of idle waiting that modern desktop CPU's experience throughout the day (when the computer is waiting on IO from the user, network, or storage).","[' What creates a “grid” from the idle resources in a network of participants?', "" What exploits the'spare' instruction cycles that typically occur during the intermittent inactivity?"", "" What do modern desktop CPU's experience throughout the day?"", ' What is the computer waiting on from the user, network, or storage?']","['CPU-scavenging, cycle-scavenging, or shared computing', 'CPU-scavenging', 'idle waiting', 'IO']"
2488,grid computing,CPU scavenging,"Many volunteer computing projects, such as BOINC, use the CPU scavenging model. Since nodes are likely to go ""offline"" from time to time, as their owners use their resources for their primary purpose, this model must be designed to handle such contingencies.
","Many volunteer computing projects, such as BOINC, use the CPU scavenging model. Since nodes are likely to go ""offline"" from time to time, as their owners use their resources for their primary purpose, this model must be designed to handle such contingencies.","[' What model is used by many volunteer computing projects?', ' What model must be designed to handle contingencies?']","['CPU scavenging model', 'CPU scavenging model']"
2489,grid computing,CPU scavenging,"Creating an Opportunistic Environment is another implementation of CPU-scavenging where special workload management system harvests the idle desktop computers for compute-intensive jobs, it also refers as Enterprise Desktop Grid (EDG). For instance, HTCondor the open-source high-throughput computing software framework for coarse-grained distributed rationalization of computationally intensive tasks can be configured to only use desktop machines where the keyboard and mouse are idle to effectively harness wasted CPU power from otherwise idle desktop workstations. Like other full-featured batch systems, HTCondor provides a job queueing mechanism, scheduling policy, priority scheme, resource monitoring, and resource management. It can be used to manage workload on a dedicated cluster of computers as well or it can seamlessly integrate both dedicated resources (rack-mounted clusters) and non-dedicated desktop machines (cycle scavenging) into one computing environment.
","Creating an Opportunistic Environment is another implementation of CPU-scavenging where special workload management system harvests the idle desktop computers for compute-intensive jobs, it also refers as Enterprise Desktop Grid (EDG). For instance, HTCondor the open-source high-throughput computing software framework for coarse-grained distributed rationalization of computationally intensive tasks can be configured to only use desktop machines where the keyboard and mouse are idle to effectively harness wasted CPU power from otherwise idle desktop workstations.","[' What does EDG stand for?', ' What is the name of the open-source high-throughput computing software framework?', ' How can coarse-grained distributed rationalization of computational intensive tasks be configured?', ' What can be configured to only use desktop machines where the keyboard and mouse are idle?']","['Enterprise Desktop Grid', 'HTCondor', 'to only use desktop machines where the keyboard and mouse are idle', 'HTCondor']"
2490,grid computing,History,"The term grid computing originated in the early 1990s as a metaphor for making computer power as easy to access as an electric power grid. The power grid metaphor for accessible computing quickly became canonical when Ian Foster and Carl Kesselman published their seminal work, ""The Grid: Blueprint for a new computing infrastructure"" (1999). This was preceded by decades by the metaphor of utility computing (1961): computing as a public utility, analogous to the phone system.","The term grid computing originated in the early 1990s as a metaphor for making computer power as easy to access as an electric power grid. The power grid metaphor for accessible computing quickly became canonical when Ian Foster and Carl Kesselman published their seminal work, ""The Grid: Blueprint for a new computing infrastructure"" (1999).","[' When did the term grid computing originate?', ' What was the power grid metaphor for accessible computing called?', ' Who wrote ""The Grid: Blueprint for a Grid""?', "" What was the title of Kesselman's seminal work?"", ' When was The Grid: Blueprint for a new computing infrastructure published?']","['early 1990s', 'grid computing', 'Ian Foster and Carl Kesselman', 'The Grid: Blueprint for a new computing infrastructure', '1999']"
2491,grid computing,History,"The ideas of the grid (including those from distributed computing, object-oriented programming, and Web services) were brought together by Ian Foster and Steve Tuecke of the University of Chicago, and Carl Kesselman of the University of Southern California's Information Sciences Institute.  The trio, who led the effort to create the Globus Toolkit, is widely regarded as the ""fathers of the grid"".  The toolkit incorporates not just computation management but also storage management, security provisioning, data movement, monitoring, and a toolkit for developing additional services based on the same infrastructure, including agreement negotiation, notification mechanisms, trigger services, and information aggregation. While the Globus Toolkit remains the de facto standard for building grid solutions, a number of other tools have been built that answer some subset of services needed to create an enterprise or global grid.","The ideas of the grid (including those from distributed computing, object-oriented programming, and Web services) were brought together by Ian Foster and Steve Tuecke of the University of Chicago, and Carl Kesselman of the University of Southern California's Information Sciences Institute. The trio, who led the effort to create the Globus Toolkit, is widely regarded as the ""fathers of the grid"".","[' Who brought together the grid ideas?', ' Who led the effort to create the grid?', ' Where did Ian Foster and Steve Tuecke come from?', ' Who led the effort to create the Globus Toolkit?', ' Who is widely regarded as the ""fathers of the grid?""']","[""Ian Foster and Steve Tuecke of the University of Chicago, and Carl Kesselman of the University of Southern California's Information Sciences Institute"", 'Ian Foster and Steve Tuecke', 'University of Chicago', 'Ian Foster and Steve Tuecke', 'Ian Foster and Steve Tuecke']"
2492,grid computing,History,"In 2007 the term cloud computing came into popularity, which is conceptually similar to the canonical Foster definition of grid computing (in terms of computing resources being consumed as electricity is from the power grid) and earlier utility computing. Indeed, grid computing is often (but not always) associated with the delivery of cloud computing systems as exemplified by the AppLogic system from 3tera.","In 2007 the term cloud computing came into popularity, which is conceptually similar to the canonical Foster definition of grid computing (in terms of computing resources being consumed as electricity is from the power grid) and earlier utility computing. Indeed, grid computing is often (but not always) associated with the delivery of cloud computing systems as exemplified by the AppLogic system from 3tera.","[' When did the term cloud computing come into popularity?', ' What is cloud computing conceptually similar to?', ' Grid computing is often associated with what?', ' What is often associated with the delivery of cloud computing systems?', ' What is exemplified by the AppLogic system from 3tera?']","['2007', 'the canonical Foster definition of grid computing', 'the delivery of cloud computing systems', 'grid computing', 'grid computing is often (but not always) associated with the delivery of cloud computing systems']"
2493,grid computing,Fastest virtual supercomputers,"Also, as of March 2019, the Bitcoin Network had a measured computing power equivalent to over 80,000 exaFLOPS (Floating-point Operations Per Second). This measurement reflects the number of FLOPS required to equal the hash output of the Bitcoin network rather than its capacity for general floating-point arithmetic operations, since the elements of the Bitcoin network (Bitcoin mining ASICs) perform only the specific cryptographic hash computation required by the Bitcoin protocol.
","Also, as of March 2019, the Bitcoin Network had a measured computing power equivalent to over 80,000 exaFLOPS (Floating-point Operations Per Second). This measurement reflects the number of FLOPS required to equal the hash output of the Bitcoin network rather than its capacity for general floating-point arithmetic operations, since the elements of the Bitcoin network (Bitcoin mining ASICs) perform only the specific cryptographic hash computation required by the Bitcoin protocol.","[' How much computing power did the Bitcoin Network have as of March 2019?', ' What is exaFLOPS?', "" The Bitcoin Network's computing power reflects the number of FLOPS required to equal what?"", ' What are the elements of the Bitcoin network called?', ' What do ASICs perform?']","['over 80,000 exaFLOPS', 'Floating-point Operations Per Second', 'the hash output', 'Bitcoin mining ASICs', 'cryptographic hash computation required by the Bitcoin protocol']"
2494,grid computing,Projects and applications,"Grid computing offers a way to solve Grand Challenge problems such as protein folding, financial modeling, earthquake simulation, and climate/weather modeling, and was integral in enabling the Large Hadron Collider at CERN. Grids offer a way of using the information technology resources optimally inside an organization. They also provide a means for offering information technology as a utility for commercial and noncommercial clients, with those clients paying only for what they use, as with electricity or water.
","Grid computing offers a way to solve Grand Challenge problems such as protein folding, financial modeling, earthquake simulation, and climate/weather modeling, and was integral in enabling the Large Hadron Collider at CERN. Grids offer a way of using the information technology resources optimally inside an organization.","[' What does grid computing offer a way to solve?', ' What was integral in enabling the Large Hadron Collider at CERN?']","['Grand Challenge problems', 'Grid computing']"
2495,grid computing,Projects and applications,"As of October 2016, over 4 million machines running the open-source Berkeley Open Infrastructure for Network Computing (BOINC) platform are members of the World Community Grid. One of the projects using BOINC is SETI@home, which was using more than 400,000 computers to achieve 0.828 TFLOPS as of October 2016. As of October 2016 Folding@home, which is not part of BOINC, achieved more than 101 x86-equivalent petaflops on over 110,000 machines.","As of October 2016, over 4 million machines running the open-source Berkeley Open Infrastructure for Network Computing (BOINC) platform are members of the World Community Grid. One of the projects using BOINC is SETI@home, which was using more than 400,000 computers to achieve 0.828 TFLOPS as of October 2016.","[' How many machines are members of the World Community Grid?', ' What is one of the projects using BOINC?', ' How many computers were used to achieve 0.828 TFLOPS?']","['over 4 million', 'SETI@home', 'more than 400,000']"
2496,grid computing,Projects and applications,"The European Union funded projects through the framework programmes of the European Commission. BEinGRID (Business Experiments in Grid) was a research project funded by the European Commission as an Integrated Project under the Sixth Framework Programme (FP6) sponsorship program. Started on June 1, 2006, the project ran 42 months, until November 2009. The project was coordinated by Atos Origin. According to the project fact sheet, their mission is “to establish effective routes to foster the adoption of grid computing across the EU and to stimulate research into innovative business models using Grid technologies”. To extract best practice and common themes from the experimental implementations, two groups of consultants are analyzing a series of pilots, one technical, one business. The project is significant not only for its long duration but also for its budget, which at 24.8 million Euros, is the largest of any FP6 integrated project. Of this, 15.7 million is provided by the European Commission and the remainder by its 98 contributing partner companies. Since the end of the project, the results of BEinGRID have been taken up and carried forward by IT-Tude.com.
",The European Union funded projects through the framework programmes of the European Commission. BEinGRID (Business Experiments in Grid) was a research project funded by the European Commission as an Integrated Project under the Sixth Framework Programme (FP6) sponsorship program.,"[' What is the acronym for Business Experiments in Grid?', ' Under what program was BEinGRID funded?', ' What was the name of the research project funded by the European Commission?']","['BEinGRID', 'Sixth Framework Programme (FP6)', 'BEinGRID']"
2497,grid computing,Projects and applications,"The Enabling Grids for E-sciencE project, based in the European Union and included sites in Asia and the United States, was a follow-up project to the European DataGrid (EDG) and evolved into the European Grid Infrastructure. This, along with the LHC Computing Grid (LCG), was developed to support experiments using the CERN Large Hadron Collider. A list of active sites participating within LCG can be found online as can real time monitoring of the EGEE infrastructure. The relevant software and documentation is also publicly accessible. There is speculation that dedicated fiber optic links, such as those installed by CERN to address the LCG's data-intensive needs, may one day be available to home users thereby providing internet services at speeds up to 10,000 times faster than a traditional broadband connection. The European Grid Infrastructure has been also used for other research activities and experiments such as the simulation of oncological clinical trials.","The Enabling Grids for E-sciencE project, based in the European Union and included sites in Asia and the United States, was a follow-up project to the European DataGrid (EDG) and evolved into the European Grid Infrastructure. This, along with the LHC Computing Grid (LCG), was developed to support experiments using the CERN Large Hadron Collider.","[' Where was the Enabling Grids for E-sciencE project based?', ' What project was a follow-up to the European DataGrid (EDG) and evolved into the European Grid Infrastructure?', ' What is LHC Computing Grid?', ' What was developed to support experiments using the CERN Large Hadron Collider?']","['European Union', 'Enabling Grids for E-sciencE', 'to support experiments using the CERN Large Hadron Collider', 'LHC Computing Grid']"
2498,grid computing,Projects and applications,"The distributed.net project was started in 1997.
The NASA Advanced Supercomputing facility (NAS) ran genetic algorithms using the Condor cycle scavenger running on about 350 Sun Microsystems and SGI workstations.
",The distributed.net project was started in 1997. The NASA Advanced Supercomputing facility (NAS) ran genetic algorithms using the Condor cycle scavenger running on about 350 Sun Microsystems and SGI workstations.,"[' When was the distributed.net project started?', ' Who ran genetic algorithms using the Condor cycle scavenger?', ' How many Sun Microsystems and SGI workstations did the NAS run?']","['1997', 'NASA Advanced Supercomputing facility', '350']"
2499,grid computing,Projects and applications,"In 2001, United Devices operated the United Devices Cancer Research Project based on its Grid MP product, which cycle-scavenges on volunteer PCs connected to the Internet. The project ran on about 3.1 million machines before its close in 2007.","In 2001, United Devices operated the United Devices Cancer Research Project based on its Grid MP product, which cycle-scavenges on volunteer PCs connected to the Internet. The project ran on about 3.1 million machines before its close in 2007.","[' What was the name of the cancer research project operated by United Devices in 2001?', ' How many machines did the Cancer Research Project run on before it closed in 2007?', ' What product was used to cycle-scavenge on PCs connected to the internet?']","['United Devices Cancer Research Project', '3.1 million', 'Grid MP']"
2500,dimensionality reduction,Summary,"Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality, and analyzing the data is usually computationally intractable (hard to control or deal with). Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as signal processing, speech recognition, neuroinformatics, and bioinformatics.","Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality, and analyzing the data is usually computationally intractable (hard to control or deal with).","[' What is the term for the transformation of data from a high-dimensional to a low-dimensional space?', ' What does dimensionality reduction do?', ' How can working in high dimensional spaces be undesirable?', ' What can be undesirable for many reasons?', ' What is often sparse as a result of the curse of dimensionality?', ' Analyzing the data is usually what?']","['Dimensionality reduction', 'transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data', 'for many reasons', 'Working in high-dimensional spaces', 'raw data', 'computationally intractable']"
2501,dimensionality reduction,Summary,"Methods are commonly divided into linear and nonlinear approaches. Approaches can also be divided into feature selection and feature extraction. Dimensionality reduction can be used for noise reduction, data visualization, cluster analysis, or as an intermediate step to facilitate other analyses.
",Methods are commonly divided into linear and nonlinear approaches. Approaches can also be divided into feature selection and feature extraction.,"[' What are methods commonly divided into?', ' What can also be divided into feature selection and feature extraction?']","['linear and nonlinear approaches', 'Approaches']"
2502,dimensionality reduction,Feature selection,"Feature selection approaches try to find a subset of the input variables (also called features or attributes). The three strategies are: the filter strategy (e.g. information gain), the wrapper strategy (e.g. search guided by accuracy), and the embedded strategy (selected features are added or removed while building the model based on prediction errors).
",Feature selection approaches try to find a subset of the input variables (also called features or attributes). The three strategies are: the filter strategy (e.g.,"[' What do feature selection approaches try to find?', ' What are features also called?', ' How many strategies are there?']","['a subset of the input variables', 'attributes', 'three']"
2503,dimensionality reduction,Feature projection,"Feature projection (also called feature extraction) transforms the data from the high-dimensional space to a space of fewer dimensions. The data transformation may be linear, as in principal component analysis (PCA), but many nonlinear dimensionality reduction techniques also exist. For multidimensional data, tensor representation can be used in dimensionality reduction through multilinear subspace learning.","Feature projection (also called feature extraction) transforms the data from the high-dimensional space to a space of fewer dimensions. The data transformation may be linear, as in principal component analysis (PCA), but many nonlinear dimensionality reduction techniques also exist.","[' What is feature projection also called?', ' What transforms the data from high-dimensional space to a space of fewer dimensions?']","['feature extraction', 'Feature projection']"
2504,dimensionality reduction,Dimension reduction,"For high-dimensional datasets (i.e. with number of dimensions more than 10), dimension reduction is usually performed prior to applying a K-nearest neighbors algorithm (k-NN) in order to avoid the effects of the curse of dimensionality.","For high-dimensional datasets (i.e. with number of dimensions more than 10), dimension reduction is usually performed prior to applying a K-nearest neighbors algorithm (k-NN) in order to avoid the effects of the curse of dimensionality.","[' What is the term for a dataset with more than 10 dimensions?', ' Why is dimension reduction performed before applying a K-nearest neighbors algorithm?']","['high-dimensional', 'to avoid the effects of the curse of dimensionality']"
2505,dimensionality reduction,Dimension reduction,"Feature extraction and dimension reduction can be combined in one step using principal component analysis (PCA), linear discriminant analysis (LDA), canonical correlation analysis (CCA), or non-negative matrix factorization (NMF) techniques as a pre-processing step followed by clustering by K-NN on feature vectors in reduced-dimension space. In machine learning this process is also called low-dimensional embedding.","Feature extraction and dimension reduction can be combined in one step using principal component analysis (PCA), linear discriminant analysis (LDA), canonical correlation analysis (CCA), or non-negative matrix factorization (NMF) techniques as a pre-processing step followed by clustering by K-NN on feature vectors in reduced-dimension space. In machine learning this process is also called low-dimensional embedding.","[' How can feature extraction and dimension reduction be combined in one step?', ' What can be used as a pre-processing step followed by clustering by K-NN?', ' In machine learning, what is low-dimensional embedding?']","['using principal component analysis (PCA), linear discriminant analysis (LDA), canonical correlation analysis (CCA), or non-negative matrix factorization (NMF) techniques', 'non-negative matrix factorization (NMF) techniques', 'clustering by K-NN on feature vectors in reduced-dimension space']"
2506,dimensionality reduction,Dimension reduction,"For very-high-dimensional datasets (e.g. when performing similarity search on live video streams, DNA data or high-dimensional time series) running a fast approximate K-NN search using locality-sensitive hashing, random projection, ""sketches"", or other high-dimensional similarity search techniques from the VLDB conference toolbox might be the only feasible option.
","For very-high-dimensional datasets (e.g. when performing similarity search on live video streams, DNA data or high-dimensional time series) running a fast approximate K-NN search using locality-sensitive hashing, random projection, ""sketches"", or other high-dimensional similarity search techniques from the VLDB conference toolbox might be the only feasible option.","[' What might be the only feasible option for very-high-dimensional datasets?', ' What is a fast approximate K-NN search?']","['running a fast approximate K-NN search using locality-sensitive hashing, random projection, ""sketches"", or other high-dimensional similarity search techniques from the VLDB conference toolbox', 'locality-sensitive hashing, random projection, ""sketches"", or other high-dimensional similarity search techniques from the VLDB conference toolbox']"
2507,constraint programming,Summary,"Constraint programming (CP) is a paradigm for solving combinatorial problems that draws on a wide range of techniques from artificial intelligence, computer science, and operations research. In constraint programming, users declaratively state the constraints on the feasible solutions for a set of decision variables. Constraints differ from the common primitives of imperative programming languages in that they do not specify a step or sequence of steps to execute, but rather the properties of a solution to be found. In addition to constraints, users also need to specify a method to solve these constraints. This typically draws upon standard methods like chronological backtracking and constraint propagation, but may use customized code like a problem specific branching heuristic.
","Constraint programming (CP) is a paradigm for solving combinatorial problems that draws on a wide range of techniques from artificial intelligence, computer science, and operations research. In constraint programming, users declaratively state the constraints on the feasible solutions for a set of decision variables.","[' What is a paradigm for solving combinatorial problems?', ' What draws on a wide range of techniques from artificial intelligence, computer science, and operations research?', ' In constraint programming, users state the constraints on what?']","['Constraint programming', 'Constraint programming', 'the feasible solutions for a set of decision variables']"
2508,constraint programming,Summary,"Constraint programming takes its root from and can be expressed in the form of constraint logic programming, which embeds constraints into a logic program. This variant of logic programming is due to Jaffar and Lassez, who extended in 1987 a specific class of constraints that were introduced in Prolog II. The first implementations of constraint logic programming were Prolog III, CLP(R), and CHIP.
","Constraint programming takes its root from and can be expressed in the form of constraint logic programming, which embeds constraints into a logic program. This variant of logic programming is due to Jaffar and Lassez, who extended in 1987 a specific class of constraints that were introduced in Prolog II.","[' What does constraint logic programming embed into a logic program?', ' Who extended a specific class of constraints that were introduced in Prolog II?']","['constraints', 'Jaffar and Lassez']"
2509,constraint programming,Summary,"Instead of logic programming, constraints can be mixed with functional programming, term rewriting, and imperative languages.
Programming languages with built-in support for constraints include Oz (functional programming) and Kaleidoscope (imperative programming). Mostly, constraints are implemented in imperative languages via constraint solving toolkits, which are separate libraries for an existing imperative language.
","Instead of logic programming, constraints can be mixed with functional programming, term rewriting, and imperative languages. Programming languages with built-in support for constraints include Oz (functional programming) and Kaleidoscope (imperative programming).","[' Instead of logic programming, what can constraints be mixed with?', ' What are two programming languages with built-in support for constraints?']","['functional programming, term rewriting, and imperative languages', 'Oz (functional programming) and Kaleidoscope (imperative programming).']"
2510,constraint programming,Constraint logic programming,"Constraint programming is an embedding of constraints in a host language. The first host languages used were logic programming languages, so the field was initially called constraint logic programming. The two paradigms share many important features, like logical variables and backtracking. Today most Prolog implementations include one or more libraries for constraint logic programming.
","Constraint programming is an embedding of constraints in a host language. The first host languages used were logic programming languages, so the field was initially called constraint logic programming.","[' What is an embedding of constraints in a host language called?', ' What were the first host languages used?', ' The field was initially called what?']","['Constraint programming', 'logic programming languages', 'constraint logic programming']"
2511,constraint programming,Constraint logic programming,"The difference between the two is largely in their styles and approaches to modeling the world. Some problems are more natural (and thus, simpler) to write as logic programs, while some are more natural to write as constraint programs.
","The difference between the two is largely in their styles and approaches to modeling the world. Some problems are more natural (and thus, simpler) to write as logic programs, while some are more natural to write as constraint programs.","[' What is the main difference between logic and constraint programs?', ' What are some problems more natural to write as?']","['their styles and approaches to modeling the world', 'logic programs']"
2512,constraint programming,Constraint logic programming,"The constraint programming approach is to search for a state of the world in which a large number of constraints are satisfied at the same time. A problem is typically stated as a state of the world containing a number of unknown variables. The constraint program searches for values for all the variables.
",The constraint programming approach is to search for a state of the world in which a large number of constraints are satisfied at the same time. A problem is typically stated as a state of the world containing a number of unknown variables.,"[' The constraint programming approach is to search for a state of the world in which a large number of constraints are satisfied at the same time?', ' A problem is typically stated as what?']","['constraint programming approach is to search for a state of the world in which a large number of constraints are satisfied at the same time.', 'a state of the world containing a number of unknown variables']"
2513,constraint programming,Constraint satisfaction problem,"Assignment is the association of a variable to a value from its domain. A partial assignment is when a subset of the variables of the problem have been assigned. A total assignment is when all the variables of the problem have been assigned.
",Assignment is the association of a variable to a value from its domain. A partial assignment is when a subset of the variables of the problem have been assigned.,"[' What is the association of a variable to a value from its domain?', ' A partial assignment is when a subset of the variables of the problem have been assigned?']","['Assignment', 'Assignment']"
2514,constraint programming,Perturbation vs refinement models,"The refinement model is more general, as it does not restrict variables to have a single value, it can lead to several solutions to the same problem. However, the perturbation model is more intuitive for programmers using mixed imperative constraint object-oriented languages.","The refinement model is more general, as it does not restrict variables to have a single value, it can lead to several solutions to the same problem. However, the perturbation model is more intuitive for programmers using mixed imperative constraint object-oriented languages.","[' What model is more general?', ' What model can lead to several solutions to the same problem?', ' For programmers using mixed imperative constraint object-oriented languages, what model is intuitive?']","['refinement model', 'refinement model', 'perturbation model']"
2515,constraint programming,Domains,"The constraints used in constraint programming are typically over some specific domains. Some popular domains for constraint programming are:
",The constraints used in constraint programming are typically over some specific domains. Some popular domains for constraint programming are:,"[' What are the constraints used in constraint programming typically over?', ' What are some popular domains for constraints?']","['some specific domains', 'domains']"
2516,constraint programming,Domains,"Finite domains is one of the most successful domains of constraint programming. In some areas (like operations research) constraint programming is often identified with constraint programming over finite domains.
",Finite domains is one of the most successful domains of constraint programming. In some areas (like operations research) constraint programming is often identified with constraint programming over finite domains.,"[' What is one of the most successful domains of constraint programming?', ' In some areas, what type of programming is often identified with constraints programming over finite domains?']","['Finite domains', 'constraint programming']"
2517,constraint programming,Constraint propagation,"Local consistency conditions are properties of constraint satisfaction problems related to the consistency of subsets of variables or constraints. They can be used to reduce the search space and make the problem easier to solve. Various kinds of local consistency conditions are leveraged, including node consistency, arc consistency, and path consistency.
",Local consistency conditions are properties of constraint satisfaction problems related to the consistency of subsets of variables or constraints. They can be used to reduce the search space and make the problem easier to solve.,"[' What are local consistency conditions?', ' Local consistency conditions are properties of what?']","['properties of constraint satisfaction problems related to the consistency of subsets of variables or constraints', 'constraint satisfaction problems']"
2518,constraint programming,Constraint propagation,"Every local consistency condition can be enforced by a transformation that changes the problem without changing its solutions. Such a transformation is called constraint propagation. Constraint propagation works by reducing domains of variables, strengthening constraints, or creating new ones. This leads to a reduction of the search space, making the problem easier to solve by some algorithms. Constraint propagation can also be used as an unsatisfiability checker, incomplete in general but complete in some particular cases.
",Every local consistency condition can be enforced by a transformation that changes the problem without changing its solutions. Such a transformation is called constraint propagation.,"[' What can be enforced by a transformation that changes the problem without changing its solutions?', ' What is the transformation called?']","['Every local consistency condition', 'constraint propagation']"
2519,constraint programming,Example,"The syntax for expressing constraints over finite domains depends on the host language. The following is a Prolog program that solves the classical alphametic puzzle SEND+MORE=MONEY in constraint logic programming:
",The syntax for expressing constraints over finite domains depends on the host language. The following is a Prolog program that solves the classical alphametic puzzle SEND+MORE=MONEY in constraint logic programming:,"[' The syntax for expressing constraints over finite domains depends on what?', ' What solves the classical alphametic puzzle SEND+MORE=MONEY in constraint logic programming?']","['the host language', 'Prolog program']"
2520,constraint programming,Example,"The interpreter creates a variable for each letter in the puzzle. The operator ins is used to specify the domains of these variables, so that they range over the set of values {0,1,2,3, ..., 9}. The constraints S#\=0 and M#\=0 means that these two variables cannot take the value zero. When the interpreter evaluates these constraints, it reduces the domains of these two variables by removing the value 0 from them. Then, the constraint all_different(Digits) is considered; it does not reduce any domain, so it is simply stored. The last constraint specifies that the digits assigned to the letters must be such that ""SEND+MORE=MONEY"" holds when each letter is replaced by its corresponding digit. From this constraint, the solver infers that M=1. All stored constraints involving variable M are awakened: in this case, constraint propagation on the all_different constraint removes value 1 from the domain of all the remaining variables. Constraint propagation may solve the problem by reducing all domains to a single value, it may prove that the problem has no solution by reducing a domain to the empty set, but may also terminate without proving satisfiability or unsatisfiability. The label literals are used to actually perform search for a solution.
","The interpreter creates a variable for each letter in the puzzle. The operator ins is used to specify the domains of these variables, so that they range over the set of values {0,1,2,3, ..., 9}.","[' What does the interpreter create for each letter in the puzzle?', ' What is used to specify the domains of the variables?']","['a variable', 'The operator ins']"
2521,optical flow,Summary,"Optical flow or optic flow is the pattern of apparent motion of objects, surfaces, and edges in a visual scene caused by the relative motion between an observer and a scene. Optical flow can also be defined as the distribution of apparent velocities of movement of brightness pattern in an image. The concept of optical flow was introduced by the American psychologist James J. Gibson in the 1940s to describe the visual stimulus provided to animals moving through the world. Gibson stressed the importance of optic flow for affordance perception, the ability to discern possibilities for action within the environment.  Followers of Gibson and his ecological approach to psychology have further demonstrated the role of the optical flow stimulus for the perception of movement by the observer in the world; perception of the shape, distance and movement of objects in the world; and the control of locomotion.","Optical flow or optic flow is the pattern of apparent motion of objects, surfaces, and edges in a visual scene caused by the relative motion between an observer and a scene. Optical flow can also be defined as the distribution of apparent velocities of movement of brightness pattern in an image.","[' What is the pattern of apparent motion of objects, surfaces, and edges in a visual scene caused by the relative motion between an observer and a scene?', ' What can also be defined as the distribution of apparent velocities of movement of brightness pattern in an image?', ' What are the apparent velocities of movement of a brightness pattern in an image?']","['Optical flow', 'Optical flow', 'Optical flow']"
2522,optical flow,Estimation,"Sequences of ordered images allow the estimation of motion as either instantaneous image velocities or discrete image displacements. Fleet and Weiss provide a tutorial introduction to gradient based optical flow.
John L. Barron, David J. Fleet, and Steven Beauchemin provide a performance analysis of a number of optical flow techniques. It emphasizes the accuracy and density of measurements.",Sequences of ordered images allow the estimation of motion as either instantaneous image velocities or discrete image displacements. Fleet and Weiss provide a tutorial introduction to gradient based optical flow.,"[' What allow the estimation of motion as instantaneous image velocities or discrete image displacements?', ' Fleet and Weiss provide a tutorial introduction to what?']","['Sequences of ordered images', 'gradient based optical flow']"
2523,optical flow,Estimation,"The optical flow methods try to calculate the motion between two image frames which are taken at times 



t


{\displaystyle t}
 and 



t
+
Δ
t


{\displaystyle t+\Delta t}
 at every voxel position. These methods are called differential since they are based on local Taylor series approximations of the image signal; that is, they use partial derivatives with respect to the spatial and temporal coordinates.
","The optical flow methods try to calculate the motion between two image frames which are taken at times 



t


{\displaystyle t}
 and 



t
+
Δ
t


{\displaystyle t+\Delta t}
 at every voxel position. These methods are called differential since they are based on local Taylor series approximations of the image signal; that is, they use partial derivatives with respect to the spatial and temporal coordinates.","[' What do optical flow methods try to calculate between two image frames?', ' What are these methods called because they are based on local Taylor series approximations?', ' What do series approximations of the image signal use with respect to the spatial and temporal coordinates?']","['the motion', 'differential', 'partial derivatives']"
2524,optical flow,Estimation,"where 




V

x


,

V

y




{\displaystyle V_{x},V_{y}}
 are the 



x


{\displaystyle x}
 and 



y


{\displaystyle y}
 components of the velocity or optical flow of 



I
(
x
,
y
,
t
)


{\displaystyle I(x,y,t)}
 and 







∂
I


∂
x






{\displaystyle {\tfrac {\partial I}{\partial x}}}
, 







∂
I


∂
y






{\displaystyle {\tfrac {\partial I}{\partial y}}}
 and 







∂
I


∂
t






{\displaystyle {\tfrac {\partial I}{\partial t}}}
 are the derivatives of the image at 



(
x
,
y
,
t
)


{\displaystyle (x,y,t)}
 in the corresponding directions. 




I

x




{\displaystyle I_{x}}
,




I

y




{\displaystyle I_{y}}
 and 




I

t




{\displaystyle I_{t}}
 can be written for the derivatives in the following.
","where 




V

x


,

V

y




{\displaystyle V_{x},V_{y}}
 are the 



x


{\displaystyle x}
 and 



y


{\displaystyle y}
 components of the velocity or optical flow of 



I
(
x
,
y
,
t
)


{\displaystyle I(x,y,t)}
 and 







∂
I


∂
x






{\displaystyle {\tfrac {\partial I}{\partial x}}}
, 







∂
I


∂
y






{\displaystyle {\tfrac {\partial I}{\partial y}}}
 and 







∂
I


∂
t






{\displaystyle {\tfrac {\partial I}{\partial t}}}
 are the derivatives of the image at 



(
x
,
y
,
t
)


{\displaystyle (x,y,t)}
 in the corresponding directions. I

x




{\displaystyle I_{x}}
,




I

y




{\displaystyle I_{y}}
 and 




I

t




{\displaystyle I_{t}}
 can be written for the derivatives in the following.","[' What are the components of the velocity or optical flow of I(x,y,t)?', ' Where are the derivatives of the image at (x,y, t ) <unk>displaystyle (x,yes ) in the corresponding directions?', ' What can be written for the derivatives in the following?']","['x\n\n\n{\\displaystyle x}\n and \n\n\n\ny', 'V_{x},V_{y}}\n are the \n\n\n\nx\n\n\n{\\displaystyle x}\n and \n\n\n\ny\n\n\n{\\displaystyle y}\n components of the velocity or optical flow of \n\n\n\nI\n(\nx\n,\ny\n,\nt\n)\n\n\n{\\displaystyle I(x,y,t)}\n and \n\n\n\n\n\n\n\n∂\nI\n\n\n∂\nx\n\n\n\n\n\n\n{\\displaystyle {\\tfrac {\\partial I}{\\partial x}}}\n, \n\n\n\n\n\n\n\n∂\nI\n\n\n∂\ny\n\n\n\n\n\n\n{\\displaystyle {\\tfrac {\\partial I}{\\partial y}}}\n and \n\n\n\n\n\n\n\n∂\nI\n\n\n∂\nt', 'x\n,\ny\n,\nt\n)\n\n\n{\\displaystyle (x,y,t)}\n in the corresponding directions. I\n\nx\n\n\n\n\n{\\displaystyle I_{x}}\n,\n\n\n\n\nI\n\ny\n\n\n\n\n{\\displaystyle I_{y}}\n and \n\n\n\n\nI\n\nt']"
2525,optical flow,Estimation,"This is an equation in two unknowns and cannot be solved as such. This is known as the aperture problem of the optical flow algorithms. To find the optical flow another set of equations is needed, given by some additional constraint. All optical flow methods introduce additional conditions for estimating the actual flow.
",This is an equation in two unknowns and cannot be solved as such. This is known as the aperture problem of the optical flow algorithms.,[' What is the name of the problem that cannot be solved as such?'],['aperture problem of the optical flow algorithms']
2526,optical flow,Uses,"Motion estimation and video compression have developed as a major aspect of optical flow research. While the optical flow field is superficially similar to a dense motion field derived from the techniques of motion estimation, optical flow is the study of not only the determination of the optical flow field itself, but also of its use in estimating the three-dimensional nature and structure of the scene, as well as the 3D motion of objects and the observer relative to the scene, most of them using the image Jacobian.","Motion estimation and video compression have developed as a major aspect of optical flow research. While the optical flow field is superficially similar to a dense motion field derived from the techniques of motion estimation, optical flow is the study of not only the determination of the optical flow field itself, but also of its use in estimating the three-dimensional nature and structure of the scene, as well as the 3D motion of objects and the observer relative to the scene, most of them using the image Jacobian.","[' Motion estimation and video compression have developed as a major aspect of what?', ' What is superficially similar to a dense motion field derived from the techniques of motion estimation?', ' What is used to estimate the three-dimensional nature and structure of a scene?', ' What image is most often used to determine the 3D motion of objects and the observer relative to the scene?<extra_id_51>']","['optical flow research', 'the optical flow field', 'optical flow', 'Jacobian']"
2527,optical flow,Uses,"Optical flow was used by robotics researchers in many areas such as: object detection and tracking, image dominant plane extraction, movement detection, robot navigation and visual odometry. Optical flow information has been recognized as being useful for controlling micro air vehicles.","Optical flow was used by robotics researchers in many areas such as: object detection and tracking, image dominant plane extraction, movement detection, robot navigation and visual odometry. Optical flow information has been recognized as being useful for controlling micro air vehicles.","[' What was used by robotics researchers in many areas?', ' What has been recognized as being useful for controlling micro air vehicles?']","['Optical flow', 'Optical flow information']"
2528,optical flow,Uses,"The application of optical flow includes the problem of inferring not only the motion of the observer and objects in the scene, but also the structure of objects and the environment. Since awareness of motion and the generation of mental maps of the structure of our environment are critical components of animal (and human) vision, the conversion of this innate ability to a computer capability is similarly crucial in the field of machine vision.","The application of optical flow includes the problem of inferring not only the motion of the observer and objects in the scene, but also the structure of objects and the environment. Since awareness of motion and the generation of mental maps of the structure of our environment are critical components of animal (and human) vision, the conversion of this innate ability to a computer capability is similarly crucial in the field of machine vision.","[' The application of optical flow includes the problem of inferring not only the motion of the observer and objects in the scene, but also the structure of objects and what else?', ' Awareness of motion and the generation of what are critical components of our environment?', ' What are critical components of animal and human vision?', ' What is the conversion of this innate ability to a computer capability crucial in the field of?']","['the environment', 'mental maps', 'awareness of motion and the generation of mental maps of the structure of our environment', 'machine vision']"
2529,optical flow,Uses,"Consider a five-frame clip of a ball moving from the bottom left of a field of vision, to the top right. Motion estimation techniques can determine that on a two dimensional plane the ball is moving up and to the right and vectors describing this motion can be extracted from the sequence of frames. For the purposes of video compression (e.g., MPEG), the sequence is now described as well as it needs to be. However, in the field of machine vision, the question of whether the ball is moving to the right or if the observer is moving to the left is unknowable yet critical information. Not even if a static, patterned background were present in the five frames, could we confidently state that the ball was moving to the right, because the pattern might have an infinite distance to the observer.
","Consider a five-frame clip of a ball moving from the bottom left of a field of vision, to the top right. Motion estimation techniques can determine that on a two dimensional plane the ball is moving up and to the right and vectors describing this motion can be extracted from the sequence of frames.","[' How many frames does a ball move from the bottom left of a field of vision to the top right?', ' Motion estimation techniques can determine that on a two dimensional plane the ball is moving up and to what?', ' What can be extracted from the sequence of frames?']","['five', 'the right', 'vectors describing this motion']"
2530,optical flow,"Optical flow sensor<span id=""flow""></span>","An optical flow sensor is a vision sensor capable of measuring optical flow or visual motion and outputting a measurement based on optical flow. Various configurations of optical flow sensors exist. One configuration is an image sensor chip connected to a processor programmed to run an optical flow algorithm. Another configuration uses a vision chip, which is an integrated circuit having both the image sensor and the processor on the same die, allowing for a compact implementation. An example of this is a generic optical mouse sensor used in an optical mouse. In some cases the processing circuitry may be implemented using analog or mixed-signal circuits to enable fast optical flow computation using minimal current consumption.
",An optical flow sensor is a vision sensor capable of measuring optical flow or visual motion and outputting a measurement based on optical flow. Various configurations of optical flow sensors exist.,"[' What is a vision sensor capable of measuring?', ' What is an optical flow sensor able to output?']","['optical flow or visual motion', 'a measurement based on optical flow']"
2531,optical flow,"Optical flow sensor<span id=""flow""></span>","One area of contemporary research is the use of neuromorphic engineering techniques to implement circuits that respond to optical flow, and thus may be appropriate for use in an optical flow sensor. Such circuits may draw inspiration from biological neural circuitry that similarly responds to optical flow.
","One area of contemporary research is the use of neuromorphic engineering techniques to implement circuits that respond to optical flow, and thus may be appropriate for use in an optical flow sensor. Such circuits may draw inspiration from biological neural circuitry that similarly responds to optical flow.","[' What do neuromorphic engineering techniques respond to?', ' What may be appropriate for use in an optical flow sensor?']","['optical flow', 'neuromorphic engineering techniques']"
2532,optical flow,"Optical flow sensor<span id=""flow""></span>","Optical flow sensors are also being used in robotics applications, primarily where there is a need to measure visual motion or relative motion between the robot and other objects in the vicinity of the robot. The use of optical flow sensors in  unmanned aerial vehicles (UAVs), for stability and obstacle avoidance, is also an area of current research.","Optical flow sensors are also being used in robotics applications, primarily where there is a need to measure visual motion or relative motion between the robot and other objects in the vicinity of the robot. The use of optical flow sensors in  unmanned aerial vehicles (UAVs), for stability and obstacle avoidance, is also an area of current research.","[' What type of sensors are being used in robotics applications?', ' What is the purpose of optical flow sensors in UAVs?', ' What does UAV stand for?', ' What do UAVs use to avoid obstacles?']","['Optical flow sensors', 'stability and obstacle avoidance', 'unmanned aerial vehicles', 'optical flow sensors']"
2533,eeg,Summary,"Electroencephalography (EEG) is a method to record an electrogram of the electrical activity on the scalp that has been shown to represent the macroscopic activity of the surface layer of the brain underneath. It is typically non-invasive, with the electrodes placed along the scalp. Electrocorticography, involving invasive electrodes, is sometimes called ""intracranial EEG"".
","Electroencephalography (EEG) is a method to record an electrogram of the electrical activity on the scalp that has been shown to represent the macroscopic activity of the surface layer of the brain underneath. It is typically non-invasive, with the electrodes placed along the scalp.","[' What is EEG?', ' What does EEG represent?', ' Where are the electrodes placed?']","['Electroencephalography', 'the macroscopic activity of the surface layer of the brain underneath', 'along the scalp']"
2534,eeg,Summary,"EEG measures voltage fluctuations resulting from ionic current within the neurons of the brain. Clinically, EEG refers to the recording of the brain's spontaneous electrical activity over a period of time, as recorded from multiple electrodes placed on the scalp. Diagnostic applications generally focus either on event-related potentials or on the spectral content of EEG. The former investigates potential fluctuations time locked to an event, such as 'stimulus onset' or 'button press'. The latter analyses the type of neural oscillations (popularly called ""brain waves"") that can be observed in EEG signals in the frequency domain.
","EEG measures voltage fluctuations resulting from ionic current within the neurons of the brain. Clinically, EEG refers to the recording of the brain's spontaneous electrical activity over a period of time, as recorded from multiple electrodes placed on the scalp.","[' What measure voltage fluctuations resulting from ionic current within the neurons of the brain?', ' What does EEG refer to in clinical terms?', ' Where are multiple electrodes placed?']","['EEG', ""the recording of the brain's spontaneous electrical activity over a period of time"", 'on the scalp']"
2535,eeg,Summary,"EEG is most often used to diagnose epilepsy, which causes abnormalities in EEG readings. It is also used to diagnose sleep disorders, depth of anesthesia, coma, encephalopathies, and brain death. EEG used to be a first-line method of diagnosis for tumors, stroke and other focal brain disorders, but this use has decreased with the advent of high-resolution anatomical imaging techniques such as magnetic resonance imaging (MRI) and computed tomography (CT). Despite limited spatial resolution, EEG continues to be a valuable tool for research and diagnosis. It is one of the few mobile techniques available and offers millisecond-range temporal resolution which is not possible with CT, PET or MRI.
","EEG is most often used to diagnose epilepsy, which causes abnormalities in EEG readings. It is also used to diagnose sleep disorders, depth of anesthesia, coma, encephalopathies, and brain death.","[' What is most often used to diagnose epilepsy?', ' What causes abnormalities in EEG readings?', ' Other than sleep disorders, depth of anesthesia, coma, encephalopathies, and brain death, what else is EEG used for?']","['EEG', 'epilepsy', 'diagnose epilepsy']"
2536,eeg,Summary,"Derivatives of the EEG technique include evoked potentials (EP), which involves averaging the EEG activity time-locked to the presentation of a stimulus of some sort (visual, somatosensory, or auditory). Event-related potentials (ERPs) refer to averaged EEG responses that are time-locked to more complex processing of stimuli; this technique is used in cognitive science, cognitive psychology, and psychophysiological research.
","Derivatives of the EEG technique include evoked potentials (EP), which involves averaging the EEG activity time-locked to the presentation of a stimulus of some sort (visual, somatosensory, or auditory). Event-related potentials (ERPs) refer to averaged EEG responses that are time-locked to more complex processing of stimuli; this technique is used in cognitive science, cognitive psychology, and psychophysiological research.","[' What is the acronym for evoked potentials?', ' What does EEG stand for?', ' How are EEG responses time-locked to more complex processing?', ' How is this technique used in cognitive science, cognitive psychology, and psychophysiological research?']","['EP', 'evoked potentials', 'Event-related potentials', 'Event-related potentials']"
2537,eeg,History,"In 1875, Richard Caton (1842–1926), a physician practicing in Liverpool, presented his findings about electrical phenomena of the exposed cerebral hemispheres of rabbits and monkeys in the British Medical Journal.  In 1890, Polish physiologist Adolf Beck published an investigation of spontaneous electrical activity of the brain of rabbits and dogs that included rhythmic oscillations altered by light. Beck started experiments on the electrical brain activity of animals. Beck placed electrodes directly on the surface of the brain to test for sensory stimulation. His observation of fluctuating brain activity led to the conclusion of brain waves.","In 1875, Richard Caton (1842–1926), a physician practicing in Liverpool, presented his findings about electrical phenomena of the exposed cerebral hemispheres of rabbits and monkeys in the British Medical Journal. In 1890, Polish physiologist Adolf Beck published an investigation of spontaneous electrical activity of the brain of rabbits and dogs that included rhythmic oscillations altered by light.","[' In what year did Richard Caton publish his findings about electrical phenomena of the exposed cerebral hemispheres of rabbits and monkeys?', ' Which physiologist published an investigation of spontaneous electrical activity of the brain of rabbit and dogs in 1890?', ' What is the name of the animal that has spontaneous electrical activity of the brain?', ' What are rhythmic oscillations altered by?']","['1875', 'Adolf Beck', 'rabbits and dogs', 'light']"
2538,eeg,History,"In 1912, Ukrainian physiologist Vladimir Vladimirovich Pravdich-Neminsky published the first animal EEG and the evoked potential of the mammalian (dog).  In 1914, Napoleon Cybulski and Jelenska-Macieszyna photographed EEG recordings of experimentally induced seizures.
","In 1912, Ukrainian physiologist Vladimir Vladimirovich Pravdich-Neminsky published the first animal EEG and the evoked potential of the mammalian (dog). In 1914, Napoleon Cybulski and Jelenska-Macieszyna photographed EEG recordings of experimentally induced seizures.","[' Who published the first animal EEG and the evoked potential of the mammalian?', ' Who photographed EEG recordings of experimentally induced seizures?']","['Vladimir Vladimirovich Pravdich-Neminsky', 'Napoleon Cybulski and Jelenska-Macieszyna']"
2539,eeg,History,"German physiologist and psychiatrist Hans Berger (1873–1941) recorded the first human EEG in 1924. Expanding on work previously conducted on animals by Richard Caton and others, Berger also invented the electroencephalogram (giving the device its name), an invention described ""as one of the most surprising, remarkable, and momentous developments in the history of clinical neurology"". His discoveries were first confirmed by British scientists Edgar Douglas Adrian and B. H. C. Matthews in 1934 and developed by them.
","German physiologist and psychiatrist Hans Berger (1873–1941) recorded the first human EEG in 1924. Expanding on work previously conducted on animals by Richard Caton and others, Berger also invented the electroencephalogram (giving the device its name), an invention described ""as one of the most surprising, remarkable, and momentous developments in the history of clinical neurology"".","[' Who recorded the first human EEG?', ' When was the first EEG recorded?', ' Who invented the electroencephalogram?', "" What was the name of Berger's invention?"", ' What is one of the most surprising, remarkable, and momentous developments in the history of clinical neurology?']","['Hans Berger', '1924', 'Hans Berger', 'electroencephalogram', 'electroencephalogram']"
2540,eeg,History,"In 1934, Fisher and Lowenbach first demonstrated epileptiform spikes. In 1935, Gibbs, Davis and Lennox described interictal spike waves and the three cycles/s pattern of clinical absence seizures, which began the field of clinical electroencephalography. Subsequently, in 1936 Gibbs and Jasper reported the interictal spike as the focal signature of epilepsy. The same year, the first EEG laboratory opened at Massachusetts General Hospital.
","In 1934, Fisher and Lowenbach first demonstrated epileptiform spikes. In 1935, Gibbs, Davis and Lennox described interictal spike waves and the three cycles/s pattern of clinical absence seizures, which began the field of clinical electroencephalography.","[' When did Fisher and Lowenbach first demonstrate epileptiform spikes?', ' When did Gibbs, Davis and Lennox describe interictal spike waves and the three cycles/s pattern of clinical absence seizures?', ' What began the field of clinical electroencephalography?']","['1934', '1935', 'clinical absence seizures']"
2541,eeg,History,"In 1947, The American EEG Society was founded and the first International EEG congress was held. In 1953 Aserinsky and Kleitman described REM sleep.
","In 1947, The American EEG Society was founded and the first International EEG congress was held. In 1953 Aserinsky and Kleitman described REM sleep.","[' When was the American EEG Society founded?', ' What was the name of the first international EEG congress?', ' When did Aserinsky and Kleitman describe REM sleep?']","['1947', 'The American EEG Society', '1953']"
2542,eeg,History,"In the 1950s, William Grey Walter developed an adjunct to EEG called EEG topography, which allowed for the mapping of electrical activity across the surface of the brain.  This enjoyed a brief period of popularity in the 1980s and seemed especially promising for psychiatry.  It was never accepted by neurologists and remains primarily a research tool.
","In the 1950s, William Grey Walter developed an adjunct to EEG called EEG topography, which allowed for the mapping of electrical activity across the surface of the brain. This enjoyed a brief period of popularity in the 1980s and seemed especially promising for psychiatry.","[' In what decade did William Grey Walter develop EEG topography?', ' What was the name of the adjunct to EEG that allowed for the mapping of electrical activity across the surface of the brain?']","['1950s', 'EEG topography']"
2543,eeg,History,An electroencephalograph system manufactured by Beckman Instruments was used on at least one of the Project Gemini manned spaceflights (1965-1966) to monitor the brain waves of astronauts on the flight. It was one of many Beckman Instruments specialized for and used by NASA.,An electroencephalograph system manufactured by Beckman Instruments was used on at least one of the Project Gemini manned spaceflights (1965-1966) to monitor the brain waves of astronauts on the flight. It was one of many Beckman Instruments specialized for and used by NASA.,"[' What company manufactured the electroencephalograph system?', ' What did Beckman Instruments monitor?', ' When did Project Gemini fly?']","['Beckman Instruments', 'brain waves', '1965-1966']"
2544,eeg,History,"In October 2018, scientists connected the brains of three people to experiment with the process of thoughts sharing. Five groups of three people participated in the experiment using EEG. The success rate of the experiment was 81%.","In October 2018, scientists connected the brains of three people to experiment with the process of thoughts sharing. Five groups of three people participated in the experiment using EEG.","[' When did scientists connect the brains of three people?', ' How many groups of people participated in the experiment using EEG?']","['October 2018', 'Five']"
2545,eeg,Medical use,"EEG is one of the main diagnostic tests for epilepsy. A routine clinical EEG recording typically lasts 20–30 minutes (plus preparation time). It is a test that detects electrical activity in the brain using small, metal discs (electrodes) attached to the scalp. Routinely, EEG is used in clinical circumstances to determine changes in brain activity that might be useful in diagnosing brain disorders, especially epilepsy or another seizure disorder. An EEG might also be helpful for diagnosing or treating the following disorders:",EEG is one of the main diagnostic tests for epilepsy. A routine clinical EEG recording typically lasts 20–30 minutes (plus preparation time).,"[' What is one of the main diagnostic tests for epilepsy?', ' How long does a routine clinical EEG recording typically last?']","['EEG', '20–30 minutes']"
2546,eeg,Medical use,"At times, a routine EEG is not sufficient to establish the diagnosis or to determine the best course of action in terms of treatment. In this case, attempts may be made to record an EEG while a seizure is occurring. This is known as an ictal recording, as opposed to an inter-ictal recording which refers to the EEG recording between seizures. To obtain an ictal recording, a prolonged EEG is typically performed accompanied by a time-synchronized video and audio recording. This can be done either as an outpatient (at home) or during a hospital admission, preferably to an Epilepsy Monitoring Unit (EMU) with nurses and other personnel trained in the care of patients with seizures. Outpatient ambulatory video EEGs typically last one to three days. An admission to an Epilepsy Monitoring Unit typically lasts several days but may last for a week or longer. While in the hospital, seizure medications are usually withdrawn to increase the odds that a seizure will occur during admission. For reasons of safety, medications are not withdrawn during an EEG outside of the hospital. Ambulatory video EEGs, therefore, have the advantage of convenience and are less expensive than a hospital admission, but the disadvantage of a decreased probability of recording a clinical event.
","At times, a routine EEG is not sufficient to establish the diagnosis or to determine the best course of action in terms of treatment. In this case, attempts may be made to record an EEG while a seizure is occurring.","[' What is not enough to establish the diagnosis or determine the best course of action in terms of treatment?', ' What can be attempted while a seizure is occurring?']","['a routine EEG', 'record an EEG']"
2547,eeg,Medical use,"Epilepsy monitoring is typically done to distinguish epileptic seizures from other types of spells, such as psychogenic non-epileptic seizures, syncope (fainting), sub-cortical movement disorders and migraine variants, to characterize seizures for the purposes of treatment, and to localize the region of brain from which a seizure originates for work-up of possible seizure surgery. Hospitals use an EEG monitor to help diagnose a seizure. They use that information to help with the treatment process as well as discovering risks. ""Many professionals have stated the importance of EEG’s when it comes to suspected seizures, for diagnosis and evaluation"". Doctors will be able to use the EEG monitoring system to help look at some treatment options as well as some risk factors. As technology advances, researchers are finding new monitors that are more accurate in regards to seizures. ""Advanced techniques with continuous EEG and simplified technique with aEEG allows clinicians to detect more seizures at the bedside”. An aEEG stands for amplitude integrated electroencephalography and can detect any electrical brain activity just like an EEG monitor. An aEEG monitor can monitor the brain function for a long period, whereas an EEG monitor can only monitor brain function for a couple hours to days. This helps the improvement of detecting more seizures faster, and the preterm babies suffering with seizures can be treated earlier and have less long term effects.
","Epilepsy monitoring is typically done to distinguish epileptic seizures from other types of spells, such as psychogenic non-epileptic seizures, syncope (fainting), sub-cortical movement disorders and migraine variants, to characterize seizures for the purposes of treatment, and to localize the region of brain from which a seizure originates for work-up of possible seizure surgery. Hospitals use an EEG monitor to help diagnose a seizure.","[' Epilepsy monitoring is done to distinguish epileptic seizures from what other type of spells?', ' Syncope, sub-cortical movement disorders and migraine variants are examples of what type of seizures?', ' What monitor does a hospital use to help diagnose a seizure?']","['psychogenic non-epileptic seizures, syncope (fainting), sub-cortical movement disorders and migraine variants', 'psychogenic non-epileptic seizures', 'EEG']"
2548,eeg,Medical use,"If a patient with epilepsy is being considered for resective surgery, it is often necessary to localize the focus (source) of the epileptic brain activity with a resolution greater than what is provided by scalp EEG. This is because the cerebrospinal fluid, skull and scalp smear the electrical potentials recorded by scalp EEG. In these cases, neurosurgeons typically implant strips and grids of electrodes (or penetrating depth electrodes) under the dura mater, through either a craniotomy or a burr hole. The recording of these signals is referred to as electrocorticography (ECoG), subdural EEG (sdEEG) or intracranial EEG (icEEG)--all terms for the same thing. The signal recorded from ECoG is on a different scale of activity than the brain activity recorded from scalp EEG. Low voltage, high frequency components that cannot be seen easily (or at all) in scalp EEG can be seen clearly in ECoG. Further, smaller electrodes (which cover a smaller parcel of brain surface) allow even lower voltage, faster components of brain activity to be seen. Some clinical sites record from penetrating microelectrodes.","If a patient with epilepsy is being considered for resective surgery, it is often necessary to localize the focus (source) of the epileptic brain activity with a resolution greater than what is provided by scalp EEG. This is because the cerebrospinal fluid, skull and scalp smear the electrical potentials recorded by scalp EEG.","[' What is needed to localize the focus of the epileptic brain activity with a resolution greater than what is provided by scalp EEG?', ' The cerebrospinal fluid, skull and scalp smear the electrical potentials recorded by what?', ' What do skull and scalp smear the electrical potentials recorded by scalp EEG?']","['the cerebrospinal fluid, skull and scalp', 'scalp EEG', 'cerebrospinal fluid']"
2549,eeg,Medical use,"EEG is not indicated for diagnosing headache. Recurring headache is a common pain problem, and this procedure is sometimes used in a search for a diagnosis, but it has no advantage over routine clinical evaluation.","EEG is not indicated for diagnosing headache. Recurring headache is a common pain problem, and this procedure is sometimes used in a search for a diagnosis, but it has no advantage over routine clinical evaluation.","[' What is not indicated for diagnosing headache?', ' What is a common pain problem?', ' EEG is sometimes used in a search for what?']","['EEG', 'Recurring headache', 'a diagnosis']"
2550,eeg,Research use,"EEG, and the related study of ERPs are used extensively in neuroscience, cognitive science, cognitive psychology, neurolinguistics and psychophysiological research, but also to study human functions such as swallowing. Many EEG techniques used in research are not standardised sufficiently for clinical use, and many ERP studies fail to report all of the necessary processing steps for data collection and reduction, limiting the reproducibility and replicability of many studies. But research on mental disabilities, such as auditory processing disorder (APD), ADD, or ADHD, is becoming more widely known and EEGs are used as research and treatment.
","EEG, and the related study of ERPs are used extensively in neuroscience, cognitive science, cognitive psychology, neurolinguistics and psychophysiological research, but also to study human functions such as swallowing. Many EEG techniques used in research are not standardised sufficiently for clinical use, and many ERP studies fail to report all of the necessary processing steps for data collection and reduction, limiting the reproducibility and replicability of many studies.","[' What is used extensively in neuroscience, cognitive science, cognitive psychology, neurolinguistics, and psychophysiophysiology?', ' What are EEG techniques not standardised sufficiently for?', ' Many ERP studies fail to report all what?', ' What do many ERP studies fail to report?', ' What is limiting the reproducibility and replicability of studies?']","['EEG', 'clinical use', 'processing steps for data collection and reduction', 'all of the necessary processing steps for data collection and reduction', 'many ERP studies fail to report all of the necessary processing steps for data collection and reduction']"
2551,eeg,Mechanisms,"The brain's electrical charge is maintained by billions of neurons.  Neurons are electrically charged (or ""polarized"") by membrane transport proteins that pump ions across their membranes.  Neurons are constantly exchanging ions with the extracellular milieu, for example to maintain resting potential and to propagate action potentials.   Ions of similar charge repel each other, and when many ions are pushed out of many neurons at the same time, they can push their neighbours, who push their neighbours, and so on, in a wave.  This process is known as volume conduction.  When the wave of ions reaches the electrodes on the scalp, they can push or pull electrons on the metal in the electrodes.  Since metal conducts the push and pull of electrons easily, the difference in push or pull voltages between any two electrodes can be measured by a voltmeter.  Recording these voltages over time gives us the EEG.","The brain's electrical charge is maintained by billions of neurons. Neurons are electrically charged (or ""polarized"") by membrane transport proteins that pump ions across their membranes.","[' How many neurons maintain the electrical charge of the brain?', ' What are neurons electrically charged by?']","['billions', 'membrane transport proteins']"
2552,eeg,Mechanisms,"The electric potential generated by an individual neuron is far too small to be picked up by EEG or MEG. EEG activity therefore always reflects the summation of the synchronous activity of thousands or millions of neurons that have similar spatial orientation.  If the cells do not have similar spatial orientation, their ions do not line up and create waves to be detected.  Pyramidal neurons of the cortex are thought to produce the most EEG signal because they are well-aligned and fire together.  Because voltage field gradients fall off with the square of distance, activity from deep sources is more difficult to detect than currents near the skull.",The electric potential generated by an individual neuron is far too small to be picked up by EEG or MEG. EEG activity therefore always reflects the summation of the synchronous activity of thousands or millions of neurons that have similar spatial orientation.,"[' What is too small to be picked up by EEG or MEG?', ' EEG activity always reflects the summation of what?']","['electric potential generated by an individual neuron', 'the synchronous activity of thousands or millions of neurons that have similar spatial orientation']"
2553,eeg,Mechanisms,"Scalp EEG activity shows oscillations at a variety of frequencies. Several of these oscillations have characteristic frequency ranges, spatial distributions and are associated with different states of brain functioning (e.g., waking and the various sleep stages). These oscillations represent synchronized activity over a network of neurons. The neuronal networks underlying some of these oscillations are understood (e.g., the thalamocortical resonance underlying sleep spindles), while many others are not (e.g., the system that generates the posterior basic rhythm). Research that measures both EEG and neuron spiking finds the relationship between the two is complex, with a combination of EEG power in the gamma band and phase in the delta band relating most strongly to neuron spike activity.","Scalp EEG activity shows oscillations at a variety of frequencies. Several of these oscillations have characteristic frequency ranges, spatial distributions and are associated with different states of brain functioning (e.g., waking and the various sleep stages).","[' Scalp EEG activity shows oscillations at a variety of what frequencies?', ' Several of these oscillation have characteristic frequency ranges, spatial distributions and are associated with what states of brain functioning?']","['frequencies', 'waking and the various sleep stages']"
2554,eeg,Method,"In conventional scalp EEG, the recording is obtained by placing electrodes on the scalp with a conductive gel or paste, usually after preparing the scalp area by light abrasion to reduce impedance due to dead skin cells. Many systems typically use electrodes, each of which is attached to an individual wire. Some systems use caps or nets into which electrodes are embedded; this is particularly common when high-density arrays of electrodes are needed.
","In conventional scalp EEG, the recording is obtained by placing electrodes on the scalp with a conductive gel or paste, usually after preparing the scalp area by light abrasion to reduce impedance due to dead skin cells. Many systems typically use electrodes, each of which is attached to an individual wire.","[' How is scalp EEG obtained?', ' What is used to prepare the scalp area?', ' How are electrodes attached to an individual?', ' How are electrodes connected?', ' How is each electrode attached?']","['by placing electrodes on the scalp with a conductive gel or paste', 'light abrasion', 'wire', 'an individual wire', 'to an individual wire']"
2555,eeg,Method,"Electrode locations and names are specified by the International 10–20 system for most clinical and research applications (except when high-density arrays are used). This system ensures that the naming of electrodes is consistent across laboratories. In most clinical applications, 19 recording electrodes (plus ground and system reference) are used. A smaller number of electrodes are typically used when recording EEG from neonates. Additional electrodes can be added to the standard set-up when a clinical or research application demands increased spatial resolution for a particular area of the brain. High-density arrays (typically via cap or net) can contain up to 256 electrodes more-or-less evenly spaced around the scalp.
",Electrode locations and names are specified by the International 10–20 system for most clinical and research applications (except when high-density arrays are used). This system ensures that the naming of electrodes is consistent across laboratories.,"[' What system specifies electrode locations and names for most clinical and research applications?', ' What system ensures the naming of electrodes is consistent across laboratories?']","['International 10–20', 'International 10–20 system']"
2556,eeg,Method,"Each electrode is connected to one input of a differential amplifier (one amplifier per pair of electrodes); a common system reference electrode is connected to the other input of each differential amplifier. These amplifiers amplify the voltage between the active electrode and the reference (typically 1,000–100,000 times, or 60–100 dB of voltage gain). In analog EEG, the signal is then filtered (next paragraph), and the EEG signal is output as the deflection of pens as paper passes underneath. Most EEG systems these days, however, are digital, and the amplified signal is digitized via an analog-to-digital converter, after being passed through an anti-aliasing filter.  Analog-to-digital sampling typically occurs at 256–512 Hz in clinical scalp EEG; sampling rates of up to 20 kHz are used in some research applications.
","Each electrode is connected to one input of a differential amplifier (one amplifier per pair of electrodes); a common system reference electrode is connected to the other input of each differential amplifier. These amplifiers amplify the voltage between the active electrode and the reference (typically 1,000–100,000 times, or 60–100 dB of voltage gain).","[' How many times is the voltage between the active electrode and the reference amplify?', ' How many amplifiers are per pair of electrodes?', ' What is connected to the other input of each differential amplifier?', ' What is the typical voltage gain between the electrode and the reference?', ' How many times is the reference typically used?']","['1,000–100,000', 'one', 'a common system reference electrode', '60–100 dB', '1,000–100,000']"
2557,eeg,Method,"During the recording, a series of activation procedures may be used. These procedures may induce normal or abnormal EEG activity that might not otherwise be seen. These procedures include hyperventilation, photic stimulation (with a strobe light), eye closure, mental activity, sleep and sleep deprivation. During (inpatient) epilepsy monitoring, a patient's typical seizure medications may be withdrawn.
","During the recording, a series of activation procedures may be used. These procedures may induce normal or abnormal EEG activity that might not otherwise be seen.","[' During recording, a series of activation procedures may be used to induce what?']",['normal or abnormal EEG activity']
2558,eeg,Method,"The digital EEG signal is stored electronically and can be filtered for display.  Typical settings for the high-pass filter and a low-pass filter are 0.5–1 Hz and 35–70 Hz respectively. The high-pass filter typically filters out slow artifact, such as electrogalvanic signals and movement artifact, whereas the low-pass filter filters out high-frequency artifacts, such as electromyographic signals. An additional notch filter is typically used to remove artifact caused by electrical power lines (60 Hz in the United States and 50 Hz in many other countries).",The digital EEG signal is stored electronically and can be filtered for display. Typical settings for the high-pass filter and a low-pass filter are 0.5–1 Hz and 35–70 Hz respectively.,"[' What is stored electronically and can be filtered for display?', ' What are the typical settings for a high-pass filter and a low-pass-filter?']","['The digital EEG signal', '0.5–1\xa0Hz and 35–70\xa0Hz']"
2559,eeg,Method,"As part of an evaluation for epilepsy surgery, it may be necessary to insert electrodes near the surface of the brain, under the surface of the dura mater. This is accomplished via burr hole or craniotomy. This is referred to variously as ""electrocorticography (ECoG)"", ""intracranial EEG (I-EEG)"" or ""subdural EEG (SD-EEG)"". Depth electrodes may also be placed into brain structures, such as the amygdala or hippocampus, structures, which are common epileptic foci and may not be ""seen"" clearly by scalp EEG. The electrocorticographic signal is processed in the same manner as digital scalp EEG (above), with a couple of caveats. ECoG is typically recorded at higher sampling rates than scalp EEG because of the requirements of Nyquist theorem—the subdural signal is composed of a higher predominance of higher frequency components. Also, many of the artifacts that affect scalp EEG do not impact ECoG, and therefore display filtering is often not needed.
","As part of an evaluation for epilepsy surgery, it may be necessary to insert electrodes near the surface of the brain, under the surface of the dura mater. This is accomplished via burr hole or craniotomy.","[' Where do electrodes need to be placed in an evaluation for epilepsy surgery?', ' What is a burr hole or craniotomy?']","['near the surface of the brain', 'insert electrodes']"
2560,eeg,Method,"Since an EEG voltage signal represents a difference between the voltages at two electrodes, the display of the EEG for the reading encephalographer may be set up in one of several ways. The representation of the EEG channels is referred to as a montage.
","Since an EEG voltage signal represents a difference between the voltages at two electrodes, the display of the EEG for the reading encephalographer may be set up in one of several ways. The representation of the EEG channels is referred to as a montage.","[' What represents a difference between the voltages at two electrodes?', ' What is the representation of the EEG channels called?']","['EEG voltage signal', 'a montage']"
2561,eeg,Method,"When analog (paper) EEGs are used, the technologist switches between montages during the recording in order to highlight or better characterize certain features of the EEG. With digital EEG, all signals are typically digitized and stored in a particular (usually referential) montage; since any montage can be constructed mathematically from any other, the EEG can be viewed by the electroencephalographer in any display montage that is desired.
","When analog (paper) EEGs are used, the technologist switches between montages during the recording in order to highlight or better characterize certain features of the EEG. With digital EEG, all signals are typically digitized and stored in a particular (usually referential) montage; since any montage can be constructed mathematically from any other, the EEG can be viewed by the electroencephalographer in any display montage that is desired.","[' When analog EEGs are used, the technologist switches between montages to highlight or better characterize what?', ' With digital EEG, all signals are typically digitized and stored in a particular (usually referential) montage what can be constructed mathematically from?', ' What type of montage can be constructed mathematically from any other?', ' What can be viewed by the electroencephalographer?']","['certain features of the EEG', 'any other', 'any montage', 'the EEG']"
2562,eeg,Method,"The EEG is read by a clinical neurophysiologist or neurologist (depending on local custom and law regarding medical specialities), optimally one who has specific training in the interpretation of EEGs for clinical purposes. This is done by visual inspection of the waveforms, called graphoelements. The use of computer signal processing of the EEG—so-called quantitative electroencephalography—is somewhat controversial when used for clinical purposes (although there are many research uses).
","The EEG is read by a clinical neurophysiologist or neurologist (depending on local custom and law regarding medical specialities), optimally one who has specific training in the interpretation of EEGs for clinical purposes. This is done by visual inspection of the waveforms, called graphoelements.","[' Who reads the EEG?', ' What is the term for the visual inspection of the waveforms?']","['a clinical neurophysiologist or neurologist', 'graphoelements']"
2563,eeg,Normal activity,"The EEG is typically described in terms of (1) rhythmic activity and (2) transients. The rhythmic activity is divided into bands by frequency. To some degree, these frequency bands are a matter of nomenclature (i.e., any rhythmic activity between 8–12 Hz can be described as ""alpha""), but these designations arose because rhythmic activity within a certain frequency range was noted to have a certain distribution over the scalp or a certain biological significance. Frequency bands are usually extracted using spectral methods (for instance Welch) as implemented for instance in freely available EEG software such as EEGLAB or the Neurophysiological Biomarker Toolbox.
Computational processing of the EEG is often named quantitative electroencephalography (qEEG).
",The EEG is typically described in terms of (1) rhythmic activity and (2) transients. The rhythmic activity is divided into bands by frequency.,"[' What is the EEG typically described in terms of?', ' What is divided into bands by frequency?']","['rhythmic activity and (2) transients', 'rhythmic activity']"
2564,eeg,Normal activity,"Most of the cerebral signal observed in the scalp EEG falls in the range of 1–20 Hz (activity below or above this range is likely to be artifactual, under standard clinical recording techniques). Waveforms are subdivided into bandwidths known as alpha, beta, theta, and delta to signify the majority of the EEG used in clinical practice.","Most of the cerebral signal observed in the scalp EEG falls in the range of 1–20 Hz (activity below or above this range is likely to be artifactual, under standard clinical recording techniques). Waveforms are subdivided into bandwidths known as alpha, beta, theta, and delta to signify the majority of the EEG used in clinical practice.","[' What frequency range does most of the cerebral signal in scalp EEG fall in?', ' What is activity below or above this range likely to be?', ' Waveforms are subdivided into what?', ' Alpha, beta, theta, and delta signify the majority of the brain signal?', ' Beta, theta, and delta signify the majority of what type of EEG used in clinical practice?']","['1–20\xa0Hz', 'artifactual', 'bandwidths known as alpha, beta, theta, and delta', 'alpha', 'alpha']"
2565,eeg,Abnormal activity,"Abnormal activity can broadly be separated into epileptiform and non-epileptiform activity. It can also be separated into focal or diffuse.
",Abnormal activity can broadly be separated into epileptiform and non-epileptiform activity. It can also be separated into focal or diffuse.,"[' Abnormal activity can be broadly separated into what two types of activity?', ' It can also be separated into focal or what other category?']","['epileptiform and non-epileptiform', 'diffuse']"
2566,eeg,Abnormal activity,"Focal epileptiform discharges represent fast, synchronous potentials in a large number of neurons in a somewhat discrete area of the brain. These can occur as interictal activity, between seizures, and represent an area of cortical irritability that may be predisposed to producing epileptic seizures. Interictal discharges are not wholly reliable for determining whether a patient has epilepsy nor where his/her seizure might originate. (See focal epilepsy.)
","Focal epileptiform discharges represent fast, synchronous potentials in a large number of neurons in a somewhat discrete area of the brain. These can occur as interictal activity, between seizures, and represent an area of cortical irritability that may be predisposed to producing epileptic seizures.","[' Focal epileptiform discharges represent fast, synchronous potentials in a large number of neurons in what area of the brain?', ' What can occur as interictal activity, between seizures, and represent an area of cortical irritability that may be predisposed to producing epileptic seizures?']","['somewhat discrete area', 'Focal epileptiform discharges']"
2567,eeg,Abnormal activity,"Generalized epileptiform discharges often have an anterior maximum, but these are seen synchronously throughout the entire brain. They are strongly suggestive of a generalized epilepsy.
","Generalized epileptiform discharges often have an anterior maximum, but these are seen synchronously throughout the entire brain. They are strongly suggestive of a generalized epilepsy.",[' Generalized epileptiform discharges are often seen synchronously throughout the entire brain.'],['Generalized epileptiform discharges often have an anterior maximum']
2568,eeg,Abnormal activity,"Focal non-epileptiform abnormal activity may occur over areas of the brain where there is focal damage of the cortex or white matter. It often consists of an increase in slow frequency rhythms and/or a loss of normal higher frequency rhythms. It may also appear as focal or unilateral decrease in amplitude of the EEG signal.
",Focal non-epileptiform abnormal activity may occur over areas of the brain where there is focal damage of the cortex or white matter. It often consists of an increase in slow frequency rhythms and/or a loss of normal higher frequency rhythms.,[' Focal non-epileptiform abnormal activity may occur over areas of the brain where there is focal damage of the cortex or what?'],['white matter']
2569,eeg,Economics,"Inexpensive EEG devices exist for the low-cost research and consumer markets. Recently, a few companies have miniaturized medical grade EEG technology to create versions accessible to the general public. Some of these companies have built commercial EEG devices retailing for less than US$100.
","Inexpensive EEG devices exist for the low-cost research and consumer markets. Recently, a few companies have miniaturized medical grade EEG technology to create versions accessible to the general public.","[' Inexpensive EEG devices exist for what?', ' What have a few companies miniaturized medical grade EEG technology to create versions accessible to the general public?']","['low-cost research and consumer markets', 'Recently']"
2570,eeg,Future research,"The EEG has been used for many purposes besides the conventional uses of clinical diagnosis and conventional cognitive neuroscience. An early use was during World War II by the U.S. Army Air Corps to screen out pilots in danger of having seizures; long-term EEG recordings in epilepsy patients are still used today for seizure prediction. Neurofeedback remains an important extension, and in its most advanced form is also attempted as the basis of brain computer interfaces. The EEG is also used quite extensively in the field of neuromarketing.
",The EEG has been used for many purposes besides the conventional uses of clinical diagnosis and conventional cognitive neuroscience. An early use was during World War II by the U.S. Army Air Corps to screen out pilots in danger of having seizures; long-term EEG recordings in epilepsy patients are still used today for seizure prediction.,"[' What has been used for many purposes other than clinical diagnosis and conventional cognitive neuroscience?', ' What was an early use of the EEG during World War II by the U.S. Army Air Corps?', ' Long-term EEG recordings in epilepsy patients are still used today?', ' What are EEG recordings in epilepsy patients used for?', ' What type of recordings are used for seizure prediction?']","['EEG', 'to screen out pilots in danger of having seizures', 'seizure prediction', 'seizure prediction', 'long-term EEG recordings']"
2571,eeg,Future research,"The EEG is altered by drugs that affect brain functions, the chemicals that are the basis for psychopharmacology.  Berger's early experiments recorded the effects of drugs on EEG.  The science of pharmaco-electroencephalography has developed methods to identify substances that systematically alter brain functions for therapeutic and recreational use.
","The EEG is altered by drugs that affect brain functions, the chemicals that are the basis for psychopharmacology. Berger's early experiments recorded the effects of drugs on EEG.","[' What is altered by drugs that affect brain functions?', ' What are the chemicals that are the basis for psychopharmacology?', ' Whose early experiments recorded the effects of drugs on EEG?']","['The EEG', 'drugs that affect brain functions', 'Berger']"
2572,eeg,Future research,"EEGs have been used as evidence in criminal trials in the Indian state of Maharashtra.  Brain Electrical Oscillation Signature Profiling (BEOS), an EEG technique, was used in the trial of State of Maharashtra v. Sharma to show Sharma remembered using arsenic to poisoning her ex-fiancé, although the reliability and scientific basis of BEOS is disputed.","EEGs have been used as evidence in criminal trials in the Indian state of Maharashtra. Brain Electrical Oscillation Signature Profiling (BEOS), an EEG technique, was used in the trial of State of Maharashtra v. Sharma to show Sharma remembered using arsenic to poisoning her ex-fiancé, although the reliability and scientific basis of BEOS is disputed.","[' In what Indian state have EEGs been used as evidence in criminal trials?', ' What technique was used in the trial of State of Maharashtra v. Sharma to show Sharma remembered using arsenic?', ' What chemical was used to poison her ex-fiancé?', ' What is disputed about the scientific basis of BEOS?']","['Maharashtra', 'Brain Electrical Oscillation Signature Profiling', 'arsenic', 'reliability']"
2573,eeg,Future research,"A lot of research is currently being carried out in order to make EEG devices smaller, more portable and easier to use. So called ""Wearable EEG"" is based upon creating low power wireless collection electronics and ‘dry’ electrodes which do not require a conductive gel to be used. Wearable EEG aims to provide small EEG devices which are present only on the head and which can record EEG for days, weeks, or months at a time, as ear-EEG. Such prolonged and easy-to-use monitoring could make a step change in the diagnosis of chronic conditions such as epilepsy, and greatly improve the end-user acceptance of BCI systems. Research is also being carried out on identifying specific solutions to increase the battery lifetime of Wearable EEG devices through the use of the data reduction approach. For example, in the context of epilepsy diagnosis, data reduction has been used to extend the battery lifetime of Wearable EEG devices by intelligently selecting, and only transmitting, diagnostically relevant EEG data.","A lot of research is currently being carried out in order to make EEG devices smaller, more portable and easier to use. So called ""Wearable EEG"" is based upon creating low power wireless collection electronics and ‘dry’ electrodes which do not require a conductive gel to be used.","[' What type of EEG devices are being researched to make them smaller, more portable and easier to use?', ' What is ""Wearable EEG"" based on creating low power wireless collection electronics and ""dry"" electrodes?']","['Wearable EEG', 'do not require a conductive gel to be used']"
2574,eeg,Future research,"In research, currently EEG is often used in combination with machine learning. EEG data are pre-processed to be passed on to machine learning algorithms. These algorithms are then trained to recognize different diseases like schizophrenia, epilepsy or dementia. Furthermore, they are increasingly used to study seizure detection. By using machine learning, the data can be analyzed automatically. In the long run this research is intended to build algorithms that support physicians in their clinical practice  and to provide further insights into diseases. In this vein, complexity measures of EEG data are often calculated, such as Lempel-Ziv complexity, fractal dimension, and spectral flatness. It has been shown that combining or multiplying such measures can reveal previously hidden information in EEG data.","In research, currently EEG is often used in combination with machine learning. EEG data are pre-processed to be passed on to machine learning algorithms.","[' EEG data are pre-processed to be passed on to what?', ' In research, what is often used in combination with machine learning?']","['machine learning algorithms', 'EEG']"
2575,eeg,Future research,"EEG signals from musical performers were used to create instant compositions and one CD by the Brainwave Music Project, run at the Computer Music Center at Columbia University by Brad Garton and Dave Soldier. Similarly, an hour-long recording of the brainwaves of Ann Druyan was included on the Voyager Golden Record, launched on the Voyager probes in 1977, in case any extraterrestrial intelligence could decode her thoughts, which included what it was like to fall in love.
","EEG signals from musical performers were used to create instant compositions and one CD by the Brainwave Music Project, run at the Computer Music Center at Columbia University by Brad Garton and Dave Soldier. Similarly, an hour-long recording of the brainwaves of Ann Druyan was included on the Voyager Golden Record, launched on the Voyager probes in 1977, in case any extraterrestrial intelligence could decode her thoughts, which included what it was like to fall in love.","[' What did the Brainwave Music Project use to create instant compositions?', ' Where is the Computer Music Center at Columbia University?', ' What was included on the Voyager Golden Recording?', "" How long was Ann Druyan's recording?"", "" What was Ann Druyan's brainwaves included on?"", ' When was the Voyager Golden Record launched?']","['EEG signals from musical performers', 'Brainwave Music Project', 'an hour-long recording of the brainwaves of Ann Druyan', 'hour', 'Voyager Golden Record', '1977']"
2576,user interface,Summary,"In the industrial design field of human–computer interaction, a user interface (UI) is the space where interactions between humans and machines occur. The goal of this interaction is to allow effective operation and control of the machine from the human end, while the machine simultaneously feeds back information that aids the operators' decision-making process. Examples of this broad concept of user interfaces include the interactive aspects of computer operating systems, hand tools, heavy machinery operator controls, and process controls. The design considerations applicable when creating user interfaces are related to, or involve such disciplines as, ergonomics and psychology.
","In the industrial design field of human–computer interaction, a user interface (UI) is the space where interactions between humans and machines occur. The goal of this interaction is to allow effective operation and control of the machine from the human end, while the machine simultaneously feeds back information that aids the operators' decision-making process.","[' What does UI stand for?', ' What is the goal of a user interface?', "" What does the machine feed back that aids the operators' decision making process?""]","['user interface', 'to allow effective operation and control of the machine from the human end', 'information']"
2577,user interface,Summary,"Generally, the goal of user interface design is to produce a user interface that makes it easy, efficient, and enjoyable (user-friendly) to operate a machine in the way which produces the desired result (i.e. maximum usability). This generally means that the operator needs to provide minimal input to achieve the desired output, and also that the machine minimizes undesired outputs to the user.
","Generally, the goal of user interface design is to produce a user interface that makes it easy, efficient, and enjoyable (user-friendly) to operate a machine in the way which produces the desired result (i.e. maximum usability).","[' What is the goal of user interface design?', ' What is a user interface designed to make it easy, efficient, and enjoyable?']","['to produce a user interface that makes it easy, efficient, and enjoyable', 'user-friendly']"
2578,user interface,Summary,"User interfaces are composed of one or more layers, including a human-machine interface (HMI) that interfaces machines with physical input hardware such as keyboards, mice, or game pads, and output hardware such as computer monitors, speakers, and printers. A device that implements an HMI is called a human interface device (HID). Other terms for human–machine interfaces are man–machine interface (MMI) and, when the machine in question is a computer, human–computer interface. Additional UI layers may interact with one or more human senses, including: tactile UI (touch), visual UI (sight), auditory UI (sound), olfactory UI (smell), equilibria UI (balance), and gustatory UI (taste).
","User interfaces are composed of one or more layers, including a human-machine interface (HMI) that interfaces machines with physical input hardware such as keyboards, mice, or game pads, and output hardware such as computer monitors, speakers, and printers. A device that implements an HMI is called a human interface device (HID).","[' What is a human-machine interface?', ' What type of input hardware does a HMI interface machines with?', ' What is a human interface device called?']","['HMI', 'physical', 'HID']"
2579,user interface,Summary,"Composite user interfaces (CUIs) are UIs that interact with two or more senses. The most common CUI is a graphical user interface (GUI), which is composed of a tactile UI and a visual UI capable of displaying graphics. When sound is added to a GUI, it becomes a multimedia user interface (MUI). There are three broad categories of CUI: standard, virtual and augmented. Standard CUI use standard human interface devices like keyboards, mice, and computer monitors. When the CUI blocks out the real world to create a virtual reality, the CUI is virtual and uses a virtual reality interface. When the CUI does not block out the real world and creates augmented reality, the CUI is augmented and uses an augmented reality interface. When a UI interacts with all human senses, it is called a qualia interface, named after the theory of qualia. CUI may also be classified by how many senses they interact with as either an X-sense virtual reality interface or X-sense augmented reality interface, where X is the number of senses interfaced with. For example, a Smell-O-Vision is a 3-sense (3S) Standard CUI with visual display, sound and smells; when virtual reality interfaces interface with smells and touch it is said to be a 4-sense (4S) virtual reality interface; and when augmented reality interfaces interface with smells and touch it is said to be a 4-sense (4S) augmented reality interface.
","Composite user interfaces (CUIs) are UIs that interact with two or more senses. The most common CUI is a graphical user interface (GUI), which is composed of a tactile UI and a visual UI capable of displaying graphics.","[' What are UIs that interact with two or more senses?', ' What is the most common CUI?', ' A graphical user interface is composed of what?']","['Composite user interfaces', 'a graphical user interface (GUI),', 'a tactile UI and a visual UI']"
2580,user interface,Overview,"The user interface or human–machine interface is the part of the machine that handles the human–machine interaction. Membrane switches, rubber keypads and touchscreens are examples of the physical part of the Human Machine Interface which we can see and touch.
","The user interface or human–machine interface is the part of the machine that handles the human–machine interaction. Membrane switches, rubber keypads and touchscreens are examples of the physical part of the Human Machine Interface which we can see and touch.","[' What is the part of the machine that handles the human-machine interaction called?', ' Membrane switches, rubber keypads and touchscreens are examples of what?']","['human–machine interface', 'the physical part of the Human Machine Interface']"
2581,user interface,Overview,"In complex systems, the human–machine interface is typically computerized. The term human–computer interface refers to this kind of system. In the context of computing, the term typically extends as well to the software dedicated to control the physical elements used for human–computer interaction.
","In complex systems, the human–machine interface is typically computerized. The term human–computer interface refers to this kind of system.","[' In complex systems, what is the human-machine interface typically computerized?', ' The term human-computer interface refers to what kind of system?']","['human–machine interface', 'complex systems']"
2582,user interface,Overview,"The engineering of human–machine interfaces is enhanced by considering ergonomics (human factors). The corresponding disciplines are human factors engineering (HFE) and usability engineering (UE), which is part of systems engineering.
","The engineering of human–machine interfaces is enhanced by considering ergonomics (human factors). The corresponding disciplines are human factors engineering (HFE) and usability engineering (UE), which is part of systems engineering.","[' What is the engineering of human-machine interfaces enhanced by?', ' What are the corresponding disciplines?']","['considering ergonomics', 'human factors engineering (HFE) and usability engineering (UE),']"
2583,user interface,Overview,"Tools used for incorporating human factors in the interface design are developed based on knowledge of computer science, such as computer graphics, operating systems, programming languages. Nowadays, we use the expression graphical user interface for human–machine interface on computers, as nearly all of them are now using graphics.","Tools used for incorporating human factors in the interface design are developed based on knowledge of computer science, such as computer graphics, operating systems, programming languages. Nowadays, we use the expression graphical user interface for human–machine interface on computers, as nearly all of them are now using graphics.","[' What are tools developed based on knowledge of computer science?', ' What does the expression graphical user interface for human-machine interface on computers mean?']","['Tools used for incorporating human factors in the interface design', 'nearly all of them are now using graphics']"
2584,user interface,Terminology,"In science fiction, HMI is sometimes used to refer to what is better described as a direct neural interface. However, this latter usage is seeing increasing application in the real-life use of (medical) prostheses—the artificial extension that replaces a missing body part (e.g., cochlear implants).","In science fiction, HMI is sometimes used to refer to what is better described as a direct neural interface. However, this latter usage is seeing increasing application in the real-life use of (medical) prostheses—the artificial extension that replaces a missing body part (e.g., cochlear implants).","[' What is HMI sometimes used to refer to in science fiction?', ' What is the artificial extension that replaces a missing body part called?']","['a direct neural interface', 'prostheses']"
2585,user interface,Terminology,"In some circumstances, computers might observe the user and react according to their actions without specific commands. A means of tracking parts of the body is required, and sensors noting the position of the head, direction of gaze and so on have been used experimentally. This is particularly relevant to immersive interfaces.","In some circumstances, computers might observe the user and react according to their actions without specific commands. A means of tracking parts of the body is required, and sensors noting the position of the head, direction of gaze and so on have been used experimentally.","[' In some circumstances, computers might observe the user and react according to their actions without what?', ' A means of tracking parts of the body is required, and sensors noting the position of the head, direction of gaze and so on have been used experimentally.']","['specific commands', 'computers might observe the user and react according to their actions without specific commands.']"
2586,path planning,Summary,"
Motion planning, also path planning (also known as the navigation problem or the piano mover's problem) is a computational problem to find a sequence of valid configurations that moves the object from the source to destination. The term is used in computational geometry, computer animation, robotics and computer games.
","
Motion planning, also path planning (also known as the navigation problem or the piano mover's problem) is a computational problem to find a sequence of valid configurations that moves the object from the source to destination. The term is used in computational geometry, computer animation, robotics and computer games.","[' What is motion planning also known as?', ' Motion planning is a computational problem to find a sequence of valid configurations that moves an object from the source to what?']","['path planning', 'destination']"
2587,path planning,Summary,"For example, consider navigating a mobile robot inside a building to a distant waypoint.  It should execute this task while avoiding walls and not falling down stairs.  A motion planning algorithm would take a description of these tasks as input, and produce the speed and turning commands sent to the robot's wheels.  Motion planning algorithms might address robots with a larger number of joints (e.g., industrial manipulators), more complex tasks (e.g. manipulation of objects), different constraints (e.g., a car that can only drive forward), and uncertainty (e.g. imperfect models of the environment or robot).
","For example, consider navigating a mobile robot inside a building to a distant waypoint. It should execute this task while avoiding walls and not falling down stairs.",[' What should a robot avoid while navigating inside a building?'],['walls']
2588,path planning,Concepts,"A basic motion planning problem is to compute a continuous path that connects a start configuration S and a goal configuration G, while avoiding collision with known obstacles.  The robot and obstacle geometry is described in a 2D or 3D workspace, while the motion is represented as a path in (possibly higher-dimensional) configuration space.
","A basic motion planning problem is to compute a continuous path that connects a start configuration S and a goal configuration G, while avoiding collision with known obstacles. The robot and obstacle geometry is described in a 2D or 3D workspace, while the motion is represented as a path in (possibly higher-dimensional) configuration space.","[' What is a basic motion planning problem?', ' How is the robot and obstacle geometry described?', ' What is represented as a path in a 3D workspace?', ' What is represented as a path in configuration space?']","['to compute a continuous path that connects a start configuration S and a goal configuration G, while avoiding collision with known obstacles', 'in a 2D or 3D workspace', 'motion', 'the motion']"
2589,path planning,Algorithms,"Exact motion planning for high-dimensional systems under complex constraints is computationally intractable.  Potential-field algorithms are efficient, but fall prey to local minima (an exception is the harmonic potential fields).  Sampling-based algorithms avoid the problem of local minima, and solve many problems quite quickly.
They are unable to determine that no path exists, but they have a probability of failure that decreases to zero as more time is spent.
","Exact motion planning for high-dimensional systems under complex constraints is computationally intractable. Potential-field algorithms are efficient, but fall prey to local minima (an exception is the harmonic potential fields).","[' What is computationally intractable for high-dimensional systems under complex constraints?', ' Potential-field algorithms are efficient but fall prey to what?', ' What is an exception to local minima?']","['Exact motion planning', 'local minima', 'harmonic potential fields']"
2590,path planning,Completeness and performance,"A motion planner is said to be complete if the planner in finite time either produces a solution or correctly reports that there is none. Most complete algorithms are geometry-based. The performance of a complete planner is assessed by its computational complexity. When proving this property mathematically, one has to make sure, that it happens in finite time and not just in the asymptotic limit. This is especially problematic, if there occur infinite sequences (that converge only in the limiting case) during a specific proving technique, since then, theoretically, the algorithm will never stop. Intuitive ""tricks"" (often based on induction) are typically mistakenly thought to converge, which they do only for the infinite limit. In other words, the solution exists, but the planner will never report it. This property therefore is related to Turing Completeness and serves in most cases as a theoretical underpinning/guidance. Planners based on a brute force approach are always complete, but are only realizable for finite and discrete setups.
",A motion planner is said to be complete if the planner in finite time either produces a solution or correctly reports that there is none. Most complete algorithms are geometry-based.,"[' What is a motion planner said to be if it either produces a solution or correctly reports that there is none?', ' Most complete algorithms are what?']","['complete', 'geometry-based']"
2591,path planning,Completeness and performance,"In practice, the termination of the algorithm can always be guaranteed by using a counter, that allows only for a maximum number of iterations and then always stops with or without solution. For realtime systems, this is typically achieved by using a watchdog timer, that will simply kill the process. The watchdog has to be independent of all processes (typically realized by low level interrupt routines). The asymptotic case described in the previous paragraph, however, will not be reached in this way. It will report the best one it has found so far (which is better than nothing) or none, but cannot correctly report that there is none. All realizations including a watchdog are always incomplete (except all cases can be evaluated in finite time). 
","In practice, the termination of the algorithm can always be guaranteed by using a counter, that allows only for a maximum number of iterations and then always stops with or without solution. For realtime systems, this is typically achieved by using a watchdog timer, that will simply kill the process.","[' How can the termination of an algorithm always be guaranteed?', ' What allows only for a maximum number of iterations and then stops with or without solution?', ' How can realtime systems achieve this?']","['by using a counter', 'a counter', 'by using a watchdog timer']"
2592,path planning,Completeness and performance,"Completeness can only be provided by a very rigorous mathematical correctness proof (often aided by tools and graph based methods) and should only be done by specialized experts if the application includes safety content. On the other hand, disproving completeness is easy, since one just needs to find one infinite loop or one wrong result returned. Formal Verification/Correctness of algorithms is a research field on its own. The correct setup of these test cases is a highly sophisticated task.
","Completeness can only be provided by a very rigorous mathematical correctness proof (often aided by tools and graph based methods) and should only be done by specialized experts if the application includes safety content. On the other hand, disproving completeness is easy, since one just needs to find one infinite loop or one wrong result returned.","[' What can only be provided by a very rigorous mathematical correctness proof?', ' Who should only be done by specialized experts if the application includes safety content?', ' Disproving completeness is easy since one just needs to find what?', ' How many infinite loops does one need to find?', ' How many wrong results can one get?']","['Completeness', 'Completeness', 'one infinite loop or one wrong result returned', 'one', 'one']"
2593,path planning,Completeness and performance,"Resolution completeness is the property that the planner is guaranteed to find a path if the resolution of an underlying grid is fine enough.  Most resolution complete planners are grid-based or interval-based.  The computational complexity of resolution complete planners is dependent on the number of points in the underlying grid, which is O(1/hd), where h is the resolution (the length of one side of a grid cell) and d is the configuration space dimension.
",Resolution completeness is the property that the planner is guaranteed to find a path if the resolution of an underlying grid is fine enough. Most resolution complete planners are grid-based or interval-based.,"[' What is the property that a planner is guaranteed to find a path if the resolution of an underlying grid is fine enough?', ' Most resolution complete planners are what?']","['Resolution completeness', 'grid-based or interval-based']"
2594,path planning,Completeness and performance,"Probabilistic completeness is the property that as more ""work"" is performed, the probability that the planner fails to find a path, if one exists, asymptotically approaches zero.  Several sample-based methods are probabilistically complete.  The performance of a probabilistically complete planner is measured by the rate of convergence. For practical applications, one usually uses this property, since it allows setting up the time-out for the watchdog based on an average convergence time.
","Probabilistic completeness is the property that as more ""work"" is performed, the probability that the planner fails to find a path, if one exists, asymptotically approaches zero. Several sample-based methods are probabilistically complete.","[' What is the property that as more work is performed, the probability that the planner fails to find a path, if one exists, asymptotically approaches zero?', ' How many sample-based methods are probabilistically complete?']","['Probabilistic completeness', 'Several']"
2595,path planning,Completeness and performance,"Incomplete planners do not always produce a feasible path when one exists (see first paragraph).  Sometimes incomplete planners do work well in practice, since they always stop after a guarantied time and allow other routines to take over.
","Incomplete planners do not always produce a feasible path when one exists (see first paragraph). Sometimes incomplete planners do work well in practice, since they always stop after a guarantied time and allow other routines to take over.","[' What do incomplete planners not always produce when one exists?', ' Incomplete planners always stop after what?']","['a feasible path', 'a guarantied time']"
2596,recurrent neural network,Summary,"A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed or undirected graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition. Recurrent neural networks are theoretically Turing complete and can run arbitrary programs to process arbitrary sequences of inputs.",A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed or undirected graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior.,"[' What is a recurrent neural network?', ' What type of behavior does an RNN exhibit?']","['connections between nodes form a directed or undirected graph along a temporal sequence', 'temporal dynamic']"
2597,recurrent neural network,Summary,"The term ""recurrent neural network"" is used to refer to the class of networks with an infinite impulse response, whereas ""convolutional neural network"" refers to the class of finite impulse response. Both classes of networks exhibit temporal dynamic behavior. A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network, while an infinite impulse recurrent network is a directed cyclic graph that can not be unrolled.
","The term ""recurrent neural network"" is used to refer to the class of networks with an infinite impulse response, whereas ""convolutional neural network"" refers to the class of finite impulse response. Both classes of networks exhibit temporal dynamic behavior.","[' What term is used to refer to the class of networks with infinite impulse response?', ' What term refers to a class of network with finite impulse reaction?', ' Both classes of networks exhibit what?']","['recurrent neural network', 'convolutional neural network', 'temporal dynamic behavior']"
2598,recurrent neural network,Summary,"Both finite impulse and infinite impulse recurrent networks can have additional stored states, and the storage can be under direct control by the neural network. The storage can also be replaced by another network or graph if that incorporates time delays or has feedback loops. Such controlled states are referred to as gated state or gated memory, and are part of long short-term memory networks (LSTMs) and gated recurrent units. This is also called Feedback Neural Network (FNN).
","Both finite impulse and infinite impulse recurrent networks can have additional stored states, and the storage can be under direct control by the neural network. The storage can also be replaced by another network or graph if that incorporates time delays or has feedback loops.","[' What can finite impulse and infinite impulse recurrent networks have?', ' What can be replaced by another network or graph if that incorporates time delays?']","['additional stored states', 'The storage']"
2599,recurrent neural network,History,"Recurrent neural networks were based on David Rumelhart's work in 1986.  Hopfield networks – a special kind of RNN – were (re-)discovered by John Hopfield in 1982. In 1993, a neural history compressor system solved a ""Very Deep Learning"" task that required more than 1000 subsequent layers in an RNN unfolded in time.",Recurrent neural networks were based on David Rumelhart's work in 1986. Hopfield networks – a special kind of RNN – were (re-)discovered by John Hopfield in 1982.,"["" When were recurrent neural networks based on David Rumelhart's work?"", ' When were Hopfield networks rediscovered by John Hopfield?']","['1986', '1982']"
2600,recurrent neural network,Related fields and models,"RNNs may behave chaotically. In such cases, dynamical systems theory may be used for analysis.
","RNNs may behave chaotically. In such cases, dynamical systems theory may be used for analysis.","[' RNNs may behave chaotically.', ' What theory may be used for analysis in such cases?']","['dynamical systems theory may be used for analysis', 'dynamical systems theory']"
2601,recurrent neural network,Related fields and models,"They are in fact recursive neural networks with a particular structure: that of a linear chain. Whereas recursive neural networks operate on any hierarchical structure, combining child representations into parent representations, recurrent neural networks operate on the linear progression of time, combining the previous time step and a hidden representation into the representation for the current time step.
","They are in fact recursive neural networks with a particular structure: that of a linear chain. Whereas recursive neural networks operate on any hierarchical structure, combining child representations into parent representations, recurrent neural networks operate on the linear progression of time, combining the previous time step and a hidden representation into the representation for the current time step.","[' What is the structure of a recursive neural network?', ' What do recurrent neural networks combine into parent representations?', ' combining the previous time step and a hidden representation into the representation for the current time step?']","['a linear chain', 'child representations', 'combining']"
2602,mathematical logic,Summary,"Mathematical logic is the study of formal logic within mathematics. Major subareas include model theory, proof theory, set theory, and recursion theory. Research in mathematical logic commonly addresses the mathematical properties of formal systems of logic such as their expressive or deductive power. However, it can also include uses of logic to characterize correct mathematical reasoning or to establish foundations of mathematics.
","Mathematical logic is the study of formal logic within mathematics. Major subareas include model theory, proof theory, set theory, and recursion theory.","[' What is the study of formal logic within mathematics?', ' What are the major subareas of mathematical logic?']","['Mathematical logic', 'model theory, proof theory, set theory, and recursion theory']"
2603,mathematical logic,Summary,"Since its inception, mathematical logic has both contributed to, and has been motivated by, the study of foundations of mathematics. This study began in the late 19th century with the development of axiomatic frameworks for geometry, arithmetic, and analysis. In the early 20th century it was shaped by David Hilbert's program to prove the consistency of foundational theories. Results of Kurt Gödel, Gerhard Gentzen, and others provided partial resolution to the program, and clarified the issues involved in proving consistency. Work in set theory showed that almost all ordinary mathematics can be formalized in terms of sets, although there are some theorems that cannot be proven in common axiom systems for set theory. Contemporary work in the foundations of mathematics often focuses on establishing which parts of mathematics can be formalized in particular formal systems (as in reverse mathematics) rather than trying to find theories in which all of mathematics can be developed.
","Since its inception, mathematical logic has both contributed to, and has been motivated by, the study of foundations of mathematics. This study began in the late 19th century with the development of axiomatic frameworks for geometry, arithmetic, and analysis.","[' In what century did mathematical logic begin?', ' What were axiomatic frameworks developed for?']","['19th', 'geometry, arithmetic, and analysis']"
2604,mathematical logic,Subfields and scope,"Additionally, sometimes the field of computational complexity theory is also included as part of mathematical logic. Each area has a distinct focus, although many techniques and results are shared among multiple areas. The borderlines amongst these fields, and the lines separating mathematical logic and other fields of mathematics, are not always sharp.  Gödel's incompleteness theorem marks not only a milestone in recursion theory and proof theory, but has also led to Löb's theorem in modal logic. The method of forcing is employed in set theory, model theory, and recursion theory, as well as in the study of intuitionistic mathematics.
","Additionally, sometimes the field of computational complexity theory is also included as part of mathematical logic. Each area has a distinct focus, although many techniques and results are shared among multiple areas.","[' What field is sometimes included as part of mathematical logic?', ' Each area has a distinct focus, although many techniques and results are shared among multiple areas?']","['computational complexity theory', 'computational complexity theory is also included as part of mathematical logic. Each']"
2605,mathematical logic,Subfields and scope,"The mathematical field of category theory uses many formal axiomatic methods, and includes the study of categorical logic, but category theory is not ordinarily considered a subfield of mathematical logic. Because of its applicability in diverse fields of mathematics, mathematicians including Saunders Mac Lane have proposed category theory as a foundational system for mathematics, independent of set theory. These foundations use toposes, which resemble generalized models of set theory that may employ classical or nonclassical logic.
","The mathematical field of category theory uses many formal axiomatic methods, and includes the study of categorical logic, but category theory is not ordinarily considered a subfield of mathematical logic. Because of its applicability in diverse fields of mathematics, mathematicians including Saunders Mac Lane have proposed category theory as a foundational system for mathematics, independent of set theory.","[' What mathematical field uses many formal axiomatic methods?', ' What is category theory not ordinarily considered a subfield of?', ' Which mathematicians have proposed category theory?', ' What has Saunders Mac Lane proposed as a foundational system for mathematics?']","['category theory', 'mathematical logic', 'Saunders Mac Lane', 'category theory']"
2606,mathematical logic,History,"Mathematical logic emerged in the mid-19th century as a subfield of mathematics, reflecting the confluence of two traditions: formal philosophical logic and mathematics.  ""Mathematical logic, also called  'logistic', 'symbolic logic', the 'algebra of logic', and, more recently, simply 'formal logic', is the set of logical theories elaborated in the course of the last [nineteenth] century with the aid of an artificial notation and a rigorously deductive method.""  Before this emergence, logic was studied with rhetoric, with calculationes, through the syllogism, and with philosophy.  The first half of the 20th century saw an explosion of fundamental results, accompanied by vigorous debate over the foundations of mathematics.
","Mathematical logic emerged in the mid-19th century as a subfield of mathematics, reflecting the confluence of two traditions: formal philosophical logic and mathematics. ""Mathematical logic, also called  'logistic', 'symbolic logic', the 'algebra of logic', and, more recently, simply 'formal logic', is the set of logical theories elaborated in the course of the last [nineteenth] century with the aid of an artificial notation and a rigorously deductive method.""","[' When did mathematical logic emerge as a subfield of mathematics?', ' What is the set of logical theories elaborated in the middle of the 19th century?', ' What is the set of logical theories elaborated in the course of the last 19th century with the aid of artificial notation and a rigorous deductive method?']","['mid-19th century', 'Mathematical logic', 'Mathematical logic']"
2607,mathematical logic,"Formal logical systems <span id=""Formal_logic""></span>","At its core, mathematical logic deals with mathematical concepts expressed using formal logical systems. These systems, though they differ in many details, share the common property of considering only expressions in a fixed formal language.  The systems of propositional logic and first-order logic are the most widely studied today, because of their applicability to foundations of mathematics and because of their desirable proof-theoretic properties.  Stronger classical logics such as second-order logic or infinitary logic are also studied, along with Non-classical logics such as intuitionistic logic.
","At its core, mathematical logic deals with mathematical concepts expressed using formal logical systems. These systems, though they differ in many details, share the common property of considering only expressions in a fixed formal language.","[' What does mathematical logic deal with at its core?', ' What do formal logical systems share?']","['mathematical concepts expressed using formal logical systems', 'the common property of considering only expressions in a fixed formal language']"
2608,mathematical logic,Set theory,"Set theory is the study of sets, which are abstract collections of objects. Many of the basic notions, such as ordinal and cardinal numbers, were developed informally by Cantor before formal axiomatizations of set theory were developed. The first such axiomatization, due to Zermelo, was extended slightly to become Zermelo–Fraenkel set theory (ZF), which is now the most widely used foundational theory for mathematics.
","Set theory is the study of sets, which are abstract collections of objects. Many of the basic notions, such as ordinal and cardinal numbers, were developed informally by Cantor before formal axiomatizations of set theory were developed.","[' What is set theory the study of?', ' What are sets?', ' Who developed many of the basic notions of set theory informally?']","['sets, which are abstract collections of objects', 'abstract collections of objects', 'Cantor']"
2609,mathematical logic,Set theory,"Other formalizations of set theory have been proposed, including von Neumann–Bernays–Gödel set theory (NBG), Morse–Kelley set theory (MK), and New Foundations (NF).  Of these, ZF, NBG, and MK are similar in describing a cumulative hierarchy of sets. New Foundations takes a different approach; it allows objects such as the set of all sets at the cost of restrictions on its set-existence axioms. The system of Kripke–Platek set theory is closely related to generalized recursion theory.
","Other formalizations of set theory have been proposed, including von Neumann–Bernays–Gödel set theory (NBG), Morse–Kelley set theory (MK), and New Foundations (NF). Of these, ZF, NBG, and MK are similar in describing a cumulative hierarchy of sets.","[' What is the acronym for von Neumann-Bernays-Gödel set theory?', ' What is NF?']","['NBG', 'New Foundations']"
2610,mathematical logic,Set theory,"Two famous statements in set theory are the axiom of choice and the continuum hypothesis. The axiom of choice, first stated by Zermelo, was proved independent of ZF by Fraenkel, but has come to be widely accepted by mathematicians.  It states that given a collection of nonempty sets there is a single set C that contains exactly one element from each set in the collection. The set C is said to ""choose"" one element from each set in the collection. While the ability to make such a choice is considered obvious by some, since each set in the collection is nonempty, the lack of a general, concrete rule by which the choice can be made renders the axiom nonconstructive. Stefan Banach and Alfred Tarski showed that the axiom of choice can be used to decompose a solid ball into a finite number of pieces which can then be rearranged, with no scaling, to make two solid balls of the original size. This theorem, known as the Banach–Tarski paradox, is one of many counterintuitive results of the axiom of choice.
","Two famous statements in set theory are the axiom of choice and the continuum hypothesis. The axiom of choice, first stated by Zermelo, was proved independent of ZF by Fraenkel, but has come to be widely accepted by mathematicians.","[' What are the two famous statements in set theory?', ' Who first stated the axiom of choice?', ' What was the first statement of the continuum hypothesis?', ' Which statement has come to be accepted by mathematicians?']","['axiom of choice and the continuum hypothesis', 'Zermelo', 'axiom of choice and the continuum hypothesis. The axiom of choice', 'The axiom of choice']"
2611,mathematical logic,Set theory,"The continuum hypothesis, first proposed as a conjecture by Cantor, was listed by David Hilbert as one of his 23 problems in 1900. Gödel showed that the continuum hypothesis cannot be disproven from the axioms of Zermelo–Fraenkel set theory (with or without the axiom of choice), by developing the constructible universe of set theory in which the continuum hypothesis must hold. In 1963, Paul Cohen showed that the continuum hypothesis cannot be proven from the axioms of Zermelo–Fraenkel set theory. This independence result did not completely settle Hilbert's question, however, as it is possible that new axioms for set theory could resolve the hypothesis. Recent work along these lines has been conducted by W. Hugh Woodin, although its importance is not yet clear.","The continuum hypothesis, first proposed as a conjecture by Cantor, was listed by David Hilbert as one of his 23 problems in 1900. Gödel showed that the continuum hypothesis cannot be disproven from the axioms of Zermelo–Fraenkel set theory (with or without the axiom of choice), by developing the constructible universe of set theory in which the continuum hypothesis must hold.","[' Who first proposed the continuum hypothesis?', ' How many problems did David Hilbert list as a problem in 1900?', ' What did Gödel show that cannot be disproved from the axioms of Zermelo-Fraenkel set theory?', ' What is the constructible universe of set theory?']","['Cantor', '23', 'the continuum hypothesis', 'the continuum hypothesis']"
2612,mathematical logic,Set theory,"Contemporary research in set theory includes the study of large cardinals and determinacy.  Large cardinals are cardinal numbers with particular properties so strong that the existence of such cardinals cannot be proved in ZFC. The existence of the smallest large cardinal typically studied, an inaccessible cardinal, already implies the consistency of ZFC.  Despite the fact that large cardinals have extremely high cardinality, their existence has many ramifications for the structure of the real line.  Determinacy refers to the possible existence of winning strategies for certain two-player games (the games are said to be determined). The existence of these strategies implies structural properties of the real line and other Polish spaces.
",Contemporary research in set theory includes the study of large cardinals and determinacy. Large cardinals are cardinal numbers with particular properties so strong that the existence of such cardinals cannot be proved in ZFC.,"[' What is the study of large cardinals and determinacy?', ' What are cardinal numbers with particular properties so strong that the existence of such cards cannot be proved in ZFC?']","['set theory', 'Large cardinals']"
2613,mathematical logic,Model theory,"Model theory studies the models of various formal theories.  Here a theory is a set of formulas in a particular formal logic and signature, while a model is a structure that gives a concrete interpretation of the theory. Model theory is closely related to universal algebra and algebraic geometry, although the methods of model theory focus more on logical considerations than those fields.
","Model theory studies the models of various formal theories. Here a theory is a set of formulas in a particular formal logic and signature, while a model is a structure that gives a concrete interpretation of the theory.","[' What does model theory study?', ' What is a theory a set of formulas in a particular formal logic and signature?', ' A structure that gives a concrete interpretation of the theory?']","['the models of various formal theories', 'Model theory', 'model']"
2614,mathematical logic,Model theory,"The method of quantifier elimination can be used to show that definable sets in particular theories cannot be too complicated. Tarski established quantifier elimination for real-closed fields, a result which also shows the theory of the field of real numbers is decidable. He also noted that his methods were equally applicable to algebraically closed fields of arbitrary characteristic. A modern subfield developing from this is concerned with o-minimal structures.
","The method of quantifier elimination can be used to show that definable sets in particular theories cannot be too complicated. Tarski established quantifier elimination for real-closed fields, a result which also shows the theory of the field of real numbers is decidable.","["" What method can be used to show that definable sets in theories can't be too complicated?"", ' Tarski established quantifier elimination for what field?', ' What result shows that the theory of the field of real numbers is decidable?']","['quantifier elimination', 'real-closed fields', 'quantifier elimination']"
2615,mathematical logic,Model theory,"Morley's categoricity theorem, proved by Michael D. Morley, states that if a first-order theory in a countable language is categorical in some uncountable cardinality, i.e. all models of this cardinality are isomorphic, then it is categorical in all uncountable cardinalities.
","Morley's categoricity theorem, proved by Michael D. Morley, states that if a first-order theory in a countable language is categorical in some uncountable cardinality, i.e. all models of this cardinality are isomorphic, then it is categorical in all uncountable cardinalities.","["" Who proved Morley's categoricity theorem?"", ' What states that if a first-order theory in a countable language is categorical in some uncountable cardinality, it is categorically in all uncounted cardinalities?']","['Michael D. Morley', ""Morley's categoricity theorem""]"
2616,mathematical logic,Model theory,"A trivial consequence of the continuum hypothesis is that a complete theory with less than continuum many nonisomorphic countable models can have only countably many. Vaught's conjecture, named after Robert Lawson Vaught, says that this is true even independently of the continuum hypothesis.  Many special cases of this conjecture have been established.
","A trivial consequence of the continuum hypothesis is that a complete theory with less than continuum many nonisomorphic countable models can have only countably many. Vaught's conjecture, named after Robert Lawson Vaught, says that this is true even independently of the continuum hypothesis.","[' What is a trivial consequence of the continuum hypothesis?', "" What is Vaught's conjecture named after?""]","['a complete theory with less than continuum many nonisomorphic countable models can have only countably many', 'Robert Lawson Vaught']"
2617,mathematical logic,Recursion theory,"Recursion theory, also called computability theory, studies the properties of computable functions and the Turing degrees, which divide the uncomputable functions into sets that have the same level of uncomputability.  Recursion theory also includes the study of generalized computability and definability.  Recursion theory grew from the work of Rózsa Péter, Alonzo Church and Alan Turing in the 1930s, which was greatly extended by Kleene and Post in the 1940s.","Recursion theory, also called computability theory, studies the properties of computable functions and the Turing degrees, which divide the uncomputable functions into sets that have the same level of uncomputability. Recursion theory also includes the study of generalized computability and definability.","[' What is another name for computability theory?', ' Recursion theory studies the properties of computable functions and what else?', ' What divides the uncomputable functions into sets that have the same level of uncompatability?']","['Recursion theory', 'Turing degrees', 'Turing degrees']"
2618,mathematical logic,Recursion theory,"Classical recursion theory focuses on the computability of functions from the natural numbers to the natural numbers. The fundamental results establish a robust, canonical class of computable functions with numerous independent, equivalent characterizations using Turing machines, λ calculus, and other systems.  More advanced results concern the structure of the Turing degrees and the lattice of recursively enumerable sets.
","Classical recursion theory focuses on the computability of functions from the natural numbers to the natural numbers. The fundamental results establish a robust, canonical class of computable functions with numerous independent, equivalent characterizations using Turing machines, λ calculus, and other systems.","[' Classical recursion theory focuses on the computability of functions from natural numbers to what?', ' The fundamental results establish a robust, canonical class of computable functions with numerous independent, equivalent characterizations using Turing machines, <unk> calculus and what other system?']","['natural numbers', 'λ']"
2619,mathematical logic,Recursion theory,"Generalized recursion theory extends the ideas of recursion theory to computations that are no longer necessarily finite. It includes the study of computability in higher types as well as areas such as hyperarithmetical theory and α-recursion theory.
",Generalized recursion theory extends the ideas of recursion theory to computations that are no longer necessarily finite. It includes the study of computability in higher types as well as areas such as hyperarithmetical theory and α-recursion theory.,"[' What extends the ideas of recursion theory to computations that are no longer necessarily finite?', ' What includes the study of computability in higher types as well as areas such as hyperarithmetical theory?']","['Generalized recursion theory', 'Generalized recursion theory']"
2620,mathematical logic,Proof theory and constructive mathematics,"Proof theory is the study of formal proofs in various logical deduction systems. These proofs are represented as formal mathematical objects, facilitating their analysis by mathematical techniques.  Several deduction systems are commonly considered, including Hilbert-style deduction systems, systems of natural deduction, and the sequent calculus developed by Gentzen.
","Proof theory is the study of formal proofs in various logical deduction systems. These proofs are represented as formal mathematical objects, facilitating their analysis by mathematical techniques.","[' What is the study of formal proofs in various logical deduction systems?', ' Proofs are represented as what?']","['Proof theory', 'formal mathematical objects']"
2621,mathematical logic,Proof theory and constructive mathematics,"The study of constructive mathematics, in the context of mathematical logic, includes the study of systems in non-classical logic such as intuitionistic logic, as well as the study of predicative systems.  An early proponent of predicativism was Hermann Weyl, who showed it is possible to develop a large part of real analysis using only predicative methods.","The study of constructive mathematics, in the context of mathematical logic, includes the study of systems in non-classical logic such as intuitionistic logic, as well as the study of predicative systems. An early proponent of predicativism was Hermann Weyl, who showed it is possible to develop a large part of real analysis using only predicative methods.","[' What type of logic is intuitionistic?', ' Who was an early proponent of predicativism?', ' What did Hermann Weyl show it is possible to develop?', ' How was it possible to develop a large part of real analysis using only predicative methods?']","['non-classical', 'Hermann Weyl', 'a large part of real analysis', 'Hermann Weyl']"
2622,mathematical logic,Proof theory and constructive mathematics,"Because proofs are entirely finitary, whereas truth in a structure is not, it is common for work in constructive mathematics to emphasize provability.   The relationship between provability in classical (or nonconstructive) systems and provability in intuitionistic (or constructive, respectively) systems is of particular interest.  Results such as the Gödel–Gentzen negative translation show that it is possible to embed (or translate) classical logic into intuitionistic logic, allowing some properties about intuitionistic proofs to be transferred back to classical proofs.
","Because proofs are entirely finitary, whereas truth in a structure is not, it is common for work in constructive mathematics to emphasize provability. The relationship between provability in classical (or nonconstructive) systems and provability in intuitionistic (or constructive, respectively) systems is of particular interest.","[' What are proofs entirely finitary?', ' What is common for work in constructive mathematics to emphasize?', ' The relationship between provability in classical and intuitionistic systems is of what interest?']","['truth in a structure is not', 'provability', 'particular interest']"
2623,mathematical logic,Applications,"""Mathematical logic has been successfully applied not only to mathematics and its foundations (G. Frege, B. Russell, D. Hilbert, P. Bernays, H. Scholz, R. Carnap, S. Lesniewski, T. Skolem), but also to physics (R. Carnap, A. Dittrich, B. Russell, C. E. Shannon, A. N. Whitehead, H. Reichenbach, P. Fevrier), to biology (J. H. Woodger, A. Tarski), to psychology (F. B. Fitch, C. G. Hempel), to law and morals (K. Menger, U. Klug, P. Oppenheim), to economics (J. Neumann, O. Morgenstern), to practical questions (E. C. Berkeley, E. Stamm), and even to metaphysics (J. [Jan] Salamucha, H. Scholz, J. M. Bochenski).  Its applications to the history of logic have proven extremely fruitful (J. Lukasiewicz, H. Scholz, B. Mates, A. Becker, E. Moody, J. Salamucha, K. Duerr, Z. Jordan, P. Boehner, J. M. Bochenski, S. [Stanislaw] T. Schayer, D. Ingalls)."" ""Applications have also been made to theology (F. Drewnowski, J. Salamucha, I. Thomas).""","""Mathematical logic has been successfully applied not only to mathematics and its foundations (G. Frege, B. Russell, D. Hilbert, P. Bernays, H. Scholz, R. Carnap, S. Lesniewski, T. Skolem), but also to physics (R. Carnap, A. Dittrich, B. Russell, C. E. Shannon, A. N. Whitehead, H. Reichenbach, P. Fevrier), to biology (J. H. Woodger, A. Tarski), to psychology (F. B. Fitch, C. G. Hempel), to law and morals (K. Menger, U. Klug, P. Oppenheim), to economics (J. Neumann, O. Morgenstern), to practical questions (E. C. Berkeley, E. Stamm), and even to metaphysics (J. [Jan] Salamucha, H. Scholz, J. M. Bochenski).","[' What has been successfully applied not only to mathematics and its foundations, but also to physics?', ' E. Shannon, A. N. Whitehead, H. Reichenbach, P. Fevrier), to biology, to psychology, to law and morals, to economics, and practical questions, to what?', ' E. C. Berkeley, E. Stamm, and O. Morgenstern are experts in what field?', ' E. C. Berkeley, E. Stamm, J. [Jan] Salamucha, H. Scholz, and J. M. Bochenski) and metaphysics?']","['Mathematical logic', 'physics', 'practical questions', 'practical questions']"
2624,mathematical logic,Connections with computer science,"The study of computability theory in computer science is closely related to the study of computability in mathematical logic.  There is a difference of emphasis, however.  Computer scientists often focus on concrete programming languages and feasible computability, while researchers in mathematical logic often focus on computability as a theoretical concept and on noncomputability.
","The study of computability theory in computer science is closely related to the study of computability in mathematical logic. There is a difference of emphasis, however.",[' The study of computability theory in computer science is closely related to the study of what?'],['computability in mathematical logic']
2625,mathematical logic,Connections with computer science,"The theory of semantics of programming languages is related to model theory, as is program verification (in particular, model checking). The Curry–Howard correspondence between proofs and programs relates to proof theory, especially intuitionistic logic. Formal calculi such as the lambda calculus and combinatory logic are now studied as idealized programming languages.
","The theory of semantics of programming languages is related to model theory, as is program verification (in particular, model checking). The Curry–Howard correspondence between proofs and programs relates to proof theory, especially intuitionistic logic.","[' What is the theory of semantics of programming languages related to?', ' What is program verification?', ' The Curry-Howard correspondence between proofs and programs relates to what theory?']","['model theory', 'model checking', 'proof theory']"
2626,mathematical logic,Connections with computer science,"Descriptive complexity theory relates logics to computational complexity. The first significant result in this area, Fagin's theorem (1974) established that NP is precisely the set of languages expressible by sentences of existential second-order logic.
","Descriptive complexity theory relates logics to computational complexity. The first significant result in this area, Fagin's theorem (1974) established that NP is precisely the set of languages expressible by sentences of existential second-order logic.","[' Descriptive complexity theory relates logics to what?', ' What is the first significant result in this area?', "" Fagin's theorem established that NP is precisely the set of languages expressible by sentences of what kind of logic?""]","['computational complexity', ""Fagin's theorem"", 'existential second-order']"
2627,mathematical logic,Foundations of mathematics,"In the 19th century, mathematicians became aware of logical gaps and inconsistencies in their field. It was shown that Euclid's axioms for geometry, which had been taught for centuries as an example of the axiomatic method, were incomplete. The use of infinitesimals, and the very definition of function, came into question in analysis, as pathological examples such as Weierstrass' nowhere-differentiable continuous function were discovered.
","In the 19th century, mathematicians became aware of logical gaps and inconsistencies in their field. It was shown that Euclid's axioms for geometry, which had been taught for centuries as an example of the axiomatic method, were incomplete.","[' When did mathematicians become aware of logical gaps and inconsistencies in their field?', "" What was shown that Euclid's axioms for geometry were incomplete?""]","['19th century', 'In the 19th century']"
2628,mathematical logic,Foundations of mathematics,"Cantor's study of arbitrary infinite sets also drew criticism. Leopold Kronecker famously stated ""God made the integers; all else is the work of man,"" endorsing a return to the study of finite, concrete objects in mathematics. Although Kronecker's argument was carried forward by constructivists in the 20th century, the mathematical community as a whole rejected them. David Hilbert argued in favor of the study of the infinite, saying ""No one shall expel us from the Paradise that Cantor has created.""
","Cantor's study of arbitrary infinite sets also drew criticism. Leopold Kronecker famously stated ""God made the integers; all else is the work of man,"" endorsing a return to the study of finite, concrete objects in mathematics.","[' Leopold Kronecker said ""God made the integers; all else is the work of what?""']",['man']
2629,mathematical logic,Foundations of mathematics,"Mathematicians began to search for axiom systems that could be used to formalize large parts of mathematics. In addition to removing ambiguity from previously naive terms such as function, it was hoped that this axiomatization would allow for consistency proofs.  In the 19th century, the main method of proving the consistency of a set of axioms was to provide a model for it. Thus, for example, non-Euclidean geometry can be proved consistent by defining point to mean a point on a fixed sphere and line to mean a great circle on the sphere. The resulting structure, a model of elliptic geometry, satisfies the axioms of plane geometry except the parallel postulate.
","Mathematicians began to search for axiom systems that could be used to formalize large parts of mathematics. In addition to removing ambiguity from previously naive terms such as function, it was hoped that this axiomatization would allow for consistency proofs.","[' Mathematicians began to search for axiom systems that could be used to formalize large parts of what?', ' Aside from removing ambiguity from previously naive terms, what other term was removed?']","['mathematics', 'function']"
2630,mathematical logic,Foundations of mathematics,"With the development of formal logic, Hilbert asked whether it would be possible to prove that an axiom system is consistent by analyzing the structure of possible proofs in the system, and showing through this analysis that it is impossible to prove a contradiction. This idea led to the study of proof theory. Moreover, Hilbert proposed that the analysis should be entirely concrete, using the term finitary to refer to the methods he would allow but not precisely defining them. This project, known as Hilbert's program, was seriously affected by Gödel's incompleteness theorems, which show that the consistency of formal theories of arithmetic cannot be established using methods formalizable in those theories. Gentzen showed that it is possible to produce a proof of the consistency of arithmetic in a finitary system augmented with axioms of transfinite induction, and the techniques he developed to do so were seminal in proof theory.
","With the development of formal logic, Hilbert asked whether it would be possible to prove that an axiom system is consistent by analyzing the structure of possible proofs in the system, and showing through this analysis that it is impossible to prove a contradiction. This idea led to the study of proof theory.","[' Hilbert asked if it would be possible to prove that an axiom system is consistent by analyzing the structure of possible proofs in the system?', ' Hilbert showed through this analysis that it is impossible to prove what?', ' To prove a contradiction led to the study of what?']","['Hilbert', 'a contradiction', 'proof theory']"
2631,mathematical logic,Foundations of mathematics,"A second thread in the history of foundations of mathematics involves nonclassical logics and constructive mathematics. The study of constructive mathematics includes many different programs with various definitions of constructive. At the most accommodating end, proofs in ZF set theory that do not use the axiom of choice are called constructive by many mathematicians. More limited versions of constructivism limit themselves to natural numbers, number-theoretic functions, and sets of natural numbers (which can be used to represent real numbers, facilitating the study of mathematical analysis). A common idea is that a concrete means of computing the values of the function must be known before the function itself can be said to exist. 
",A second thread in the history of foundations of mathematics involves nonclassical logics and constructive mathematics. The study of constructive mathematics includes many different programs with various definitions of constructive.,"[' What is a second thread in the history of foundations of mathematics?', ' Nonclassical logics and constructive mathematics are what?', ' The study of constructive mathematics includes many different programs with various definitions of constructive?']","['nonclassical logics and constructive mathematics', 'A second thread in the history of foundations of mathematics', 'mathematics']"
2632,mathematical logic,Foundations of mathematics,"In the early 20th century, Luitzen Egbertus Jan Brouwer founded intuitionism as a part of philosophy of mathematics . This philosophy, poorly understood at first, stated that in order for a mathematical statement to be true to a mathematician, that person must be able to intuit the statement, to not only believe its truth but understand the reason for its truth. A consequence of this definition of truth was the rejection of the law of the excluded middle, for there are statements that, according to Brouwer, could not be claimed to be true while their negations also could not be claimed true.  Brouwer's philosophy was influential, and the cause of bitter disputes among prominent mathematicians. Later, Kleene and Kreisel would study formalized versions of intuitionistic logic (Brouwer rejected formalization, and presented his work in unformalized natural language). With the advent of the BHK interpretation and Kripke models, intuitionism became easier to reconcile with classical mathematics.
","In the early 20th century, Luitzen Egbertus Jan Brouwer founded intuitionism as a part of philosophy of mathematics . This philosophy, poorly understood at first, stated that in order for a mathematical statement to be true to a mathematician, that person must be able to intuit the statement, to not only believe its truth but understand the reason for its truth.","[' In what century did Luitzen Egbertus Jan Brouwer start intuitionism?', ' What philosophy did Luiten Egebertus J Jan Brouer start?', ' Intuitionism stated that a mathematician must be able to what?', ' What must a person be able to intuit?']","['20th', 'intuitionism', 'intuit the statement', 'mathematical statement']"
2633,gesture recognition,Summary,"Gesture recognition is a topic in computer science and language technology with the goal of interpreting human gestures via mathematical algorithms. It is a subdiscipline of computer vision.  Gestures can originate from any bodily motion or state but commonly originate from the face or hand.  Current focuses in the field include emotion recognition from face and hand gesture recognition. Users can use simple gestures to control or interact with devices without physically touching them. Many approaches have been made using cameras and computer vision algorithms to interpret sign language. However, the identification and recognition of posture, gait, proxemics, and human behaviors is also the subject of gesture recognition techniques.
Gesture recognition can be seen as a way for computers to begin to understand human body language, thus building a richer bridge between machines and humans than primitive text user interfaces or even GUIs (graphical user interfaces), which still limit the majority of input to keyboard and mouse and interact naturally without any mechanical devices.
",Gesture recognition is a topic in computer science and language technology with the goal of interpreting human gestures via mathematical algorithms. It is a subdiscipline of computer vision.,"[' Gesture recognition is a topic in computer science and language technology with the goal of interpreting human gestures via what kind of algorithms?', ' What subdiscipline of computer vision is gesture recognition?']","['mathematical', 'Gesture recognition']"
2634,gesture recognition,Overview,"Gesture recognition and pen computing:
Pen computing reduces the hardware impact of a system and also increases the range of physical world objects usable for control beyond traditional digital objects like keyboards and mice. Such implementations could enable a new range of hardware that does not require monitors. This idea may lead to the creation of holographic display. The term gesture recognition has been used to refer more narrowly to non-text-input handwriting symbols, such as inking on a graphics tablet, multi-touch gestures, and mouse gesture recognition. This is computer interaction through the drawing of symbols with a pointing device cursor. (see Pen computing)
","Gesture recognition and pen computing:
Pen computing reduces the hardware impact of a system and also increases the range of physical world objects usable for control beyond traditional digital objects like keyboards and mice. Such implementations could enable a new range of hardware that does not require monitors.","[' Gesture recognition and pen computing: Pen computing reduces the hardware impact of what?', ' Pen computing increases the range of physical world objects usable for control beyond traditional digital objects like keyboards and what else?', "" Implementations of pen computing could enable a new range of hardware that doesn't require monitors?""]","['a system', 'mice', 'Pen computing']"
2635,gesture recognition,Gesture types,"In computer interfaces, two types of gestures are distinguished: We consider online gestures, which can also be regarded as direct manipulations like scaling and rotating. In contrast, offline gestures are usually processed after the interaction is finished; e. g. a circle is drawn to activate a context menu.
","In computer interfaces, two types of gestures are distinguished: We consider online gestures, which can also be regarded as direct manipulations like scaling and rotating. In contrast, offline gestures are usually processed after the interaction is finished; e. g. a circle is drawn to activate a context menu.","[' What two types of gestures are distinguished in computer interfaces?', ' What are online gestures also regarded as?', ' When are offline gestures processed?']","['online gestures', 'direct manipulations', 'after the interaction is finished']"
2636,gesture recognition,Touchless interface,"Touchless user interface is an emerging type of technology in relation to gesture control. Touchless user interface (TUI) is the process of commanding the computer via body motion and gestures without touching a keyboard, mouse, or screen.  Touchless interface in addition to gesture controls are becoming widely popular as they provide the abilities to interact with devices without physically touching them.
","Touchless user interface is an emerging type of technology in relation to gesture control. Touchless user interface (TUI) is the process of commanding the computer via body motion and gestures without touching a keyboard, mouse, or screen.","[' What type of technology is emerging in relation to gesture control?', ' What is the process of commanding the computer via body motion and gestures without touching a keyboard, mouse, or screen?']","['Touchless user interface', 'Touchless user interface']"
2637,gesture recognition,Input devices,"The ability to track a person's movements and determine what gestures they may be performing can be achieved through various tools. The kinetic user interfaces (KUIs) are an emerging type of user interfaces that allow users to interact with computing devices through the motion of objects and bodies. Examples of KUIs include tangible user interfaces and motion-aware games such as Wii and Microsoft's Kinect, and other interactive projects.",The ability to track a person's movements and determine what gestures they may be performing can be achieved through various tools. The kinetic user interfaces (KUIs) are an emerging type of user interfaces that allow users to interact with computing devices through the motion of objects and bodies.,[' What type of user interfaces allow users to interact with computing devices through the motion of objects?'],['kinetic user interfaces']
2638,gesture recognition,Input devices,"Another example of this is mouse gesture trackings, where the motion of the mouse is correlated to a symbol being drawn by a person's hand which can study changes in acceleration over time to represent gestures.  The software also compensates for human tremor and inadvertent movement.  The sensors of these smart light emitting cubes can be used to sense hands and fingers as well as other objects nearby, and can be used to process data. Most applications are in music and sound synthesis, but can be applied to other fields.
","Another example of this is mouse gesture trackings, where the motion of the mouse is correlated to a symbol being drawn by a person's hand which can study changes in acceleration over time to represent gestures. The software also compensates for human tremor and inadvertent movement.","[' What is another example of mouse gesture tracking?', ' What can study changes in acceleration over time to represent gestures?']","[""the motion of the mouse is correlated to a symbol being drawn by a person's hand"", 'mouse gesture trackings']"
2639,gesture recognition,Algorithms,"Depending on the type of the input data, the approach for interpreting a gesture could be done in different ways. However, most of the techniques rely on key pointers represented in a 3D coordinate system. Based on the relative motion of these, the gesture can be detected with a high accuracy, depending on the quality of the input and the algorithm's approach.
In order to interpret movements of the body, one has to classify them according to common properties and the message the movements may express. For example, in sign language each gesture represents a word or phrase.
","Depending on the type of the input data, the approach for interpreting a gesture could be done in different ways. However, most of the techniques rely on key pointers represented in a 3D coordinate system.","[' What type of data determines the approach for interpreting a gesture?', ' What do most techniques rely on?', ' How are key pointers represented?']","['input', 'key pointers represented in a 3D coordinate system', '3D coordinate system']"
2640,gesture recognition,Algorithms,"Some literature differentiates 2 different approaches in gesture recognition: a 3D model based and an appearance-based. The foremost method makes use of 3D information of key elements of the body parts in order to obtain several important parameters, like palm position or joint angles. On the other hand, Appearance-based systems use images or videos for direct interpretation.
","Some literature differentiates 2 different approaches in gesture recognition: a 3D model based and an appearance-based. The foremost method makes use of 3D information of key elements of the body parts in order to obtain several important parameters, like palm position or joint angles.","[' How many different approaches does literature differentiate in gesture recognition?', ' What is the foremost method of gesture recognition based on?', ' How does 3D information of key elements of the body parts help?']","['2', '3D information of key elements of the body parts', 'to obtain several important parameters']"
2641,gesture recognition,Challenges,"There are many challenges associated with the accuracy and usefulness of gesture recognition software. For image-based gesture recognition there are limitations on the equipment used and image noise. Images or video may not be under consistent lighting, or in the same location. Items in the background or distinct features of the users may make recognition more difficult.
",There are many challenges associated with the accuracy and usefulness of gesture recognition software. For image-based gesture recognition there are limitations on the equipment used and image noise.,"[' What are some challenges associated with the accuracy and usefulness of gesture recognition software?', ' For image-based gesture recognition there are limitations on the equipment used and what else?']","['limitations on the equipment used and image noise', 'image noise']"
2642,gesture recognition,Challenges,"The variety of implementations for image-based gesture recognition may also cause issue for viability of the technology to general usage. For example, an algorithm calibrated for one camera may not work for a different camera.  The amount of background noise also causes tracking and recognition difficulties, especially when occlusions (partial and full) occur.  Furthermore, the distance from the camera, and the camera's resolution and quality, also cause variations in recognition accuracy.
","The variety of implementations for image-based gesture recognition may also cause issue for viability of the technology to general usage. For example, an algorithm calibrated for one camera may not work for a different camera.","[' What may cause issue for viability of the technology to general usage?', ' An algorithm calibrated for one camera may not work for a different camera?']","['The variety of implementations for image-based gesture recognition', 'image-based gesture recognition']"
2643,decision making,Summary,"In psychology, decision-making (also spelled decision making and decisionmaking) is regarded as the cognitive process resulting in the selection of a belief or a course of action among several possible alternative options. It could be either rational or irrational. Decision-making process is a reasoning process based on assumptions of values, preferences and beliefs of the decision-maker. Every decision-making process produces a final choice, which may or may not prompt action.
","In psychology, decision-making (also spelled decision making and decisionmaking) is regarded as the cognitive process resulting in the selection of a belief or a course of action among several possible alternative options. It could be either rational or irrational.","[' What is the cognitive process resulting in the selection of a belief or a course of action among several possible alternative options?', ' Decision-making could be either rational or what?']","['decision-making', 'irrational']"
2644,decision making,Overview,"Decision-making can be regarded as a problem-solving activity yielding a solution deemed to be optimal, or at least satisfactory. It is therefore a process which can be more or less rational or irrational and can be based on explicit or tacit knowledge and beliefs. Tacit knowledge is often used to fill the gaps in complex decision-making processes. Usually, both of these types of knowledge, tacit and explicit, are used together in the decision-making process.
","Decision-making can be regarded as a problem-solving activity yielding a solution deemed to be optimal, or at least satisfactory. It is therefore a process which can be more or less rational or irrational and can be based on explicit or tacit knowledge and beliefs.","[' Decision-making can be regarded as a problem-solving activity yielding what?', ' What can be more or less rational or irrational?']","['a solution deemed to be optimal, or at least satisfactory', 'Decision-making']"
2645,decision making,Overview,"A major part of decision-making, involves the analysis of a finite set of alternatives described in terms of evaluative criteria. Then the task might be to rank these alternatives in terms of how attractive they are to the decision-maker(s) when all the criteria are considered simultaneously. Another task might be to find the best alternative or to determine the relative total priority of each alternative (for instance, if alternatives represent projects competing for funds) when all the criteria are considered simultaneously. Solving such problems is the focus of multiple-criteria decision analysis (MCDA). This area of decision-making, although very old, has attracted the interest of many researchers and practitioners and is still highly debated as there are many MCDA methods which may yield very different results when they are applied on exactly the same data. This leads to the formulation of a decision-making paradox. Logical decision-making is an important part of all science-based professions, where specialists apply their knowledge in a given area to make informed decisions. For example, medical decision-making often involves a diagnosis and the selection of appropriate treatment. But naturalistic decision-making research shows that in situations with higher time pressure, higher stakes, or increased ambiguities, experts may use intuitive decision-making rather than structured approaches. They may follow a recognition primed decision that fits their experience, and arrive at a course of action without weighing alternatives.","A major part of decision-making, involves the analysis of a finite set of alternatives described in terms of evaluative criteria. Then the task might be to rank these alternatives in terms of how attractive they are to the decision-maker(s) when all the criteria are considered simultaneously.","[' What part of decision-making involves the analysis of a finite set of alternatives described in terms of evaluative criteria?', ' What might the task be to rank the alternatives according to when all the criteria are considered simultaneously?']","['major', 'how attractive they are to the decision-maker(s']"
2646,decision making,Overview,"The decision-maker's environment can play a part in the decision-making process. For example, environmental complexity is a factor that influences cognitive function. A complex environment is an environment with a large number of different possible states which come and go over time. Studies done at the University of Colorado have shown that more complex environments correlate with higher cognitive function, which means that a decision can be influenced by the location. One experiment measured complexity in a room by the number of small objects and appliances present; a simple room had less of those things. Cognitive function was greatly affected by the higher measure of environmental complexity, making it easier to think about the situation and make a better decision.","The decision-maker's environment can play a part in the decision-making process. For example, environmental complexity is a factor that influences cognitive function.","[' What can play a part in the decision-making process?', ' What is a factor that influences cognitive function?']","[""The decision-maker's environment"", 'environmental complexity']"
2647,decision making,Problem solving vs. decision making,"It is important to differentiate between problem solving, or problem analysis, and decision-making. Problem solving is the process of investigating the given information and finding all possible solutions through invention or discovery. Traditionally, it is argued that problem solving is a step towards decision making, so that the information gathered in that process may be used towards decision-making.","It is important to differentiate between problem solving, or problem analysis, and decision-making. Problem solving is the process of investigating the given information and finding all possible solutions through invention or discovery.",[' What is the process of investigating the given information and finding all possible solutions through invention or discovery?'],['Problem solving']
2648,decision making,Neuroscience,"Decision-making is a region of intense study in the fields of systems neuroscience, and cognitive neuroscience. Several brain structures, including the anterior cingulate cortex (ACC), orbitofrontal cortex, and the overlapping ventromedial prefrontal cortex are believed to be involved in decision-making processes. A neuroimaging study found distinctive patterns of neural activation in these regions depending on whether decisions were made on the basis of perceived personal volition or following directions from someone else. Patients with damage to the ventromedial prefrontal cortex have difficulty making advantageous decisions.","Decision-making is a region of intense study in the fields of systems neuroscience, and cognitive neuroscience. Several brain structures, including the anterior cingulate cortex (ACC), orbitofrontal cortex, and the overlapping ventromedial prefrontal cortex are believed to be involved in decision-making processes.","[' What is a region of intense study in the fields of systems neuroscience and cognitive neuroscience?', ' What are believed to be involved in decision-making processes?']","['Decision-making', 'anterior cingulate cortex (ACC), orbitofrontal cortex, and the overlapping ventromedial prefrontal cortex']"
2649,decision making,Neuroscience,"A common laboratory paradigm for studying neural decision-making is the two-alternative forced choice task (2AFC), in which a subject has to choose between two alternatives within a certain time. A study of a two-alternative forced choice task involving rhesus monkeys found that neurons in the parietal cortex not only represent the formation of a decision but also signal the degree of certainty (or ""confidence"") associated with the decision. A 2012 study found that rats and humans can optimally accumulate incoming sensory evidence, to make statistically optimal decisions.  Another study found that lesions to the ACC in the macaque resulted in impaired decision-making in the long run of reinforcement guided tasks suggesting that the ACC may be involved in evaluating past reinforcement information and guiding future action. It has recently been argued that the development of formal frameworks will allow neuroscientists to study richer and more naturalistic paradigms than simple 2AFC decision tasks; in particular, such decisions may involve planning and information search across temporally extended environments.","A common laboratory paradigm for studying neural decision-making is the two-alternative forced choice task (2AFC), in which a subject has to choose between two alternatives within a certain time. A study of a two-alternative forced choice task involving rhesus monkeys found that neurons in the parietal cortex not only represent the formation of a decision but also signal the degree of certainty (or ""confidence"") associated with the decision.","[' What is a common laboratory paradigm for studying neural decision-making?', ' What is the two-alternative forced choice task?', ' How many alternatives must a subject choose within a certain time?', ' A study of what monkeys found that neurons in the parietal cortex represent?', ' What do neurons in the parietal cortex represent?']","['two-alternative forced choice task', '2AFC', 'two', 'rhesus', 'the formation of a decision']"
2650,decision making,Decision-making techniques,"Decision-making techniques can be separated into two broad categories: group decision-making techniques and individual decision-making techniques. Individual decision-making techniques can also often be applied by a group.
",Decision-making techniques can be separated into two broad categories: group decision-making techniques and individual decision-making techniques. Individual decision-making techniques can also often be applied by a group.,"[' How many broad categories can decision-making techniques be divided into?', ' What can be applied by a group?']","['two', 'Individual decision-making techniques']"
2651,decision making,Rational and irrational,"In economics, it is thought that if humans are rational and free to make their own decisions, then they would behave according to rational choice theory.: 368–370  Rational choice theory says that a person consistently makes choices that lead to the best situation for themselves, taking into account all available considerations including costs and benefits; the rationality of these considerations is from the point of view of the person themselves, so a decision is not irrational just because someone else finds it questionable.
","In economics, it is thought that if humans are rational and free to make their own decisions, then they would behave according to rational choice theory. : 368–370  Rational choice theory says that a person consistently makes choices that lead to the best situation for themselves, taking into account all available considerations including costs and benefits; the rationality of these considerations is from the point of view of the person themselves, so a decision is not irrational just because someone else finds it questionable.","[' In economics, it is thought that if humans are rational and free to make their own decisions, then they would behave according to what theory?', ' Rational choice theory says that a person consistently makes choices that lead to the best situation for themselves, taking into account what?', "" What is the rationality of a person's considerations from the point of view of the person themselves?""]","['rational choice theory', 'all available considerations including costs and benefits', 'a decision is not irrational just because someone else finds it questionable']"
2652,decision making,Rational and irrational,"Rational decision making is a multi-step process for making choices between alternatives. The process of rational decision making favors logic, objectivity, and analysis over subjectivity and insight. Irrational decision is more counter to logic. The decisions are made in haste and outcomes are not considered.","Rational decision making is a multi-step process for making choices between alternatives. The process of rational decision making favors logic, objectivity, and analysis over subjectivity and insight.","[' Rational decision making is a multi-step process for making choices between what?', ' The process of rational decision making favors logic, objectivity, and what else?']","['alternatives', 'analysis']"
2653,decision making,Rational and irrational,"One of the most prominent theories of decision making is subjective expected utility (SEU) theory, which describes the rational behavior of the decision maker. The decision maker assesses different alternatives by their utilities and the subjective probability of occurrence.","One of the most prominent theories of decision making is subjective expected utility (SEU) theory, which describes the rational behavior of the decision maker. The decision maker assesses different alternatives by their utilities and the subjective probability of occurrence.","[' What is one of the most prominent theories of decision making?', ' What describes the rational behavior of the decision maker?', ' The decision maker assesses different alternatives by what?']","['subjective expected utility (SEU) theory', 'subjective expected utility', 'their utilities']"
2654,decision making,Rational and irrational,"Rational decision-making is often grounded on experience and theories that are able to put this approach on solid mathematical grounds so that subjectivity is reduced to a minimum, see e.g. scenario optimization.
","Rational decision-making is often grounded on experience and theories that are able to put this approach on solid mathematical grounds so that subjectivity is reduced to a minimum, see e.g. scenario optimization.","[' Rational decision-making is often grounded on what?', ' What is able to put this approach on solid mathematical grounds so that subjectivity is reduced to a minimum?']","['experience and theories', 'theories']"
2655,decision making,Cognitive and personal biases,"Biases usually affect decision-making processes. They appear more when decision task has time pressure, is done under high stress and/or task is highly complex.","Biases usually affect decision-making processes. They appear more when decision task has time pressure, is done under high stress and/or task is highly complex.","[' Biases usually affect what?', ' Bias appear more when task has time pressure, is done under high stress and/or is highly complex?']","['decision-making processes', 'Biases']"
2656,decision making,Cognitive limitations in groups,"In groups, people generate decisions through active and complex processes. One method consists of three steps: initial preferences are expressed by members; the members of the group then gather and share information concerning those preferences; finally, the members combine their views and make a single choice about how to face the problem. Although these steps are relatively ordinary, judgements are often distorted by cognitive and motivational biases, include ""sins of commission"", ""sins of omission"", and ""sins of imprecision"".","In groups, people generate decisions through active and complex processes. One method consists of three steps: initial preferences are expressed by members; the members of the group then gather and share information concerning those preferences; finally, the members combine their views and make a single choice about how to face the problem.","["" How many steps are involved in a group's decision making?"", ' What are the three steps in the group decision making process?', ' How are the members of the group able to make a single choice about how to face the world?', ' What is the single choice about how to face the problem?']","['three', 'initial preferences are expressed by members', 'combine their views', 'the members combine their views']"
2657,quality of service,Summary,"Quality of service (QoS) is the description or measurement of the overall performance of a service, such as a telephony or computer network or a cloud computing service, particularly the performance seen by the users of the network. To quantitatively measure quality of service, several related aspects of the network service are often considered, such as packet loss, bit rate, throughput, transmission delay, availability, jitter, etc.
","Quality of service (QoS) is the description or measurement of the overall performance of a service, such as a telephony or computer network or a cloud computing service, particularly the performance seen by the users of the network. To quantitatively measure quality of service, several related aspects of the network service are often considered, such as packet loss, bit rate, throughput, transmission delay, availability, jitter, etc.","[' What does QoS stand for?', ' What is the description of the overall performance of a service?', ' The performance seen by the users of the network is called what?', ' What measure quality of service?', ' What are some related aspects of the network service often considered?']","['Quality of service', 'Quality of service', 'Quality of service', 'several related aspects of the network service are often considered', 'packet loss, bit rate, throughput, transmission delay, availability, jitter']"
2658,quality of service,Summary,"In the field of computer networking and other packet-switched telecommunication networks, quality of service refers to traffic prioritization and resource reservation control mechanisms rather than the achieved service quality. Quality of service is the ability to provide different priorities to different applications, users, or data flows, or to guarantee a certain level of performance to a data flow.
","In the field of computer networking and other packet-switched telecommunication networks, quality of service refers to traffic prioritization and resource reservation control mechanisms rather than the achieved service quality. Quality of service is the ability to provide different priorities to different applications, users, or data flows, or to guarantee a certain level of performance to a data flow.","[' What refers to traffic prioritization and resource reservation control mechanisms rather than the achieved service quality?', ' What is the ability to provide different priorities to different applications, users, or data flows?', ' What is another term for a guarantee of performance to a data flow?']","['quality of service', 'Quality of service', 'Quality of service']"
2659,quality of service,Summary,"Quality of service is particularly important for the transport of traffic with special requirements. In particular, developers have introduced Voice over IP technology to allow computer networks to become as useful as telephone networks for audio conversations, as well as supporting new applications with even stricter network performance requirements.
","Quality of service is particularly important for the transport of traffic with special requirements. In particular, developers have introduced Voice over IP technology to allow computer networks to become as useful as telephone networks for audio conversations, as well as supporting new applications with even stricter network performance requirements.","[' What is particularly important for the transport of traffic with special requirements?', ' What has developers introduced to allow computer networks to become as useful as telephone networks for audio conversations?']","['Quality of service', 'Voice over IP technology']"
2660,quality of service,Definitions,"In the field of telephony, quality of service was defined by the ITU in 1994. Quality of service comprises requirements on all the aspects of a connection, such as service response time, loss, signal-to-noise ratio, crosstalk, echo, interrupts, frequency response, loudness levels, and so on. A subset of telephony QoS is grade of service (GoS) requirements, which comprises aspects of a connection relating to capacity and coverage of a network, for example guaranteed maximum blocking probability and outage probability.","In the field of telephony, quality of service was defined by the ITU in 1994. Quality of service comprises requirements on all the aspects of a connection, such as service response time, loss, signal-to-noise ratio, crosstalk, echo, interrupts, frequency response, loudness levels, and so on.","[' When was quality of service defined by the ITU?', ' What is a requirement on all aspects of a connection?']","['1994', 'Quality of service']"
2661,quality of service,Definitions,"In the field of computer networking and other packet-switched telecommunication networks, teletraffic engineering refers to traffic prioritization and resource reservation control mechanisms rather than the achieved service quality. Quality of service is the ability to provide different priorities to different applications, users, or data flows, or to guarantee a certain level of performance to a data flow. For example, a required bit rate, delay, delay variation, packet loss or bit error rates may be guaranteed. Quality of service is important for real-time streaming multimedia applications such as voice over IP, multiplayer online games and IPTV, since these often require fixed bit rate and are delay sensitive. Quality of service is especially important in networks where the capacity is a limited resource, for example in cellular data communication.
","In the field of computer networking and other packet-switched telecommunication networks, teletraffic engineering refers to traffic prioritization and resource reservation control mechanisms rather than the achieved service quality. Quality of service is the ability to provide different priorities to different applications, users, or data flows, or to guarantee a certain level of performance to a data flow.","[' What does teletraffic engineering refer to?', ' What is the ability to provide different priorities to different applications, users, or data flows?', ' Applications, users, or data flows or to guarantee a certain level of performance to a data flow?']","['traffic prioritization and resource reservation control mechanisms', 'Quality of service', 'Quality of service']"
2662,quality of service,Definitions,"A network or protocol that supports QoS may agree on a traffic contract with the application software and reserve capacity in the network nodes, for example during a session establishment phase. During the session it may monitor the achieved level of performance, for example the data rate and delay, and dynamically control scheduling priorities in the network nodes. It may release the reserved capacity during a tear down phase.
","A network or protocol that supports QoS may agree on a traffic contract with the application software and reserve capacity in the network nodes, for example during a session establishment phase. During the session it may monitor the achieved level of performance, for example the data rate and delay, and dynamically control scheduling priorities in the network nodes.","[' What may a network or protocol that supports QoS agree on with the application software and reserve capacity in the network nodes?', ' During the session establishment phase, what may it monitor the achieved level of performance?', ' What are two examples of performance?', ' How do you dynamically control scheduling priorities in the network nodes?']","['a traffic contract', 'the data rate and delay', 'data rate and delay', 'During the session it may monitor the achieved level of performance']"
2663,quality of service,Definitions,"A best-effort network or service does not support quality of service. An alternative to complex QoS control mechanisms is to provide high quality communication over a best-effort network by over-provisioning the capacity so that it is sufficient for the expected peak traffic load. The resulting absence of network congestion reduces or eliminates the need for QoS mechanisms.
",A best-effort network or service does not support quality of service. An alternative to complex QoS control mechanisms is to provide high quality communication over a best-effort network by over-provisioning the capacity so that it is sufficient for the expected peak traffic load.,"[' What does not support quality of service?', ' What is an alternative to complex QoS control mechanisms?', ' How do you provide high quality communication over a best effort network?']","['A best-effort network', 'to provide high quality communication over a best-effort network by over-provisioning the capacity so that it is sufficient for the expected peak traffic load', 'by over-provisioning the capacity']"
2664,quality of service,Definitions,"QoS is sometimes used as a quality measure, with many alternative definitions, rather than referring to the ability to reserve resources. Quality of service sometimes refers to the level of quality of service, i.e. the guaranteed service quality. High QoS is often confused with a high level of performance, for example high bit rate, low latency and low bit error rate.
","QoS is sometimes used as a quality measure, with many alternative definitions, rather than referring to the ability to reserve resources. Quality of service sometimes refers to the level of quality of service, i.e.","[' What is sometimes used as a quality measure?', ' What sometimes refers to the level of quality of service?']","['QoS', 'Quality of service']"
2665,quality of service,Definitions,"QoS is sometimes used in application layer services such as telephony and streaming video to describe a metric that reflects or predicts the subjectively experienced quality. In this context, QoS is the acceptable cumulative effect on subscriber satisfaction of all imperfections affecting the service. Other terms with similar meaning are the quality of experience (QoE), mean opinion score (MOS), perceptual speech quality measure (PSQM) and perceptual evaluation of video quality (PEVQ).
","QoS is sometimes used in application layer services such as telephony and streaming video to describe a metric that reflects or predicts the subjectively experienced quality. In this context, QoS is the acceptable cumulative effect on subscriber satisfaction of all imperfections affecting the service.","[' What is sometimes used in application layer services such as telephony and streaming video to describe a metric that reflects or predicts the subjective experienced quality?', ' What is the acceptable cumulative effect on subscriber satisfaction of all imperfections affecting the service?']","['QoS', 'QoS']"
2666,quality of service,History,"A number of attempts for layer 2 technologies that add QoS tags to the data have gained popularity in the past. Examples are frame relay, asynchronous transfer mode (ATM) and multiprotocol label switching (MPLS) (a technique between layer 2 and 3). Despite these network technologies remaining in use today, this kind of network lost attention after the advent of Ethernet networks. Today Ethernet is, by far, the most popular layer 2 technology. Conventional Internet routers and network switches operate on a best effort basis. This equipment is less expensive, less complex and faster and thus more popular than earlier more complex technologies that provide QoS mechanisms.
","A number of attempts for layer 2 technologies that add QoS tags to the data have gained popularity in the past. Examples are frame relay, asynchronous transfer mode (ATM) and multiprotocol label switching (MPLS) (a technique between layer 2 and 3).","[' What type of tags do layer 2 technologies add to data?', ' What is ATM?', ' MPLS is a technique between layers 2 and 3.']","['QoS', 'asynchronous transfer mode', 'multiprotocol label switching']"
2667,quality of service,History,"There were four type of service bits and three precedence bits originally provided in each IP packet header, but they were not generally respected.  These bits were later re-defined as Differentiated services code points (DSCP).
","There were four type of service bits and three precedence bits originally provided in each IP packet header, but they were not generally respected. These bits were later re-defined as Differentiated services code points (DSCP).","[' How many type of service bits were initially provided in each IP packet header?', ' How many precedence bits were originally provided?', ' What were these bits later re-defined as?']","['four', 'three', 'Differentiated services code points']"
2668,quality of service,Qualities of traffic,"In packet-switched networks, quality of service is affected by various factors, which can be divided into human and technical factors. Human factors include: stability of service quality, availability of service, waiting times and user information. Technical factors include: reliability, scalability, effectiveness, maintainability and network congestion.","In packet-switched networks, quality of service is affected by various factors, which can be divided into human and technical factors. Human factors include: stability of service quality, availability of service, waiting times and user information.","[' In packet-switched networks, what affects the quality of service?', ' What are human and technical factors divided into?']","['various factors', 'stability of service quality, availability of service, waiting times and user information']"
2669,quality of service,Applications,"These types of service are called inelastic, meaning that they require a certain minimum bit rate and a certain maximum latency to function. By contrast, elastic applications can take advantage of however much or little bandwidth is available. Bulk file transfer applications that rely on TCP are generally elastic.
","These types of service are called inelastic, meaning that they require a certain minimum bit rate and a certain maximum latency to function. By contrast, elastic applications can take advantage of however much or little bandwidth is available.","[' What are elastic services called?', ' What do elastic services require to function?', ' How much or little bandwidth is available for elastic services?']","['inelastic', 'a certain minimum bit rate and a certain maximum latency', 'however much']"
2670,quality of service,Mechanisms,"Circuit switched networks, especially those intended for voice transmission, such as Asynchronous Transfer Mode (ATM) or GSM, have QoS in the core protocol, resources are reserved at each step on the network for the call as it is set up, there is no need for additional procedures to achieve required performance. Shorter data units and built-in QoS were some of the unique selling points of ATM for applications such as video on demand.
","Circuit switched networks, especially those intended for voice transmission, such as Asynchronous Transfer Mode (ATM) or GSM, have QoS in the core protocol, resources are reserved at each step on the network for the call as it is set up, there is no need for additional procedures to achieve required performance. Shorter data units and built-in QoS were some of the unique selling points of ATM for applications such as video on demand.","[' What type of network is intended for voice transmission?', ' What is in the core protocol of circuit switched networks?', ' Where are resources reserved?', ' What were some of the unique selling points of ATM for applications such as video on demand?', ' What did ATM not require to achieve required performance?']","['Circuit switched networks', 'QoS', 'at each step on the network for the call as it is set up', 'Shorter data units and built-in QoS', 'additional procedures']"
2671,quality of service,End-to-end quality of service,"End-to-end quality of service can require a method of coordinating resource allocation between one autonomous system and another. The Internet Engineering Task Force (IETF) defined the Resource Reservation Protocol (RSVP) for bandwidth reservation as a proposed standard in 1997. RSVP is an end-to-end bandwidth reservation and admission control protocol. RSVP was not widely adopted due to scalability limitations. The more scalable traffic engineering version, RSVP-TE, is used in many networks to establish traffic-engineered Multiprotocol Label Switching (MPLS) label-switched paths. The IETF also defined Next Steps in Signaling (NSIS) with QoS signalling as a target. NSIS is a development and simplification of RSVP.
",End-to-end quality of service can require a method of coordinating resource allocation between one autonomous system and another. The Internet Engineering Task Force (IETF) defined the Resource Reservation Protocol (RSVP) for bandwidth reservation as a proposed standard in 1997.,"[' What can require a method of coordinating resource allocation between one autonomous system and another?', ' What did the Internet Engineering Task Force define as a proposed standard in 1997?']","['End-to-end quality of service', 'Resource Reservation Protocol']"
2672,quality of service,End-to-end quality of service,"Research consortia such as ""end-to-end quality of service support over heterogeneous networks"" (EuQoS, from 2004 through 2007) and fora such as the IPsphere Forum developed more mechanisms for handshaking QoS invocation from one domain to the next. IPsphere defined the Service Structuring Stratum (SSS) signaling bus in order to establish, invoke and (attempt to) assure network services. EuQoS conducted experiments to integrate Session Initiation Protocol, Next Steps in Signaling and IPsphere's SSS with an estimated cost of about 15.6 million Euro and published a book.","Research consortia such as ""end-to-end quality of service support over heterogeneous networks"" (EuQoS, from 2004 through 2007) and fora such as the IPsphere Forum developed more mechanisms for handshaking QoS invocation from one domain to the next. IPsphere defined the Service Structuring Stratum (SSS) signaling bus in order to establish, invoke and (attempt to) assure network services.","[' What is the term for end-to-end quality of service support over heterogeneous networks?', ' When did EuQoS begin?', ' What did IPsphere define?', ' What is the acronym for Service Structuring Stratum?', ' What is SSS?']","['EuQoS', '2004', 'Service Structuring Stratum (SSS) signaling bus', 'SSS', 'Service Structuring Stratum']"
2673,quality of service,End-to-end quality of service,"A research project Multi Service Access Everywhere (MUSE) defined another QoS concept in a first phase from January 2004 through February 2006, and a second phase from January 2006 through 2007. Another research project named PlaNetS was proposed for European funding circa 2005.
A broader European project called ""Architecture and design for the future Internet"" known as 4WARD had a budget estimated at 23.4 million Euro and was funded from January 2008 through June 2010.
It included a ""Quality of Service Theme"" and published a book. Another European project, called WIDENS (Wireless Deployable Network System), proposed a bandwidth reservation approach for mobile wireless multirate adhoc networks.","A research project Multi Service Access Everywhere (MUSE) defined another QoS concept in a first phase from January 2004 through February 2006, and a second phase from January 2006 through 2007. Another research project named PlaNetS was proposed for European funding circa 2005.","[' What was the name of the research project that defined another QoS concept in a first phase?', ' When was the first phase of MUSE?', ' What was proposed for European funding circa 2005?']","['Multi Service Access Everywhere', 'January 2004 through February 2006', 'PlaNetS']"
2674,quality of service,Limitations,"Strong cryptography network protocols such as Secure Sockets Layer, I2P, and virtual private networks obscure the data transferred using them.  As all electronic commerce on the Internet requires the use of such strong cryptography protocols, unilaterally downgrading the performance of encrypted traffic creates an unacceptable hazard for customers.  Yet, encrypted traffic is otherwise unable to undergo deep packet inspection for QoS.
","Strong cryptography network protocols such as Secure Sockets Layer, I2P, and virtual private networks obscure the data transferred using them. As all electronic commerce on the Internet requires the use of such strong cryptography protocols, unilaterally downgrading the performance of encrypted traffic creates an unacceptable hazard for customers.","[' Secure Sockets Layer, I2P, and virtual private networks obscure what?', ' All electronic commerce on the internet requires the use of what type of protocols?', ' What creates an unacceptable hazard for customers?']","['the data transferred using them', 'Strong cryptography', 'unilaterally downgrading the performance of encrypted traffic']"
2675,quality of service,Limitations,"Protocols like ICA and RDP may encapsulate other traffic (e.g. printing, video streaming) with varying requirements that can make optimization difficult.
","Protocols like ICA and RDP may encapsulate other traffic (e.g. printing, video streaming) with varying requirements that can make optimization difficult.",[' Protocols like ICA and RDP may encapsulate other traffic with varying requirements that can make optimization difficult?'],"['Protocols like ICA and RDP may encapsulate other traffic (e.g. printing, video streaming) with varying requirements that can make optimization difficult.']"
2676,quality of service,Limitations,"The Internet2 project found, in 2001, that the QoS protocols were probably not deployable inside its Abilene Network with equipment available at that time. The group predicted that “logistical, financial, and organizational barriers will block the way toward any bandwidth guarantees” by protocol modifications aimed at QoS.
They believed that the economics would encourage network providers to deliberately erode the quality of best effort traffic as a way to push customers to higher priced QoS services. Instead they proposed over-provisioning of capacity as more cost-effective at the time.","The Internet2 project found, in 2001, that the QoS protocols were probably not deployable inside its Abilene Network with equipment available at that time. The group predicted that “logistical, financial, and organizational barriers will block the way toward any bandwidth guarantees” by protocol modifications aimed at QoS.","[' What project found that the QoS protocols were probably not deployable inside its Abilene Network?', ' What did the Internet2 project predict that logistical, financial, and organizational barriers would block the way toward?']","['Internet2', 'any bandwidth guarantees']"
2677,quality of service,Limitations,The Abilene network study was the basis for the testimony of Gary Bachula to the US Senate Commerce Committee's hearing on Network Neutrality in early 2006. He expressed the opinion that adding more bandwidth was more effective than any of the various schemes for accomplishing QoS they examined. Bachula's testimony has been cited by proponents of a law banning quality of service as proof that no legitimate purpose is served by such an offering. This argument is dependent on the assumption that over-provisioning isn't a form of QoS and that it is always possible. Cost and other factors affect the ability of carriers to build and maintain permanently over-provisioned networks.,The Abilene network study was the basis for the testimony of Gary Bachula to the US Senate Commerce Committee's hearing on Network Neutrality in early 2006. He expressed the opinion that adding more bandwidth was more effective than any of the various schemes for accomplishing QoS they examined.,"["" What was the basis for the testimony of Gary Bachula to the US Senate Commerce Committee's hearing on Network Neutrality in early 2006?"", ' Who expressed the opinion that adding more bandwidth was more effective than any scheme for accomplishing QoS?']","['The Abilene network study', 'Gary Bachula']"
2678,quality of service,Mobile (cellular) QoS,"Mobile cellular service providers may offer mobile QoS to customers just as the wired public switched telephone network services providers and Internet service providers may offer QoS. QoS mechanisms are always provided for circuit switched services, and are essential for inelastic services, for example streaming multimedia.
","Mobile cellular service providers may offer mobile QoS to customers just as the wired public switched telephone network services providers and Internet service providers may offer QoS. QoS mechanisms are always provided for circuit switched services, and are essential for inelastic services, for example streaming multimedia.","[' What type of service providers may offer mobile QoS to customers?', ' What are always provided for circuit switched services and are essential for inelastic services?']","['Mobile cellular', 'QoS mechanisms']"
2679,quality of service,Mobile (cellular) QoS,"Mobility adds complications to QoS mechanisms. A phone call or other session may be interrupted after a handover if the new base station is overloaded. Unpredictable handovers make it impossible to give an absolute QoS guarantee during the session initiation phase.
",Mobility adds complications to QoS mechanisms. A phone call or other session may be interrupted after a handover if the new base station is overloaded.,"[' What adds complications to QoS mechanisms?', ' What may be interrupted after a handover if the new base station is overloaded?']","['Mobility', 'A phone call or other session']"
2680,quality of service,Standards,"Quality of service in the field of telephony was first defined in 1994 in ITU-T Recommendation E.800. This definition is very broad, listing 6 primary components: Support, Operability, Accessibility, Retainability, Integrity and Security. In 1998 the ITU published a document discussing QoS in the field of data networking. X.641 offers a means of developing or enhancing standards related to QoS and provide concepts and terminology that should assist in maintaining the consistency of related standards.","Quality of service in the field of telephony was first defined in 1994 in ITU-T Recommendation E.800. This definition is very broad, listing 6 primary components: Support, Operability, Accessibility, Retainability, Integrity and Security.","[' When was quality of service first defined in the field of telephony?', ' What is the main component of the ITU-T Recommendation E.800?']","['1994', 'Support, Operability, Accessibility, Retainability, Integrity and Security']"
2681,quality of service,Standards,"Some QoS-related IETF Request for Comments (RFC)s are Baker, Fred; Black, David L.; Nichols, Kathleen; Blake, Steven L. (December 1998), Definition of the Differentiated services Field (DS Field) in the IPv4 and IPv6 Headers, RFC 2474, and Braden, Robert T.; Zhang, Lixia; Berson, Steven; Herzog, Shai; Jamin, Sugih (September 1997), Resource ReSerVation Protocol (RSVP), RFC 2205; both these are discussed above. The IETF has also published two RFCs giving background on QoS: Huston, Geoff (November 2000), Next Steps for the IP QoS Architecture, RFC 2990, and IAB Concerns Regarding Congestion Control for Voice Traffic in the Internet, RFC 3714.
","Some QoS-related IETF Request for Comments (RFC)s are Baker, Fred; Black, David L.; Nichols, Kathleen; Blake, Steven L. (December 1998), Definition of the Differentiated services Field (DS Field) in the IPv4 and IPv6 Headers, RFC 2474, and Braden, Robert T.; Zhang, Lixia; Berson, Steven; Herzog, Shai; Jamin, Sugih (September 1997), Resource ReSerVation Protocol (RSVP), RFC 2205; both these are discussed above. The IETF has also published two RFCs giving background on QoS: Huston, Geoff (November 2000), Next Steps for the IP QoS Architecture, RFC 2990, and IAB Concerns Regarding Congestion Control for Voice Traffic in the Internet, RFC 3714.","[' What is the DS Field in the IPv4 and IPv6 Headers RFC 2474?', ' Who is the author of Definition of the Differentiated services Field?', "" What is Jamin's occupation?"", ' What is RSVP?', ' What is RFC 2205?', ' Who published Next Steps for the IP QoS Architecture?', ' How many RFCs has the IETF published?', ' What is RFC 2990?', ' What is IAB Concerns Regarding Congestion Control for Voice Traffic in the Internet?']","['Differentiated services Field', 'Blake, Steven L.', 'Sugih', 'Resource ReSerVation Protocol', 'Resource ReSerVation Protocol', 'The IETF', 'two', 'Resource ReSerVation Protocol', 'RFC\xa03714']"
2682,quality of service,Standards,"The IETF has also published Baker, Fred; Babiarz, Jozef; Chan, Kwok Ho (August 2006), Configuration Guidelines for DiffServ Service Classes, RFC 4594 as an informative or best practices document about the practical aspects of designing a QoS solution for a DiffServ network. The document tries to identify applications commonly run over an IP network, groups them into traffic classes, studies the treatment required by these classes from the network, and suggests which of the QoS mechanisms commonly available in routers can be used to implement those treatments.
","The IETF has also published Baker, Fred; Babiarz, Jozef; Chan, Kwok Ho (August 2006), Configuration Guidelines for DiffServ Service Classes, RFC 4594 as an informative or best practices document about the practical aspects of designing a QoS solution for a DiffServ network. The document tries to identify applications commonly run over an IP network, groups them into traffic classes, studies the treatment required by these classes from the network, and suggests which of the QoS mechanisms commonly available in routers can be used to implement those treatments.","[' Who published the Configuration Guidelines for DiffServ Service Classes?', ' What is the name of the document that describes the practical aspects of designing a QoS solution?', ' What does the document try to identify commonly run over an IP network?', ' What does it group applications into?']","['The IETF', 'Configuration Guidelines for DiffServ Service Classes, RFC\xa04594', 'applications', 'traffic classes']"
2683,semantics,Summary,"Semantics (from Ancient Greek: σημαντικός sēmantikós, ""significant"") is the study of reference, meaning, or truth. The term can be used to refer to subfields of several distinct disciplines, including philosophy, linguistics and computer science.
","Semantics (from Ancient Greek: σημαντικός sēmantikós, ""significant"") is the study of reference, meaning, or truth. The term can be used to refer to subfields of several distinct disciplines, including philosophy, linguistics and computer science.","[' What is the Greek word for the study of reference, meaning, or truth?', ' Semantics can be used to refer to subfields of several distinct disciplines, including philosophy, linguistics and computer science?']","['Semantics', 'Semantics']"
2684,semantics,Linguistics,"In linguistics, semantics is the subfield that studies meaning. Semantics can address meaning at the levels of words, phrases, sentences, or larger units of discourse. Two of the fundamental issues in the field of semantics are that of compositional semantics (which pertains on how smaller parts, like words, combine and interact to form the meaning of larger expressions such as sentences) and lexical semantics (the nature of the meaning of words). Other prominent issues are those of context and its role on interpretation, opaque contexts, ambiguity, vagueness, entailment and presuppositions.","In linguistics, semantics is the subfield that studies meaning. Semantics can address meaning at the levels of words, phrases, sentences, or larger units of discourse.","[' What subfield studies meaning in linguistics?', ' What can semantics address at the levels of words, phrases, sentences, or larger units of discourse?']","['semantics', 'meaning']"
2685,semantics,Linguistics,"Several disciplines and approaches have contributed to the often contentious field of semantics. One of the crucial questions which unites different approaches to linguistic semantics is that of the relationship between form and meaning, and some major contributions to the study of semantics have derived from studies in the 1980–1990s in related subjects of the syntax–semantics interface and pragmatics.","Several disciplines and approaches have contributed to the often contentious field of semantics. One of the crucial questions which unites different approaches to linguistic semantics is that of the relationship between form and meaning, and some major contributions to the study of semantics have derived from studies in the 1980–1990s in related subjects of the syntax–semantics interface and pragmatics.","[' What has contributed to the field of semantics?', ' What is one of the crucial questions that unites different approaches to linguistic semantics that is the relationship between form and meaning?', ' When did studies in the 1980-1990s make major contributions to the study of semantic?', ' Study of semantics have derived from studies in the 1980-1990s in related subjects of what?']","['Several disciplines and approaches', 'semantics', 'syntax–semantics interface and pragmatics', 'syntax–semantics interface and pragmatics']"
2686,semantics,Linguistics,"The semantic level of language interacts with other modules or levels (like syntax) in which language is traditionally divided. In linguistics, it is typical to talk in terms of ""interfaces"" regarding such interactions between modules or levels. For semantics, the most crucial interfaces are considered those with syntax (the syntax–semantics interface), pragmatics and phonology (regarding prosody and intonation).","The semantic level of language interacts with other modules or levels (like syntax) in which language is traditionally divided. In linguistics, it is typical to talk in terms of ""interfaces"" regarding such interactions between modules or levels.","[' What interacts with other modules or levels in which language is traditionally divided?', ' In linguistics, it is typical to talk in terms of what?']","['The semantic level of language', 'interfaces']"
2687,semantics,Philosophy,"Many of the formal approaches to semantics in mathematical logic and computer science originated in early twentieth century philosophy of language and philosophical logic. Initially, the most influential semantic theory stemmed from Gottlob Frege and Bertrand Russell. Frege and Russell are seen as the originators of a tradition in analytic philosophy to explain meaning compositionally via syntax and mathematical functionality. Ludwig Wittgenstein, a former student of Russell, is also seen as one of the seminal figures in the analytic tradition. All three of these early philosophers of language were concerned with how sentences expressed information in the form of propositions. They also dealt with the truth values or truth conditions a given sentence has in virtue of the proposition it expresses.","Many of the formal approaches to semantics in mathematical logic and computer science originated in early twentieth century philosophy of language and philosophical logic. Initially, the most influential semantic theory stemmed from Gottlob Frege and Bertrand Russell.","[' When did the formal approaches to semantics in mathematical logic and computer science originate?', ' Who were the most influential semantic theory?']","['early twentieth century philosophy of language and philosophical logic', 'Gottlob Frege and Bertrand Russell']"
2688,semantics,Philosophy,"In present day philosophy, the term ""semantics"" is often used to refer to linguistic formal semantics, which bridges both linguistics and philosophy. There is also an active tradition of metasemantics, which studies the foundations of natural language semantics.","In present day philosophy, the term ""semantics"" is often used to refer to linguistic formal semantics, which bridges both linguistics and philosophy. There is also an active tradition of metasemantics, which studies the foundations of natural language semantics.","[' What term is often used to refer to linguistic formal semantics?', ' What bridges linguistics and philosophy?', ' There is also an active tradition of what?']","['semantics', 'linguistic formal semantics', 'metasemantics']"
2689,semantics,Computer science,"In computer science, the term semantics refers to the meaning of language constructs, as opposed to their form (syntax). According to Euzenat, semantics ""provides the rules for interpreting the syntax which do not provide the meaning directly but constrains the possible interpretations of what is declared"".","In computer science, the term semantics refers to the meaning of language constructs, as opposed to their form (syntax). According to Euzenat, semantics ""provides the rules for interpreting the syntax which do not provide the meaning directly but constrains the possible interpretations of what is declared"".","[' In computer science, what term refers to the meaning of language constructs?', ' What does semantics refer to?', ' According to Euzenat, semantics provides the rules for what?']","['semantics', 'the meaning of language constructs', 'interpreting the syntax']"
2690,convex hull,Summary,"In geometry, the convex hull or convex envelope or convex closure of a shape is the smallest convex set that contains it. The convex hull may be defined either as the intersection of all convex sets containing a given subset of a Euclidean space, or equivalently as the set of all convex combinations of points in the subset. For a bounded subset of the plane, the convex hull may be visualized as the shape enclosed by a rubber band stretched around the subset.
","In geometry, the convex hull or convex envelope or convex closure of a shape is the smallest convex set that contains it. The convex hull may be defined either as the intersection of all convex sets containing a given subset of a Euclidean space, or equivalently as the set of all convex combinations of points in the subset.","[' What is the smallest convex set that contains a shape?', ' What may be defined either as the intersection of all convexed sets containing a given subset of a Euclidean space or equivalently as the set of Euclidesan space?', ' What is equivalent to the set of all convex combinations of points in the subset?']","['convex hull', 'The convex hull', 'The convex hull']"
2691,convex hull,Summary,"Convex hulls of open sets are open, and convex hulls of compact sets are compact. Every compact convex set is the convex hull of its extreme points. The convex hull operator is an example of a closure operator, and every antimatroid can be represented by applying this closure operator to finite sets of points.
The algorithmic problems of finding the convex hull of a finite set of points in the plane or other low-dimensional Euclidean spaces, and its dual problem of intersecting half-spaces, are fundamental problems of computational geometry. They can be solved in time 



O
(
n
log
⁡
n
)


{\displaystyle O(n\log n)}
 for two or three dimensional point sets, and in time matching the worst-case output complexity given by the upper bound theorem in higher dimensions.
","Convex hulls of open sets are open, and convex hulls of compact sets are compact. Every compact convex set is the convex hull of its extreme points.","[' What are convex hulls of open sets?', ' What is the convective hull of its extreme points?']","['open', 'Every compact convex set']"
2692,convex hull,Summary,"As well as for finite point sets, convex hulls have also been studied for simple polygons, Brownian motion, space curves, and epigraphs of functions. Convex hulls have wide applications in mathematics, statistics, combinatorial optimization, economics, geometric modeling, and ethology. Related structures include the orthogonal convex hull, convex layers, Delaunay triangulation and Voronoi diagram, and convex skull.
","As well as for finite point sets, convex hulls have also been studied for simple polygons, Brownian motion, space curves, and epigraphs of functions. Convex hulls have wide applications in mathematics, statistics, combinatorial optimization, economics, geometric modeling, and ethology.","[' What has been studied for simple polygons, Brownian motion, space curves, and epigraphs of functions?', ' Convex hulls have wide applications in what fields?']","['convex hulls', 'mathematics, statistics, combinatorial optimization, economics, geometric modeling, and ethology']"
2693,convex hull,Definitions,"A set of points in a Euclidean space is defined to be convex if it contains the line segments connecting each pair of its points. The convex hull of a given set 



X


{\displaystyle X}
 may be defined as","A set of points in a Euclidean space is defined to be convex if it contains the line segments connecting each pair of its points. The convex hull of a given set 



X


{\displaystyle X}
 may be defined as","[' What is defined to be convex if it contains the line segments connecting each pair of its points?', ' What may be defined as the hull of a given set X <unk>displaystyle X<unk>?']","['A set of points in a Euclidean space', 'convex']"
2694,convex hull,Definitions,"For bounded sets in the Euclidean plane, not all on one line, the boundary of the convex hull is the simple closed curve with minimum perimeter containing 



X


{\displaystyle X}
. One may imagine stretching a rubber band so that it surrounds the entire set 



S


{\displaystyle S}
 and then releasing it, allowing it to contract; when it becomes taut, it encloses the convex hull of 



S


{\displaystyle S}
. This formulation does not immediately generalize to higher dimensions: for a finite set of points in three-dimensional space, a neighborhood of a spanning tree of the points encloses them with arbitrarily small surface area, smaller than the surface area of the convex hull. However, in higher dimensions, variants of the obstacle problem of finding a minimum-energy surface above a given shape can have the convex hull as their solution.","For bounded sets in the Euclidean plane, not all on one line, the boundary of the convex hull is the simple closed curve with minimum perimeter containing 



X


{\displaystyle X}
. One may imagine stretching a rubber band so that it surrounds the entire set 



S


{\displaystyle S}
 and then releasing it, allowing it to contract; when it becomes taut, it encloses the convex hull of 



S


{\displaystyle S}
.","[' What is the boundary of the convex hull for bounded sets in the Euclidean plane?', ' What would one imagine stretching a rubber band so that it surrounds the entire set?', ' What does the entire set of S <unk>displaystyle S<unk> release?', ' When it becomes taut, it encloses the convex hull of what?']","['X', 'S', 'S', 'S']"
2695,convex hull,Definitions,"For objects in three dimensions, the first definition states that the convex hull is the smallest possible convex bounding volume of the objects.
The definition using intersections of convex sets may be extended to non-Euclidean geometry, and the definition using convex combinations may be extended from Euclidean spaces to arbitrary real vector spaces or affine spaces; convex hulls may also be generalized in a more abstract way, to oriented matroids.","For objects in three dimensions, the first definition states that the convex hull is the smallest possible convex bounding volume of the objects. The definition using intersections of convex sets may be extended to non-Euclidean geometry, and the definition using convex combinations may be extended from Euclidean spaces to arbitrary real vector spaces or affine spaces; convex hulls may also be generalized in a more abstract way, to oriented matroids.","[' What is the smallest possible convex bounding volume of the objects?', ' What may be extended from Euclidean spaces to arbitrary spaces?', ' For objects in three dimensions, what is the first definition?', ' What may be extended from Euclidean spaces to arbitrary real vector spaces or affine spaces?', ' Convex hulls may also be generalized in a more abstract way, to what?']","['convex hull', 'the definition using convex combinations', 'the convex hull is the smallest possible convex bounding volume of the objects', 'the definition using convex combinations', 'oriented matroids']"
2696,convex hull,Computation,"In computational geometry, a number of algorithms are known for computing the convex hull for a finite set of points and for other geometric objects.
Computing the convex hull means constructing an unambiguous, efficient representation of the required convex shape. Output representations that have been considered for convex hulls of point sets include a list of linear inequalities describing the facets of the hull, an undirected graph of facets and their adjacencies, or the full face lattice of the hull. In two dimensions, it may suffice more simply to list the points that are vertices, in their cyclic order around the hull.","In computational geometry, a number of algorithms are known for computing the convex hull for a finite set of points and for other geometric objects. Computing the convex hull means constructing an unambiguous, efficient representation of the required convex shape.","[' In computational geometry, a number of algorithms are known for computing what for a finite set of points?', ' Computing the convex hull means constructing what?']","['convex hull', 'an unambiguous, efficient representation of the required convex shape']"
2697,convex hull,Computation,"For convex hulls in two or three dimensions, the complexity of the corresponding algorithms is usually estimated in terms  of 



n


{\displaystyle n}
, the number of input points, and 



h


{\displaystyle h}
, the number of points on the convex hull, which may be significantly smaller than 



n


{\displaystyle n}
. For higher-dimensional hulls, the number of faces of other dimensions may also come into the analysis. Graham scan can compute the convex hull of 



n


{\displaystyle n}
 points in the plane in time 



O
(
n
log
⁡
n
)


{\displaystyle O(n\log n)}
. For points in two and three dimensions, more complicated output-sensitive algorithms are known that compute the convex hull in time 



O
(
n
log
⁡
h
)


{\displaystyle O(n\log h)}
. These include Chan's algorithm and the Kirkpatrick–Seidel algorithm. For dimensions 



d
>
3


{\displaystyle d>3}
, the time for computing the convex hull is 



O
(

n

⌊
d

/

2
⌋


)


{\displaystyle O(n^{\lfloor d/2\rfloor })}
, matching the worst-case output complexity of the problem. The convex hull of a simple polygon in the plane can be constructed in linear time.","For convex hulls in two or three dimensions, the complexity of the corresponding algorithms is usually estimated in terms  of 



n


{\displaystyle n}
, the number of input points, and 



h


{\displaystyle h}
, the number of points on the convex hull, which may be significantly smaller than 



n


{\displaystyle n}
. For higher-dimensional hulls, the number of faces of other dimensions may also come into the analysis.","[' How is the complexity of algorithms estimated for convex hulls in two or three dimensions?', ' What is n <unk>displaystyle n<unk>, the number of input points?', ' How is complexity of the corresponding algorithms estimated?', ' What may be significantly smaller than n?', ' For higher-dimensional hulls, the number of faces of other dimensions may also come into the analysis?']","['n\n\n\n{\\displaystyle n}', 'n', 'n', 'points on the convex hull', 'n']"
2698,convex hull,Computation,"Dynamic convex hull data structures can be used to keep track of the convex hull of a set of points undergoing insertions and deletions of points, and kinetic convex hull structures can keep track of the convex hull for points moving continuously.
The construction of convex hulls also serves as a tool, a building block for a number of other computational-geometric algorithms such as the rotating calipers method for computing the width and diameter of a point set.","Dynamic convex hull data structures can be used to keep track of the convex hull of a set of points undergoing insertions and deletions of points, and kinetic convex hull structures can keep track of the convex hull for points moving continuously. The construction of convex hulls also serves as a tool, a building block for a number of other computational-geometric algorithms such as the rotating calipers method for computing the width and diameter of a point set.","[' What can be used to keep track of the convex hull of a set of points undergoing insertions and deletions of points?', ' What can also serve as a useful tool for keeping track of points moving continuously?', ' What is a building block for a number of other computational-geometric algorithms?', ' What method is used to compute the width and diameter of a point set?']","['Dynamic convex hull data structures', 'kinetic convex hull structures', 'convex hulls', 'rotating calipers']"
2699,convex hull,Related structures,"Several other shapes can be defined from a set of points in a similar way to the convex hull, as the minimal superset with some property, the intersection of all shapes containing the points from a given family of shapes, or the union of all combinations of points for a certain type of combination. For instance:
","Several other shapes can be defined from a set of points in a similar way to the convex hull, as the minimal superset with some property, the intersection of all shapes containing the points from a given family of shapes, or the union of all combinations of points for a certain type of combination. For instance:","[' What can be defined from a set of points in a similar way to the convex hull?', ' What is the minimal superset with some property?', ' The union of all combinations of points for a given family of shapes?', ' What is the union of all combinations of points for a certain type of combination?']","['Several other shapes', 'the intersection of all shapes containing the points from a given family of shapes', 'intersection of all shapes containing the points from a given family of shapes, or the union of all combinations of points for a certain type of combination', 'the intersection of all shapes containing the points from a given family of shapes']"
2700,convex hull,Related structures,"The Delaunay triangulation of a point set and its dual, the Voronoi diagram, are mathematically  related to convex hulls: the Delaunay triangulation of a point set in 





R


n




{\displaystyle \mathbb {R} ^{n}}
 can be viewed as the projection of a convex hull in 





R


n
+
1


.


{\displaystyle \mathbb {R} ^{n+1}.}

The alpha shapes of a finite point set give a nested family of (non-convex) geometric objects describing the shape of a point set at different levels of detail.
Each of alpha shape is the union of some of the features of the Delaunay triangulation, selected by comparing their circumradius to the parameter alpha. The point set itself forms one endpoint of this family of shapes, and its convex hull forms the other endpoint.
The convex layers of a point set are a nested family of convex polygons, the outermost of which is the convex hull, with the inner layers constructed recursively from the points that are not vertices of the convex hull.","The Delaunay triangulation of a point set and its dual, the Voronoi diagram, are mathematically  related to convex hulls: the Delaunay triangulation of a point set in 





R


n




{\displaystyle \mathbb {R} ^{n}}
 can be viewed as the projection of a convex hull in 





R


n
+
1


. {\displaystyle \mathbb {R} ^{n+1}.}","[' The Delaunay triangulation of a point set and its dual, the Voronoi diagram, are mathematically related to what?', ' What can be viewed as the projection of the convex hull in R n + 1?']","['convex hulls', 'R\n\n\nn\n+\n1\n\n\n. {\\displaystyle \\mathbb']"
2701,convex hull,Related structures,"The convex skull of a polygon is the largest convex polygon contained inside it. It can be found in polynomial time, but the exponent of the algorithm is high.","The convex skull of a polygon is the largest convex polygon contained inside it. It can be found in polynomial time, but the exponent of the algorithm is high.","[' What is the largest convex polygon contained inside a polygon?', ' What can be found in polynomial time?', ' The exponent of the algorithm is what?']","['The convex skull', 'The convex skull', 'high']"
2702,convex hull,Applications,"Convex hulls have wide applications in many fields. Within mathematics, convex hulls are used to study polynomials, matrix eigenvalues, and unitary elements, and several theorems in discrete geometry involve convex hulls. They are used in robust statistics as the outermost contour of Tukey depth, are part of the bagplot visualization of two-dimensional data, and define risk sets of randomized decision rules. Convex hulls of indicator vectors of solutions to combinatorial problems are central to combinatorial optimization and polyhedral combinatorics. In economics, convex hulls can be used to apply methods of convexity in economics to non-convex markets. In geometric modeling, the convex hull property Bézier curves helps find their crossings, and convex hulls are part of the measurement of boat hulls. And in the study of animal behavior, convex hulls are used in a standard definition of the home range.
","Convex hulls have wide applications in many fields. Within mathematics, convex hulls are used to study polynomials, matrix eigenvalues, and unitary elements, and several theorems in discrete geometry involve convex hulls.","[' What are convex hulls used to study in mathematics?', ' What are some theorems in discrete geometry using?']","['polynomials, matrix eigenvalues, and unitary elements', 'convex hulls']"
2703,convex hull,History,"The lower convex hull of points in the plane appears, in the form of a Newton polygon, in a letter from Isaac Newton to Henry Oldenburg in 1676. The term ""convex hull"" itself appears as early as the work of Garrett Birkhoff (1935), and the corresponding term in German appears earlier, for instance in Hans Rademacher's review of Kőnig (1922). Other terms, such as ""convex envelope"", were also used in this time frame. By 1938, according to Lloyd Dines, the term ""convex hull"" had become standard; Dines adds that he finds the term unfortunate, because the colloquial meaning of the word ""hull"" would suggest that it refers to the surface of a shape, whereas the convex hull includes the interior and not just the surface.","The lower convex hull of points in the plane appears, in the form of a Newton polygon, in a letter from Isaac Newton to Henry Oldenburg in 1676. The term ""convex hull"" itself appears as early as the work of Garrett Birkhoff (1935), and the corresponding term in German appears earlier, for instance in Hans Rademacher's review of Kőnig (1922).","[' What is the form of the lower convex hull of points in the plane?', ' In what year did Isaac Newton send a letter to Henry Oldenburg?', ' Who wrote the letter in 1676?', ' When did Garrett Birkhoff write his work?', ' In what year was Garrett Birkhoff born?', ' In what language does the term ""Birkhoff"" appear?', ' Who wrote a review of K<unk>nig?']","['a Newton polygon', '1676', 'Isaac Newton', '1935', '1935', 'German', 'Hans Rademacher']"
2704,biometrics,Summary,"Biometrics are body measurements and calculations related to human characteristics. Biometric authentication (or realistic authentication) is used in computer science as a form of identification and access control. It is also used to identify individuals in groups that are under surveillance.
",Biometrics are body measurements and calculations related to human characteristics. Biometric authentication (or realistic authentication) is used in computer science as a form of identification and access control.,"[' What are body measurements and calculations related to?', ' What is used in computer science as a form of identification and access control?']","['human characteristics', 'Biometric authentication']"
2705,biometrics,Summary,"Biometric identifiers are the distinctive, measurable characteristics used to label and describe individuals. Biometric identifiers are often categorized as physiological characteristics, which are related to the shape of the body. Examples include, but are not limited to mouse movement, fingerprint, palm veins, face recognition, DNA, palm print, hand geometry, iris recognition, retina and odor/scent. Behavioral characteristics are related to the pattern of behavior of a person, including but not limited to typing rhythm, gait, signature, behavioral profiling, and voice. Some researchers have coined the term 'behaviometrics' to describe the latter class of biometrics.","Biometric identifiers are the distinctive, measurable characteristics used to label and describe individuals. Biometric identifiers are often categorized as physiological characteristics, which are related to the shape of the body.","[' What are the distinctive, measurable characteristics used to label and describe individuals?', ' What are biometric identifiers often categorized as?']","['Biometric identifiers', 'physiological characteristics']"
2706,biometrics,Summary,"More traditional means of access control include token-based identification systems, such as a driver's license or passport, and knowledge-based identification systems, such as a password or personal identification number. Since biometric identifiers are unique to individuals, they are more reliable in verifying identity than token and knowledge-based methods; however, the collection of biometric identifiers raises privacy concerns about the ultimate use of this information.
","More traditional means of access control include token-based identification systems, such as a driver's license or passport, and knowledge-based identification systems, such as a password or personal identification number. Since biometric identifiers are unique to individuals, they are more reliable in verifying identity than token and knowledge-based methods; however, the collection of biometric identifiers raises privacy concerns about the ultimate use of this information.","[' What type of identifiers are more reliable than token and knowledge-based methods?', ' What are two examples of token-based access control systems?', ' What raises privacy concerns about the ultimate use of biometric identifiers?', ' What methods are more effective in verifying identity than token and knowledge-based methods?']","['biometric', ""driver's license or passport"", 'the collection of biometric identifiers', 'biometric identifiers']"
2707,biometrics,Biometric functionality,"Many different aspects of human physiology, chemistry or behavior can be used for biometric authentication. The selection of a particular biometric for use in a specific application involves a weighting of several factors. Jain et al. (1999) identified seven such factors to be used when assessing the suitability of any trait for use in biometric authentication. 
","Many different aspects of human physiology, chemistry or behavior can be used for biometric authentication. The selection of a particular biometric for use in a specific application involves a weighting of several factors.","[' How many different aspects of human physiology, chemistry or behavior can be used for biometric authentication?', ' The selection of a particular biometric for use in a specific application involves a weighting of several factors?']","['Many', 'biometric authentication']"
2708,biometrics,Biometric functionality,Proper biometric use is very application dependent. Certain biometrics will be better than others based on the required levels of convenience and security. No single biometric will meet all the requirements of every possible application.,Proper biometric use is very application dependent. Certain biometrics will be better than others based on the required levels of convenience and security.,"[' Proper biometric use is very application dependent what?', ' Certain biometrics will be better than others based on the required levels of convenience and security?']","['Certain biometrics', 'Proper biometric use is very application dependent']"
2709,biometrics,Biometric functionality,"The block diagram illustrates the two basic modes of a biometric system. First, in verification (or authentication) mode the system performs a one-to-one comparison of a captured biometric with a specific template stored in a biometric database in order to verify the individual is the person they claim to be. Three steps are involved in the verification of a person. In the first step, reference models for all the users are generated and stored in the model database. In the second step, some samples are matched with reference models to generate the genuine and impostor scores and calculate the threshold. The third step is the testing step. This process may use a smart card, username or ID number (e.g. PIN) to indicate which template should be used for comparison. 'Positive recognition' is a common use of the verification mode, ""where the aim is to prevent multiple people from using the same identity"".","The block diagram illustrates the two basic modes of a biometric system. First, in verification (or authentication) mode the system performs a one-to-one comparison of a captured biometric with a specific template stored in a biometric database in order to verify the individual is the person they claim to be.","[' How many basic modes of a biometric system are illustrated in the block diagram?', ' What is the first mode in which a system performs a one-to-one comparison of captured biometric with a specific template?']","['two', 'verification']"
2710,biometrics,Biometric functionality,"Second, in identification mode the system performs a one-to-many comparison against a biometric database in an attempt to establish the identity of an unknown individual. The system will succeed in identifying the individual if the comparison of the biometric sample to a template in the database falls within a previously set threshold. Identification mode can be used either for 'positive recognition' (so that the user does not have to provide any information about the template to be used) or for 'negative recognition' of the person ""where the system establishes whether the person is who she (implicitly or explicitly) denies to be"". The latter function can only be achieved through biometrics since other methods of personal recognition such as passwords, PINs or keys are ineffective.
","Second, in identification mode the system performs a one-to-many comparison against a biometric database in an attempt to establish the identity of an unknown individual. The system will succeed in identifying the individual if the comparison of the biometric sample to a template in the database falls within a previously set threshold.","[' In identification mode, the system performs a one-to-many comparison against a biometric database in an attempt to establish the identity of an unknown individual?', ' The system will succeed in identifying the individual if the comparison of the biometric sample to a template in the database falls within what?', ' What does a template in the database fall within a previously set threshold?']","['The system will succeed in identifying the individual if the comparison of the biometric sample to a template in the database falls within a previously set threshold', 'a previously set threshold', 'comparison of the biometric sample']"
2711,biometrics,Biometric functionality,"The first time an individual uses a biometric system is called enrollment. During enrollment, biometric information from an individual is captured and stored. In subsequent uses, biometric information is detected and compared with the information stored at the time of enrollment. Note that it is crucial that storage and retrieval of such systems themselves be secure if the biometric system is to be robust. The first block (sensor) is the interface between the real world and the system; it has to acquire all the necessary data. Most of the times it is an image acquisition system, but it can change according to the characteristics desired. The second block performs all the necessary pre-processing: it has to remove artifacts from the sensor, to enhance the input (e.g. removing background noise), to use some kind of normalization, etc. In the third block, necessary features are extracted. This step is an important step as the correct features need to be extracted in an optimal way. A vector of numbers or an image with particular properties is used to create a template. A template is a synthesis of the relevant characteristics extracted from the source. Elements of the biometric measurement that are not used in the comparison algorithm are discarded in the template to reduce the file size and to protect the identity of the enrollee. However, depending on the scope of the biometric system, original biometric image sources may be retained such as the PIV-cards used in the Federal Information Processing Standard Personal Identity Verification (PIV) of Federal Employees and Contractors (FIPS 201).","The first time an individual uses a biometric system is called enrollment. During enrollment, biometric information from an individual is captured and stored.","[' What is the first time an individual uses a biometric system?', ' What is captured and stored during enrollment?']","['enrollment', 'biometric information from an individual']"
2712,biometrics,Biometric functionality,"During the enrollment phase, the template is simply stored somewhere (on a card or within a database or both). During the matching phase, the obtained template is passed to a matcher that compares it with other existing templates, estimating the distance between them using any algorithm (e.g. Hamming distance). The matching program will analyze the template with the input. This will then be output for a specified use or purpose (e.g. entrance in a restricted area), though it is a fear that the use of biometric data may face mission creep.
Selection of biometrics in any practical application depending upon the characteristic measurements and user requirements. In selecting a particular biometric, factors to consider include, performance, social acceptability, ease of circumvention and/or spoofing, robustness, population coverage, size of equipment needed and identity theft deterrence. The selection of a biometric is based on user requirements and considers sensor and device availability, computational time and reliability, cost, sensor size, and power consumption.
","During the enrollment phase, the template is simply stored somewhere (on a card or within a database or both). During the matching phase, the obtained template is passed to a matcher that compares it with other existing templates, estimating the distance between them using any algorithm (e.g.","[' What is stored somewhere during the enrollment phase?', ' What is passed to a matcher that compares the template with other existing templates?']","['the template', 'the obtained template']"
2713,biometrics,Multimodal biometric system,"Multimodal biometric systems use multiple sensors or biometrics to overcome the limitations of unimodal biometric systems. For instance iris recognition systems can be compromised by aging irises and electronic fingerprint recognition can be worsened by worn-out or cut fingerprints. While unimodal biometric systems are limited by the integrity of their identifier, it is unlikely that several unimodal systems will suffer from identical limitations. Multimodal biometric systems can obtain sets of information from the same marker (i.e., multiple images of an iris, or scans of the same finger) or information from different biometrics (requiring fingerprint scans and, using voice recognition, a spoken passcode).",Multimodal biometric systems use multiple sensors or biometrics to overcome the limitations of unimodal biometric systems. For instance iris recognition systems can be compromised by aging irises and electronic fingerprint recognition can be worsened by worn-out or cut fingerprints.,"[' What do multimodal biometric systems use to overcome the limitations of?', ' What can be compromised by aging irises?', ' Weared-out or cut fingerprints can worsen what?']","['multiple sensors or biometrics', 'iris recognition systems', 'electronic fingerprint recognition']"
2714,biometrics,Multimodal biometric system,"Multimodal biometric systems can fuse these unimodal systems sequentially, simultaneously, a combination thereof, or in series, which refer to sequential, parallel, hierarchical and serial integration modes, respectively.
Fusion of the biometrics information can occur at different stages of a recognition system. In case of feature level fusion, the data itself or the features extracted from multiple biometrics are fused. Matching-score level fusion consolidates the scores generated by multiple classifiers pertaining to different modalities. Finally, in case of decision level fusion the final results of multiple classifiers are combined via techniques such as majority voting. Feature level fusion is believed to be more effective than the other levels of fusion because the feature set contains richer information about the input biometric data than the matching score or the output decision of a classifier. Therefore, fusion at the feature level is expected to provide better recognition results.","Multimodal biometric systems can fuse these unimodal systems sequentially, simultaneously, a combination thereof, or in series, which refer to sequential, parallel, hierarchical and serial integration modes, respectively. Fusion of the biometrics information can occur at different stages of a recognition system.","[' What can multimodal biometric systems fuse sequentially, simultaneously, a combination thereof, or in series?', ' What refers to sequential, parallel, hierarchical and serial integration modes?', ' Fusion of biometrics information can occur at different stages of a recognition system?']","['unimodal systems', 'Multimodal biometric systems can fuse these unimodal systems sequentially, simultaneously, a combination thereof, or in series', 'Multimodal biometric systems']"
2715,biometrics,Multimodal biometric system,"Spoof attacks consist in submitting fake biometric traits to biometric systems, and are a major threat that can curtail their security. Multi-modal biometric systems are commonly believed to be intrinsically more robust to spoof attacks, but recent studies have shown that they can be evaded by spoofing even a single biometric trait.
","Spoof attacks consist in submitting fake biometric traits to biometric systems, and are a major threat that can curtail their security. Multi-modal biometric systems are commonly believed to be intrinsically more robust to spoof attacks, but recent studies have shown that they can be evaded by spoofing even a single biometric trait.","[' What do spoof attacks consist of?', ' What is a major threat that can curtail their security?', ' Multi-modal biometric systems are commonly believed to be more robust to what?', ' What can be evaded by spoofing even a single biometric trait?']","['submitting fake biometric traits to biometric systems', 'Spoof attacks', 'spoof attacks', 'Multi-modal biometric systems']"
2716,biometrics,Performance,"The discriminating powers of all biometric technologies depend on the amount of entropy they are able to encode and use in matching. 
The following are used as performance metrics for biometric systems:",The discriminating powers of all biometric technologies depend on the amount of entropy they are able to encode and use in matching. The following are used as performance metrics for biometric systems:,"[' What determines the discriminating powers of all biometric technologies?', ' What are performance metrics for biometric systems?']","['the amount of entropy they are able to encode and use in matching', 'The following']"
2717,biometrics,History,"An early cataloguing of fingerprints dates back to 1885 when Juan Vucetich started a collection of fingerprints of criminals in Argentina. Josh Ellenbogen and Nitzan Lebovic argued that Biometrics originated in the identification systems of criminal activity developed by Alphonse Bertillon (1853–1914) and by Francis Galton's theory of fingerprints and physiognomy. According to Lebovic, Galton's work ""led to the application of mathematical models to fingerprints, phrenology, and facial characteristics"", as part of ""absolute identification"" and ""a key to both inclusion and exclusion"" of populations. Accordingly, ""the biometric system is the absolute political weapon of our era"" and a form of ""soft control"". The theoretician David Lyon showed that during the past two decades biometric systems have penetrated the civilian market, and blurred the lines between governmental forms of control and private corporate control. Kelly A. Gates identified 9/11 as the turning point for the cultural language of our present: ""in the language of cultural studies, the aftermath of 9/11 was a moment of articulation, where objects or events that have no necessary connection come together and a new discourse formation is established: automated facial recognition as a homeland security technology.""",An early cataloguing of fingerprints dates back to 1885 when Juan Vucetich started a collection of fingerprints of criminals in Argentina. Josh Ellenbogen and Nitzan Lebovic argued that Biometrics originated in the identification systems of criminal activity developed by Alphonse Bertillon (1853–1914) and by Francis Galton's theory of fingerprints and physiognomy.,"[' When did an early cataloguing of fingerprints date back to?', ' When did Juan Vucetich start collecting fingerprints of criminals in Argentina?', ' Who argued that Biometrics originated in the identification systems of criminal activity developed by Alphonse Bertillon?', ' Bertillon was born in what year?', "" Bertillon's theory of fingerprints and physiognomy was based on what theory?""]","['1885', '1885', 'Josh Ellenbogen and Nitzan Lebovic', '1853', 'Francis Galton']"
2718,biometrics,Adaptive biometric systems,"Adaptive biometric systems aim to auto-update the templates or model to the intra-class variation of the operational data. The two-fold advantages of these systems are solving the problem of limited training data and tracking the temporal variations of the input data through adaptation. Recently, adaptive biometrics have received a significant attention from the research community. This research direction is expected to gain momentum because of their key promulgated advantages. First, with an adaptive biometric system, one no longer needs to collect a large number of biometric samples during the enrollment process. Second, it is no longer necessary to enroll again or retrain the system from scratch in order to cope with the changing environment. This convenience can significantly reduce the cost of maintaining a biometric system. Despite these advantages, there are several open issues involved with these systems. For mis-classification error (false acceptance) by the biometric system, cause adaptation using impostor sample. However, continuous research efforts are directed to resolve the open issues associated to the field of adaptive biometrics. More information about adaptive biometric systems can be found in the critical review by Rattani et al.
",Adaptive biometric systems aim to auto-update the templates or model to the intra-class variation of the operational data. The two-fold advantages of these systems are solving the problem of limited training data and tracking the temporal variations of the input data through adaptation.,"[' Adaptive biometric systems aim to auto-update the templates or model to what?', ' What are the two-fold advantages of adaptive biometrics?', ' How do adaptive systems solve the problem of limited training data?']","['the intra-class variation of the operational data', 'solving the problem of limited training data and tracking the temporal variations of the input data through adaptation', 'tracking the temporal variations of the input data through adaptation']"
2719,biometrics,Recent advances in emerging biometrics,"In recent times, biometrics based on brain (electroencephalogram) and heart (electrocardiogram) signals have emerged.  An example is finger vein recognition, using pattern-recognition techniques, based on images of human vascular patterns. The advantage of such 'futuristic' technology is that it is more fraud resistant compared to conventional biometrics like fingerprints. However, such technology is generally more cumbersome and still has issues such as lower accuracy and poor reproducibility over time. 
","In recent times, biometrics based on brain (electroencephalogram) and heart (electrocardiogram) signals have emerged. An example is finger vein recognition, using pattern-recognition techniques, based on images of human vascular patterns.","[' What is a biometric based on brain and heart signals?', ' What is an example of finger vein recognition?', ' Finger vein recognition uses what techniques?']","['electroencephalogram) and heart (electrocardiogram', 'biometrics based on brain (electroencephalogram) and heart (electrocardiogram) signals have emerged. An example is finger vein recognition, using pattern-recognition techniques, based on images of human vascular patterns', 'pattern-recognition']"
2720,biometrics,Countries applying biometrics,"There are also numerous countries applying biometrics for voter registration and similar electoral purposes. According to the International IDEA's ICTs in Elections Database, some of the countries using (2017) Biometric Voter Registration (BVR) are Armenia, Angola, Bangladesh, Bhutan, Bolivia, Brazil, Burkina Faso, Cambodia, Cameroon, Chad, Colombia, Comoros, Congo (Democratic Republic of), Costa Rica, Ivory Coast, Dominican Republic, Fiji, Gambia, Ghana, Guatemala, India, Iraq, Kenya, Lesotho, Liberia, Malawi, Mali, Mauritania, Mexico, Morocco, Mozambique, Namibia, Nepal, Nicaragua, Nigeria, Panama, Peru, The Philippines, Senegal, Sierra Leone, Solomon Islands, Somaliland, Swaziland, Tanzania, Uganda, Uruguay, Venezuela, Yemen, Zambia, and Zimbabwe.","There are also numerous countries applying biometrics for voter registration and similar electoral purposes. According to the International IDEA's ICTs in Elections Database, some of the countries using (2017) Biometric Voter Registration (BVR) are Armenia, Angola, Bangladesh, Bhutan, Bolivia, Brazil, Burkina Faso, Cambodia, Cameroon, Chad, Colombia, Comoros, Congo (Democratic Republic of), Costa Rica, Ivory Coast, Dominican Republic, Fiji, Gambia, Ghana, Guatemala, India, Iraq, Kenya, Lesotho, Liberia, Malawi, Mali, Mauritania, Mexico, Morocco, Mozambique, Namibia, Nepal, Nicaragua, Nigeria, Panama, Peru, The Philippines, Senegal, Sierra Leone, Solomon Islands, Somaliland, Swaziland, Tanzania, Uganda, Uruguay, Venezuela, Yemen, Zambia, and Zimbabwe.","[' What are some of the countries using (2017) Biometric Voter Registration (BVR)?', ' Burkina Faso, Cameroon, Chad, Colombia, Comoros, Congo (Democratic Republic of), Costa Rica, Ivory Coast, Dominican Republic, Fiji, Gambia, Ghana, Guatemala, India, Iraq, Kenya, Lesotho, Liberia, Malawi, Mali, Mauritania, Mozambique, Namibia, Nepal, Nicaragua, Nigeria, Panama, Peru, The Philippines, Senegal, Sierra Leone, Swaziland, Tanzania, Uganda,', ' What is the name of the country that is in the Solomon Islands?', ' Where is Swaziland located?']","['Armenia', 'Biometric Voter Registration', 'Sierra Leone', 'Somaliland']"
2721,approximation algorithms,Summary,"In computer science and operations research, approximation algorithms are efficient algorithms that find approximate solutions to optimization problems (in particular NP-hard problems) with provable guarantees on the distance of the returned solution to the optimal one. Approximation algorithms naturally arise in the field of theoretical computer science as a consequence of the widely believed P ≠ NP conjecture. Under this conjecture, a wide class of optimization problems cannot be solved exactly in polynomial time. The field of approximation algorithms, therefore, tries to understand how closely it is possible to approximate optimal solutions to such problems in polynomial time. In an overwhelming majority of the cases, the guarantee of such algorithms is a multiplicative one expressed as an approximation ratio or approximation factor i.e., the optimal solution is always guaranteed to be within a (predetermined) multiplicative factor of the returned solution. However, there are also many approximation algorithms that provide an additive guarantee on the quality of the returned solution. A notable example of an approximation algorithm that provides both is the classic approximation algorithm of Lenstra, Shmoys and Tardos for scheduling on unrelated parallel machines.
","In computer science and operations research, approximation algorithms are efficient algorithms that find approximate solutions to optimization problems (in particular NP-hard problems) with provable guarantees on the distance of the returned solution to the optimal one. Approximation algorithms naturally arise in the field of theoretical computer science as a consequence of the widely believed P ≠ NP conjecture.","[' What are efficient algorithms that find approximate solutions to optimization problems?', ' What do approximation algorithms find with provable guarantees on the distance of the returned solution to the optimal one?', ' What conjecture is widely believed in the field of theoretical computer science?']","['approximation algorithms', 'approximate solutions to optimization problems', 'P ≠ NP']"
2722,approximation algorithms,Summary,"The design and analysis of approximation algorithms crucially involves a mathematical proof certifying the quality of the returned solutions in the worst case. This distinguishes them from heuristics such as annealing or genetic algorithms, which find reasonably good solutions on some inputs, but provide no clear indication at the outset on when they may succeed or fail.
","The design and analysis of approximation algorithms crucially involves a mathematical proof certifying the quality of the returned solutions in the worst case. This distinguishes them from heuristics such as annealing or genetic algorithms, which find reasonably good solutions on some inputs, but provide no clear indication at the outset on when they may succeed or fail.","[' What does the design and analysis of approximation algorithms involve?', ' What distinguishes them from heuristics such as annealing or genetic algorithms?', ' What does not provide a clear indication at the outset on when inputs may succeed or fail?']","['a mathematical proof certifying the quality of the returned solutions in the worst case', 'mathematical proof certifying the quality of the returned solutions in the worst case', 'genetic algorithms']"
2723,approximation algorithms,Summary,"There is widespread interest in theoretical computer science to better understand the limits to which we can approximate certain famous optimization problems. For example, one of the long-standing open questions in computer science is to determine whether there is an algorithm that outperforms the 1.5 approximation algorithm of Christofides to the metric traveling salesman problem. The desire to understand hard optimization problems from the perspective of approximability is motivated by the discovery of surprising mathematical connections and broadly applicable techniques to design algorithms for hard optimization problems. One well-known example of the former is the Goemans–Williamson algorithm for maximum cut, which solves a graph theoretic problem using high dimensional geometry.","There is widespread interest in theoretical computer science to better understand the limits to which we can approximate certain famous optimization problems. For example, one of the long-standing open questions in computer science is to determine whether there is an algorithm that outperforms the 1.5 approximation algorithm of Christofides to the metric traveling salesman problem.","[' What is a long-standing open question in computer science?', ' What algorithm outperforms the 1.5 approximation algorithm?', ' What algorithm outperforms the 1.5 approximation algorithm of Christofides to the metric traveling salesman problem?']","['to determine whether there is an algorithm that outperforms the 1.5 approximation algorithm of Christofides to the metric traveling salesman problem', 'Christofides', 'algorithm']"
2724,approximation algorithms,Introduction,"A simple example of an approximation algorithm is one for the minimum vertex cover problem, where the goal is to choose the smallest set of vertices such that every edge in the input graph contains at least one chosen vertex. One way to find a vertex cover is to repeat the following process: find an uncovered edge, add both its endpoints to the cover, and remove all edges incident to either vertex from the graph. As any vertex cover of the input graph must use a distinct vertex to cover each edge that was considered in the process (since it forms a matching), the vertex cover produced, therefore, is at most twice as large as the optimal one. In other words, this is a constant factor approximation algorithm with an approximation factor of 2. Under the recent unique games conjecture, this factor is even the best possible one.","A simple example of an approximation algorithm is one for the minimum vertex cover problem, where the goal is to choose the smallest set of vertices such that every edge in the input graph contains at least one chosen vertex. One way to find a vertex cover is to repeat the following process: find an uncovered edge, add both its endpoints to the cover, and remove all edges incident to either vertex from the graph.","[' What is the goal of an approximation algorithm?', ' What is one way to find a vertex cover?', ' How do you find an uncovered edge, add both its endpoints to the cover and remove all edges incident to either vertex from the graph?']","['to choose the smallest set of vertices', 'to repeat the following process', 'repeat the following process']"
2725,approximation algorithms,Introduction,"NP-hard problems vary greatly in their approximability; some, such as the knapsack problem, can be approximated within a multiplicative factor 



1
+
ϵ


{\displaystyle 1+\epsilon }
, for any fixed 



ϵ
>
0


{\displaystyle \epsilon >0}
, and therefore produce solutions arbitrarily close to the optimum (such a family of approximation algorithms is called a polynomial time approximation scheme or PTAS). Others are impossible to approximate within any constant, or even polynomial, factor unless P = NP, as in the case of the maximum clique problem. Therefore, an important benefit of studying approximation algorithms is a fine-grained classification of the difficulty of various NP-hard problems beyond the one afforded by the theory of NP-completeness. In other words, although NP-complete problems may be equivalent (under polynomial time reductions) to each other from the perspective of exact solutions, the corresponding optimization problems behave very differently from the perspective of approximate solutions.
","NP-hard problems vary greatly in their approximability; some, such as the knapsack problem, can be approximated within a multiplicative factor 



1
+
ϵ


{\displaystyle 1+\epsilon }
, for any fixed 



ϵ
>
0


{\displaystyle \epsilon >0}
, and therefore produce solutions arbitrarily close to the optimum (such a family of approximation algorithms is called a polynomial time approximation scheme or PTAS). Others are impossible to approximate within any constant, or even polynomial, factor unless P = NP, as in the case of the maximum clique problem.","[' What type of problem can be approximated within a multiplicative factor 1 + <unk> <unk>displaystyle 1+<unk>epsilon <unk>?', ' What is a family of approximation algorithms called?', ' What is the family of approximation algorithms called?', ' What is PTAS?']","['knapsack problem', 'polynomial time approximation scheme', 'polynomial time approximation scheme', 'polynomial time approximation scheme']"
2726,approximation algorithms,Algorithm design techniques,"By now there are several established techniques to design approximation algorithms. These include the following ones.
",By now there are several established techniques to design approximation algorithms. These include the following ones.,[' How many established techniques are there to design approximation algorithms?'],['several']
2727,approximation algorithms,A posteriori guarantees,"While approximation algorithms always provide an a priori worst case guarantee (be it additive or multiplicative), in some cases they also provide an a posteriori guarantee that is often much better. This is often the case for algorithms that work by solving a convex relaxation of the optimization problem on the given input. For example, there is a different approximation algorithm for minimum vertex cover that solves a linear programming relaxation to find a vertex cover that is at most twice the value of the relaxation. Since the value of the relaxation is never larger than the size of the optimal vertex cover, this yields another 2-approximation algorithm. While this is similar to the a priori guarantee of the previous approximation algorithm, the guarantee of the latter can be much better (indeed when the value of the LP relaxation is far from the size of the optimal vertex cover).
","While approximation algorithms always provide an a priori worst case guarantee (be it additive or multiplicative), in some cases they also provide an a posteriori guarantee that is often much better. This is often the case for algorithms that work by solving a convex relaxation of the optimization problem on the given input.","[' What guarantee do approximation algorithms always provide?', ' What is often the case for algorithms that work by solving a convex relaxation of the optimization problem?', ' By solving a convex relaxation of the optimization problem on the given input?']","['a priori worst case guarantee', 'a posteriori guarantee', 'approximation algorithms']"
2728,approximation algorithms,Hardness of approximation,"Approximation algorithms as a research area is closely related to and informed by inapproximability theory where the non-existence of efficient algorithms with certain approximation ratios is proved (conditioned on widely believed hypotheses such as the P ≠ NP conjecture) by means of reductions. In the case of the metric traveling salesman problem, the best known inapproximability result rules out algorithms with an approximation ratio less than 123/122 ≈ 1.008196 unless P = NP, Karpinski, Lampis, Schmied. Coupled with the knowledge of the existence of Christofides' 1.5 approximation algorithm, this tells us that the threshold of approximability for metric traveling salesman (if it exists) is somewhere between 123/122 and 1.5.
","Approximation algorithms as a research area is closely related to and informed by inapproximability theory where the non-existence of efficient algorithms with certain approximation ratios is proved (conditioned on widely believed hypotheses such as the P ≠ NP conjecture) by means of reductions. In the case of the metric traveling salesman problem, the best known inapproximability result rules out algorithms with an approximation ratio less than 123/122 ≈ 1.008196 unless P = NP, Karpinski, Lampis, Schmied.","[' Approximation algorithms are closely related to and informed by what theory?', ' The non-existence of efficient algorithms with certain approximations ratios is proved by means of what?', ' What is the best known inapproximability result in the case of the metric traveling salesman problem?', ' What rules out algorithms with an approximation ratio less than 123/122 <unk> 1.008196?']","['inapproximability', 'reductions', 'rules out algorithms with an approximation ratio less than 123/122 ≈ 1.008196', 'inapproximability result']"
2729,approximation algorithms,Hardness of approximation,"While inapproximability results have been proved since the 1970s, such results were obtained by ad hoc means and no systematic understanding was available at the time. It is only since the 1990 result of Feige, Goldwasser, Lovász, Safra and Szegedy on the inapproximability of Independent Set and the famous PCP theorem, that modern tools for proving inapproximability results were uncovered. The PCP theorem, for example, shows that Johnson's 1974 approximation algorithms for Max SAT, set cover, independent set and coloring all achieve the optimal approximation ratio, assuming P ≠ NP.","While inapproximability results have been proved since the 1970s, such results were obtained by ad hoc means and no systematic understanding was available at the time. It is only since the 1990 result of Feige, Goldwasser, Lovász, Safra and Szegedy on the inapproximability of Independent Set and the famous PCP theorem, that modern tools for proving inapproximability results were uncovered.","[' Since when have inapproximability results been proved?', ' What was the only way to obtain the inapprovability results?', ' When did Feige, Goldwasser, Lovász, Safra and Szegedy prove the inprovability of Independent Set and PCP?', ' What is the famous PCP theorem?', ' Modern tools for proving inapproximability results were uncovered?']","['since the 1970s', 'ad hoc means', '1990', 'inapproximability of Independent Set', '1990 result of Feige, Goldwasser, Lovász, Safra and Szegedy on the inapproximability of Independent Set and the famous PCP theorem, that modern']"
2730,approximation algorithms,Practicality,"Not all approximation algorithms are suitable for direct practical applications. Some involve solving non-trivial linear programming/semidefinite relaxations (which may themselves invoke the ellipsoid algorithm), complex data structures, or sophisticated algorithmic techniques, leading to difficult implementation issues or improved running time performance (over exact algorithms) only on impractically large inputs. Implementation and running time issues aside, the guarantees provided by approximation algorithms may themselves not be strong enough to justify their consideration in practice. Despite their inability to be used ""out of the box"" in practical applications, the ideas and insights behind the design of such algorithms can often be incorporated in other ways in practical algorithms. In this way, the study of even very expensive algorithms is not a completely theoretical pursuit as they can yield valuable insights.
","Not all approximation algorithms are suitable for direct practical applications. Some involve solving non-trivial linear programming/semidefinite relaxations (which may themselves invoke the ellipsoid algorithm), complex data structures, or sophisticated algorithmic techniques, leading to difficult implementation issues or improved running time performance (over exact algorithms) only on impractically large inputs.","[' Not all approximation algorithms are suitable for what kind of applications?', ' Non-trivial linear programming/semidefinite relaxations may invoke what?']","['direct practical', 'the ellipsoid algorithm']"
2731,approximation algorithms,Practicality,"In other cases, even if the initial results are of purely theoretical interest, over time, with an improved understanding, the algorithms may be refined to become more practical. One such example is the initial PTAS for Euclidean TSP by Sanjeev Arora (and independently by Joseph Mitchell) which had a prohibitive running time of 




n

O
(
1

/

ϵ
)




{\displaystyle n^{O(1/\epsilon )}}
 for a 



1
+
ϵ


{\displaystyle 1+\epsilon }
 approximation. Yet, within a year these ideas were incorporated into a near-linear time 



O
(
n
log
⁡
n
)


{\displaystyle O(n\log n)}
 algorithm for any constant 



ϵ
>
0


{\displaystyle \epsilon >0}
.","In other cases, even if the initial results are of purely theoretical interest, over time, with an improved understanding, the algorithms may be refined to become more practical. One such example is the initial PTAS for Euclidean TSP by Sanjeev Arora (and independently by Joseph Mitchell) which had a prohibitive running time of 




n

O
(
1

/

ϵ
)




{\displaystyle n^{O(1/\epsilon )}}
 for a 



1
+
ϵ


{\displaystyle 1+\epsilon }
 approximation.","[' What may be refined over time to become more practical?', "" What was Sanjeev Arora's initial PTAS for Euclidean TSP?"", ' What was the prohibitive running time of Arora?', "" What was Joseph Mitchell's running time?""]","['algorithms', 'n\n\nO\n(\n1\n\n/\n\nϵ\n)\n\n\n\n\n{\\displaystyle n^{O(1/\\epsilon )}}\n for a \n\n\n\n1\n+\nϵ', 'n\n\nO\n(\n1\n\n/\n\nϵ\n)\n\n\n\n\n{\\displaystyle n^{O(1/\\epsilon )}}\n for a \n\n\n\n1\n+\nϵ', 'n^{O(1/\\epsilon )}}\n for a \n\n\n\n1\n+\nϵ']"
2732,approximation algorithms,Performance guarantees,"For some approximation algorithms it is possible to prove certain properties about the approximation of the optimum result. For example, a ρ-approximation algorithm A is defined to be an algorithm for which it has been proven that the value/cost, f(x), of the approximate solution A(x) to an instance x will not be more (or less, depending on the situation) than a factor ρ times the value, OPT, of an optimum solution.
","For some approximation algorithms it is possible to prove certain properties about the approximation of the optimum result. For example, a ρ-approximation algorithm A is defined to be an algorithm for which it has been proven that the value/cost, f(x), of the approximate solution A(x) to an instance x will not be more (or less, depending on the situation) than a factor ρ times the value, OPT, of an optimum solution.","[' What is it possible to prove about the approximation of the optimum result?', ' What is defined to be an algorithm for which it has been proven that the value/cost, f(x), of the approximate solution A(x) to an instance x will be proved?', ' What is a factor <unk> times the value of an optimum solution?']","['certain properties', 'ρ-approximation algorithm A', 'ρ']"
2733,approximation algorithms,Performance guarantees,"The factor ρ is called the relative performance guarantee. An approximation algorithm has an absolute performance guarantee or bounded error c, if it has been proven for every instance x that
","The factor ρ is called the relative performance guarantee. An approximation algorithm has an absolute performance guarantee or bounded error c, if it has been proven for every instance x that","[' What is the factor <unk> called?', ' An approximation algorithm has an absolute performance guarantee or what else?']","['relative performance guarantee', 'bounded error c']"
2734,approximation algorithms,Performance guarantees,"where f(y) is the value/cost of the solution y for the instance x. Clearly, the performance guarantee is greater than or equal to 1 and equal to 1 if and only if y is an optimal solution. If an algorithm A guarantees to return solutions with a performance guarantee of at most r(n), then A is said to be an r(n)-approximation algorithm and has an approximation ratio of r(n). Likewise, a problem with an r(n)-approximation algorithm is said to be r(n)-approximable or have an approximation ratio of r(n).","where f(y) is the value/cost of the solution y for the instance x. Clearly, the performance guarantee is greater than or equal to 1 and equal to 1 if and only if y is an optimal solution.","[' Where f(y) is the value/cost of the solution y for the instance x?', ' The performance guarantee is greater than or equal to what?']","['performance guarantee is greater than or equal to 1 and equal to 1 if and only if y is an optimal solution', '1']"
2735,approximation algorithms,Performance guarantees,"For minimization problems, the two different guarantees provide the same result and that for maximization problems, a relative performance guarantee of ρ is equivalent to a performance guarantee of 



r
=

ρ

−
1




{\displaystyle r=\rho ^{-1}}
. In the literature, both definitions are common but it is clear which definition is used since, for maximization problems, as ρ ≤ 1 while r ≥ 1.
","For minimization problems, the two different guarantees provide the same result and that for maximization problems, a relative performance guarantee of ρ is equivalent to a performance guarantee of 



r
=

ρ

−
1




{\displaystyle r=\rho ^{-1}}
. In the literature, both definitions are common but it is clear which definition is used since, for maximization problems, as ρ ≤ 1 while r ≥ 1.","[' How many different guarantees provide the same result for minimization problems?', ' What is equivalent to a performance guarantee of r = <unk> <unk> 1?', ' In the literature, both definitions are common but it is unclear which definition is used?', ' What is used for maximization problems?', ' What is the definition used for?']","['two', '\\rho', 'r=\\rho ^{-1', 'r=\\rho ^{-1', 'maximization problems']"
2736,approximation algorithms,Performance guarantees,"The absolute performance guarantee 





P


A




{\displaystyle \mathrm {P} _{A}}
 of some approximation algorithm A, where x refers to an instance of a problem, and where 




R

A


(
x
)


{\displaystyle R_{A}(x)}
 is the performance guarantee of A on x (i.e. ρ for problem instance x) is:
","The absolute performance guarantee 





P


A




{\displaystyle \mathrm {P} _{A}}
 of some approximation algorithm A, where x refers to an instance of a problem, and where 




R

A


(
x
)


{\displaystyle R_{A}(x)}
 is the performance guarantee of A on x (i.e. ρ for problem instance x) is:","[' What is the absolute performance guarantee of some approximation algorithm A?', ' What refers to an instance of a problem?']","['\\mathrm', 'x']"
2737,approximation algorithms,Performance guarantees,"That is to say that 





P


A




{\displaystyle \mathrm {P} _{A}}
 is the largest bound on the approximation ratio, r, that one sees over all possible instances of the problem. Likewise, the asymptotic performance ratio 




R

A


∞




{\displaystyle R_{A}^{\infty }}
 is:
","That is to say that 





P


A




{\displaystyle \mathrm {P} _{A}}
 is the largest bound on the approximation ratio, r, that one sees over all possible instances of the problem. Likewise, the asymptotic performance ratio 




R

A


∞




{\displaystyle R_{A}^{\infty }}
 is:","[' What is the largest bound on the approximation ratio that one sees over all possible instances of the problem?', ' The asymptotic performance ratio R A <unk> <unk>displaystyle R_<unk>A<unk>infty <unk> is:']","['P\n\n\nA\n\n\n\n\n{\\displaystyle \\mathrm {P} _{A}}\n is the largest bound on the approximation ratio, r', 'R\n\nA\n\n\n∞']"
2738,approximation algorithms,Performance guarantees,"That is to say that it is the same as the absolute performance ratio, with a lower bound n on the size of problem instances. These two types of ratios are used because there exist algorithms where the difference between these two is significant.
","That is to say that it is the same as the absolute performance ratio, with a lower bound n on the size of problem instances. These two types of ratios are used because there exist algorithms where the difference between these two is significant.","[' What is the same as the absolute performance ratio?', ' What is a lower bound on the size of problem instances?', ' Why are these two types of ratios used?']","['it is the same as the absolute performance ratio, with a lower bound n on the size of problem instances', 'n', 'because there exist algorithms where the difference between these two is significant']"
2739,approximation algorithms,Epsilon terms,"In the literature, an approximation ratio for a maximization (minimization) problem of c - ϵ (min: c + ϵ) means that the algorithm has an approximation ratio of c ∓ ϵ  for arbitrary ϵ > 0 but that the ratio has not (or cannot) be shown for ϵ = 0. An example of this is the optimal inapproximability — inexistence of approximation — ratio of 7 / 8 + ϵ for satisfiable MAX-3SAT instances due to Johan Håstad. As mentioned previously, when c = 1, the problem is said to have a polynomial-time approximation scheme.
","In the literature, an approximation ratio for a maximization (minimization) problem of c - ϵ (min: c + ϵ) means that the algorithm has an approximation ratio of c ∓ ϵ  for arbitrary ϵ > 0 but that the ratio has not (or cannot) be shown for ϵ = 0. An example of this is the optimal inapproximability — inexistence of approximation — ratio of 7 / 8 + ϵ for satisfiable MAX-3SAT instances due to Johan Håstad.","[' What is an approximation ratio for a maximization (minimization) problem of c - <unk> (min: c + <unk>)?', ' What does the literature say the algorithm has a ratio of for arbitrary <unk> > 0 but cannot be shown?', ' What is the ratio of 7 / 8 + <unk> for satisfiable MAX-3SAT instances due to Johan H<unk>stad?', ' What is an example of an optimal inapproximability — inexistence of approximation?']","['c ∓ ϵ  for arbitrary ϵ > 0', 'c ∓ ϵ', 'optimal inapproximability', '7 / 8 + ϵ for satisfiable MAX-3SAT instances due to Johan Håstad']"
2740,approximation algorithms,Epsilon terms,"An ϵ-term may appear when an approximation algorithm introduces a multiplicative error and a constant error while the minimum optimum of instances of size n goes to infinity as n does. In this case, the approximation ratio is c ∓ k / OPT = c ∓ o(1) for some constants c and k. Given arbitrary ϵ > 0, one can choose a large enough N such that the term k / OPT < ϵ for every n ≥ N. For every fixed ϵ, instances of size n < N can be solved by brute force, thereby showing an approximation ratio — existence of approximation algorithms with a guarantee — of c ∓ ϵ for every ϵ > 0.
","An ϵ-term may appear when an approximation algorithm introduces a multiplicative error and a constant error while the minimum optimum of instances of size n goes to infinity as n does. In this case, the approximation ratio is c ∓ k / OPT = c ∓ o(1) for some constants c and k. Given arbitrary ϵ > 0, one can choose a large enough N such that the term k / OPT < ϵ for every n ≥ N. For every fixed ϵ, instances of size n < N can be solved by brute force, thereby showing an approximation ratio — existence of approximation algorithms with a guarantee — of c ∓ ϵ for every ϵ > 0.","[' What may appear when an approximation algorithm introduces a multiplicative error and a constant error?', ' The minimum optimum of instances of size n goes to what as n does?', ' What does k / OPT = c <unk> o(1) for some constants c and k?', ' What can one choose given arbitrary <unk> > 0, given a large enough N?', ' For every fixed <unk>, instances of size n <unk> N can be what?', ' How can every fixed <unk>, instances of size n <unk> N be solved by brute force?', ' What is the existence of approximation algorithms with a guarantee?']","['An ϵ-term', 'infinity', 'approximation ratio', 'k / OPT < ϵ', 'solved by brute force', 'an approximation ratio', 'approximation ratio']"
2741,integrity constraint,Summary,"Data integrity is the maintenance of, and the assurance of, data accuracy and consistency over its entire life-cycle and is a critical aspect to the design, implementation, and usage of any system that stores, processes, or retrieves data. The term is broad in scope and may have widely different meanings depending on the specific context –  even under the same general umbrella of computing. It is at times used as a proxy term for data quality, while data validation is a prerequisite for data integrity.
Data integrity is the opposite of data corruption. The overall intent of any data integrity technique is the same: ensure data is recorded exactly as intended (such as a database correctly rejecting mutually exclusive possibilities). Moreover, upon later retrieval, ensure the data is the same as when it was originally recorded. In short, data integrity aims to prevent unintentional changes to information. Data integrity is not to be confused with data security, the discipline of protecting data from unauthorized parties.
","Data integrity is the maintenance of, and the assurance of, data accuracy and consistency over its entire life-cycle and is a critical aspect to the design, implementation, and usage of any system that stores, processes, or retrieves data. The term is broad in scope and may have widely different meanings depending on the specific context –  even under the same general umbrella of computing.","[' What is the maintenance of and the assurance of data accuracy and consistency over its entire life-cycle?', ' Data integrity is a critical aspect to the design, implementation, and usage of any system that stores, processes, or retrieves what?', ' What is broad in scope and may have widely different meanings?', ' What is broad in scope and may have widely different meanings depending on the specific context?']","['Data integrity', 'data', 'Data integrity', 'Data integrity']"
2742,integrity constraint,Summary,"Any unintended changes to data as the result of a storage, retrieval or processing operation, including malicious intent, unexpected hardware failure, and human error, is failure of data integrity. If the changes are the result of unauthorized access, it may also be a failure of data security. Depending on the data involved this could manifest itself as benign as a single pixel in an image appearing a different color than was originally recorded, to the loss of vacation pictures or a business-critical database, to even catastrophic loss of human life in a life-critical system.
","Any unintended changes to data as the result of a storage, retrieval or processing operation, including malicious intent, unexpected hardware failure, and human error, is failure of data integrity. If the changes are the result of unauthorized access, it may also be a failure of data security.","[' Unintended changes to data as a result of a storage, retrieval or processing operation are what?', ' What is a failure of data integrity?', ' If the changes are the result of an unauthorized access, it may also be what type of failure?']","['failure of data integrity', 'Any unintended changes to data', 'failure of data security']"
2743,integrity constraint,Databases,"Data integrity contains guidelines for data retention, specifying or guaranteeing the length of time data can be retained in a particular database.  To achieve data integrity, these rules are consistently and routinely applied to all data entering the system, and any relaxation of enforcement could cause errors in the data. Implementing checks on the data as close as possible to the source of input (such as human data entry), causes less erroneous data to enter the system. Strict enforcement of data integrity rules results in lower error rates, and time saved troubleshooting and tracing erroneous data and the errors it causes to algorithms.
","Data integrity contains guidelines for data retention, specifying or guaranteeing the length of time data can be retained in a particular database. To achieve data integrity, these rules are consistently and routinely applied to all data entering the system, and any relaxation of enforcement could cause errors in the data.","[' What is a guideline for data retention?', ' What does data integrity specify or guarantee?', ' How are data integrity rules applied to all data entering the system?', ' Any relaxation of enforcement could cause errors in what?']","['Data integrity', 'the length of time data can be retained in a particular database', 'consistently and routinely', 'the data']"
2744,integrity constraint,Databases,"Data integrity also includes rules defining the relations a piece of data can have to other pieces of data, such as a Customer record being allowed to link to purchased Products, but not to unrelated data such as Corporate Assets. Data integrity often includes checks and correction for invalid data, based on a fixed schema or a predefined set of rules. An example being textual data entered where a date-time value is required. Rules for data derivation are also applicable, specifying how a data value is derived based on algorithm, contributors and conditions. It also specifies the conditions on how the data value could be re-derived.
","Data integrity also includes rules defining the relations a piece of data can have to other pieces of data, such as a Customer record being allowed to link to purchased Products, but not to unrelated data such as Corporate Assets. Data integrity often includes checks and correction for invalid data, based on a fixed schema or a predefined set of rules.","[' What is a rule that defines the relations a piece of data can have to other pieces of data?', ' What does data integrity often include checks and correction for?', ' Data integrity often includes checks and correction for what?', ' Data integrity is often based on a fixed schema or a predefined set of rules?']","['Data integrity', 'invalid data', 'invalid data', 'checks and correction for invalid data']"
2745,integrity constraint,File systems,"Some filesystems (including Btrfs and ZFS) provide internal data and metadata checksumming that is used for detecting silent data corruption and improving data integrity.  If a corruption is detected that way and internal RAID mechanisms provided by those filesystems are also used, such filesystems can additionally reconstruct corrupted data in a transparent way.  This approach allows improved data integrity protection covering the entire data paths, which is usually known as end-to-end data protection.","Some filesystems (including Btrfs and ZFS) provide internal data and metadata checksumming that is used for detecting silent data corruption and improving data integrity. If a corruption is detected that way and internal RAID mechanisms provided by those filesystems are also used, such filesystems can additionally reconstruct corrupted data in a transparent way.","[' What are two filesystems that provide internal data and metadata checksumming?', ' What is used for detecting silent data corruption?', ' How can filesystems reconstruct corrupted data in a transparent way?']","['Btrfs and ZFS', 'internal data and metadata checksumming', 'internal RAID mechanisms']"
2746,business process management,Summary,"Business process management (BPM) is the discipline in which people use various methods to discover, model, analyze, measure, improve, optimize, and automate business processes. Any combination of methods used to manage a company's business processes is BPM. Processes can be structured and repeatable or unstructured and variable. Though not required, enabling technologies are often used with BPM.","Business process management (BPM) is the discipline in which people use various methods to discover, model, analyze, measure, improve, optimize, and automate business processes. Any combination of methods used to manage a company's business processes is BPM.","[' What is the discipline in which people use various methods to discover, model, analyze, measure, improve, optimize, and automate business processes?', "" What is any combination of methods used to manage a company's business processes called?""]","['Business process management', 'BPM']"
2747,business process management,Summary,"It can be differentiated from program management in that program management is concerned with managing a group of inter-dependent projects. From another viewpoint, process management includes program management. In project management, process management is the use of a repeatable process to improve the outcome of the project.","It can be differentiated from program management in that program management is concerned with managing a group of inter-dependent projects. From another viewpoint, process management includes program management.","[' How is process management different from program management?', ' What is program management concerned with managing?']","['program management is concerned with managing a group of inter-dependent projects', 'a group of inter-dependent projects']"
2748,business process management,Summary,"Key distinctions between process management and project management are repeatability and predictability. If the structure and sequence of work is unique, then it is a project. In business process management, a sequence of work can vary from instance to instance: there are gateways, conditions; business rules etc. The key is predictability: no matter how many forks in the road, we know all of them in advance, and we understand the conditions for the process to take one route or another. If this condition is met, we are dealing with a process.","Key distinctions between process management and project management are repeatability and predictability. If the structure and sequence of work is unique, then it is a project.","[' What are two key distinctions between process management and project management?', ' If the structure and sequence of work is unique, then it is a what?']","['repeatability and predictability', 'project']"
2749,business process management,Summary,"As an approach, BPM sees processes as important assets of an organization that must be understood, managed, and developed to announce and deliver value-added products and services to clients or customers. This approach closely resembles other total quality management or continual improvement process methodologies.
","As an approach, BPM sees processes as important assets of an organization that must be understood, managed, and developed to announce and deliver value-added products and services to clients or customers. This approach closely resembles other total quality management or continual improvement process methodologies.","[' What does BPM see as important assets of an organization that must be understood, managed, and developed to deliver value-added products and services to clients or customers?', ' What approach closely resembles other total quality management or continuous improvement process methodologies?']","['processes', 'BPM']"
2750,business process management,Summary,"BPM proponents also claim that this approach can be supported, or enabled, through technology. As such, many BPM articles and scholars frequently discuss BPM from one of two viewpoints: people and/or technology.
","BPM proponents also claim that this approach can be supported, or enabled, through technology. As such, many BPM articles and scholars frequently discuss BPM from one of two viewpoints: people and/or technology.","[' BPM proponents claim that what can be supported or enabled through technology?', ' Many BPM articles and scholars often discuss BPM from one of two viewpoints: people and/or what?']","['BPM', 'technology']"
2751,business process management,Summary,"BPM streamlines business processing by automating workflows; while RPA automates tasks by recording a set of repetitive activities implemented by human. Organizations maximize their business automation leveraging both technologies to achieve better results.
",BPM streamlines business processing by automating workflows; while RPA automates tasks by recording a set of repetitive activities implemented by human. Organizations maximize their business automation leveraging both technologies to achieve better results.,"[' BPM streamlines business processing by automating what?', ' RPA automates tasks by recording a set of repetitive activities implemented by who?', ' Organizations maximize their business automation by leveraging which technologies to achieve better results?']","['workflows', 'human', 'both technologies']"
2752,business process management,Definitions,"It is common to confuse BPM with a BPM suite (BPMS).   BPM is a professional discipline done by people, whereas a BPMS is a technological suite of tools designed to help the BPM professionals accomplish their goals.  BPM should also not be confused with an application or solution developed to support a particular process.  Suites and solutions represent ways of automating business processes, but automation is only one aspect of BPM.
","It is common to confuse BPM with a BPM suite (BPMS). BPM is a professional discipline done by people, whereas a BPMS is a technological suite of tools designed to help the BPM professionals accomplish their goals.","[' What does BPMS stand for?', ' What is BPM a professional discipline done by?', ' A BPM suite is a technological suite of tools designed to help what?']","['BPM suite', 'people', 'BPM professionals accomplish their goals']"
2753,business process management,Changes,"The concept of business process may be as traditional as concepts of tasks, department, production, and outputs, arising from job shop scheduling problems in the early 20th century. The management and improvement approach as of 2010, with formal definitions and technical modeling, has been around since the early 1990s (see business process modeling). Note that the term ""business process"" is sometimes used by IT practitioners as synonymous with the management of middleware processes or with integrating application software tasks.","The concept of business process may be as traditional as concepts of tasks, department, production, and outputs, arising from job shop scheduling problems in the early 20th century. The management and improvement approach as of 2010, with formal definitions and technical modeling, has been around since the early 1990s (see business process modeling).","[' When did the management and improvement approach begin?', ' What is the term for a problem with job shop scheduling?', ' Since when has technical modeling been around?']","['early 1990s', 'business process', 'early 1990s']"
2754,business process management,Changes,"Although BPM initially focused on the automation of business processes with the use of information technology, it has since been extended to integrate human-driven processes in which human interaction takes place in series or parallel with the use of technology. For example, workflow management systems can assign individual steps requiring deploying human intuition or judgment to relevant humans and other tasks in a workflow to a relevant automated system.","Although BPM initially focused on the automation of business processes with the use of information technology, it has since been extended to integrate human-driven processes in which human interaction takes place in series or parallel with the use of technology. For example, workflow management systems can assign individual steps requiring deploying human intuition or judgment to relevant humans and other tasks in a workflow to a relevant automated system.","[' What did BPM initially focus on?', ' What has BPM since been extended to integrate?', ' How can workflow management systems assign individual steps?', ' What can workflow management systems assign individual steps requiring deploying human intuition or judgment to?', ' What can other tasks in a workflow be assigned to a relevant automated system?']","['automation of business processes with the use of information technology', 'human-driven processes', 'requiring deploying human intuition or judgment to relevant humans', 'relevant humans and other tasks in a workflow to a relevant automated system', 'individual steps requiring deploying human intuition or judgment']"
2755,business process management,Changes,"As of 2010, technology has allowed the coupling of BPM with other methodologies, such as Six Sigma. Some BPM tools such as SIPOCs, process flows, RACIs, CTQs and histograms allow users to:
","As of 2010, technology has allowed the coupling of BPM with other methodologies, such as Six Sigma. Some BPM tools such as SIPOCs, process flows, RACIs, CTQs and histograms allow users to:","[' As of 2010, technology has allowed the coupling of BPM with what other methodology?', ' Some BPM tools allow users to do what?']","['Six Sigma', ':']"
2756,business process management,Changes,"This brings with it the benefit of being able to simulate changes to business processes based on real-world data (not just on assumed knowledge). Also, the coupling of BPM to industry methodologies allows users to continually streamline and optimize the process to ensure that it is tuned to its market need.","This brings with it the benefit of being able to simulate changes to business processes based on real-world data (not just on assumed knowledge). Also, the coupling of BPM to industry methodologies allows users to continually streamline and optimize the process to ensure that it is tuned to its market need.","[' What is the benefit of using BPM to simulate business processes based on real-world data?', ' What allows users to continuously streamline and optimize the process to ensure it is tuned to its market?', ' What is the process to ensure that it is tuned to its market need?']","['not just on assumed knowledge', 'the coupling of BPM to industry methodologies', 'BPM to industry methodologies allows users to continually streamline and optimize']"
2757,business process management,Changes,"As of 2012 research on BPM has paid increasing attention to the compliance of business processes. Although a key aspect of business processes is flexibility, as business processes continuously need to adapt to changes in the environment, compliance with business strategy, policies, and government regulations should also be ensured.
The compliance aspect in BPM is highly important for governmental organizations. As of 2010 BPM approaches in a governmental context largely focus on operational processes and knowledge representation.
There have been many technical studies on operational business processes in the public and private sectors, but researchers rarely take legal compliance activities into account—for instance, the legal implementation processes in public-administration bodies.","As of 2012 research on BPM has paid increasing attention to the compliance of business processes. Although a key aspect of business processes is flexibility, as business processes continuously need to adapt to changes in the environment, compliance with business strategy, policies, and government regulations should also be ensured.","[' As of 2012 research on BPM has paid increasing attention to what?', ' What is a key aspect of business processes?', ' As business processes continuously need to adapt to changes in the environment what should be ensured?']","['compliance of business processes', 'flexibility', 'compliance with business strategy, policies, and government regulations']"
2758,business process management,Suites,"A market has developed for enterprise software leveraging the business process management concepts to organize and automate processes. The recent convergence of this software from distinct pieces such as business rules engine, business process modelling, business activity monitoring and Human Workflow has given birth to integrated Business Process Management Suites.
Forrester Research, Inc recognize the BPM suite space through three different lenses: 
","A market has developed for enterprise software leveraging the business process management concepts to organize and automate processes. The recent convergence of this software from distinct pieces such as business rules engine, business process modelling, business activity monitoring and Human Workflow has given birth to integrated Business Process Management Suites.","[' What has developed a market for enterprise software leveraging business process management concepts?', ' What has given birth to integrated Business Process Management Suites?', ' Business rules engine, business process modelling, business activity monitoring and Human Workflow have all been combined into what?']","['Business Process Management Suites', 'The recent convergence of this software', 'integrated Business Process Management Suites']"
2759,business process management,Suites,"Rapid application development using no-code/low-code principles is becoming an ever prevalent feature of BPMS platforms. RAD enables businesses to deploy applications more quickly and more cost effectively, while also offering improved change and version management. Gartner notes that as businesses embrace these systems, their budgets rely less on the maintenance of existing systems and show more investment in growing and transforming them.","Rapid application development using no-code/low-code principles is becoming an ever prevalent feature of BPMS platforms. RAD enables businesses to deploy applications more quickly and more cost effectively, while also offering improved change and version management.","[' Rapid application development using no-code/low-code principles is becoming an ever-present feature of what platforms?', ' RAD enables businesses to deploy applications more quickly and more cost effectively while offering what?']","['BPMS', 'improved change and version management']"
2760,business process management,Practice,"While the steps can be viewed as a cycle, economic or time constraints are likely to limit the process to only a few iterations.  This is often the case when an organization uses the approach for short to medium term objectives rather than trying to transform the organizational culture.  True iterations are only possible through the collaborative efforts of process participants.  In a majority of organizations, complexity requires enabling technology (see below) to support the process participants in these daily process management challenges.
","While the steps can be viewed as a cycle, economic or time constraints are likely to limit the process to only a few iterations. This is often the case when an organization uses the approach for short to medium term objectives rather than trying to transform the organizational culture.","[' What can be viewed as a cycle?', ' What are likely to limit the process to a few iterations?', ' When an organization uses the approach for short to medium term objectives rather than trying to transform the organizational culture?']","['the steps', 'economic or time constraints', 'often']"
2761,business process management,Practice,"Currently, the international standards for the task have limited BPM to the application in the IT sector, and ISO/IEC 15944 covers the operational aspects of the business. However, some corporations with the culture of best practices do use standard operating procedures to regulate their operational process.  Other standards are currently being worked upon to assist in BPM implementation (BPMN, enterprise architecture, Business Motivation Model).
","Currently, the international standards for the task have limited BPM to the application in the IT sector, and ISO/IEC 15944 covers the operational aspects of the business. However, some corporations with the culture of best practices do use standard operating procedures to regulate their operational process.","[' What does ISO/IEC 15944 cover?', ' What do some corporations with a culture of best practices use?']","['the operational aspects of the business', 'standard operating procedures']"
2762,real-time,Summary,"Real-time or real time describes various operations in computing or other processes that must guarantee response times within a specified time (deadline), usually a relatively short time. A real-time process is generally one that happens in defined time steps of maximum duration and fast enough to affect the environment in which it occurs, such as inputs to a computing system.
","Real-time or real time describes various operations in computing or other processes that must guarantee response times within a specified time (deadline), usually a relatively short time. A real-time process is generally one that happens in defined time steps of maximum duration and fast enough to affect the environment in which it occurs, such as inputs to a computing system.","[' What does real-time refer to?', ' What is the term for a process that must guarantee response times within a specified time?', ' How long is a real time process usually?', ' What is the maximum duration?', ' What is fast enough to affect the environment in which it occurs?']","['various operations in computing or other processes', 'Real-time or real time', 'relatively short time', 'real-time process is generally one that happens in defined time steps', 'real-time process']"
2763,predicate symbol,Summary,"In mathematical logic, a predicate variable is a predicate letter which functions as a ""placeholder"" for a relation (between terms), but which has not been specifically assigned any particular relation (or meaning). Common symbols for denoting predicate variables include capital roman letters such as 



P


{\displaystyle P}
, 



Q


{\displaystyle Q}
 and 



R


{\displaystyle R}
, and common variables such as 



x


{\displaystyle x}
. In first-order logic, they can be more properly called metalinguistic variables. In higher-order logic, predicate variables correspond to propositional variables which can stand for well-formed formulas of the same logic, and such variables can be quantified by means of (at least) second-order quantifiers.
","In mathematical logic, a predicate variable is a predicate letter which functions as a ""placeholder"" for a relation (between terms), but which has not been specifically assigned any particular relation (or meaning). Common symbols for denoting predicate variables include capital roman letters such as 



P


{\displaystyle P}
, 



Q


{\displaystyle Q}
 and 



R


{\displaystyle R}
, and common variables such as 



x


{\displaystyle x}
.","[' What is a predicate letter that functions as a ""placeholder"" for a relation (between terms)?', ' What are some common symbols for denoting predicates variables?', ' What are some examples of roman letters?', ' What are examples of common variables?']","['a predicate variable', 'P\n\n\n{\\displaystyle P}\n, \n\n\n\nQ\n\n\n{\\displaystyle Q}\n and \n\n\n\nR\n\n\n{\\displaystyle R}\n, and common variables such as \n\n\n\nx', 'P\n\n\n{\\displaystyle P}\n, \n\n\n\nQ', 'x\n\n\n{\\displaystyle x}']"
2764,predicate symbol,Usage,"In the metavariable sense, a predicate variable can be used to define an axiom schema. Predicate variables should be distinguished from predicate constants, which could be represented either with a different (exclusive) set of predicate letters, or by their own symbols which really do have their own specific meaning in their domain of discourse: e.g. 



=
,
 
∈
,
 
≤
,
 
<
,
 
⊂
,
.
.
.


{\displaystyle =,\ \in ,\ \leq ,\ <,\ \subset ,...}
.
","In the metavariable sense, a predicate variable can be used to define an axiom schema. Predicate variables should be distinguished from predicate constants, which could be represented either with a different (exclusive) set of predicate letters, or by their own symbols which really do have their own specific meaning in their domain of discourse: e.g.","[' What can a predicate variable be used to define in the metavariable sense?', ' Predicate variables should be distinguished from what?', ' What are symbols that have their own specific meaning in their domain of discourse?']","['an axiom schema', 'predicate constants', 'their own symbols']"
2765,predicate symbol,Usage,"If letters are used for predicate constants as well as for predicate variables, then there has to be a way of distinguishing between them. For example, letters W, X, Y, Z could be designated to represent predicate variables, whereas letters A, B, C,..., U, V could represent predicate ""constants"". If these letters are not enough, then numerical subscripts can be appended after the letter in question (as in X1, X2, X3). However, if the predicate variables are not perceived (or defined) as belonging to the vocabulary of the predicate calculus, then they are predicate metavariables, whereas the rest of the predicate letters are just called ""predicate letters"". The metavariables are thus understood to be used to code for axiom schemata and theorem schemata (derived from the axiom schemata).
","If letters are used for predicate constants as well as for predicate variables, then there has to be a way of distinguishing between them. For example, letters W, X, Y, Z could be designated to represent predicate variables, whereas letters A, B, C,..., U, V could represent predicate ""constants"".","[' What could be designated to represent predicate constants?', ' What would be the letter A, B, C,..., U, V used for?']","['letters A, B, C,..., U, V', 'predicate ""constants']"
2766,predicate symbol,Usage,"Another option is to use Greek lower-case letters to represent such metavariable predicates. Then, such letters could be used to represent entire well-formed formulae (wff) of the predicate calculus: any free variable terms of the wff could be incorporated as terms of the Greek-letter predicate. This is the first step towards creating a higher-order logic.
","Another option is to use Greek lower-case letters to represent such metavariable predicates. Then, such letters could be used to represent entire well-formed formulae (wff) of the predicate calculus: any free variable terms of the wff could be incorporated as terms of the Greek-letter predicate.","[' What could be used to represent whole well-formed formulae of the predicate calculus?', ' What could free variable terms of the wff be incorporated as terms of?']","['Greek lower-case letters', 'Greek-letter predicate']"
2767,twitter,Summary,"Twitter is an American microblogging and social networking service on which users post and interact with messages known as ""tweets"". Registered users can post, like, and retweet tweets, but unregistered users can only read those that are publicly available. Users interact with Twitter through browser or mobile frontend software, or programmatically via its APIs. Prior to April 2020, services were accessible via SMS. The service is provided by Twitter, Inc., a corporation based in San Francisco, California, and has more than 25 offices around the world. Tweets were originally restricted to 140 characters, but the limit was doubled to 280 for non-CJK languages in November 2017. Audio and video tweets remain limited to 140 seconds for most accounts.
","Twitter is an American microblogging and social networking service on which users post and interact with messages known as ""tweets"". Registered users can post, like, and retweet tweets, but unregistered users can only read those that are publicly available.","[' What is the name of the American microblogging and social networking service?', ' What are tweets?', ' Registered users can post, like, and retweet what?', ' Unregistered users can only read tweets that are publicly available?']","['Twitter', 'users post and interact with messages', 'tweets', 'Twitter is an American microblogging and social networking service on which users post and interact with messages known as ""tweets"". Registered users can post, like, and retweet tweets']"
2768,twitter,Summary,"Twitter was created by Jack Dorsey, Noah Glass, Biz Stone, and Evan Williams in March 2006 and launched in July of that year. By 2012, more than 100 million users posted 340 million tweets a day, and the service handled an average of 1.6 billion search queries per day. In 2013, it was one of the ten most-visited websites and has been described as ""the SMS of the Internet"". As of Q1 2019, Twitter had more than 330 million monthly active users. In practice, the vast majority of tweets are written by a minority of users.","Twitter was created by Jack Dorsey, Noah Glass, Biz Stone, and Evan Williams in March 2006 and launched in July of that year. By 2012, more than 100 million users posted 340 million tweets a day, and the service handled an average of 1.6 billion search queries per day.","[' Who created Twitter?', ' When did Jack Dorsey, Noah Glass, Biz Stone, and Evan Williams start using the service?', ' By 2012, how many users had posted 340 million tweets a day?', ' How many search queries per day did the service handle?']","['Jack Dorsey, Noah Glass, Biz Stone, and Evan Williams', 'March 2006', 'more than 100 million', '1.6 billion']"
2769,twitter,Leadership,"As chief executive officer, Dorsey saw the startup through two rounds of capital funding by the venture capitalists who backed the company. On October 16, 2008, Williams took over the role of CEO, and Dorsey became chairman of the board. On October 4, 2010, Williams announced that he was stepping down as CEO. Dick Costolo, formerly Twitter's chief operating officer, became CEO. On October 4, 2010, Williams made an announcement saying that he will stay with the company and ""be completely focused on product strategy"".","As chief executive officer, Dorsey saw the startup through two rounds of capital funding by the venture capitalists who backed the company. On October 16, 2008, Williams took over the role of CEO, and Dorsey became chairman of the board.","[' How many rounds of capital funding did Dorsey oversee?', ' When did Williams take over the CEO role?', ' Who became chairman of the board in 2008?']","['two', 'October 16, 2008', 'Williams']"
2770,twitter,Leadership,"According to The New York Times, ""Mr. Dorsey and Mr. Costolo forged a close relationship"" when Williams was away. According to PC Magazine, Williams was ""no longer involved in the day-to-day goings on at the company"". He was focused on developing a new startup, and became a member of Twitter's board of directors, and promised to ""help in any way [he could]"". In 2011, Stone was still with Twitter but was working with AOL as an ""advisor on volunteer efforts and philanthropy"". In January 2014, Stone announced the release of Jelly, a 'social Q&A network for mobile'. Dorsey rejoined Twitter in March 2011, as executive chairman focusing on product development. At that time, he split his schedule with Square (where he is CEO), whose offices are within walking distance of Twitter's in San Francisco.","According to The New York Times, ""Mr. Dorsey and Mr. Costolo forged a close relationship"" when Williams was away. According to PC Magazine, Williams was ""no longer involved in the day-to-day goings on at the company"".","[' Who did Dorsey and Costolo forged a close relationship with when Williams was away?', ' What magazine reported that Williams was no longer involved in the day-to-day goings on at the company?']","['Mr. Dorsey and Mr. Costolo', 'PC Magazine']"
2771,twitter,Leadership,"In September 2011, board members and investors Fred Wilson and Bijan Sabet resigned from Twitter's board of directors. In October 2012, Twitter announced it had hired former Google executive Matt Derella to become their new director of business agency development. Twitter named former Goldman Sachs executive Anthony Noto as the company's CFO in July 2014, with an ""annual salary of $250,000 and one-time restricted stock options of 1.5 million shares ... valued at $61.5 million"". On June 10, 2015, Twitter announced its CEO Dick Costolo would resign on July 1, 2015. Noto was said to be considered a potential replacement for outgoing CEO Costolo. On October 14, 2015, former Google chief business officer Omid Kordestani became executive chairman, replacing Dorsey who remains CEO. On January 26, 2016, Leslie Berland, former executive vice president of global advertising, marketing, and digital partnerships at American Express, was named chief marketing officer. In November 2016, COO Adam Bain announced his resignation and CFO Anthony Noto took over Bain's role. A month later, on December 20, 2016, CTO Adam Messinger announced that he too was leaving.","In September 2011, board members and investors Fred Wilson and Bijan Sabet resigned from Twitter's board of directors. In October 2012, Twitter announced it had hired former Google executive Matt Derella to become their new director of business agency development.","["" When did Fred Wilson and Bijan Sabet resign from Twitter's board of directors?"", ' When did Twitter announce that it had hired Matt Derella to become their new director of business agency development?']","['September 2011', 'October 2012']"
2772,twitter,Leadership,"In February 2020, it was reported that Elliott Management Corporation had acquired a stake in Twitter, with activist shareholder and Republican Party supporter Paul Singer expected to seek the removal of Dorsey as CEO. Twitter agreed to appoint a new independent director and two new board members, and to perform $2 billion in share buybacks.","In February 2020, it was reported that Elliott Management Corporation had acquired a stake in Twitter, with activist shareholder and Republican Party supporter Paul Singer expected to seek the removal of Dorsey as CEO. Twitter agreed to appoint a new independent director and two new board members, and to perform $2 billion in share buybacks.","[' When did Elliott Management Corporation acquire a stake in Twitter?', ' Who was expected to seek the removal of Dorsey as CEO?', ' How many new board members did Twitter agree to appoint?', ' How many new board members will be added to the board?', ' How much money will be spent on share buybacks?']","['February 2020', 'Paul Singer', 'two', 'two', '$2\xa0billion']"
2773,twitter,Leadership,"On November 29, 2021, Jack Dorsey stepped down as CEO. He was replaced by CTO Parag Agrawal.","On November 29, 2021, Jack Dorsey stepped down as CEO. He was replaced by CTO Parag Agrawal.","[' On what date did Jack Dorsey resign as CEO?', "" Who was Jack's replacement?"", ' What was the name of the new CTO?']","['November 29, 2021', 'Parag Agrawal', 'Parag Agrawal']"
2774,twitter,Usage,"Daily user estimates vary as the company does not publish statistics on active accounts. A February 2009 Compete.com blog entry ranked Twitter as the third most used social network based on their count of 6 million unique monthly visitors and 55 million monthly visits. In 2009, Twitter had a monthly user retention rate of forty percent. Twitter had annual growth of 1,382 percent, increasing from 475,000 unique visitors in February 2008 to 7 million in February 2009. Twitter's annual growth rate decreased from 7.8 percent in 2015 to 3.4 percent in 2017. An April 2017 a statista.com blog entry ranked Twitter as the tenth most used social network based on their count of 319 million monthly visitors. Its global user base in 2017 was 328 million. As per August 2018, Twitter light (data saving app) is available in 45 countries.",Daily user estimates vary as the company does not publish statistics on active accounts. A February 2009 Compete.com blog entry ranked Twitter as the third most used social network based on their count of 6 million unique monthly visitors and 55 million monthly visits.,"[' How many unique monthly visitors does Twitter receive?', ' How many monthly visits does Twitter get?', ' What is the third most used social network?']","['6\xa0million', '55\xa0million', 'Twitter']"
2775,twitter,Branding,"Twitter has become internationally identifiable by its signature bird logo, or the Twitter Bird. The original logo, which was simply the word Twitter, was in use from its launch in March 2006. It was accompanied by an image of a bird which was later discovered to be a piece of clip art created by the British graphic designer Simon Oxley. A new logo had to be redesigned by founder Biz Stone with help from designer Philip Pascuzzo, which resulted in a more cartoon-like bird in 2009. This version had been named ""Larry the Bird"" after Larry Bird of the NBA's Boston Celtics fame.","Twitter has become internationally identifiable by its signature bird logo, or the Twitter Bird. The original logo, which was simply the word Twitter, was in use from its launch in March 2006.","["" What is Twitter's signature bird logo?"", ' What was the original logo for Twitter?', ' When was the Twitter Bird logo first used?']","['Twitter Bird', 'Twitter Bird. The original logo, which was simply the word Twitter', 'March 2006']"
2776,twitter,Branding,"Within a year, the Larry the Bird logo underwent a redesign by Stone and Pascuzzo to eliminate the cartoon features, leaving a solid silhouette of Larry the Bird that was used from 2010 through 2012. In 2012, Douglas Bowman created a further simplified version of Larry the Bird, keeping the solid silhouette but making it more similar to a mountain bluebird. This new logo was called simply the ""Twitter Bird"" and has been used as the company's branding since.","Within a year, the Larry the Bird logo underwent a redesign by Stone and Pascuzzo to eliminate the cartoon features, leaving a solid silhouette of Larry the Bird that was used from 2010 through 2012. In 2012, Douglas Bowman created a further simplified version of Larry the Bird, keeping the solid silhouette but making it more similar to a mountain bluebird.","[' When did Stone and Pascuzzo redesign the Larry the Bird logo?', ' Who created a simplified version of Larry the bird in 2012?', ' What is a more simplified version of Larry the Bird?']","['Within a year', 'Douglas Bowman', 'Douglas Bowman']"
2777,twitter,"Finances <span id=""Revenue""></span>","
For the fiscal year 2017, Twitter reported losses of US$108 million, with an annual revenue of $2.443 billion, a decrease of 3.9% over the previous fiscal cycle. Twitter's shares traded at over $17 per share, and its market capitalization was valued at over US$25.6 billion in October 2018.
","
For the fiscal year 2017, Twitter reported losses of US$108 million, with an annual revenue of $2.443 billion, a decrease of 3.9% over the previous fiscal cycle. Twitter's shares traded at over $17 per share, and its market capitalization was valued at over US$25.6 billion in October 2018.","[' How much revenue did Twitter generate in 2017?', "" How much did Twitter's revenue decrease in 2017 relative to the previous fiscal year?"", ' What was the market capitalization of Twitter in October 2018?']","['$2.443\xa0billion', '3.9%', 'US$25.6\xa0billion']"
2778,twitter,Developers,"Twitter is recognized for having one of the most open and powerful developer APIs of any major technology company. Developer interest in Twitter began immediately following its launch, prompting the company to release the first version of its public API in September 2006. The API quickly became iconic as a reference implementation for public REST APIs and is widely cited in programming tutorials.","Twitter is recognized for having one of the most open and powerful developer APIs of any major technology company. Developer interest in Twitter began immediately following its launch, prompting the company to release the first version of its public API in September 2006.","[' What is one of the most open and powerful developer APIs of any major technology company?', ' When did Twitter release the first version of its public API?']","['Twitter', 'September 2006']"
2779,twitter,Developers,"From 2006 until 2010, Twitter's developer platform experienced strong growth and a highly favorable reputation. Developers built upon the public API to create the first Twitter mobile phone clients as well as the first URL shortener. Between 2010 and 2012, however, Twitter made a number of decisions that were received unfavorably by the developer community. In 2010, Twitter mandated that all developers adopt OAuth authentication with just 9 weeks of notice. Later that year, Twitter launched its own URL shortener, in direct competition with some of its most well-known 3rd-party developers. And in 2012, Twitter introduced strict usage limits for its API, ""completely crippling"" some developers. While these moves successfully increased the stability and security of the service, they were broadly perceived as hostile to developers, causing them to lose trust in the platform.","From 2006 until 2010, Twitter's developer platform experienced strong growth and a highly favorable reputation. Developers built upon the public API to create the first Twitter mobile phone clients as well as the first URL shortener.","["" When did Twitter's developer platform experience strong growth and a favorable reputation?"", ' What did developers build upon the public API to create the first Twitter mobile phone clients?']","['2006 until 2010', 'URL shortener']"
2780,twitter,Developers,"In an effort to reset its relationship with developers, Twitter acquired Crashlytics on January 28, 2013, for over US$100 million, its largest acquisition to date. Twitter committed to continue supporting and expanding the service.","In an effort to reset its relationship with developers, Twitter acquired Crashlytics on January 28, 2013, for over US$100 million, its largest acquisition to date. Twitter committed to continue supporting and expanding the service.","[' How much did Twitter pay for Crashlytics?', ' What company did Twitter buy in January of 2013?', "" How much money did Twitter spend to acquire Crashlys?<extra_id_51> What was Twitter's largest acquisition to date?""]","['$100\xa0million', 'Crashlytics', 'US$100\xa0million']"
2781,twitter,Developers,"In October 2014, Twitter announced Fabric, a suite of mobile developer tools built around Crashlytics. Fabric brought together Crashlytics, Answers (mobile app analytics), Beta (mobile app distribution), Digits (mobile app identity and authentication services), MoPub, and TwitterKit (login with Twitter and Tweet display functionality) into a single, modular SDK, allowing developers to pick and choose which features they needed while guaranteeing ease of installation and compatibility. By building Fabric on top of Crashlytics, Twitter was able to take advantage of Crashlytics' large adoption and device footprint to rapidly scale usage of MoPub and TwitterKit. Fabric reached active distribution across 1 billion mobile devices just 8 months after its launch.","In October 2014, Twitter announced Fabric, a suite of mobile developer tools built around Crashlytics. Fabric brought together Crashlytics, Answers (mobile app analytics), Beta (mobile app distribution), Digits (mobile app identity and authentication services), MoPub, and TwitterKit (login with Twitter and Tweet display functionality) into a single, modular SDK, allowing developers to pick and choose which features they needed while guaranteeing ease of installation and compatibility.","[' What did Twitter announce in October 2014?', ' What was the name of the suite of mobile developer tools built around Crashlytics?', ' Which mobile app distribution service was part of Fabric?', ' What kind of SDK allows developers to pick and choose which features they needed while guaranteeing ease of installation and compatibility?']","['Fabric', 'Fabric', 'Beta', 'modular']"
2782,twitter,Developers,"In early 2016, Twitter announced that Fabric was installed on more than 2 billion active devices and used by more than 225,000 developers. Fabric is recognized as the #1 most popular crash reporting and also the #1 mobile analytics solution among the top 200 iOS apps, beating out Google Analytics, Flurry, and MixPanel.","In early 2016, Twitter announced that Fabric was installed on more than 2 billion active devices and used by more than 225,000 developers. Fabric is recognized as the #1 most popular crash reporting and also the #1 mobile analytics solution among the top 200 iOS apps, beating out Google Analytics, Flurry, and MixPanel.","[' When did Twitter announce that Fabric was installed on more than 2 billion active devices?', ' How many developers used Fabric?', ' What is Fabric recognized as the #1 most popular crash reporting?', ' What were the top 200 apps for iOS?', ' What was the top app for iOS beat out?']","['early 2016', '225,000', 'mobile analytics solution among the top 200 iOS apps, beating out Google Analytics', 'Google Analytics, Flurry, and MixPanel', 'Google Analytics']"
2783,twitter,Television,"Twitter is increasingly used for TV to be more interactive. This effect is sometimes referred to as the second screen, ""virtual watercooler"" or social television—the practice has been called ""chatterboxing"". Twitter has been successfully used to encourage people to watch live TV events, such as the Oscars, the Super Bowl and the MTV Video Music Awards; however this strategy has proven less effective with regularly scheduled TV shows. Such direct cross-promotions have been banned from French television due to regulations against secret advertising.","Twitter is increasingly used for TV to be more interactive. This effect is sometimes referred to as the second screen, ""virtual watercooler"" or social television—the practice has been called ""chatterboxing"".","[' What is Twitter increasingly used for to be more interactive?', ' What is the second screen sometimes referred to as?']","['TV', 'virtual watercooler']"
2784,twitter,Television,"In December 2012, Twitter and Nielsen entered a multi-year agreement to produce social TV ratings, which are expected to be commercially available for the fall 2013 season as the Nielsen Twitter TV Rating. Advertising Age said Twitter had become the new TV Guide. Then in February 2013, Twitter acquired Bluefin Labs for an estimated US$50 million to $100 million. Founded in 2008 at the MIT Media Lab, Bluefin is a data miner whose analysis tells which brands (e.g., TV shows and companies) are chatted about the most in social media. MIT Technology Review said that Bluefin gives Twitter part of the US$72 billion television advertising market.","In December 2012, Twitter and Nielsen entered a multi-year agreement to produce social TV ratings, which are expected to be commercially available for the fall 2013 season as the Nielsen Twitter TV Rating. Advertising Age said Twitter had become the new TV Guide.","[' When did Twitter and Nielsen enter a multi-year agreement to produce social TV ratings?', ' What is expected to be commercially available for the fall 2013 season as the Nielsen Twitter TV Rating?', ' Advertising Age said Twitter had become what?']","['December 2012', 'social TV ratings', 'the new TV Guide']"
2785,twitter,Television,"In May 2013, it launched Twitter Amplify—an advertising product for media and consumer brands. With Amplify, Twitter runs video highlights from major live broadcasts, with advertisers' names and messages playing before the clip. In October 2013, Comcast announced that it had partnered with Twitter to implement its ""See It"" feature within the service, allowing posts promoting programs on selected NBCUniversal channels to contain direct links to TV Everywhere streaming to the program. On launch, the concept was limited to NBCUniversal channels and Xfinity cable television subscribers.","In May 2013, it launched Twitter Amplify—an advertising product for media and consumer brands. With Amplify, Twitter runs video highlights from major live broadcasts, with advertisers' names and messages playing before the clip.","[' In what month and year did Twitter launch Amplify?', "" What is Twitter's advertising product for media and consumer brands?""]","['May 2013', 'Twitter Amplify']"
2786,twitter,Television,"In an attempt to compete with Twitter's leadership in TV, Facebook introduced a number of features in 2013 to drive conversation about TV including hashtags, verified profiles and embeddable posts. It also opened up new data visualization APIs for TV news and other media outlets, enabling them to search for a word and see a firehose of public posts that mention it as well as show how many people mentioned a word in both public and private posts during a set time frame, with a demographic breakdown of the age, gender, and location of these people. In January 2014, Facebook announced a partnership with UK-based social TV analytics company SecondSync which saw the social network make its social TV available outside the company for the first time. Facebook struck the partnership to help marketers understand how people are using the social network to talk about topics such as TV. However, Twitter responded by acquiring SecondSync and Parisian social TV firm Mesagraph three months later. These acquisitions, as well as a partnership with research company Kantar (which it had been working with to develop a suite of analytics tools for the British TV industry since August 2013) strengthened Twitter's dominance of the ""second screen"" – TV viewers using tablets and smartphones to share their TV experience on social media. With the additional analytic tools, Twitter was able to improve the firm's offering to advertisers, allowing them to, for instance, only promote a tweet onto the timelines of users who were watching a certain programme.","In an attempt to compete with Twitter's leadership in TV, Facebook introduced a number of features in 2013 to drive conversation about TV including hashtags, verified profiles and embeddable posts. It also opened up new data visualization APIs for TV news and other media outlets, enabling them to search for a word and see a firehose of public posts that mention it as well as show how many people mentioned a word in both public and private posts during a set time frame, with a demographic breakdown of the age, gender, and location of these people.","[' What was Facebook trying to compete with in TV?', ' What were some of the features Facebook introduced in 2013?', ' In addition to hashtags and embeddable posts, what other feature did Facebook offer TV news outlets?', ' What does news and other media outlets allow them to search for?', ' How many people mentioned a word in both public and private posts during a set time frame?', ' What is a demographic breakdown of?', ' What is a set time frame with a demographic breakdown of the age, gender, and location of these people?']","[""Twitter's leadership"", 'hashtags, verified profiles and embeddable posts', 'verified profiles', 'a word', 'Facebook introduced a number of features in 2013 to drive conversation about TV including hashtags, verified profiles and embeddable posts. It also opened up new data visualization APIs for TV news and other media outlets, enabling them to search for a word and see a firehose of public posts that mention it as well as show how many people mentioned a word in both public and private posts during a set time frame, with a demographic breakdown of the age, gender, and location of these people', 'age, gender, and location of these people', 'Facebook introduced a number of features in 2013 to drive conversation about TV including hashtags, verified profiles and embeddable posts. It also opened up new data visualization APIs for TV news and other media outlets, enabling them to search for a word and see a firehose of public posts that mention it as well as show how many people mentioned a word in both public and private posts']"
2787,twitter,Television,"By February 2014, all four major U.S. TV networks had signed up to the Amplify program, bringing a variety of premium TV content onto the social platform in the form of in-tweet real-time video clips. In March 2014, ITV became the first major broadcaster in the UK to sign up to Twitter Amplify and Twitter introduced one-tap video playback across its mobile apps to further enhance the consumer experience.","By February 2014, all four major U.S. TV networks had signed up to the Amplify program, bringing a variety of premium TV content onto the social platform in the form of in-tweet real-time video clips. In March 2014, ITV became the first major broadcaster in the UK to sign up to Twitter Amplify and Twitter introduced one-tap video playback across its mobile apps to further enhance the consumer experience.","[' How many major U.S. TV networks had signed up to the Amplify program by February 2014?', ' What is the name of the program that brings premium TV content onto the social platform in the form of in-tweet real-time video clips?', ' In what month and year did ITV become the first major broadcaster in the UK to sign up?', ' What was the first broadcaster in the UK to sign up to Twitter Amplify?', ' What did Twitter introduce to its mobile apps?']","['four', 'Amplify', 'March 2014', 'ITV', 'one-tap video playback']"
2788,twitter,Television,"In June 2014, Twitter acquired its Amplify partner in the U.S., SnappyTV. In Europe, Twitter's Amplify partner is London-based Grabyo, which has also struck numerous deals with broadcasters and rights holders to share video content across Facebook and Twitter. In July 2017, Twitter announced that it would wind down SnappyTV as a separate company, and integrate its features into the Media Studio suite on Twitter.","In June 2014, Twitter acquired its Amplify partner in the U.S., SnappyTV. In Europe, Twitter's Amplify partner is London-based Grabyo, which has also struck numerous deals with broadcasters and rights holders to share video content across Facebook and Twitter.","[' In what year did Twitter acquire SnappyTV?', "" What is Twitter's Amplify partner in the U.S.?"", ' In what city is Grabyo based?']","['2014', 'SnappyTV', 'London']"
2789,membership function,Summary,"In mathematics, an indicator function or a characteristic function of a subset of a set is a function that maps elements of the subset to one, and all other elements of the set to zero. The indicator function of a subset
A of a set X maps X to the two-element set 



{
0
,
1
}

;


{\displaystyle \{0,1\}\,;}
 





1


A


(
x
)
=
1


{\displaystyle \mathbf {1} _{A}(x)=1}
 if an element 



x


{\displaystyle x}
 in X belongs to A, and 





1


A


(
x
)
=
0


{\displaystyle \mathbf {1} _{A}(x)=0}
 if 



x


{\displaystyle x}
 does not belong to A. It may be denoted as 





1


A


,


{\displaystyle \mathbf {1} _{A},}
 by 




I

A


,


{\displaystyle I_{A},}
 or by 




χ

A


.


{\displaystyle \chi _{A}.}

","In mathematics, an indicator function or a characteristic function of a subset of a set is a function that maps elements of the subset to one, and all other elements of the set to zero. The indicator function of a subset
A of a set X maps X to the two-element set 



{
0
,
1
}

;


{\displaystyle \{0,1\}\,;}
 





1


A


(
x
)
=
1


{\displaystyle \mathbf {1} _{A}(x)=1}
 if an element 



x


{\displaystyle x}
 in X belongs to A, and 





1


A


(
x
)
=
0


{\displaystyle \mathbf {1} _{A}(x)=0}
 if 



x


{\displaystyle x}
 does not belong to A.","[' What is an indicator function?', ' What is a characteristic function of a subset?', ' How does the indicator function map elements of the subset to one?', ' The indicator function of subset A maps X to what?', ' Subset A of a set X maps X to what two-element set?', ' If an element x <unk>displaystyle x<unk> in X belongs to A, what is the value of subset A?']","['a function that maps elements of the subset to one, and all other elements of the set to zero', 'indicator function', '1', 'the two-element set', '{\n0\n,\n1\n}', '0']"
2790,membership function,Definition,"The Iverson bracket provides the equivalent notation, 



[
x
∈
A
]


{\displaystyle [x\in A]}
 or ⧙ x ϵ A ⧘, to be used instead of 





1


A


(
x
)

.


{\displaystyle \mathbf {1} _{A}(x)\,.}

","The Iverson bracket provides the equivalent notation, 



[
x
∈
A
]


{\displaystyle [x\in A]}
 or ⧙ x ϵ A ⧘, to be used instead of 





1


A


(
x
)

. {\displaystyle \mathbf {1} _{A}(x)\,.}","[' What does the Iverson bracket provide?', ' What is the equivalent notation to 1 A?']","['[\nx\n∈\nA\n]\n\n\n{\\displaystyle [x\\in A]}\n or ⧙ x ϵ A ⧘, to be used instead of \n\n\n\n\n\n1\n\n\nA\n\n\n(\nx\n)\n\n. {\\displaystyle \\mathbf {1} _{A}', '[\nx\n∈\nA\n]\n\n\n{\\displaystyle [x\\in A]}\n or ⧙ x ϵ A ⧘, to be used instead of \n\n\n\n\n\n1\n\n\nA\n\n\n(\nx\n)\n\n. {\\displaystyle \\mathbf {1} _{A}']"
2791,membership function,Notation and terminology,"A related concept in statistics is that of a dummy variable. (This must not be confused with ""dummy variables"" as that term is usually used in mathematics, also called a bound variable.)
","A related concept in statistics is that of a dummy variable. (This must not be confused with ""dummy variables"" as that term is usually used in mathematics, also called a bound variable.)","[' What is a related concept in statistics?', ' What term is usually used in mathematics?']","['a dummy variable', 'dummy variables']"
2792,membership function,Notation and terminology,"The term ""characteristic function"" has an unrelated meaning in classic probability theory. For this reason, traditional probabilists use the term indicator function for the function defined here almost exclusively, while mathematicians in other fields are more likely to use the term characteristic function to describe the function that indicates membership in a set.
","The term ""characteristic function"" has an unrelated meaning in classic probability theory. For this reason, traditional probabilists use the term indicator function for the function defined here almost exclusively, while mathematicians in other fields are more likely to use the term characteristic function to describe the function that indicates membership in a set.","[' What term has an unrelated meaning in classic probability theory?', ' Traditional probabilists use the term indicator function for the function defined here almost exclusively.', ' Mathematicians in other fields are more likely to use what term to describe the function that indicates membership?', ' What term describes the function that indicates membership in a set?']","['characteristic function', 'characteristic function', 'characteristic function', 'characteristic function']"
2793,membership function,Notation and terminology,"In fuzzy logic and modern many-valued logic, predicates are the characteristic functions of a probability distribution. That is, the strict true/false valuation of the predicate is replaced by a quantity interpreted as the degree of truth.
","In fuzzy logic and modern many-valued logic, predicates are the characteristic functions of a probability distribution. That is, the strict true/false valuation of the predicate is replaced by a quantity interpreted as the degree of truth.","[' What are the characteristic functions of a probability distribution?', ' What is replaced by a quantity interpreted as the degree of truth?']","['predicates', 'strict true/false valuation of the predicate']"
2794,membership function,Basic properties,"This mapping is surjective only when A is a non-empty proper subset of X. If 



A
≡
X
,


{\displaystyle A\equiv X,}
 then 





1


A


=
1.


{\displaystyle \mathbf {1} _{A}=1.}
 By a similar argument, if 



A
≡
∅


{\displaystyle A\equiv \emptyset }
 then  





1


A


=
0.


{\displaystyle \mathbf {1} _{A}=0.}

","This mapping is surjective only when A is a non-empty proper subset of X. If 



A
≡
X
,


{\displaystyle A\equiv X,}
 then 





1


A


=
1.","[' What mapping is surjective only when A is a non-empty proper subset of X?', ' If A <unk> X, <unk>displaystyle A<unk>equiv X,<unk> then 1 A = what?']","['A\n≡\nX', '1']"
2795,membership function,Basic properties,"In the following, the dot represents multiplication, 



1
⋅
1
=
1
,


{\displaystyle 1\cdot 1=1,}
 



1
⋅
0
=
0
,


{\displaystyle 1\cdot 0=0,}
 etc. ""+"" and ""−"" represent addition and subtraction. ""



∩


{\displaystyle \cap }
"" and ""



∪


{\displaystyle \cup }
"" is intersection and union, respectively.
","In the following, the dot represents multiplication, 



1
⋅
1
=
1
,


{\displaystyle 1\cdot 1=1,}
 



1
⋅
0
=
0
,


{\displaystyle 1\cdot 0=0,}
 etc. ""+"" and ""−"" represent addition and subtraction. ""","[' What represents multiplication?', ' What represents addition and subtraction?']","['dot', '1']"
2796,membership function,Basic properties,"and the indicator function of the complement of 



A


{\displaystyle A}
 i.e. 




A

C




{\displaystyle A^{C}}
 is:
","and the indicator function of the complement of 



A


{\displaystyle A}
 i.e. A

C




{\displaystyle A^{C}}
 is:",[' What is the indicator function of the complement of A <unk>displaystyle A<unk>?'],['A\n\nC']
2797,membership function,Basic properties,"More generally, suppose 




A

1


,
…
,

A

n




{\displaystyle A_{1},\dotsc ,A_{n}}
 is a collection of subsets of X. For any 



x
∈
X
:


{\displaystyle x\in X:}

","More generally, suppose 




A

1


,
…
,

A

n




{\displaystyle A_{1},\dotsc ,A_{n}}
 is a collection of subsets of X. For any 



x
∈
X
:


{\displaystyle x\in X:}","[' What is a collection of subsets of X?', ' For any x <unk> X : <unk>displaystyle x<unk>in X:<unk>?']","['A\n\nn', '\\dotsc']"
2798,membership function,Basic properties,"is clearly a product of 0s and 1s.  This product has the value 1 at precisely those 



x
∈
X


{\displaystyle x\in X}
 that belong to none of the sets 




A

k




{\displaystyle A_{k}}
 and is 0 otherwise. That is
","is clearly a product of 0s and 1s. This product has the value 1 at precisely those 



x
∈
X


{\displaystyle x\in X}
 that belong to none of the sets 




A

k




{\displaystyle A_{k}}
 and is 0 otherwise.","[' What is clearly a product of 0s and 1s?', ' What has the value 1 at precisely those x <unk> X <unk>displaystyle x<unk>in X<unk>?']","['x\n∈\nX', 'x\n∈\nX']"
2799,membership function,Basic properties,"As suggested by the previous example, the indicator function is a useful notational device in combinatorics.  The notation is used in other places as well, for instance in probability theory: if X is a probability space with probability measure 



P


{\displaystyle \operatorname {P} }
 and A is a measurable set, then 





1


A




{\displaystyle \mathbf {1} _{A}}
 becomes a random variable whose expected value is equal to the probability of A:
","As suggested by the previous example, the indicator function is a useful notational device in combinatorics. The notation is used in other places as well, for instance in probability theory: if X is a probability space with probability measure 



P


{\displaystyle \operatorname {P} }
 and A is a measurable set, then 





1


A




{\displaystyle \mathbf {1} _{A}}
 becomes a random variable whose expected value is equal to the probability of A:","[' What is a useful notational device in combinatorics?', ' What is used in other places as well?', ' What is a measurable set?', ' What becomes a random variable whose expected value is equal to the probability of A?']","['the indicator function', 'The notation', 'A', '1\n\n\nA']"
2800,membership function,Basic properties,"In many cases, such as order theory, the inverse of the indicator function may be defined. This is commonly called the generalized Möbius function, as a generalization of the inverse of the indicator function in elementary number theory, the Möbius function. (See paragraph below about the use of the inverse in classical recursion theory.)
","In many cases, such as order theory, the inverse of the indicator function may be defined. This is commonly called the generalized Möbius function, as a generalization of the inverse of the indicator function in elementary number theory, the Möbius function.","[' In order theory, what is the inverse of the indicator function called?', ' What is the generalized Möbius function?']","['generalized Möbius function', 'the inverse of the indicator function']"
2801,membership function,"Mean, variance and covariance","Given a probability space 




(
Ω
,


F


,
P
)



{\displaystyle \textstyle (\Omega ,{\mathcal {F}},\operatorname {P} )}
 with 



A
∈


F


,


{\displaystyle A\in {\mathcal {F}},}
 the indicator random variable 





1


A


:
Ω
→

R



{\displaystyle \mathbf {1} _{A}\colon \Omega \rightarrow \mathbb {R} }
 is defined by 





1


A


(
ω
)
=
1


{\displaystyle \mathbf {1} _{A}(\omega )=1}
 if 



ω
∈
A
,


{\displaystyle \omega \in A,}
 otherwise 





1


A


(
ω
)
=
0.


{\displaystyle \mathbf {1} _{A}(\omega )=0.}

","Given a probability space 




(
Ω
,


F


,
P
)



{\displaystyle \textstyle (\Omega ,{\mathcal {F}},\operatorname {P} )}
 with 



A
∈


F


,


{\displaystyle A\in {\mathcal {F}},}
 the indicator random variable 





1


A


:
Ω
→

R



{\displaystyle \mathbf {1} _{A}\colon \Omega \rightarrow \mathbb {R} }
 is defined by 





1


A


(
ω
)
=
1


{\displaystyle \mathbf {1} _{A}(\omega )=1}
 if 



ω
∈
A
,


{\displaystyle \omega \in A,}
 otherwise 





1


A


(
ω
)
=
0. {\displaystyle \mathbf {1} _{A}(\omega )=0.}","[' What is the indicator random variable given?', ' What is defined by 1 A ( <unk> ) = 1?']","['0', '0']"
2802,membership function,"Characteristic function in recursion theory, Gödel's and Kleene's representing function","Kurt Gödel described the representing function in his 1934 paper ""On undecidable propositions of formal mathematical systems"" (the ""¬"" indicates logical inversion, i.e. ""NOT""):: 42 ","Kurt Gödel described the representing function in his 1934 paper ""On undecidable propositions of formal mathematical systems"" (the ""¬"" indicates logical inversion, i.e. ""NOT""):: 42","[' Who described the representing function in his 1934 paper?', ' What does the ""<unk>"" indicate?']","['Kurt Gödel', 'logical inversion']"
2803,membership function,"Characteristic function in recursion theory, Gödel's and Kleene's representing function","There shall correspond to each class or relation R a representing function 



ϕ
(

x

1


,
…

x

n


)
=
0


{\displaystyle \phi (x_{1},\ldots x_{n})=0}
 if 



R
(

x

1


,
…

x

n


)


{\displaystyle R(x_{1},\ldots x_{n})}
 and 



ϕ
(

x

1


,
…

x

n


)
=
1


{\displaystyle \phi (x_{1},\ldots x_{n})=1}
 if 



¬
R
(

x

1


,
…

x

n


)
.


{\displaystyle \neg R(x_{1},\ldots x_{n}).}
","There shall correspond to each class or relation R a representing function 



ϕ
(

x

1


,
…

x

n


)
=
0


{\displaystyle \phi (x_{1},\ldots x_{n})=0}
 if 



R
(

x

1


,
…

x

n


)


{\displaystyle R(x_{1},\ldots x_{n})}
 and 



ϕ
(

x

1


,
…

x

n


)
=
1


{\displaystyle \phi (x_{1},\ldots x_{n})=1}
 if 



¬
R
(

x

1


,
…

x

n


)
. {\displaystyle \neg R(x_{1},\ldots x_{n}).}",[' What shall correspond to each class or relation R a representing function?'],['0']
2804,membership function,"Characteristic function in recursion theory, Gödel's and Kleene's representing function","For example, because the product of characteristic functions 




ϕ

1


∗

ϕ

2


∗
⋯
∗

ϕ

n


=
0


{\displaystyle \phi _{1}*\phi _{2}*\cdots *\phi _{n}=0}
 whenever any one of the functions equals 0, it plays the role of logical OR: IF 




ϕ

1


=
0


{\displaystyle \phi _{1}=0}
 OR 




ϕ

2


=
0


{\displaystyle \phi _{2}=0}
 OR ... OR 




ϕ

n


=
0


{\displaystyle \phi _{n}=0}
 THEN their product is 0. What appears to the modern reader as the representing function's logical inversion, i.e. the representing function is 0 when the function R is ""true"" or satisfied"", plays a useful role in Kleene's definition of the logical functions OR, AND, and IMPLY,: 228  the bounded-: 228  and unbounded-: 279 ff  mu operators and the CASE function.: 229 ","For example, because the product of characteristic functions 




ϕ

1


∗

ϕ

2


∗
⋯
∗

ϕ

n


=
0


{\displaystyle \phi _{1}*\phi _{2}*\cdots *\phi _{n}=0}
 whenever any one of the functions equals 0, it plays the role of logical OR: IF 




ϕ

1


=
0


{\displaystyle \phi _{1}=0}
 OR 




ϕ

2


=
0


{\displaystyle \phi _{2}=0}
 OR ... OR 




ϕ

n


=
0


{\displaystyle \phi _{n}=0}
 THEN their product is 0. What appears to the modern reader as the representing function's logical inversion, i.e.","[' What is the product of characteristic functions?', ' When any one of the functions equals 0, what plays the role of logical OR?', "" What appears to the modern reader as the representing function's logical inversion?""]","['0', 'the product of characteristic functions \n\n\n\n\nϕ\n\n1\n\n\n∗\n\nϕ\n\n2\n\n\n∗\n⋯\n∗\n\nϕ\n\nn', '0\n\n\n{\\displaystyle \\phi _{n}=0}\n THEN their product is 0']"
2805,membership function,Characteristic function in fuzzy set theory,"In classical mathematics, characteristic functions of sets only take values 1 (members) or 0 (non-members). In fuzzy set theory, characteristic functions are generalized to take value in the real unit interval [0, 1], or more generally, in some algebra or structure (usually required to be at least a poset or lattice). Such generalized characteristic functions are more usually called membership functions, and the corresponding ""sets"" are called fuzzy sets. Fuzzy sets model the gradual change in the membership degree seen in many real-world predicates like ""tall"", ""warm"", etc.
","In classical mathematics, characteristic functions of sets only take values 1 (members) or 0 (non-members). In fuzzy set theory, characteristic functions are generalized to take value in the real unit interval [0, 1], or more generally, in some algebra or structure (usually required to be at least a poset or lattice).","[' In classical mathematics, characteristic functions of sets only take values what?', ' In what theory are characteristic functions generalized to take value in the real unit interval [0, 1], or more generally in some algebra or structure?', ' What type of structure is usually required to be at least?']","['1 (members) or 0', 'fuzzy set theory', 'algebra']"
2806,membership function,Derivatives of the indicator function,"A particular indicator function is the Heaviside step function. The Heaviside step function H(x) is the indicator function of the one-dimensional positive half-line, i.e. the domain [0, ∞). The distributional derivative of the Heaviside step function is equal to the Dirac delta function, i.e.
","A particular indicator function is the Heaviside step function. The Heaviside step function H(x) is the indicator function of the one-dimensional positive half-line, i.e.","[' What is a particular indicator function called?', ' What is the indicator function of the one-dimensional positive half-line?']","['Heaviside step function', 'Heaviside step function H(x)']"
2807,membership function,Derivatives of the indicator function,"The derivative of the Heaviside step function can be seen as the inward normal derivative at the boundary of the domain given by the positive half-line. In higher dimensions, the derivative naturally generalises to the inward normal derivative, while the Heaviside step function naturally generalises to the indicator function of some domain D. The surface of D will be denoted by S. Proceeding, it can be derived that the inward normal derivative of the indicator gives rise to a 'surface delta function', which can be indicated by 




δ

S


(

x

)
:


{\displaystyle \delta _{S}(\mathbf {x} ):}

","The derivative of the Heaviside step function can be seen as the inward normal derivative at the boundary of the domain given by the positive half-line. In higher dimensions, the derivative naturally generalises to the inward normal derivative, while the Heaviside step function naturally generalises to the indicator function of some domain D. The surface of D will be denoted by S. Proceeding, it can be derived that the inward normal derivative of the indicator gives rise to a 'surface delta function', which can be indicated by 




δ

S


(

x

)
:


{\displaystyle \delta _{S}(\mathbf {x} ):}","[' What is the derivative of the Heaviside step function seen as?', ' What does the derivative naturally generalise to in higher dimensions?', ' What naturally generalises to the indicator function of some domain D?', ' The surface of D will be denoted by what?', ' What gives rise to a surface delta function?', ' What can be indicated by <unk> S ( x ): <unk>displaystyle <unk>delta_<unk>S<unk>(<unk>mathbf <unk>x<unk> )?']","['the inward normal derivative', 'the inward normal derivative', 'Heaviside step function', 'S', 'inward normal derivative of the indicator', 'surface delta function']"
2808,formal verification,Summary,"The verification of these systems is done by providing a formal proof on an abstract mathematical model of the system, the correspondence between the mathematical model and the nature of the system being otherwise known by construction.  Examples of mathematical objects often used to model systems are: finite state machines, labelled transition systems, Petri nets, vector addition systems, timed automata, hybrid automata, process algebra, formal semantics of programming languages such as operational semantics, denotational semantics, axiomatic semantics and Hoare logic.","The verification of these systems is done by providing a formal proof on an abstract mathematical model of the system, the correspondence between the mathematical model and the nature of the system being otherwise known by construction. Examples of mathematical objects often used to model systems are: finite state machines, labelled transition systems, Petri nets, vector addition systems, timed automata, hybrid automata, process algebra, formal semantics of programming languages such as operational semantics, denotational semantics, axiomatic semantics and Hoare logic.","[' What is done by providing a formal proof on an abstract mathematical model of a system?', ' What is the correspondence between the mathematical model and the nature of the system known by?', ' Finite state machines are examples of what?', ' What are some examples of objects used to model systems?', ' What is an example of formal semantics of programming languages?']","['The verification of these systems', 'construction', 'mathematical objects often used to model systems', 'finite state machines, labelled transition systems, Petri nets, vector addition systems, timed automata, hybrid automata, process algebra', 'operational semantics']"
2809,formal verification,Approaches,"One approach and formation is model checking, which consists of a systematically exhaustive exploration of the mathematical model (this is possible for finite models, but also for some infinite models where infinite sets of states can be effectively represented finitely by using abstraction or taking advantage of symmetry).  Usually this consists of exploring all states and transitions in the model, by using smart and domain-specific abstraction techniques to consider whole groups of states in a single operation and reduce computing time. Implementation techniques include state space enumeration, symbolic state space enumeration, abstract interpretation, symbolic simulation, abstraction refinement.  The properties to be verified are often described in temporal logics, such as linear temporal logic (LTL), Property Specification Language (PSL), SystemVerilog Assertions (SVA), or computational tree logic (CTL). The great advantage of model checking is that it is often fully automatic; its primary disadvantage is that it does not in general scale to large systems; symbolic models are typically limited to a few hundred bits of state, while explicit state enumeration requires the state space being explored to be relatively small.
","One approach and formation is model checking, which consists of a systematically exhaustive exploration of the mathematical model (this is possible for finite models, but also for some infinite models where infinite sets of states can be effectively represented finitely by using abstraction or taking advantage of symmetry). Usually this consists of exploring all states and transitions in the model, by using smart and domain-specific abstraction techniques to consider whole groups of states in a single operation and reduce computing time.","[' What is one approach and formation?', ' What consists of a systematically exhaustive exploration of the mathematical model?', ' By using abstraction or taking advantage of symmetry?', ' Exploring all states and transitions in the model is usually done by what?', ' Smart and domain-specific abstraction techniques are used to consider whole groups of states in a single operation and reduce computing time?']","['model checking', 'model checking', 'model checking', 'model checking', 'model checking']"
2810,formal verification,Approaches,"Another approach is deductive verification.  It consists of generating from the system and its specifications (and possibly other annotations) a collection of mathematical proof obligations, the truth of which imply conformance of the system to its specification, and discharging these obligations using either proof assistants (interactive theorem provers) (such as HOL, ACL2, Isabelle, Coq or PVS), or automatic theorem provers, including in particular satisfiability modulo theories (SMT) solvers. This approach has the disadvantage that it may require the user to understand in detail why the system works correctly, and to convey this information to the verification system, either in the form of a sequence of theorems to be proved or in the form of specifications (invariants, preconditions, postconditions) of system components (e.g. functions or procedures) and perhaps subcomponents (such as loops or data structures).
","Another approach is deductive verification. It consists of generating from the system and its specifications (and possibly other annotations) a collection of mathematical proof obligations, the truth of which imply conformance of the system to its specification, and discharging these obligations using either proof assistants (interactive theorem provers) (such as HOL, ACL2, Isabelle, Coq or PVS), or automatic theorem provers, including in particular satisfiability modulo theories (SMT) solvers.","[' What is another approach to deductive verification?', ' What is a proof assistant?', ' What are interactive theorem provers?', ' What are SMT solvers?']","['generating from the system and its specifications (and possibly other annotations) a collection of mathematical proof obligations', 'interactive theorem provers', 'proof assistants', 'satisfiability modulo theories']"
2811,formal verification,Verification and validation,"Verification is one aspect of testing a product's fitness for purpose. Validation is the complementary aspect. Often one refers to the overall checking process as V & V.
",Verification is one aspect of testing a product's fitness for purpose. Validation is the complementary aspect.,"["" What is one aspect of testing a product's fitness for purpose?"", ' What is the complementary aspect?']","['Verification', 'Validation']"
2812,formal verification,Verification and validation,"The verification process consists of static/structural and dynamic/behavioral aspects. E.g., for a software product one can inspect the source code (static) and run against specific test cases (dynamic). Validation usually can be done only dynamically, i.e., the product is tested by putting it through typical and atypical usages (""Does it satisfactorily meet all use cases?"").
","The verification process consists of static/structural and dynamic/behavioral aspects. E.g., for a software product one can inspect the source code (static) and run against specific test cases (dynamic).","[' What are the two main aspects of the verification process?', ' What is the source code of a software product called?']","['static/structural and dynamic/behavioral', 'static']"
2813,formal verification,Automated program repair,"Program repair is performed with respect to an oracle, encompassing the desired functionality of the program which is used for validation of the generated fix. A simple example is a test-suite—the input/output pairs specify the functionality of the program. A variety of techniques are employed, most notably using satisfiability modulo theories (SMT) solvers, and genetic programming, using evolutionary computing to generate and evaluate possible candidates for fixes. The former method is deterministic, while the latter is randomized.
","Program repair is performed with respect to an oracle, encompassing the desired functionality of the program which is used for validation of the generated fix. A simple example is a test-suite—the input/output pairs specify the functionality of the program.","[' What is performed with respect to an oracle?', ' What is used for validation of the generated fix?', ' The input/output pairs specify what?']","['Program repair', 'Program repair is performed with respect to an oracle', 'the functionality of the program']"
2814,formal verification,Automated program repair,"Program repair combines techniques from formal verification and program synthesis. Fault-localization techniques in formal verification are used to compute program points which might be possible bug-locations, which can be targeted by the synthesis modules. Repair systems often focus on a small pre-defined class of bugs in order to reduce the search space. Industrial use is limited owing to the computational cost of existing techniques.
","Program repair combines techniques from formal verification and program synthesis. Fault-localization techniques in formal verification are used to compute program points which might be possible bug-locations, which can be targeted by the synthesis modules.","[' What is a combination of formal verification and program synthesis?', ' What techniques are used in formal verification to compute program points?']","['Program repair', 'Fault-localization']"
2815,formal verification,Industry use,"The growth in complexity of designs increases the importance of formal verification techniques in the hardware industry. At present, formal verification is used by most or all leading hardware companies, but its use in the software industry is still languishing. This could be attributed to the greater need in the hardware industry, where errors have greater commercial significance. Because of the potential subtle interactions between components, it is increasingly difficult to exercise a realistic set of possibilities by simulation. Important aspects of hardware design are amenable to automated proof methods, making formal verification easier to introduce and more productive.","The growth in complexity of designs increases the importance of formal verification techniques in the hardware industry. At present, formal verification is used by most or all leading hardware companies, but its use in the software industry is still languishing.","[' What increases the importance of formal verification techniques in the hardware industry?', ' What industry is still languishing?']","['The growth in complexity of designs', 'software']"
2816,formal verification,Industry use,"As of 2016, Yale and Columbia professors Zhong Shao and Ronghui Gu developed a formal verification protocol for blockchain called CertiKOS. The program is the first example of formal verification in the blockchain world, and an example of formal verification being used explicitly as a security program.","As of 2016, Yale and Columbia professors Zhong Shao and Ronghui Gu developed a formal verification protocol for blockchain called CertiKOS. The program is the first example of formal verification in the blockchain world, and an example of formal verification being used explicitly as a security program.","[' In what year did Zhong Shao and Ronghui Gu develop a formal verification protocol for blockchain?', ' CertiKOS is the first example of what type of verification in the blockchain world?']","['2016', 'formal']"
2817,formal verification,Industry use,"As of 2017, formal verification has been applied to the design of large computer networks through a mathematical model of the network, and as part of a new network technology category, intent-based networking. Network software vendors that offer formal verification solutions include Cisco Forward Networks and Veriflow Systems.","As of 2017, formal verification has been applied to the design of large computer networks through a mathematical model of the network, and as part of a new network technology category, intent-based networking. Network software vendors that offer formal verification solutions include Cisco Forward Networks and Veriflow Systems.","[' As of 2017, what has been applied to the design of large computer networks?', ' What is a new network technology category?', ' Cisco Forward Networks and Veriflow Systems offer what type of verification solutions?']","['formal verification', 'intent-based networking', 'formal']"
2818,software product line,Description,"Manufacturers have long employed analogous engineering techniques to create a product line of similar products using a common factory that assembles and configures parts designed to be reused across the product line. For example, automotive manufacturers can create unique variations of one car model using a single pool of carefully designed parts and a factory specifically designed to configure and assemble those parts.
","Manufacturers have long employed analogous engineering techniques to create a product line of similar products using a common factory that assembles and configures parts designed to be reused across the product line. For example, automotive manufacturers can create unique variations of one car model using a single pool of carefully designed parts and a factory specifically designed to configure and assemble those parts.","[' What have manufacturers long employed to create a product line of similar products?', ' What assembles and configures parts designed to be reused across the product line?', ' How can manufacturers create unique variations of one car model?', ' How many carefully designed parts does one car model use?', ' What is the factory specifically designed to configure and assemble those parts?']","['analogous engineering techniques', 'a common factory', 'using a single pool of carefully designed parts and a factory specifically designed to configure and assemble those parts', 'single pool', 'automotive manufacturers can create unique variations of one car model']"
2819,software product line,Description,"The characteristic that distinguishes software product lines from previous efforts is predictive versus opportunistic software reuse. Rather than put general software components into a library in the hope that opportunities for reuse will arise, software product lines only call for software artifacts to be created when reuse is predicted in one or more products in a well defined product line.","The characteristic that distinguishes software product lines from previous efforts is predictive versus opportunistic software reuse. Rather than put general software components into a library in the hope that opportunities for reuse will arise, software product lines only call for software artifacts to be created when reuse is predicted in one or more products in a well defined product line.","[' What is the characteristic that distinguishes software product lines from previous efforts?', ' Software product lines only call for what to be created when reuse is predicted in the hope that opportunities for reuse will arise?', ' What are created when reuse is predicted in one or more products in a well defined product line?']","['predictive versus opportunistic software reuse', 'software artifacts', 'software artifacts']"
2820,software product line,Description,"Recent advances in the software product line field have demonstrated that narrow and strategic application of these concepts can yield order of magnitude improvements in software engineering capability. The result is often a discontinuous jump in competitive business advantage, similar to that seen when manufacturers adopt mass production and mass customization paradigms.
","Recent advances in the software product line field have demonstrated that narrow and strategic application of these concepts can yield order of magnitude improvements in software engineering capability. The result is often a discontinuous jump in competitive business advantage, similar to that seen when manufacturers adopt mass production and mass customization paradigms.","[' What has shown that narrow and strategic application of these concepts can yield order of magnitude improvements in software engineering capability?', ' What is often a discontinuous jump in competitive advantage?', ' When do manufacturers adopt mass production and mass customization?']","['Recent advances in the software product line field', 'Recent advances in the software product line field have demonstrated that narrow and strategic application of these concepts can yield order of magnitude improvements in software engineering capability', 'discontinuous jump in competitive business advantage']"
2821,software product line,Development,"While early software product line methods at the genesis of the field provided the best software engineering improvement metrics seen in four decades, the latest generation of software product line methods and tools are exhibiting even greater improvements. New generation methods are extending benefits beyond product creation into maintenance and evolution, lowering the overall complexity of product line development, increasing the scalability of product line portfolios, and enabling organizations to make the transition to software product line practice with orders of magnitude less time, cost and effort.
","While early software product line methods at the genesis of the field provided the best software engineering improvement metrics seen in four decades, the latest generation of software product line methods and tools are exhibiting even greater improvements. New generation methods are extending benefits beyond product creation into maintenance and evolution, lowering the overall complexity of product line development, increasing the scalability of product line portfolios, and enabling organizations to make the transition to software product line practice with orders of magnitude less time, cost and effort.","[' What were the best software engineering improvement metrics seen in four decades?', ' What are the latest generation of software product line methods and tools extending beyond product creation into?', ' What are methods extending benefits beyond product creation into?', ' What is lowering the overall complexity of product line development?', ' How much less time, cost and effort does the transition to software product line practice take?']","['early software product line methods', 'maintenance and evolution', 'maintenance and evolution', 'New generation methods', 'orders of magnitude']"
2822,software product line,Development,Recently the concepts of software product lines have been extended to cover systems and software engineering holistically. This is reflected by the emergence of industry standard families like ISO 265xx on systems and software engineering practices for product lines.,Recently the concepts of software product lines have been extended to cover systems and software engineering holistically. This is reflected by the emergence of industry standard families like ISO 265xx on systems and software engineering practices for product lines.,"[' What has been extended to cover systems and software engineering holistically?', ' What is ISO 265xx?']","['software product lines', 'industry standard families']"
2823,communication,Summary,"Communication (from Latin communicare, meaning ""to share"" or ""to be in relation with"") is ""an apparent answer to the painful divisions between self and other, private and public, and inner thought and outer world."" As this definition indicates, communication is difficult to define in a consistent manner, because it is commonly used to refer to a wide range of different behaviors (broadly: ""the transfer of information""), or to limit what can be included in the category of communication (for example, requiring a ""conscious intent"" to persuade). John Peters argues the difficulty of defining communication emerges from the fact that communication is both a universal phenomenon (because everyone communicates) and a specific discipline of institutional academic study.","Communication (from Latin communicare, meaning ""to share"" or ""to be in relation with"") is ""an apparent answer to the painful divisions between self and other, private and public, and inner thought and outer world."" As this definition indicates, communication is difficult to define in a consistent manner, because it is commonly used to refer to a wide range of different behaviors (broadly: ""the transfer of information""), or to limit what can be included in the category of communication (for example, requiring a ""conscious intent"" to persuade).","[' What Latin word means ""to share"" or ""to be in relation with""?', ' What is communication an apparent answer to?', ' What is difficult to define in a consistent manner?', ' What is commonly used to refer to a wide range of different behaviors?']","['Communication (from Latin communicare', 'the painful divisions between self and other, private and public, and inner thought and outer world', 'Communication', 'Communication']"
2824,communication,Summary,"In Claude Shannon's and Warren Weaver's influential model, human communication was imagined to function like a telephone or telegraph. Accordingly, they conceptualized communication as involving discrete steps:
","In Claude Shannon's and Warren Weaver's influential model, human communication was imagined to function like a telephone or telegraph. Accordingly, they conceptualized communication as involving discrete steps:","["" What was Claude Shannon's and Warren Weaver's influential model of human communication?"", ' What was human communication imagined to function like?']","['a telephone or telegraph', 'a telephone or telegraph']"
2825,communication,Summary,"These elements are now understood to be substantially overlapping and recursive activities rather than steps in a sequence. For example, communicative actions can commence before a communicator formulates a conscious attempt to do so, as in the case of phatics; likewise, communicators modify their intentions and formulations of a message in response to real-time feedback (e.g., a change in facial expression). Practices of decoding and interpretation are culturally enacted, not just by individuals (genre conventions, for instance, trigger anticipatory expectations for how a message is to be received), and receivers of any message operationalize their own frames of reference in interpretation.","These elements are now understood to be substantially overlapping and recursive activities rather than steps in a sequence. For example, communicative actions can commence before a communicator formulates a conscious attempt to do so, as in the case of phatics; likewise, communicators modify their intentions and formulations of a message in response to real-time feedback (e.g., a change in facial expression).","[' What are overlapping and recursive activities now understood to be?', ' What can communicative actions begin before a communicator formulates a conscious attempt to do?', ' What do communicators do in response to real-time feedback?']","['steps in a sequence', 'phatics', 'modify their intentions and formulations of a message']"
2826,communication,Summary,"Communication can be realized visually (through images and written language), through auditory, tactile/haptic (e.g. Braille or other physical means), olfactory, electromagnetic, or biochemical means (or any combination thereof). Human communication is unique for its extensive use of abstract language.
","Communication can be realized visually (through images and written language), through auditory, tactile/haptic (e.g. Braille or other physical means), olfactory, electromagnetic, or biochemical means (or any combination thereof).","[' How can communication be realized?', ' What can be realized through visual means?']","['visually', 'Communication']"
2827,communication,Communication models,"The first major model for communication was introduced by Claude Shannon and Warren Weaver for Bell Laboratories in 1949 The original model was designed to mirror the functioning of radio and telephone technologies. Their initial model consisted of three primary parts: sender, channel, and receiver. The sender was the part of a telephone a person spoke into, the channel was the telephone itself, and the receiver was the part of the phone where one could hear the other person. Shannon and Weaver also recognized that often there is static that interferes with one listening to a telephone conversation, which they deemed noise.
","The first major model for communication was introduced by Claude Shannon and Warren Weaver for Bell Laboratories in 1949 The original model was designed to mirror the functioning of radio and telephone technologies. Their initial model consisted of three primary parts: sender, channel, and receiver.","[' When was the first major model for communication introduced?', ' What was the original model designed to mirror?', ' How many primary parts did the initial model consist of?']","['1949', 'the functioning of radio and telephone technologies', 'three']"
2828,communication,Communication models,"In a simple model, often referred to as the transmission model or standard view of communication, information or content (e.g. a message in natural language) is sent in some form (as spoken language) from an emitter (emisor in the picture)/sender/encoder to a destination/receiver/decoder. This common conception of communication simply views communication as a means of sending and receiving information. The strengths of this model are simplicity, generality, and quantifiability. Claude Shannon and Warren Weaver structured this model based on the following elements:
","In a simple model, often referred to as the transmission model or standard view of communication, information or content (e.g. a message in natural language) is sent in some form (as spoken language) from an emitter (emisor in the picture)/sender/encoder to a destination/receiver/decoder.","[' What is a simple model often referred to as?', ' What is sent in some form as spoken language?']","['the transmission model', 'information or content']"
2829,communication,Communication models,"In 1960, David Berlo expanded on Shannon and Weaver's (1949) linear model of communication and created the SMCR Model of Communication. The Sender-Message-Channel-Receiver Model of communication separated the model into clear parts and has been expanded upon by other scholars.
","In 1960, David Berlo expanded on Shannon and Weaver's (1949) linear model of communication and created the SMCR Model of Communication. The Sender-Message-Channel-Receiver Model of communication separated the model into clear parts and has been expanded upon by other scholars.","[' In what year did David Berlo create the SMCR Model of Communication?', ' What model of communication was expanded upon?', "" Who expanded on Shannon and Weaver's model of communications?""]","['1960', 'Sender-Message-Channel-Receiver Model', 'David Berlo']"
2830,communication,Communication models,"Communication is usually described along a few major dimensions: message (what type of things are communicated), source/emisor/sender/encoder (from whom), form (in which form), channel (through which medium), destination/receiver/target/decoder (to whom). Wilbur Schram (1954) also indicated that we should also examine the impact that a message has (both desired and undesired) on the target of the message. Between parties, communication includes acts that confer knowledge and experiences, give advice and commands, and ask questions. These acts may take many forms, in one of the various manners of communication. The form depends on the abilities of the group communicating. Together, communication content and form make messages that are sent towards a destination. The target can be oneself, another person or being, another entity (such as a corporation or group of beings).
","Communication is usually described along a few major dimensions: message (what type of things are communicated), source/emisor/sender/encoder (from whom), form (in which form), channel (through which medium), destination/receiver/target/decoder (to whom). Wilbur Schram (1954) also indicated that we should also examine the impact that a message has (both desired and undesired) on the target of the message.","[' What is communication usually described along a few major dimensions?', ' Who indicated that we should also examine the impact that a message has (both desired and undesired)?', ' What is the impact that a message has on the target of the message?']","['message', 'Wilbur Schram', 'both desired and undesired']"
2831,communication,Communication models,"Therefore, communication is social interaction where at least two interacting agents share a common set of signs and a common set of semiotic rules. This commonly held rule in some sense ignores autocommunication, including intrapersonal communication via diaries or self-talk, both secondary phenomena that followed the primary acquisition of communicative competences within social interactions.
","Therefore, communication is social interaction where at least two interacting agents share a common set of signs and a common set of semiotic rules. This commonly held rule in some sense ignores autocommunication, including intrapersonal communication via diaries or self-talk, both secondary phenomena that followed the primary acquisition of communicative competences within social interactions.","[' What is social interaction?', ' How many interacting agents share a common set of signs and semiotic rules?', ' What ignores autocommunication?', ' What are two secondary phenomena that followed the primary acquisition of communicative competences within social interactions?']","['communication is social interaction where at least two interacting agents share a common set of signs and a common set of semiotic rules', 'at least two', 'communication', 'intrapersonal communication via diaries or self-talk']"
2832,communication,Communication models,"In light of these weaknesses, Barnlund (2008) proposed a transactional model of communication. The basic premise of the transactional model of communication is that individuals are simultaneously engaging in the sending and receiving of messages.
","In light of these weaknesses, Barnlund (2008) proposed a transactional model of communication. The basic premise of the transactional model of communication is that individuals are simultaneously engaging in the sending and receiving of messages.","[' What did Barnlund propose in light of these weaknesses?', ' What is the basic premise of the transactional model of communication?']","['a transactional model of communication', 'individuals are simultaneously engaging in the sending and receiving of messages']"
2833,communication,Communication models,"In a slightly more complex form a sender and a receiver are linked reciprocally. This second attitude of communication, referred to as the constitutive model or constructionist view, focuses on how an individual communicates as the determining factor of the way the message will be interpreted. Communication is viewed as a conduit; a passage in which information travels from one individual to another and this information becomes separate from the communication itself. A particular instance of communication is called a speech act. The sender's personal filters and the receiver's personal filters may vary depending upon different regional traditions, cultures, or gender; which may alter the intended meaning of message contents. In the presence of ""communication noise"" on the transmission channel (air, in this case), reception and decoding of content may be faulty, and thus the speech act may not achieve the desired effect. One problem with this encode-transmit-receive-decode model is that the processes of encoding and decoding imply that the sender and receiver each possess something that functions as a codebook, and that these two code books are, at the very least, similar if not identical. Although something like code books is implied by the model, they are nowhere represented in the model, which creates many conceptual difficulties.
","In a slightly more complex form a sender and a receiver are linked reciprocally. This second attitude of communication, referred to as the constitutive model or constructionist view, focuses on how an individual communicates as the determining factor of the way the message will be interpreted.","[' In a slightly more complex form a sender and a receiver are linked what?', ' What is the second attitude of communication referred to as?', ' The constitutive model or constructionist view focuses on how an individual communicates as the determining factor of the way the message will be interpreted?']","['reciprocally', 'the constitutive model or constructionist view', 'In a slightly more complex form a sender and a receiver are linked reciprocally']"
2834,communication,Communication models,"Theories of coregulation describe communication as a creative and dynamic continuous process, rather than a discrete exchange of information. Canadian media scholar Harold Innis had the theory that people use different types of media to communicate and which one they choose to use will offer different possibilities for the shape and durability of society. His famous example of this is using ancient Egypt and looking at the ways they built themselves out of media with very different properties stone and papyrus. Papyrus is what he called 'Space Binding'. it made possible the transmission of written orders across space, empires and enables the waging of distant military campaigns and colonial administration. The other is stone and 'Time Binding', through the construction of temples and the pyramids can sustain their authority generation to generation, through this media they can change and shape communication in their society.","Theories of coregulation describe communication as a creative and dynamic continuous process, rather than a discrete exchange of information. Canadian media scholar Harold Innis had the theory that people use different types of media to communicate and which one they choose to use will offer different possibilities for the shape and durability of society.","[' What describe communication as a creative and dynamic continuous process rather than a discrete exchange of information?', ' What Canadian media scholar had the theory that people use different types of media to communicate?', ' What will offer different possibilities for the shape and durability of society?']","['Theories of coregulation', 'Harold Innis', 'which one they choose to use']"
2835,communication,As academic discipline with distinct fields of study,"The academic discipline that deals with processes of human communication is communication studies. The discipline encompasses a range of topics, from face-to-face conversation to mass media outlets such as television broadcasting. Communication studies also examines how messages are interpreted through the political, cultural, economic, semiotic, hermeneutic, and social dimensions of their contexts. Statistics, as a quantitative approach to communication science, has also been incorporated into research on communication science in order to help substantiate claims.","The academic discipline that deals with processes of human communication is communication studies. The discipline encompasses a range of topics, from face-to-face conversation to mass media outlets such as television broadcasting.","[' What is the academic discipline that deals with processes of human communication called?', ' Communication studies covers a range of topics from face-to-face conversation to mass media outlets such as what?']","['communication studies', 'television broadcasting']"
2836,communication,Barriers to effectiveness,"Barriers to effective communication can retard or distort the message or intention of the message being conveyed. This may result in failure of the communication process or cause an effect that is undesirable. These include filtering, selective perception, information overload, emotions, language, silence, communication apprehension, gender differences and political correctness.",Barriers to effective communication can retard or distort the message or intention of the message being conveyed. This may result in failure of the communication process or cause an effect that is undesirable.,"[' Barriers to effective communication can retard or distort what?', ' What can result in failure of the communication process or cause an effect that is undesirable?']","['the message or intention of the message being conveyed', 'Barriers to effective communication']"
2837,communication,Nonhuman,"Every information exchange between living organisms — i.e. transmission of signals that involve a living sender and receiver can be considered a form of communication; and even primitive creatures such as corals are competent to communicate. Nonhuman communication also include cell signaling, cellular communication, and chemical transmissions between primitive organisms like bacteria and within the plant and fungal kingdoms.
",Every information exchange between living organisms — i.e. transmission of signals that involve a living sender and receiver can be considered a form of communication; and even primitive creatures such as corals are competent to communicate.,"[' What can be considered a form of communication between living organisms?', ' Even primitive creatures such as corals are competent to communicate?']","['Every information exchange', 'information exchange between living organisms — i.e. transmission of signals that involve a living sender and receiver can be considered a form of communication']"
2838,online algorithm,Summary,"In contrast, an offline algorithm is given the whole problem data from the beginning and is required to output an answer which solves the problem at hand. In operations research, the area in which online algorithms are developed is called online optimization.
","In contrast, an offline algorithm is given the whole problem data from the beginning and is required to output an answer which solves the problem at hand. In operations research, the area in which online algorithms are developed is called online optimization.","[' What is the name of the area in which online algorithms are developed?', ' What is given to an offline algorithm from the beginning?']","['online optimization', 'the whole problem data']"
2839,online algorithm,Summary,"As an example, consider the sorting algorithms selection sort and insertion sort: selection sort repeatedly selects the minimum element from the unsorted remainder and places it at the front, which requires access to the entire input; it is thus an offline algorithm. On the other hand, insertion sort considers one input element per iteration and produces a partial solution without considering future elements. Thus insertion sort is an online algorithm.
","As an example, consider the sorting algorithms selection sort and insertion sort: selection sort repeatedly selects the minimum element from the unsorted remainder and places it at the front, which requires access to the entire input; it is thus an offline algorithm. On the other hand, insertion sort considers one input element per iteration and produces a partial solution without considering future elements.","[' What sorting algorithm repeatedly selects the minimum element from the unsorted remainder and places it at the front?', ' What requires access to the entire input?', ' What does insertion sort consider per iteration and produce a partial solution without considering future elements?']","['selection sort', 'selection sort', 'one input element']"
2840,online algorithm,Summary,"Note that the final result of an insertion sort is optimum, i.e., a correctly sorted list. For many problems, online algorithms cannot match the performance of offline algorithms. If the ratio between the performance of an online algorithm and an optimal offline algorithm is bounded, the online algorithm is called competitive.","Note that the final result of an insertion sort is optimum, i.e., a correctly sorted list. For many problems, online algorithms cannot match the performance of offline algorithms.","[' What is the final result of an insertion sort?', ' Online algorithms cannot match the performance of what?']","['optimum, i.e., a correctly sorted list', 'offline algorithms']"
2841,online algorithm,Definition,"Because it does not know the whole input, an online algorithm is forced to make decisions that may later turn out not to be optimal, and the study of online algorithms has focused on the quality of decision-making that is possible in this setting. Competitive analysis formalizes this idea by comparing the relative performance of an online and offline algorithm for the same problem instance. Specifically, the competitive ratio of an algorithm, is defined as the worst-case ratio of its cost divided by the optimal cost, over all possible inputs. The competitive ratio of an online problem is the best competitive ratio achieved by an online algorithm. Intuitively, the competitive ratio of an algorithm gives a measure on the quality of solutions produced by this algorithm, while the competitive ratio of a problem shows the importance of knowing the future for this problem.
","Because it does not know the whole input, an online algorithm is forced to make decisions that may later turn out not to be optimal, and the study of online algorithms has focused on the quality of decision-making that is possible in this setting. Competitive analysis formalizes this idea by comparing the relative performance of an online and offline algorithm for the same problem instance.","[' Why is an online algorithm forced to make decisions that may later turn out not optimal?', ' What has the study of online algorithms focused on?', ' Competitive analysis formalizes this idea by formalizing what?', ' Competitive analysis formalizes this idea by comparing the relative performance of an online and offline algorithm for the same problem instance.']","['Because it does not know the whole input', 'the quality of decision-making that is possible in this setting', 'comparing the relative performance of an online and offline algorithm for the same problem instance', 'Competitive analysis']"
2842,online algorithm,Online problems,"A problem exemplifying the concepts of online algorithms is the Canadian Traveller Problem. The goal of this problem is to minimize the cost of reaching a target in a weighted graph where some of the edges are unreliable and may have been removed from the graph. However, that an edge has been removed (failed) is only revealed to the traveller when she/he reaches one of the edge's endpoints. The worst case for this problem is simply that all of the unreliable edges fail and the problem reduces to the usual Shortest Path Problem. An alternative analysis of the problem can be made with the help of competitive analysis. For this method of analysis, the offline algorithm knows in advance which edges will fail and the goal is to minimize the ratio between the online and offline algorithms' performance. This problem is PSPACE-complete.
",A problem exemplifying the concepts of online algorithms is the Canadian Traveller Problem. The goal of this problem is to minimize the cost of reaching a target in a weighted graph where some of the edges are unreliable and may have been removed from the graph.,"[' What is a problem exemplifying the concepts of online algorithms?', ' What is the goal of the Canadian Traveller Problem?']","['Canadian Traveller Problem', 'to minimize the cost of reaching a target in a weighted graph']"
2843,conjunctive normal form,Summary,"In Boolean logic, a formula is in conjunctive normal form (CNF) or clausal normal form if it is a conjunction of one or more clauses, where a clause is a disjunction of literals; otherwise put, it is a product of sums or an AND of ORs. As a canonical normal form, it is useful in automated theorem proving and circuit theory.
","In Boolean logic, a formula is in conjunctive normal form (CNF) or clausal normal form if it is a conjunction of one or more clauses, where a clause is a disjunction of literals; otherwise put, it is a product of sums or an AND of ORs. As a canonical normal form, it is useful in automated theorem proving and circuit theory.","[' In Boolean logic, if a formula is a conjunction of one or more clauses, it is in what form?', ' If a clause is disjunction of literals, the formula is what?', ' What is the formula a product of sums or?', ' What is an AND of ORs?', ' What is a canonical normal form useful in?']","['conjunctive normal form', 'conjunctive normal form', 'an AND of ORs', 'formula', 'automated theorem proving and circuit theory']"
2844,conjunctive normal form,Summary,"All conjunctions of literals and all disjunctions of literals are in CNF, as they can be seen as conjunctions of one-literal clauses and conjunctions of a single clause, respectively. As in the disjunctive normal form (DNF), the only propositional connectives a formula in CNF can contain are and, or, and not. The not operator can only be used as part of a literal, which means that it can only precede a propositional variable or a predicate symbol.
","All conjunctions of literals and all disjunctions of literals are in CNF, as they can be seen as conjunctions of one-literal clauses and conjunctions of a single clause, respectively. As in the disjunctive normal form (DNF), the only propositional connectives a formula in CNF can contain are and, or, and not.","[' What are the only propositional connectives a formula in CNF can contain?', ' What can be seen as conjunctions of one-literal clauses?', ' What can a formula in CNF contain?']","['and, or, and not', 'All conjunctions of literals and all disjunctions of literals', 'propositional connectives']"
2845,conjunctive normal form,Examples and non-examples,"For clarity, the disjunctive clauses are written inside parentheses above. In disjunctive normal form with parenthesized conjunctive clauses, the last case is the same, but the next to last is 



(
A
)
∨
(
B
)


{\displaystyle (A)\lor (B)}
. The constants true and false are denoted by the empty conjunct and one clause consisting of the empty disjunct, but are normally written explicitly.","For clarity, the disjunctive clauses are written inside parentheses above. In disjunctive normal form with parenthesized conjunctive clauses, the last case is the same, but the next to last is 



(
A
)
∨
(
B
)


{\displaystyle (A)\lor (B)}
.","[' What are the disjunctive clauses written inside?', ' What is the last case in a normal form?', ' The next to last case is what?']","['parentheses', 'the same', '(\nA']"
2846,conjunctive normal form,Examples and non-examples,"Every formula can be equivalently written as a formula in conjunctive normal form. The three non-examples in CNF are:
",Every formula can be equivalently written as a formula in conjunctive normal form. The three non-examples in CNF are:,"[' How many non-examples in CNF are there?', ' How many formulas can be written as a formula in conjunctive normal form?']","['three', 'Every formula']"
2847,conjunctive normal form,Conversion into CNF,"Every propositional formula can be converted into an equivalent formula that is in CNF. This transformation is based on rules about logical equivalences: double negation elimination, De Morgan's laws, and the distributive law.
","Every propositional formula can be converted into an equivalent formula that is in CNF. This transformation is based on rules about logical equivalences: double negation elimination, De Morgan's laws, and the distributive law.","[' What can be converted into an equivalent formula that is in CNF?', ' What are the rules about logical equivalences based on?']","['Every propositional formula', ""double negation elimination, De Morgan's laws, and the distributive law""]"
2848,conjunctive normal form,Conversion into CNF,"Since all propositional formulas can be converted into an equivalent formula in conjunctive normal form, proofs are often based on the assumption that all formulae are CNF. However, in some cases this conversion to CNF can lead to an exponential explosion of the formula. For example, translating the following non-CNF formula into CNF produces a formula with 




2

n




{\displaystyle 2^{n}}
 clauses:
","Since all propositional formulas can be converted into an equivalent formula in conjunctive normal form, proofs are often based on the assumption that all formulae are CNF. However, in some cases this conversion to CNF can lead to an exponential explosion of the formula.","[' What can be converted into an equivalent formula in conjunctive normal form?', ' Proofs are often based on the assumption that all formulae are what?', ' What can conversion to CNF lead to?']","['all propositional formulas', 'CNF', 'an exponential explosion of the formula']"
2849,conjunctive normal form,Conversion into CNF,"There exist transformations into CNF that avoid an exponential increase in size by preserving satisfiability rather than equivalence. These transformations are guaranteed to only linearly increase the size of the formula, but introduce new variables. For example, the above formula can be transformed into CNF by adding variables 




Z

1


,
…
,

Z

n




{\displaystyle Z_{1},\ldots ,Z_{n}}
 as follows:
","There exist transformations into CNF that avoid an exponential increase in size by preserving satisfiability rather than equivalence. These transformations are guaranteed to only linearly increase the size of the formula, but introduce new variables.","[' What do transformations into CNF avoid by preserving satisfiability rather than equivalence?', ' What are transformations guaranteed to only linearly increase the size of?']","['an exponential increase in size', 'the formula']"
2850,conjunctive normal form,Conversion into CNF,"An interpretation satisfies this formula only if at least one of the new variables is true. If this variable is 




Z

i




{\displaystyle Z_{i}}
, then both 




X

i




{\displaystyle X_{i}}
 and 




Y

i




{\displaystyle Y_{i}}
 are true as well. This means that every model that satisfies this formula also satisfies the original one. On the other hand, only some of the models of the original formula satisfy this one: since the 




Z

i




{\displaystyle Z_{i}}
 are not mentioned in the original formula, their values are irrelevant to satisfaction of it, which is not the case in the last formula. This means that the original formula and the result of the translation are equisatisfiable but not equivalent.
","An interpretation satisfies this formula only if at least one of the new variables is true. If this variable is 




Z

i




{\displaystyle Z_{i}}
, then both 




X

i




{\displaystyle X_{i}}
 and 




Y

i




{\displaystyle Y_{i}}
 are true as well.","[' What satisfies the formula if at least one of the new variables is true?', ' If the variable is Z i <unk>displaystyle Z_<unk>i<unk>, then both X i and Y i are true as well?']","['An interpretation', 'X\n\ni']"
2851,conjunctive normal form,Conversion into CNF,"An alternative translation, the Tseitin transformation, includes also the clauses 




Z

i


∨
¬

X

i


∨
¬

Y

i




{\displaystyle Z_{i}\vee \neg X_{i}\vee \neg Y_{i}}
. With these clauses, the formula implies 




Z

i


≡

X

i


∧

Y

i




{\displaystyle Z_{i}\equiv X_{i}\wedge Y_{i}}
; this formula is often regarded to ""define"" 




Z

i




{\displaystyle Z_{i}}
 to be a name for 




X

i


∧

Y

i




{\displaystyle X_{i}\wedge Y_{i}}
.
","An alternative translation, the Tseitin transformation, includes also the clauses 




Z

i


∨
¬

X

i


∨
¬

Y

i




{\displaystyle Z_{i}\vee \neg X_{i}\vee \neg Y_{i}}
. With these clauses, the formula implies 




Z

i


≡

X

i


∧

Y

i




{\displaystyle Z_{i}\equiv X_{i}\wedge Y_{i}}
; this formula is often regarded to ""define"" 




Z

i




{\displaystyle Z_{i}}
 to be a name for 




X

i


∧

Y

i




{\displaystyle X_{i}\wedge Y_{i}}
.","[' What is an alternative translation of the Tseitin transformation?', ' What is a name for X i <unk> Y i?']","['Z\n\ni', 'equiv']"
2852,conjunctive normal form,First-order logic,"In first order logic, conjunctive normal form can be taken further to yield the clausal normal form of a logical formula, which can be then used to perform first-order resolution.
In resolution-based automated theorem-proving, a CNF formula 
","In first order logic, conjunctive normal form can be taken further to yield the clausal normal form of a logical formula, which can be then used to perform first-order resolution. In resolution-based automated theorem-proving, a CNF formula","[' What can be taken further to yield the clausal normal form of a logical formula?', ' What can then be used to perform first-order resolution?', ' In resolution-based automated theorem-proving, what is a CNF formula used?']","['conjunctive normal form', 'logical formula', 'first-order resolution']"
2853,conjunctive normal form,Computational complexity,"An important set of problems in computational complexity involves finding assignments to the variables of a boolean formula expressed in Conjunctive Normal Form, such that the formula is true. The k-SAT problem is the problem of finding a satisfying assignment to a boolean formula expressed in CNF in which each disjunction contains at most k variables. 3-SAT is NP-complete (like any other k-SAT problem with k>2) while 2-SAT is known to have solutions in polynomial time. As a consequence, the task of converting a formula into a DNF, preserving satisfiability, is NP-hard; dually, converting into CNF, preserving validity, is also NP-hard; hence equivalence-preserving conversion into DNF or CNF is again NP-hard.
","An important set of problems in computational complexity involves finding assignments to the variables of a boolean formula expressed in Conjunctive Normal Form, such that the formula is true. The k-SAT problem is the problem of finding a satisfying assignment to a boolean formula expressed in CNF in which each disjunction contains at most k variables.","[' The k-SAT problem is the problem of finding a satisfying assignment to a boolean formula expressed in what form?', ' What is a boolean formula expressed in?']","['CNF', 'Conjunctive Normal Form']"
2854,conjunctive normal form,Computational complexity,"Typical problems in this case involve formulas in ""3CNF"": conjunctive normal form with no more than three variables per conjunct. Examples of such formulas encountered in practice can be very large, for example with 100,000 variables and 1,000,000 conjuncts.
","Typical problems in this case involve formulas in ""3CNF"": conjunctive normal form with no more than three variables per conjunct. Examples of such formulas encountered in practice can be very large, for example with 100,000 variables and 1,000,000 conjuncts.","[' How many variables can a formula have?', ' How many conjuncts are there in a 3CNF formula?']","['100,000', '1,000,000']"
2855,conjunctive normal form,Converting from first-order logic,"Informally, the Skolem function 



g
(
x
)


{\displaystyle g(x)}
 can be thought of as yielding the person by whom 



x


{\displaystyle x}
 is loved, while 



f
(
x
)


{\displaystyle f(x)}
 yields the animal (if any) that 



x


{\displaystyle x}
 doesn't love. The 3rd last line from below then reads as ""



x


{\displaystyle x}
 doesn't love the animal 



f
(
x
)


{\displaystyle f(x)}
, or else 



x


{\displaystyle x}
 is loved by 



g
(
x
)


{\displaystyle g(x)}
"".
","Informally, the Skolem function 



g
(
x
)


{\displaystyle g(x)}
 can be thought of as yielding the person by whom 



x


{\displaystyle x}
 is loved, while 



f
(
x
)


{\displaystyle f(x)}
 yields the animal (if any) that 



x


{\displaystyle x}
 doesn't love. The 3rd last line from below then reads as ""



x


{\displaystyle x}
 doesn't love the animal 



f
(
x
)


{\displaystyle f(x)}
, or else 



x


{\displaystyle x}
 is loved by 



g
(
x
)


{\displaystyle g(x)}
"".","[' Informally, the Skolem function can be thought of as yielding the person by whom x <unk>displaystyle x<unk> is loved?', ' The 3rd last line from below then reads as what?', "" What does x <unk>displaystyle x<unk> don't love?""]","['g', ""x\n\n\n{\\displaystyle x}\n doesn't love the animal \n\n\n\nf"", 'f\n(\nx\n)\n\n\n{\\displaystyle f(x)}\n yields the animal (if any) that \n\n\n\nx\n\n\n{\\displaystyle x}\n doesn\'t love. The 3rd last line from below then reads as ""\n\n\n\nx\n\n\n{\\displaystyle x}\n doesn\'t love the animal \n\n\n\nf']"
2856,sparse representation,Summary,"Sparse approximation (also known as sparse representation) theory deals with sparse solutions for systems of linear equations. Techniques for finding these solutions and exploiting them in applications have found wide use in image processing, signal processing, machine learning, medical imaging, and more.
","Sparse approximation (also known as sparse representation) theory deals with sparse solutions for systems of linear equations. Techniques for finding these solutions and exploiting them in applications have found wide use in image processing, signal processing, machine learning, medical imaging, and more.","[' What is another name for the theory of sparse approximation?', ' What does the theory deal with?', ' Where have techniques for finding these solutions found wide use?']","['sparse representation', 'sparse solutions for systems of linear equations', 'image processing, signal processing, machine learning, medical imaging']"
2857,sparse representation,Variations,"Structured sparsity: In the original version of the problem, any of the atoms in the dictionary can be picked. In the structured (block) sparsity model, instead of picking atoms individually, groups of them are to be picked. These groups can be overlapping and of varying size. The objective is to represent 



x


{\displaystyle x}
 such that it is sparse while forcing this block-structure.","Structured sparsity: In the original version of the problem, any of the atoms in the dictionary can be picked. In the structured (block) sparsity model, instead of picking atoms individually, groups of them are to be picked.","[' What is structured sparsity?', ' In the original version of the problem, any of the atoms in the dictionary can be picked which way?', ' Instead of picking individually, what is to be picked?']","['block', 'Structured sparsity', 'groups of them']"
2858,sparse representation,Variations,"Collaborative (joint) sparse coding: The original version of the problem is defined for a single signal 



x


{\displaystyle x}
. In the collaborative (joint) sparse coding model, a set of signals is available, each believed to emerge from (nearly) the same set of atoms from 



D


{\displaystyle D}
. In this case, the pursuit task aims to recover a set of sparse representations that best describe the data while forcing them to share the same (or close-by) support.","Collaborative (joint) sparse coding: The original version of the problem is defined for a single signal 



x


{\displaystyle x}
. In the collaborative (joint) sparse coding model, a set of signals is available, each believed to emerge from (nearly) the same set of atoms from 



D


{\displaystyle D}
.","[' What is collaborative sparse coding?', ' The original version of the problem is defined for a single signal x <unk>displaystyle x<unk>?']","['a set of signals is available, each believed to emerge from (nearly) the same set of atoms from \n\n\n\nD', 'x']"
2859,sparse representation,Variations,"Other structures: More broadly, the sparse approximation problem can be cast while forcing a specific desired structure on the pattern of non-zero locations in 



α


{\displaystyle \alpha }
. Two cases of interest that have been extensively studied are tree-based structure, and more generally, a Boltzmann distributed support.","Other structures: More broadly, the sparse approximation problem can be cast while forcing a specific desired structure on the pattern of non-zero locations in 



α


{\displaystyle \alpha }
. Two cases of interest that have been extensively studied are tree-based structure, and more generally, a Boltzmann distributed support.","[' What can be cast while forcing a specific desired structure on the pattern of non-zero locations in <unk> <unk>displaystyle <unk>alpha <unk>?', ' What are two cases of interest that have been extensively studied?']","['the sparse approximation problem', 'tree-based structure, and more generally, a Boltzmann distributed support']"
2860,sparse representation,Applications,"Sparse approximation ideas and algorithms have been extensively used in signal processing, image processing, machine learning, medical imaging, array processing, data mining, and more. In most of these applications, the unknown signal of interest is modeled as a sparse combination of a few atoms from a given dictionary, and this is used as the regularization of the problem. These problems are typically accompanied by a dictionary learning mechanism that aims to fit 



D


{\displaystyle D}
 to best match the model to the given data. The use of sparsity-inspired models has led to state-of-the-art results in a wide set of applications. Recent work suggests that there is a tight connection between sparse representation modeling and deep-learning.","Sparse approximation ideas and algorithms have been extensively used in signal processing, image processing, machine learning, medical imaging, array processing, data mining, and more. In most of these applications, the unknown signal of interest is modeled as a sparse combination of a few atoms from a given dictionary, and this is used as the regularization of the problem.","[' What has been extensively used in signal processing, image processing, machine learning, medical imaging, array processing, data mining, and more?', ' What is modeled as a sparse combination of a few atoms from a given dictionary?', ' What is used as the regularization of the problem?']","['Sparse approximation ideas and algorithms', 'the unknown signal of interest', 'the unknown signal of interest']"
2861,entropy,Summary,"Entropy is a scientific concept as well as a measurable physical property that is most commonly associated with a state of disorder, randomness, or uncertainty. The term and the concept are used in diverse fields, from classical thermodynamics, where it was first recognized, to the microscopic description of nature in statistical physics, and to the principles of information theory. It has found far-ranging applications in chemistry and physics, in biological systems and their relation to life, in cosmology, economics, sociology, weather science, climate change, and information systems including the transmission of information in telecommunication.","Entropy is a scientific concept as well as a measurable physical property that is most commonly associated with a state of disorder, randomness, or uncertainty. The term and the concept are used in diverse fields, from classical thermodynamics, where it was first recognized, to the microscopic description of nature in statistical physics, and to the principles of information theory.","[' What is entropy a scientific concept as well as a measurable physical property?', ' What is most commonly associated with a state of disorder, randomness, or uncertainty?', ' The term and concept are used in diverse fields from classical thermodynamics to what?', ' When was the microscopic description of nature in statistical physics first recognized?']","['Entropy is a scientific concept as well as a measurable physical property that is most commonly associated with a state of disorder, randomness, or uncertainty', 'Entropy', 'the microscopic description of nature in statistical physics', 'classical thermodynamics']"
2862,entropy,Summary,"The thermodynamic concept was referred to by Scottish scientist and engineer Macquorn Rankine in 1850 with the names thermodynamic function and heat-potential. In 1865, German physicist Rudolf Clausius, one of the leading founders of the field of thermodynamics, defined it as the quotient of an infinitesimal amount of heat to the instantaneous temperature. He initially described it as transformation-content, in German Verwandlungsinhalt, and later coined the term entropy from a Greek word for transformation. Referring to microscopic constitution and structure, in 1862, Clausius interpreted the concept as meaning disgregation.","The thermodynamic concept was referred to by Scottish scientist and engineer Macquorn Rankine in 1850 with the names thermodynamic function and heat-potential. In 1865, German physicist Rudolf Clausius, one of the leading founders of the field of thermodynamics, defined it as the quotient of an infinitesimal amount of heat to the instantaneous temperature.","[' Who first referred to the thermodynamic concept in 1850?', ' What were the names of the two names used by Macquorn Rankine?', ' Who was one of the founding founders of the field of thermodynamics?', ' In what year did Rudolf Clausius define the concept as the quotient of infinite amounts of heat?', ' What is the quotient of an infinitesimal amount of heat to the instantaneous temperature?']","['Macquorn Rankine', 'thermodynamic function and heat-potential', 'Rudolf Clausius', '1865', 'The thermodynamic concept']"
2863,entropy,Summary,"A consequence of entropy is that certain processes are irreversible or impossible, aside from the requirement of not violating the conservation of energy, the latter being expressed in the first law of thermodynamics. Entropy is central to the second law of thermodynamics, which states that the entropy of isolated systems left to spontaneous evolution cannot decrease with time, as they always arrive at a state of thermodynamic equilibrium, where the entropy is highest.
","A consequence of entropy is that certain processes are irreversible or impossible, aside from the requirement of not violating the conservation of energy, the latter being expressed in the first law of thermodynamics. Entropy is central to the second law of thermodynamics, which states that the entropy of isolated systems left to spontaneous evolution cannot decrease with time, as they always arrive at a state of thermodynamic equilibrium, where the entropy is highest.","[' What is a consequence of entropy?', ' What is the requirement of not violating the conservation of energy expressed in?', ' Entropy is central to the second law of what?', ' What states that the entropy of isolated systems left to spontaneous evolution cannot decrease with time?']","['certain processes are irreversible or impossible', 'the first law of thermodynamics', 'thermodynamics', 'second law of thermodynamics']"
2864,entropy,Summary,"Austrian physicist Ludwig Boltzmann explained entropy as the measure of the number of possible microscopic arrangements or states of individual atoms and molecules of a system that comply with the macroscopic condition of the system. He thereby introduced the concept of statistical disorder and probability distributions into a new field of thermodynamics, called statistical mechanics, and found the link between the microscopic interactions, which fluctuate about an average configuration, to the macroscopically observable behavior, in form of a simple logarithmic law, with a proportionality constant, the Boltzmann constant, that has become one of the defining universal constants for the modern International System of Units (SI).
","Austrian physicist Ludwig Boltzmann explained entropy as the measure of the number of possible microscopic arrangements or states of individual atoms and molecules of a system that comply with the macroscopic condition of the system. He thereby introduced the concept of statistical disorder and probability distributions into a new field of thermodynamics, called statistical mechanics, and found the link between the microscopic interactions, which fluctuate about an average configuration, to the macroscopically observable behavior, in form of a simple logarithmic law, with a proportionality constant, the Boltzmann constant, that has become one of the defining universal constants for the modern International System of Units (SI).","[' Who explained entropy as the measure of the number of possible microscopic arrangements or states of individual atoms and molecules of a system that comply with the macroscopic condition of the system?', ' Who introduced the concept of statistical disorder and probability distributions into a new field of study?', ' What is the new field of thermodynamics called?', ' What is statistical mechanics?', ' How do microscopic interactions compare to macroscopically observed behavior?', ' What is the Boltzmann constant?', ' What has become one of the defining universal constants for the modern International System of Units?']","['Ludwig Boltzmann', 'Ludwig Boltzmann', 'statistical mechanics', 'field of thermodynamics', 'fluctuate about an average configuration', 'proportionality constant', 'proportionality constant, the Boltzmann constant']"
2865,entropy,Summary,"In 1948, Bell Labs scientist Claude Shannon developed similar statistical concepts of measuring microscopic uncertainty and multiplicity to the problem of random losses of information in telecommunication signals. Upon John von Neumann's suggestion, Shannon named this entity of missing information in analogous manner to its use in statistical mechanics as entropy, and gave birth to the field of information theory. This description has been identified as a universal definition of the concept of entropy.","In 1948, Bell Labs scientist Claude Shannon developed similar statistical concepts of measuring microscopic uncertainty and multiplicity to the problem of random losses of information in telecommunication signals. Upon John von Neumann's suggestion, Shannon named this entity of missing information in analogous manner to its use in statistical mechanics as entropy, and gave birth to the field of information theory.","[' When did Claude Shannon develop statistical concepts of measuring microscopic uncertainty and multiplicity to the problem of random losses of information in telecommunication signals?', ' Who suggested that Shannon name this entity of missing information in analogous manner to its use in statistical mechanics?', ' In what field was information theory born?']","['1948', 'John von Neumann', 'statistical mechanics']"
2866,entropy,History,"In his 1803 paper, Fundamental Principles of Equilibrium and Movement, the French mathematician Lazare Carnot proposed that in any machine, the accelerations and shocks of the moving parts represent losses of moment of activity; in any natural process there exists an inherent tendency towards the dissipation of useful energy. In 1824, building on that work, Lazare's son, Sadi Carnot, published Reflections on the Motive Power of Fire, which posited that in all heat-engines, whenever ""caloric"" (what is now known as heat) falls through a temperature difference, work or motive power can be produced from the actions of its fall from a hot to cold body. He used an analogy with how water falls in a water wheel. That was an early insight into the second law of thermodynamics. Carnot based his views of heat partially on the early 18th-century ""Newtonian hypothesis"" that both heat and light were types of indestructible forms of matter, which are attracted and repelled by other matter, and partially on the contemporary views of Count Rumford, who showed in 1789 that heat could be created by friction, as when cannon bores are machined. Carnot reasoned that if the body of the working substance, such as a body of steam, is returned to its original state at the end of a complete engine cycle, ""no change occurs in the condition of the working body"".
","In his 1803 paper, Fundamental Principles of Equilibrium and Movement, the French mathematician Lazare Carnot proposed that in any machine, the accelerations and shocks of the moving parts represent losses of moment of activity; in any natural process there exists an inherent tendency towards the dissipation of useful energy. In 1824, building on that work, Lazare's son, Sadi Carnot, published Reflections on the Motive Power of Fire, which posited that in all heat-engines, whenever ""caloric"" (what is now known as heat) falls through a temperature difference, work or motive power can be produced from the actions of its fall from a hot to cold body.","[' Who wrote Fundamental Principles of Equilibrium and Movement in 1803?', ' What did Lazare Carnot believe were losses of moment of activity?', ' In any natural process, what is there an inherent tendency towards?', ' In what year did Sadi Carnot publish Reflections on the Motive Power of Fire?', ' What is now known as ""caloric""?', ' What falls through a temperature difference?', ' What can be produced from the actions of its fall from a hot to cold body?']","['Lazare Carnot', 'accelerations and shocks of the moving parts', 'dissipation of useful energy', '1824', 'heat', 'caloric', 'work or motive power']"
2867,entropy,History,"In the 1850s and 1860s, German physicist Rudolf Clausius objected to the supposition that no change occurs in the working body, and gave that change a mathematical interpretation, by questioning the nature of the inherent loss of usable heat when work is done, e.g., heat produced by friction. He described his observations as a dissipative use of energy, resulting in a transformation-content (Verwandlungsinhalt in German), of a thermodynamic system or working body of chemical species during a change of state. That was in contrast to earlier views, based on the theories of Isaac Newton, that heat was an indestructible particle that had mass. Clausius discovered that the non-usable energy increases as steam proceeds from inlet to exhaust in a steam engine. From the prefix en-, as in 'energy', and from the Greek word τροπή [tropē], which is translated in an established lexicon as turning or change and that he rendered in German as Verwandlung, a word often translated into English as transformation, in 1865 Clausius coined the name of that property as entropy. The word was adopted into the English language in 1868.
","In the 1850s and 1860s, German physicist Rudolf Clausius objected to the supposition that no change occurs in the working body, and gave that change a mathematical interpretation, by questioning the nature of the inherent loss of usable heat when work is done, e.g., heat produced by friction. He described his observations as a dissipative use of energy, resulting in a transformation-content (Verwandlungsinhalt in German), of a thermodynamic system or working body of chemical species during a change of state.","[' What German physicist objected to the supposition that no change occurs in the working body?', ' What did Clausius question the nature of when work is done?', ' What is done, e.g., by friction?', ' What did he describe as a dissipative use of energy?']","['Rudolf Clausius', 'loss of usable heat', 'work', 'Rudolf Clausius objected to the supposition that no change occurs in the working body, and gave that change a mathematical interpretation, by questioning the nature of the inherent loss of usable heat when work is done, e.g., heat produced by friction. He described his observations']"
2868,entropy,History,"Later, scientists such as Ludwig Boltzmann, Josiah Willard Gibbs, and James Clerk Maxwell gave entropy a statistical basis. In 1877, Boltzmann visualized a probabilistic way to measure the entropy of an ensemble of ideal gas particles, in which he defined entropy as proportional to the natural logarithm of the number of microstates such a gas could occupy. The proportionality constant in this definition, called the Boltzmann constant, has become one of the defining universal constants for the modern International System of Units (SI). Henceforth, the essential problem in statistical thermodynamics has been to determine the distribution of a given amount of energy E over N identical systems. Constantin Carathéodory, a Greek mathematician, linked entropy with a mathematical definition of irreversibility, in terms of trajectories and integrability.
","Later, scientists such as Ludwig Boltzmann, Josiah Willard Gibbs, and James Clerk Maxwell gave entropy a statistical basis. In 1877, Boltzmann visualized a probabilistic way to measure the entropy of an ensemble of ideal gas particles, in which he defined entropy as proportional to the natural logarithm of the number of microstates such a gas could occupy.","[' What did Ludwig Boltzmann, Josiah Willard Gibbs, and James Clerk Maxwell give entropy a statistical basis for?', ' In what year was a probabilistic way to measure the etropy of an ensemble of ideal gas particles visualized?', ' What did Boltsmann define as proportional to the natural logarithm of the number of particles?', ' What is the natural logarithm of the number of microstates such a gas could occupy?']","['probabilistic way to measure the entropy of an ensemble of ideal gas particles', '1877', 'entropy', 'entropy as proportional']"
2869,entropy,Etymology,"In 1865, Clausius named the concept of ""the differential of a quantity which depends on the configuration of the system,"" entropy (Entropie) after the Greek word for 'transformation'. He gave ""transformational content"" (Verwandlungsinhalt) as a synonym, paralleling his ""thermal and ergonal content"" (Wärme- und Werkinhalt) as the name of 



U


{\displaystyle U}
, but preferring the term entropy as a close parallel of the word energy, as he found the concepts nearly ""analogous in their physical significance."" This term was formed by replacing the root of ἔργον ('ergon', 'work') by that of τροπή ('tropy', 'transformation').","In 1865, Clausius named the concept of ""the differential of a quantity which depends on the configuration of the system,"" entropy (Entropie) after the Greek word for 'transformation'. He gave ""transformational content"" (Verwandlungsinhalt) as a synonym, paralleling his ""thermal and ergonal content"" (Wärme- und Werkinhalt) as the name of 



U


{\displaystyle U}
, but preferring the term entropy as a close parallel of the word energy, as he found the concepts nearly ""analogous in their physical significance.""","[' In what year did Clausius name the concept of differential of a quantity which depends on the configuration of the system?', ' What Greek word does entropy come from?', ' In 1865, what was the term for transformational content?', ' Which term was used to describe thermo- and ergonal content in 1865?', ' The term for thermo-and-workinhalt came from what Greek word?', ' What was the name of U <unk>displaystyle U<unk>?', ' What term was ergonal content a close parallel to?']","['1865', 'transformation', 'Verwandlungsinhalt', 'transformational content', 'Verwandlungsinhalt', 'transformational content"" (Verwandlungsinhalt) as a synonym, paralleling his ""thermal and ergonal content"" (Wärme- und Werkinhalt', 'energy']"
2870,entropy,Definitions and descriptions,"The concept of entropy is described by two principal approaches, the macroscopic perspective of classical thermodynamics, and the microscopic description central to statistical mechanics. The classical approach defines entropy in terms of macroscopically measurable physical properties, such as bulk mass, volume, pressure, and temperature.  The statistical definition of entropy defines it in terms of the statistics of the motions of the microscopic constituents of a system – modeled at first classically, e.g. Newtonian particles constituting a gas, and later quantum-mechanically (photons, phonons, spins, etc.). The two approaches form a consistent, unified view of the same phenomenon as expressed in the second law of thermodynamics, which has found universal applicability to physical processes.
","The concept of entropy is described by two principal approaches, the macroscopic perspective of classical thermodynamics, and the microscopic description central to statistical mechanics. The classical approach defines entropy in terms of macroscopically measurable physical properties, such as bulk mass, volume, pressure, and temperature.","[' The concept of entropy is described by what two principal approaches?', ' The macroscopic perspective of classical thermodynamics and the microscopic description central to what?']","['the macroscopic perspective of classical thermodynamics, and the microscopic description central to statistical mechanics', 'statistical mechanics']"
2871,entropy,Second law of thermodynamics,"The second law of thermodynamics requires that, in general, the total entropy of any system does not decrease other than by increasing the entropy of some other system. Hence, in a system isolated from its environment, the entropy of that system tends not to decrease. It follows that heat cannot flow from a colder body to a hotter body without the application of work to the colder body. Secondly, it is impossible for any device operating on a cycle to produce net work from a single temperature reservoir; the production of net work requires flow of heat from a hotter reservoir to a colder reservoir, or a single expanding reservoir undergoing adiabatic cooling, which performs adiabatic work. As a result, there is no possibility of a perpetual motion machine. It follows that a reduction in the increase of entropy in a specified process, such as a chemical reaction, means that it is energetically more efficient.
","The second law of thermodynamics requires that, in general, the total entropy of any system does not decrease other than by increasing the entropy of some other system. Hence, in a system isolated from its environment, the entropy of that system tends not to decrease.","[' The second law of thermodynamics requires that the total entropy of any system does not decrease other than by increasing what?', ' In a system isolated from its environment, what tends not to decrease?']","['the entropy of some other system', 'the entropy']"
2872,entropy,Second law of thermodynamics,"It follows from the second law of thermodynamics that the entropy of a system that is not isolated may decrease. An air conditioner, for example, may cool the air in a room, thus reducing the entropy of the air of that system. The heat expelled from the room (the system), which the air conditioner transports and discharges to the outside air, always makes a bigger contribution to the entropy of the environment than the decrease of the entropy of the air of that system. Thus, the total of entropy of the room plus the entropy of the environment increases, in agreement with the second law of thermodynamics.
","It follows from the second law of thermodynamics that the entropy of a system that is not isolated may decrease. An air conditioner, for example, may cool the air in a room, thus reducing the entropy of the air of that system.","[' What is the second law of thermodynamics?', ' An air conditioner may cool the air in a room by reducing what?']","['the entropy of a system that is not isolated may decrease', 'the entropy of the air of that system']"
2873,entropy,Second law of thermodynamics,"In mechanics, the second law in conjunction with the fundamental thermodynamic relation places limits on a system's ability to do useful work. The entropy change of a system at temperature 



T


{\textstyle T}
 absorbing an infinitesimal amount of heat 



δ
q


{\textstyle \delta q}
 in a reversible way, is given by 



δ
q

/

T


{\textstyle \delta q/T}
. More explicitly, an energy 




T

R


S


{\textstyle T_{R}S}
 is not available to do useful work, where 




T

R




{\textstyle T_{R}}
 is the temperature of the coldest accessible reservoir or heat sink external to the system. For further discussion, see Exergy.
","In mechanics, the second law in conjunction with the fundamental thermodynamic relation places limits on a system's ability to do useful work. The entropy change of a system at temperature 



T


{\textstyle T}
 absorbing an infinitesimal amount of heat 



δ
q


{\textstyle \delta q}
 in a reversible way, is given by 



δ
q

/

T


{\textstyle \delta q/T}
.","[' In mechanics, the second law in conjunction with the fundamental thermodynamic relation places limits on what?', ' The entropy change of a system at temperature T <unk>textstyle T absorbs infinitesimal amount of heat in a reversible way?', ' What is given by q / T <unk>textstyle <unk>delta q/T?']","[""a system's ability to do useful work"", 'T\n\n\n{\\textstyle T}\n absorbing an infinitesimal amount of heat \n\n\n\nδ\nq', 'δ']"
2874,entropy,Second law of thermodynamics,"Statistical mechanics demonstrates that entropy is governed by probability, thus allowing for a decrease in disorder even in an isolated system. Although this is possible, such an event has a small probability of occurring, making it unlikely.","Statistical mechanics demonstrates that entropy is governed by probability, thus allowing for a decrease in disorder even in an isolated system. Although this is possible, such an event has a small probability of occurring, making it unlikely.","[' What demonstrates that entropy is governed by probability?', ' What allows for a decrease in disorder even in an isolated system?']","['Statistical mechanics', 'entropy is governed by probability']"
2875,entropy,Second law of thermodynamics,"The applicability of a second law of thermodynamics is limited to systems in or sufficiently near equilibrium state, so that they have defined entropy. Some inhomogeneous systems out of thermodynamic equilibrium still satisfy the hypothesis of local thermodynamic equilibrium, so that entropy density is locally defined as an intensive quantity. For such systems, there may apply a principle of maximum time rate of entropy production. It states that such a system may evolve to a steady state that maximizes its time rate of entropy production. This does not mean that such a system is necessarily always in a condition of maximum time rate of entropy production; it means that it may evolve to such a steady state.","The applicability of a second law of thermodynamics is limited to systems in or sufficiently near equilibrium state, so that they have defined entropy. Some inhomogeneous systems out of thermodynamic equilibrium still satisfy the hypothesis of local thermodynamic equilibrium, so that entropy density is locally defined as an intensive quantity.","[' The applicability of a second law of thermodynamics is limited to systems in or sufficiently near what state?', ' Some inhomogeneous systems out of what state satisfy the hypothesis of local thermodynamic equilibrium?', ' Entropy density is locally defined as what?']","['equilibrium', 'thermodynamic equilibrium', 'an intensive quantity']"
2876,finite automaton,Summary,"A finite-state machine (FSM) or finite-state automaton (FSA, plural: automata), finite automaton, or simply a state machine, is a mathematical model of computation. It is an abstract machine that can be in exactly one of a finite number of states at any given time. The FSM can change from one state to another in response to some inputs; the change from one state to another is called a transition. An FSM is defined by a list of its states, its initial state, and the inputs that trigger each transition. Finite-state machines are of two types—deterministic finite-state machines and non-deterministic finite-state machines. A deterministic finite-state machine can be constructed equivalent to any non-deterministic one.
","A finite-state machine (FSM) or finite-state automaton (FSA, plural: automata), finite automaton, or simply a state machine, is a mathematical model of computation. It is an abstract machine that can be in exactly one of a finite number of states at any given time.","[' What is a finite-state machine?', ' What is an abstract machine that can be in exactly one of finite number of states at any given time?']","['FSM', 'A finite-state machine']"
2877,finite automaton,Summary,"The behavior of state machines can be observed in many devices in modern society that perform a predetermined sequence of actions depending on a sequence of events with which they are presented. Simple examples are vending machines, which dispense products when the proper combination of coins is deposited, elevators, whose sequence of stops is determined by the floors requested by riders, traffic lights, which change sequence when cars are waiting, and combination locks, which require the input of a sequence of numbers in the proper order.
","The behavior of state machines can be observed in many devices in modern society that perform a predetermined sequence of actions depending on a sequence of events with which they are presented. Simple examples are vending machines, which dispense products when the proper combination of coins is deposited, elevators, whose sequence of stops is determined by the floors requested by riders, traffic lights, which change sequence when cars are waiting, and combination locks, which require the input of a sequence of numbers in the proper order.","[' What can be observed in many devices in modern society that perform a predetermined sequence of actions depending on a sequence of events?', ' What do vending machines dispense when the proper combination of coins is deposited?', ' What is determined by the floors requested by riders?', ' Traffic lights change sequence when cars are waiting?', ' What requires the input of a sequence of numbers in the proper order?']","['The behavior of state machines', 'products', 'sequence of stops', 'traffic lights', 'combination locks']"
2878,finite automaton,Summary,"The finite-state machine has less computational power than some other models of computation such as the Turing machine. The computational power distinction means there are computational tasks that a Turing machine can do but an FSM cannot. This is because an FSM's memory is limited by the number of states it has. A finite-state machine has the same computational power as a Turing machine that is restricted such that its head may only perform ""read"" operations, and always has to move from left to right. FSMs are studied in the more general field of automata theory.
",The finite-state machine has less computational power than some other models of computation such as the Turing machine. The computational power distinction means there are computational tasks that a Turing machine can do but an FSM cannot.,"[' The finite-state machine has less computational power than what other model of computation?', ' The computational power distinction means there are computational tasks that a Turing machine can do but an FSM cannot?']","['Turing machine', 'cannot']"
2879,finite automaton,Example: coin-operated turnstile,"An example of a simple mechanism that can be modeled by a state machine is a turnstile. A turnstile, used to control access to subways and amusement park rides, is a gate with three rotating arms at waist height, one across the entryway. Initially the arms are locked, blocking the entry, preventing patrons from passing through. Depositing a coin or token in a slot on the turnstile unlocks the arms, allowing a single customer to push through. After the customer passes through, the arms are locked again until another coin is inserted.
","An example of a simple mechanism that can be modeled by a state machine is a turnstile. A turnstile, used to control access to subways and amusement park rides, is a gate with three rotating arms at waist height, one across the entryway.","[' What is an example of a simple mechanism that can be modeled by a state machine?', ' What is used to control access to subways and amusement park rides?', ' A turnstile is a gate with three rotating arms at what height?']","['a turnstile', 'A turnstile', 'waist']"
2880,finite automaton,Example: coin-operated turnstile,"Considered as a state machine, the turnstile has two possible states: Locked and Unlocked. There are two possible inputs that affect its state: putting a coin in the slot (coin) and pushing the arm (push). In the locked state, pushing on the arm has no effect; no matter how many times the input push is given, it stays in the locked state. Putting a coin in – that is, giving the machine a coin input – shifts the state from Locked to Unlocked. In the unlocked state, putting additional coins in has no effect; that is, giving additional coin inputs does not change the state. However, a customer pushing through the arms, giving a push input, shifts the state back to Locked.
","Considered as a state machine, the turnstile has two possible states: Locked and Unlocked. There are two possible inputs that affect its state: putting a coin in the slot (coin) and pushing the arm (push).","[' What are the two possible states of the turnstile?', "" How many inputs affect the state of a turnstile's state?"", ' What is a coin put in the slot?']","['Locked and Unlocked', 'two', 'coin']"
2881,finite automaton,Example: coin-operated turnstile,"The turnstile state machine can also be represented by a directed graph called a state diagram (above). Each state is represented by a node (circle). Edges (arrows) show the transitions from one state to another. Each arrow is labeled with the input that triggers that transition. An input that doesn't cause a change of state (such as a coin input in the Unlocked state) is represented by a circular arrow returning to the original state. The arrow into the Locked node from the black dot indicates it is the initial state.
",The turnstile state machine can also be represented by a directed graph called a state diagram (above). Each state is represented by a node (circle).,"[' What is another name for a graph of the turnstile state machine?', ' What is a node called?']","['state diagram', 'circle']"
2882,finite automaton,Concepts and terminology,"A state is a description of the status of a system that is waiting to execute a transition. A transition is a set of actions to be executed when a condition is fulfilled or when an event is received.
For example, when using an audio system to listen to the radio (the system is in the ""radio"" state), receiving a ""next"" stimulus results in moving to the next station. When the system is in the ""CD"" state, the ""next"" stimulus results in moving to the next track. Identical stimuli trigger different actions depending on the current state.
",A state is a description of the status of a system that is waiting to execute a transition. A transition is a set of actions to be executed when a condition is fulfilled or when an event is received.,"[' A state describes the status of a system that is waiting to execute what?', ' A transition is a set of actions to be executed when a condition is fulfilled or when an event is received?']","['a transition', 'A state']"
2883,finite automaton,Usage,"In addition to their use in modeling reactive systems presented here, finite-state machines are significant in many different areas, including electrical engineering, linguistics, computer science, philosophy, biology, mathematics, video game programming, and logic. Finite-state machines are a class of automata studied in automata theory and the theory of computation.
In computer science, finite-state machines are widely used in modeling of application behavior, design of hardware digital systems, software engineering, compilers, network protocols, and the study of computation and languages.
","In addition to their use in modeling reactive systems presented here, finite-state machines are significant in many different areas, including electrical engineering, linguistics, computer science, philosophy, biology, mathematics, video game programming, and logic. Finite-state machines are a class of automata studied in automata theory and the theory of computation.","[' What is a class of automata studied in automata theory and the theory of computation?', ' What are finite-state machines important in?']","['Finite-state machines', 'electrical engineering, linguistics, computer science, philosophy, biology, mathematics, video game programming, and logic']"
2884,finite automaton,Alternative semantics,"There are other sets of semantics available to represent state machines. For example, there are tools for modeling and designing logic for embedded controllers. They combine hierarchical state machines (which usually have more than one current state), flow graphs, and truth tables into one language, resulting in a different formalism and set of semantics.  These charts, like Harel's original state machines, support hierarchically nested states, orthogonal regions, state actions, and transition actions.","There are other sets of semantics available to represent state machines. For example, there are tools for modeling and designing logic for embedded controllers.","[' What other sets of semantics are available to represent state machines?', ' What are tools for modeling and designing?']","['tools for modeling and designing logic for embedded controllers', 'logic for embedded controllers']"
2885,finite automaton,Mathematical model,"For both deterministic and non-deterministic FSMs, it is conventional to allow 



δ


{\displaystyle \delta }
 to be a partial function, i.e. 



δ
(
s
,
x
)


{\displaystyle \delta (s,x)}
 does not have to be defined for every combination of 



s
∈
S


{\displaystyle s\in S}
 and 



x
∈
Σ


{\displaystyle x\in \Sigma }
. If an FSM 



M


{\displaystyle M}
 is in a state 



s


{\displaystyle s}
, the next symbol is 



x


{\displaystyle x}
 and 



δ
(
s
,
x
)


{\displaystyle \delta (s,x)}
 is not defined, then 



M


{\displaystyle M}
 can announce an error (i.e. reject the input). This is useful in definitions of general state machines, but less useful when transforming the machine. Some algorithms in their default form may require total functions.
","For both deterministic and non-deterministic FSMs, it is conventional to allow 



δ


{\displaystyle \delta }
 to be a partial function, i.e. δ
(
s
,
x
)


{\displaystyle \delta (s,x)}
 does not have to be defined for every combination of 



s
∈
S


{\displaystyle s\in S}
 and 



x
∈
Σ


{\displaystyle x\in \Sigma }
.","[' What is a partial function for both deterministic and non-deterministic FSMs?', ' What does not have to be defined for every combination?']","['δ', 'δ\n(\ns\n,\nx\n)']"
2886,finite automaton,Mathematical model,"A finite-state machine has the same computational power as a Turing machine that is restricted such that its head may only perform ""read"" operations, and always has to move from left to right. That is, each formal language accepted by a finite-state machine is accepted by such a kind of restricted Turing machine, and vice versa.","A finite-state machine has the same computational power as a Turing machine that is restricted such that its head may only perform ""read"" operations, and always has to move from left to right. That is, each formal language accepted by a finite-state machine is accepted by such a kind of restricted Turing machine, and vice versa.","[' What does a finite-state machine have the same computational power as?', ' What is restricted such that its head may only perform ""read"" operations and always has to move from left to right?', ' What is accepted by a restricted Turing machine?']","['a Turing machine', 'Turing machine', 'each formal language']"
2887,finite automaton,Mathematical model,"If the output function depends on the state and input symbol (



ω
:
S
×
Σ
→
Γ


{\displaystyle \omega :S\times \Sigma \rightarrow \Gamma }
) that definition corresponds to the Mealy model, and can be modelled as a Mealy machine. If the output function depends only on the state (



ω
:
S
→
Γ


{\displaystyle \omega :S\rightarrow \Gamma }
) that definition corresponds to the Moore model, and can be modelled as a Moore machine. A finite-state machine with no output function at all is known as a semiautomaton or transition system.
","If the output function depends on the state and input symbol (



ω
:
S
×
Σ
→
Γ


{\displaystyle \omega :S\times \Sigma \rightarrow \Gamma }
) that definition corresponds to the Mealy model, and can be modelled as a Mealy machine. If the output function depends only on the state (



ω
:
S
→
Γ


{\displaystyle \omega :S\rightarrow \Gamma }
) that definition corresponds to the Moore model, and can be modelled as a Moore machine.","[' If the output function depends on the state and input symbol, what does that definition correspond to?', ' What can be modelled as a Mealy machine?', ' What state corresponds to the Moore model?', ' What can be modelled as a Moore machine?']","['the Mealy model', 'S\n×\nΣ\n→\nΓ', 'ω\n:\nS\n→\nΓ', 'ω\n:\nS\n×\nΣ\n→\nΓ']"
2888,finite automaton,Mathematical model,"If we disregard the first output symbol of a Moore machine, 



ω
(

s

0


)


{\displaystyle \omega (s_{0})}
, then it can be readily converted to an output-equivalent Mealy machine by setting the output function of every Mealy transition (i.e. labeling every edge) with the output symbol given of the destination Moore state. The converse transformation is less straightforward because a Mealy machine state may have different output labels on its incoming transitions (edges). Every such state needs to be split in multiple Moore machine states, one for every incident output symbol.","If we disregard the first output symbol of a Moore machine, 



ω
(

s

0


)


{\displaystyle \omega (s_{0})}
, then it can be readily converted to an output-equivalent Mealy machine by setting the output function of every Mealy transition (i.e. labeling every edge) with the output symbol given of the destination Moore state.",[' What can be easily converted to an output-equivalent Mealy machine if we disregard the first output symbol of a Moore machine?'],['ω\n(\n\ns\n\n0\n\n\n)\n\n\n{\\displaystyle \\omega (s_{0})}']
2889,finite automaton,Optimization,"Optimizing an FSM means finding a machine with the minimum number of states that performs the same function. The fastest known algorithm doing this is the Hopcroft minimization algorithm. Other techniques include using an implication table, or the Moore reduction procedure. Additionally, acyclic FSAs can be minimized in linear time.",Optimizing an FSM means finding a machine with the minimum number of states that performs the same function. The fastest known algorithm doing this is the Hopcroft minimization algorithm.,"[' What does optimizing an FSM mean?', ' What is the fastest known algorithm to do this?']","['finding a machine with the minimum number of states that performs the same function', 'Hopcroft minimization algorithm']"
2890,linear discriminant analysis,Summary,"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.
","Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.","["" What is a generalization of Fisher's linear discriminant?"", ' What method is used in statistics and other fields?', ' What may be used as a linear classifier or dimensionality reduction before later classification?']","['Linear discriminant analysis', ""Fisher's linear discriminant"", ""Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination""]"
2891,linear discriminant analysis,Summary,"LDA is closely related to analysis of variance (ANOVA) and regression analysis, which also attempt to express one dependent variable as a linear combination of other features or measurements. However, ANOVA uses categorical independent variables and a continuous dependent variable, whereas discriminant analysis has continuous independent variables and a categorical dependent variable (i.e. the class label). Logistic regression and probit regression are more similar to LDA than ANOVA is, as they also explain a categorical variable by the values of continuous independent variables. These other methods are preferable in applications where it is not reasonable to assume that the independent variables are normally distributed, which is a fundamental assumption of the LDA method.
","LDA is closely related to analysis of variance (ANOVA) and regression analysis, which also attempt to express one dependent variable as a linear combination of other features or measurements. However, ANOVA uses categorical independent variables and a continuous dependent variable, whereas discriminant analysis has continuous independent variables and a categorical dependent variable (i.e.","[' LDA is closely related to what?', ' ANOVA and regression analysis attempt to express one dependent variable as a linear combination of other features or what else?', ' What uses categorical independent variables and a continuous dependent variable?', ' What type of analysis has continuous independent variables and a categorical dependent variable?']","['analysis of variance', 'measurements', 'ANOVA', 'discriminant']"
2892,linear discriminant analysis,Summary,"LDA is also closely related to principal component analysis (PCA) and factor analysis in that they both look for linear combinations of variables which best explain the data. LDA explicitly attempts to model the difference between the classes of data. PCA, in contrast, does not take into account any difference in class, and factor analysis builds the feature combinations based on differences rather than similarities. Discriminant analysis is also different from factor analysis in that it is not an interdependence technique: a distinction between independent variables and dependent variables (also called criterion variables) must be made.
",LDA is also closely related to principal component analysis (PCA) and factor analysis in that they both look for linear combinations of variables which best explain the data. LDA explicitly attempts to model the difference between the classes of data.,"[' What is LDA closely related to?', ' What do both PCA and factor analysis look for?', ' LDA explicitly attempts to model the difference between what classes of data?']","['principal component analysis', 'linear combinations of variables', 'principal component analysis (PCA) and factor analysis in that they both look for linear combinations of variables which best explain the data. LDA explicitly attempts to model the difference between the classes of data']"
2893,linear discriminant analysis,Summary,"LDA works when the measurements made on independent variables for each observation are continuous quantities. When dealing with categorical independent variables, the equivalent technique is discriminant correspondence analysis.","LDA works when the measurements made on independent variables for each observation are continuous quantities. When dealing with categorical independent variables, the equivalent technique is discriminant correspondence analysis.","[' What does LDA work when the measurements made on independent variables for each observation are continuous quantities?', ' What is the equivalent technique when dealing with categorical independent variables?']","['discriminant correspondence analysis', 'discriminant correspondence analysis']"
2894,linear discriminant analysis,Summary,"Discriminant analysis is used when groups are known a priori (unlike in cluster analysis). Each case must have a score on one or more quantitative predictor measures, and a score on a group measure. In simple terms, discriminant function analysis is classification - the act of distributing things into groups, classes or categories of the same type.
","Discriminant analysis is used when groups are known a priori (unlike in cluster analysis). Each case must have a score on one or more quantitative predictor measures, and a score on a group measure.","[' What type of analysis is used when groups are known a priori?', ' Each case must have a score on what?']","['Discriminant', 'one or more quantitative predictor measures']"
2895,linear discriminant analysis,History,"The original dichotomous discriminant analysis was developed by Sir Ronald Fisher in 1936. It is different from an ANOVA or MANOVA, which is used to predict one (ANOVA) or multiple (MANOVA) continuous dependent variables by one or more independent categorical variables. Discriminant function analysis is useful in determining whether a set of variables is effective in predicting category membership.","The original dichotomous discriminant analysis was developed by Sir Ronald Fisher in 1936. It is different from an ANOVA or MANOVA, which is used to predict one (ANOVA) or multiple (MANOVA) continuous dependent variables by one or more independent categorical variables.","[' Who developed the original dichotomous discriminant analysis?', ' What is different from an ANOVA or MANOVA?']","['Sir Ronald Fisher', 'dichotomous discriminant analysis']"
2896,linear discriminant analysis,LDA for two classes,"Consider a set of observations 






x
→





{\displaystyle {\vec {x}}}
 (also called features, attributes, variables or measurements) for each sample of an object or event with known class 



y


{\displaystyle y}
. This set of samples is called the training set. The classification problem is then to find a good predictor for the class 



y


{\displaystyle y}
 of any sample of the same distribution (not necessarily from the training set) given only an observation 






x
→





{\displaystyle {\vec {x}}}
.: 338 ","Consider a set of observations 






x
→





{\displaystyle {\vec {x}}}
 (also called features, attributes, variables or measurements) for each sample of an object or event with known class 



y


{\displaystyle y}
. This set of samples is called the training set.","[' What is another name for features, attributes, variables or measurements?', ' What is the training set?']","['the training set', 'observations \n\n\n\n\n\n\nx\n→\n\n\n\n\n\n{\\displaystyle {\\vec {x}}}\n (also called features, attributes, variables or measurements) for each sample of an object or event with known class \n\n\n\ny\n\n\n{\\displaystyle y}\n. This set of samples']"
2897,linear discriminant analysis,LDA for two classes,"LDA approaches the problem by assuming that the conditional probability density functions 



p
(



x
→




|

y
=
0
)


{\displaystyle p({\vec {x}}|y=0)}
 and 



p
(



x
→




|

y
=
1
)


{\displaystyle p({\vec {x}}|y=1)}
 are both the normal distribution with mean and covariance parameters 




(





μ
→




0


,

Σ

0



)



{\displaystyle \left({\vec {\mu }}_{0},\Sigma _{0}\right)}
 and 




(





μ
→




1


,

Σ

1



)



{\displaystyle \left({\vec {\mu }}_{1},\Sigma _{1}\right)}
, respectively. Under this assumption, the Bayes optimal solution is to predict points as being from the second class if the log of the likelihood ratios is bigger than some threshold T, so that:
","LDA approaches the problem by assuming that the conditional probability density functions 



p
(



x
→




|

y
=
0
)


{\displaystyle p({\vec {x}}|y=0)}
 and 



p
(



x
→




|

y
=
1
)


{\displaystyle p({\vec {x}}|y=1)}
 are both the normal distribution with mean and covariance parameters 




(





μ
→




0


,

Σ

0



)



{\displaystyle \left({\vec {\mu }}_{0},\Sigma _{0}\right)}
 and 




(





μ
→




1


,

Σ

1



)



{\displaystyle \left({\vec {\mu }}_{1},\Sigma _{1}\right)}
, respectively. Under this assumption, the Bayes optimal solution is to predict points as being from the second class if the log of the likelihood ratios is bigger than some threshold T, so that:","[' How does LDA approach the problem?', ' What is the Bayes optimal solution?', ' What is the Bayes optimal solution to predict points as being from the second class if the log of the likelihood ratios is bigger than some threshold T?']","['0\n\n\n,\n\nΣ\n\n0\n\n\n\n)\n\n\n\n{\\displaystyle \\left({\\vec {\\mu }}_{0},\\Sigma _{0}\\right)}\n and \n\n\n\n\n(\n\n\n\n\n\nμ\n→\n\n\n\n\n1\n\n\n,\n\nΣ\n\n1\n\n\n\n)', 'to predict points', '0\n\n\n,\n\nΣ\n\n0\n\n\n\n)\n\n\n\n{\\displaystyle \\left({\\vec {\\mu }}_{0},\\Sigma _{0}\\right)}\n and \n\n\n\n\n(\n\n\n\n\n\nμ\n→\n\n\n\n\n1\n\n\n,\n\nΣ\n\n1\n\n\n\n)']"
2898,linear discriminant analysis,LDA for two classes,"LDA instead makes the additional simplifying homoscedasticity assumption (i.e. that the class covariances are identical, so 




Σ

0


=

Σ

1


=
Σ


{\displaystyle \Sigma _{0}=\Sigma _{1}=\Sigma }
) and that the covariances have full rank.
In this case, several terms cancel:
","LDA instead makes the additional simplifying homoscedasticity assumption (i.e. that the class covariances are identical, so 




Σ

0


=

Σ

1


=
Σ


{\displaystyle \Sigma _{0}=\Sigma _{1}=\Sigma }
) and that the covariances have full rank.","[' What assumption does LDA make instead of simplifying homoscedasticity assumption?', ' What is the assumption that class covariances are identical?']","['the class covariances are identical, so \n\n\n\n\nΣ\n\n0\n\n\n=\n\nΣ\n\n1\n\n\n=\nΣ\n\n\n{\\displaystyle \\Sigma _{0}=\\Sigma _{1}=\\Sigma }\n) and that the covariances have full rank', 'homoscedasticity']"
2899,markov decision process,Summary,"In mathematics, a Markov decision process (MDP) is a discrete-time stochastic control process. It provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful for studying optimization problems solved via dynamic programming. MDPs were known at least as early as the 1950s; a core body of research on Markov decision processes resulted from Ronald Howard's 1960 book, Dynamic Programming and Markov Processes. They are used in many disciplines, including robotics, automatic control, economics and manufacturing. The name of MDPs comes from the Russian mathematician Andrey Markov as they are an extension of Markov chains.
","In mathematics, a Markov decision process (MDP) is a discrete-time stochastic control process. It provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker.","[' What is a Markov decision process in mathematics?', ' What does MDP stand for?']","['a discrete-time stochastic control process', 'Markov decision process']"
2900,markov decision process,Summary,"At each time step, the process is in some state 



s


{\displaystyle s}
, and the decision maker may choose any action 



a


{\displaystyle a}
 that is available in state 



s


{\displaystyle s}
. The process responds at the next time step by randomly moving into a new state 




s
′



{\displaystyle s'}
, and giving the decision maker a corresponding reward 




R

a


(
s
,

s
′

)


{\displaystyle R_{a}(s,s')}
.
","At each time step, the process is in some state 



s


{\displaystyle s}
, and the decision maker may choose any action 



a


{\displaystyle a}
 that is available in state 



s


{\displaystyle s}
. The process responds at the next time step by randomly moving into a new state 




s
′



{\displaystyle s'}
, and giving the decision maker a corresponding reward 




R

a


(
s
,

s
′

)


{\displaystyle R_{a}(s,s')}
.","[' At each time step, the process is in what state?', "" The decision maker may choose any action that is available in state s 'displaystyle s'?"", ' At the next time step the process responds by randomly moving into what state?', ' What is the name of the reward given to the decision maker?', "" What is a 'displaystyle'?""]","['s', 'a', ""s\n′\n\n\n\n{\\displaystyle s'}"", 'R\n\na', 's\n\n\n{\\displaystyle s}\n, and the decision maker may choose any action \n\n\n\na']"
2901,markov decision process,Summary,"The probability that the process moves into its new state 




s
′



{\displaystyle s'}
 is influenced by the chosen action. Specifically, it is given by the state transition function 




P

a


(
s
,

s
′

)


{\displaystyle P_{a}(s,s')}
. Thus, the next state 




s
′



{\displaystyle s'}
 depends on the current state 



s


{\displaystyle s}
 and the decision maker's action 



a


{\displaystyle a}
. But given 



s


{\displaystyle s}
 and 



a


{\displaystyle a}
, it is conditionally independent of all previous states and actions; in other words, the state transitions of an MDP satisfy the Markov property.
","The probability that the process moves into its new state 




s
′



{\displaystyle s'}
 is influenced by the chosen action. Specifically, it is given by the state transition function 




P

a


(
s
,

s
′

)


{\displaystyle P_{a}(s,s')}
.","[' What is the probability that the process moves into its new state influenced by?', ' What is given by the state transition function P a?']","['the chosen action', ""The probability that the process moves into its new state \n\n\n\n\ns\n′\n\n\n\n{\\displaystyle s'}\n is influenced by the chosen action""]"
2902,markov decision process,Summary,"Markov decision processes are an extension of Markov chains; the difference is the addition of actions (allowing choice) and rewards (giving motivation). Conversely, if only one action exists for each state (e.g. ""wait"") and all rewards are the same (e.g. ""zero""), a Markov decision process reduces to a Markov chain.
","Markov decision processes are an extension of Markov chains; the difference is the addition of actions (allowing choice) and rewards (giving motivation). Conversely, if only one action exists for each state (e.g.","[' What are Markov decision processes an extension of?', ' What is the difference between actions and rewards?', ' How many actions exist for each state?']","['Markov chains', 'giving motivation', 'one']"
2903,markov decision process,Definition,"The state and action spaces may be finite or infinite, for example the set of real numbers. Some processes with countably infinite state and action spaces can be reduced to ones with finite state and action spaces.","The state and action spaces may be finite or infinite, for example the set of real numbers. Some processes with countably infinite state and action spaces can be reduced to ones with finite state and action spaces.","[' The state and action spaces may be finite or what?', ' The set of real numbers is an example of what kind of space?', ' Some processes can be reduced to processes with what type of space and state?']","['infinite', 'state and action spaces', 'countably infinite']"
2904,markov decision process,Algorithms,"Solutions for MDPs with finite state and action spaces may be found through a variety of methods such as dynamic programming. The algorithms in this section apply to MDPs with finite state and action spaces and explicitly given transition probabilities and reward functions, but the basic concepts may be extended to handle other problem classes, for example using function approximation.
","Solutions for MDPs with finite state and action spaces may be found through a variety of methods such as dynamic programming. The algorithms in this section apply to MDPs with finite state and action spaces and explicitly given transition probabilities and reward functions, but the basic concepts may be extended to handle other problem classes, for example using function approximation.","[' What can be found through a variety of methods such as dynamic programming?', ' The algorithms in this section apply to MDPs with finite state and action spaces and explicitly given what?', ' What is an example of a function approximation that can be extended to handle other problem classes?']","['Solutions for MDPs with finite state and action spaces', 'transition probabilities and reward functions', 'basic concepts']"
2905,markov decision process,Algorithms,"The standard family of algorithms to calculate optimal policies for finite state and action MDPs requires storage for two arrays indexed by state: value 



V


{\displaystyle V}
, which contains real values, and policy 



π


{\displaystyle \pi }
, which contains actions. At the end of the algorithm, 



π


{\displaystyle \pi }
 will contain the solution and 



V
(
s
)


{\displaystyle V(s)}
 will contain the discounted sum of the rewards to be earned (on average) by following that solution from state 



s


{\displaystyle s}
.
","The standard family of algorithms to calculate optimal policies for finite state and action MDPs requires storage for two arrays indexed by state: value 



V


{\displaystyle V}
, which contains real values, and policy 



π


{\displaystyle \pi }
, which contains actions. At the end of the algorithm, 



π


{\displaystyle \pi }
 will contain the solution and 



V
(
s
)


{\displaystyle V(s)}
 will contain the discounted sum of the rewards to be earned (on average) by following that solution from state 



s


{\displaystyle s}
.","[' What does the standard family of algorithms to calculate optimal policies for finite state and action MDPs require storage for?', ' What does value V <unk>displaystyle V<unk> contain?', ' At the end of the algorithm, <unk> <unk>displaystyle <unk>pi <unk> will contain what?', ' V ( s ) will contain the discounted sum of the rewards to be earned (on average) by following that solution from what state?']","['two arrays indexed by state', 'real values', 'actions. At the end of the algorithm, \n\n\n\nπ\n\n\n{\\displaystyle \\pi }\n will contain the solution', 's']"
2906,markov decision process,Algorithms,"The algorithm has two steps, (1) a value update and (2) a policy update, which are repeated in some order for all the states until no further changes take place.  Both recursively update a new estimation of the optimal policy and state value using an older estimation of those values.
","The algorithm has two steps, (1) a value update and (2) a policy update, which are repeated in some order for all the states until no further changes take place. Both recursively update a new estimation of the optimal policy and state value using an older estimation of those values.","[' How many steps does the algorithm have?', ' What are the two steps that the algorithm has?']","['two', '(1) a value update and (2) a policy update']"
2907,markov decision process,Algorithms,"Their order depends on the variant of the algorithm; one can also do them for all states at once or state by state, and more often to some states than others. As long as no state is permanently excluded from either of the steps, the algorithm will eventually arrive at the correct solution.","Their order depends on the variant of the algorithm; one can also do them for all states at once or state by state, and more often to some states than others. As long as no state is permanently excluded from either of the steps, the algorithm will eventually arrive at the correct solution.","[' What depends on the variant of the algorithm?', ' One can do them for all states at once or state by state and more often to some states?', ' Which algorithm will eventually arrive at the correct solution?']","['Their order', 'Their order depends on the variant of the algorithm', 'As long as no state is permanently excluded from either of the steps, the algorithm']"
2908,markov decision process,Continuous-time Markov decision process,"In discrete-time Markov Decision Processes, decisions are made at discrete time intervals. However, for continuous-time Markov decision processes, decisions can be made at any time the decision maker chooses. In comparison to discrete-time Markov decision processes, continuous-time Markov decision processes can better model the decision making process for a system that has continuous dynamics, i.e., the system dynamics is defined by partial differential equations (PDEs).
","In discrete-time Markov Decision Processes, decisions are made at discrete time intervals. However, for continuous-time Markov decision processes, decisions can be made at any time the decision maker chooses.","[' Where are decisions made in discrete-time Markov Decision Processes?', ' Where can decisions be made in continuous-time markov decision processes?']","['at discrete time intervals', 'at any time the decision maker chooses']"
2909,markov decision process,Alternative notations,"The terminology and notation for MDPs are not entirely settled. There are two main streams — one focuses on maximization problems from contexts like economics, using the terms action, reward, value, and calling the discount factor β or γ, while the other focuses on minimization problems from engineering and navigation, using the terms control, cost, cost-to-go, and calling the discount factor α. In addition, the notation for the transition probability varies.
","The terminology and notation for MDPs are not entirely settled. There are two main streams — one focuses on maximization problems from contexts like economics, using the terms action, reward, value, and calling the discount factor β or γ, while the other focuses on minimization problems from engineering and navigation, using the terms control, cost, cost-to-go, and calling the discount factor α.","[' The terminology and notation for MDPs are not entirely settled what?', ' What are the two main streams?', ' What are the terms used for minimization problems from engineering and navigation?', ' What is the discount factor called?']","['There are two main streams', 'one focuses on maximization problems from contexts like economics', 'control, cost, cost-to-go, and calling the discount factor α', 'α']"
2910,markov decision process,Alternative notations,"In addition, transition probability is sometimes written 



Pr
(
s
,
a
,

s
′

)


{\displaystyle \Pr(s,a,s')}
, 



Pr
(

s
′

∣
s
,
a
)


{\displaystyle \Pr(s'\mid s,a)}
 or, rarely, 




p


s
′

s


(
a
)
.


{\displaystyle p_{s's}(a).}

","In addition, transition probability is sometimes written 



Pr
(
s
,
a
,

s
′

)


{\displaystyle \Pr(s,a,s')}
, 



Pr
(

s
′

∣
s
,
a
)


{\displaystyle \Pr(s'\mid s,a)}
 or, rarely, 




p


s
′

s


(
a
)
. {\displaystyle p_{s's}(a).}","["" What is sometimes written Pr(s,a,s')?"", "" Pr (s ′ <unk> s, a ) <unk>displaystyle <unk>Pr(s'<unk>mid s.a)], or, rarer, what is written p_<unk>s's?""]","['transition probability', ""p_{s's""]"
2911,markov decision process,Constrained Markov decision processes,Constrained Markov decision processes (CMDPs) are extensions to Markov decision process (MDPs). There are three fundamental differences between MDPs and CMDPs.,Constrained Markov decision processes (CMDPs) are extensions to Markov decision process (MDPs). There are three fundamental differences between MDPs and CMDPs.,"[' Constrained Markov decision processes are extensions to what?', ' What are the three fundamental differences between MDPs and CMDPs?']","['Markov decision process', 'Constrained Markov decision processes']"
2912,markov decision process,Constrained Markov decision processes,There are a number of applications for CMDPs. It has recently been used in motion planning scenarios in robotics.,There are a number of applications for CMDPs. It has recently been used in motion planning scenarios in robotics.,[' CMDPs have been used in motion planning scenarios in what field?'],['robotics']
2913,supply chain,Summary,"In commerce, a supply chain is a system within organizations, people, activities, information, and resources involved in
supplying a product or service to a consumer. Supply chain activities involve the transformation of natural resources, raw materials, and components into a finished product and delivering the same to the end customer. In sophisticated supply chain systems, used products may re-enter the supply chain at any point where residual value is recyclable. Supply chains link value chains. Suppliers in a supply chain are often ranked by ""tier"", first-tier suppliers being those who supply direct to the client business, second-tier being suppliers to the first tier, etc.","In commerce, a supply chain is a system within organizations, people, activities, information, and resources involved in
supplying a product or service to a consumer. Supply chain activities involve the transformation of natural resources, raw materials, and components into a finished product and delivering the same to the end customer.","[' What is a supply chain in commerce?', ' What is the process of transforming natural resources, raw materials, and components into a finished product?']","['a system within organizations, people, activities, information, and resources involved in\nsupplying a product or service to a consumer', 'Supply chain activities']"
2914,supply chain,Overview,"A typical supply chain begins with the ecological, biological, and political regulation of natural resources, followed by the human extraction of raw material, and includes several production links (e.g., component construction, assembly, and merging) before moving on to several layers of storage facilities of ever-decreasing size and increasingly remote geographical locations, and finally reaching the consumer. At the end of the supply chain, materials and finished products only flow there because of customer behaviour at the end of the chain; academics Alan Harrison and Janet Godsell argue that ""supply chain processes
should be co-ordinated in order to focus on end customer buying behaviour"", and look for ""customer responsiveness"" as an indicator confirming that materials are able to flow ""through a sequence of supply chain processes in order to meet end customer buying behaviour"".","A typical supply chain begins with the ecological, biological, and political regulation of natural resources, followed by the human extraction of raw material, and includes several production links (e.g., component construction, assembly, and merging) before moving on to several layers of storage facilities of ever-decreasing size and increasingly remote geographical locations, and finally reaching the consumer. At the end of the supply chain, materials and finished products only flow there because of customer behaviour at the end of the chain; academics Alan Harrison and Janet Godsell argue that ""supply chain processes
should be co-ordinated in order to focus on end customer buying behaviour"", and look for ""customer responsiveness"" as an indicator confirming that materials are able to flow ""through a sequence of supply chain processes in order to meet end customer buying behaviour"".","[' What does a typical supply chain begin with?', ' What is the first step in a supply chain?', ' How many production links do supply chains include?', ' What do academics Alan Harrison and Janet Godsell argue about the supply chain?', ' Academics Alan Harrison and Janet Godsell argue that ""supply chain processes should be co-ordinated in order to focus on end customer buying behaviour""?', ' What does the supply chain need to meet in order to meet end customer buying behaviour?']","['the ecological, biological, and political regulation of natural resources', 'ecological, biological, and political regulation of natural resources', 'several', 'supply chain processes\nshould be co-ordinated in order to focus on end customer buying behaviour', 'At the end of the supply chain', 'customer responsiveness']"
2915,supply chain,Overview,"Many of the exchanges encountered in the supply chain take place between varied companies that seek to maximize their revenue within their sphere of interest but may have little or no knowledge or interest in the remaining players in the supply chain. More recently, the loosely coupled, self-organizing network of businesses who cooperate to provide product and service offerings has been called the extended enterprise, and the use of the term ""chain"" and the linear structure it appears to represent have been criticised as ""harder to relate ... to the way supply networks really operate. A chain is actually a complex and dynamic supply and demand network.","Many of the exchanges encountered in the supply chain take place between varied companies that seek to maximize their revenue within their sphere of interest but may have little or no knowledge or interest in the remaining players in the supply chain. More recently, the loosely coupled, self-organizing network of businesses who cooperate to provide product and service offerings has been called the extended enterprise, and the use of the term ""chain"" and the linear structure it appears to represent have been criticised as ""harder to relate ... to the way supply networks really operate.","[' What do many of the exchanges encountered in the supply chain take place between?', ' What do companies seek to maximize their revenue within their sphere of interest but may have little or no knowledge or interest in?', ' The loosely coupled, self-organizing network of what?', ' What has been called the extended enterprise?', ' What has the use of the term ""chain"" and its linear structure been criticised for?', ' What have been criticised as harder to relate to the way supply networks really operate?']","['varied companies', 'the remaining players in the supply chain', 'businesses who cooperate to provide product and service offerings', 'the loosely coupled, self-organizing network of businesses who cooperate to provide product and service offerings', 'harder to relate', 'the use of the term ""chain"" and the linear structure']"
2916,supply chain,Overview,"As part of their efforts to demonstrate ethical practices, many large companies and global brands are integrating codes of conduct and guidelines into their corporate cultures and management systems. Through these, corporations are making demands on their suppliers (facilities, farms, subcontracted services such as cleaning, canteen, security etc.) and verifying, through social audits, that they are complying with the required standard. A lack of transparency in the supply chain can bar consumers from knowledge of where their purchases originated and facilitate socially irresponsible practices. In 2018, the Loyola University Chicago's Supply and Value Chain Center found in a survey that 53% of supply chain professionals considered ethics to be ""extremely"" important to their organization. Supply-chain managers are under constant scrutiny to secure the best pricing for their resources, which becomes a difficult task when faced with the inherent lack of transparency. Cost benchmarking is one effective method for identifying competitive pricing within the industry. This gives negotiators a solid basis to form their strategy on and drive overall spend down.
","As part of their efforts to demonstrate ethical practices, many large companies and global brands are integrating codes of conduct and guidelines into their corporate cultures and management systems. Through these, corporations are making demands on their suppliers (facilities, farms, subcontracted services such as cleaning, canteen, security etc.)","[' What are large companies and global brands integrating into their corporate cultures and management systems?', ' What are corporations making demands on their suppliers?']","['codes of conduct and guidelines', 'codes of conduct and guidelines']"
2917,supply chain,Typologies,"Marshall L. Fisher (1977) asks the question in a key article, ""Which is the right supply chain for your product?"" Fisher, and also Naylor, Naim and Berry (1999), identify two matching characteristics of supply chain strategy: a combination of ""functional"" and ""efficient"", or a combination of ""responsive"" and ""innovative"" (Harrison and Godsell).","Marshall L. Fisher (1977) asks the question in a key article, ""Which is the right supply chain for your product?"" Fisher, and also Naylor, Naim and Berry (1999), identify two matching characteristics of supply chain strategy: a combination of ""functional"" and ""efficient"", or a combination of ""responsive"" and ""innovative"" (Harrison and Godsell).","[' Who wrote the article ""Which is the right supply chain for your product?""', ' In what year did Fisher and Naylor, Naim and Berry identify two matching characteristics of supply chain strategy?', ' What is the combination of ""responsive"" and ""innovative""?']","['Marshall L. Fisher', '1999', 'functional"" and ""efficient']"
2918,supply chain,Typologies,"
Brown et al. refer to supply chains as either ""loosely coupled"" or ""tightly coupled"":","
Brown et al. refer to supply chains as either ""loosely coupled"" or ""tightly coupled"":",[' What do Brown et al. refer to supply chains as?'],"['loosely coupled"" or ""tightly coupled']"
2919,supply chain,Typologies," These ideas refer to two polar models of collaboration: tightly coupled, or ""hard-wired"", also known as ""linked"", collaboration represents a close relationship between a buyer and supplier within the chain, whereas a loosely-coupled link relates to low interdependency between buyer and seller and therefore greater flexibility. The Chartered Institute of Procurement & Supply's professional guidance suggests that the aim of a tightly coupled relationship is to reduce inventory and avoid stock-outs."," These ideas refer to two polar models of collaboration: tightly coupled, or ""hard-wired"", also known as ""linked"", collaboration represents a close relationship between a buyer and supplier within the chain, whereas a loosely-coupled link relates to low interdependency between buyer and seller and therefore greater flexibility. The Chartered Institute of Procurement & Supply's professional guidance suggests that the aim of a tightly coupled relationship is to reduce inventory and avoid stock-outs.","[' What are the two polar models of collaboration?', ' What is tightly coupled, or hard-wired, also known as?', ' A loosely-coupled link relates to low interdependency between buyer and what?', ' What is the aim of a tightly coupled relationship?', ' What does the Chartered Institute of Procurement & Supply suggest?']","['tightly coupled, or ""hard-wired"",', 'linked', 'seller', 'to reduce inventory and avoid stock-outs', 'the aim of a tightly coupled relationship is to reduce inventory and avoid stock-outs']"
2920,supply chain,Modeling,"There are a variety of supply-chain models, which address both the upstream and downstream elements of supply-chain management (SCM). The SCOR (Supply-Chain Operations Reference) model, developed by a consortium of industry and the non-profit Supply Chain Council (now part of APICS) became the cross-industry de facto standard defining the scope of supply-chain management. SCOR measures total supply-chain performance. It is a process reference model for supply-chain management, spanning from the supplier's supplier to the customer's customer. It includes delivery and order fulfillment performance, production flexibility, warranty and returns processing costs, inventory and asset turns, and other factors in evaluating the overall effective performance of a supply chain.
","There are a variety of supply-chain models, which address both the upstream and downstream elements of supply-chain management (SCM). The SCOR (Supply-Chain Operations Reference) model, developed by a consortium of industry and the non-profit Supply Chain Council (now part of APICS) became the cross-industry de facto standard defining the scope of supply-chain management.","[' What is the acronym for Supply-Chain Operations Reference?', ' Who developed the SCOR model?', ' What organization is the Supply Chain Council a part of?', ' What became the cross-industry de facto standard defining the scope of supply-chain management?']","['SCOR', 'a consortium of industry and the non-profit Supply Chain Council', 'APICS', 'The SCOR (Supply-Chain Operations Reference) model']"
2921,supply chain,Modeling,"The Global Supply Chain Forum has introduced another supply chain model. This framework is built on eight key business processes that are both cross-functional and cross-firm in nature. Each process is managed by a cross-functional team including representatives from logistics, production, purchasing, finance, marketing, and research and development. While each process interfaces with key customers and suppliers, the processes of customer relationship management and supplier relationship management form the critical linkages in the supply chain.
",The Global Supply Chain Forum has introduced another supply chain model. This framework is built on eight key business processes that are both cross-functional and cross-firm in nature.,"[' What has the Global Supply Chain Forum introduced?', ' How many key business processes are in the global supply chain model?']","['another supply chain model', 'eight']"
2922,supply chain,Modeling,"The American Productivity and Quality Center (APQC) Process Classification Framework (PCF) SM is a high-level, industry-neutral enterprise process model that allows organizations to see their business processes from a cross-industry viewpoint. The PCF was developed by APQC and its member organizations as an open standard to facilitate improvement through process management and benchmarking, regardless of industry, size, or geography. The PCF organizes operating and management processes into 12 enterprise-level categories, including process groups, and over 1,000 processes and associated activities.
","The American Productivity and Quality Center (APQC) Process Classification Framework (PCF) SM is a high-level, industry-neutral enterprise process model that allows organizations to see their business processes from a cross-industry viewpoint. The PCF was developed by APQC and its member organizations as an open standard to facilitate improvement through process management and benchmarking, regardless of industry, size, or geography.","["" What is the acronym for the American Productivity and Quality Center's Process Classification Framework?"", ' What does the PCF stand for?', ' The PCF SM allows organizations to see their business processes from a cross-industry perspective?', ' Organizations as an open standard to facilitate improvement through what?', ' What does process management and benchmarking facilitate?']","['PCF', 'Process Classification Framework', 'The American Productivity and Quality Center (APQC) Process Classification Framework', 'process management and benchmarking', 'improvement']"
2923,supply chain,Management,"In the 1980s, the term supply-chain management (SCM) was developed to express the need to integrate the key business processes, from end user through original suppliers. Original suppliers are those that provide products, services, and information that add value for customers and other stakeholders. The basic idea behind SCM is that companies and corporations involve themselves in a supply chain by exchanging information about market demand, distribution capacity and production capabilities. Keith Oliver, a consultant at Booz Allen Hamilton, is credited with the term's invention after using it in an interview for the Financial Times in 1982. The term was used earlier by Alizamir et al. in 1981.","In the 1980s, the term supply-chain management (SCM) was developed to express the need to integrate the key business processes, from end user through original suppliers. Original suppliers are those that provide products, services, and information that add value for customers and other stakeholders.","[' When was the term supply-chain management (SCM) developed?', ' What does SCM stand for?', ' Who provides products, services and information that add value for customers and other stakeholders?']","['1980s', 'supply-chain management', 'Original suppliers']"
2924,supply chain,Management,"If all relevant information is accessible to any relevant company, every company in the supply chain has the ability to help optimize the entire supply chain rather than to sub-optimize based on local optimization. This will lead to better-planned overall production and distribution, which can cut costs and give a more attractive final product, leading to better sales and better overall results for the companies involved.  This is one form of vertical integration. Yet, it has been shown that the motives for and performance efficacy of vertical integration differ by global region.","If all relevant information is accessible to any relevant company, every company in the supply chain has the ability to help optimize the entire supply chain rather than to sub-optimize based on local optimization. This will lead to better-planned overall production and distribution, which can cut costs and give a more attractive final product, leading to better sales and better overall results for the companies involved.","[' How can every company in the supply chain help optimize the entire supply chain?', ' What will lead to better-planned overall production and distribution?', ' What can reduce costs and give a more attractive final product?', ' What can lead to better sales and better results for the companies involved?']","['If all relevant information is accessible to any relevant company', 'If all relevant information is accessible to any relevant company', 'better-planned overall production and distribution', 'better-planned overall production and distribution']"
2925,supply chain,Management,"The primary objective of SCM is to fulfill customer demands through the most efficient use of resources, including distribution capacity, inventory, and labor. In theory, a supply chain seeks to match demand with supply and do so with minimal inventory. Various aspects of optimizing the supply chain include liaising with suppliers to eliminate bottlenecks; sourcing strategically to strike a balance between lowest material cost and transportation, implementing just-in-time techniques to optimize manufacturing flow; maintaining the right mix and location of factories and warehouses to serve customer markets; and using location allocation, vehicle routing analysis, dynamic programming, and traditional logistics optimization to maximize the efficiency of distribution.
","The primary objective of SCM is to fulfill customer demands through the most efficient use of resources, including distribution capacity, inventory, and labor. In theory, a supply chain seeks to match demand with supply and do so with minimal inventory.","[' What is the primary objective of SCM?', ' How does a supply chain match demand with supply?']","['to fulfill customer demands through the most efficient use of resources', 'with minimal inventory']"
2926,supply chain,Management,"Starting in the 1990s, several companies chose to outsource the logistics aspect of supply-chain management by partnering with a third-party logistics provider (3PL). Companies also outsource production to contract manufacturers. Technology companies have risen to meet the demand to help manage these complex systems. Cloud-based SCM technologies are at the forefront of next-generation supply chains due to their impact on optimization of time, resources, and inventory visibility. Cloud technologies facilitate work being processed offline from a mobile app which solves the common issue of inventory residing in areas with no online coverage or connectivity.","Starting in the 1990s, several companies chose to outsource the logistics aspect of supply-chain management by partnering with a third-party logistics provider (3PL). Companies also outsource production to contract manufacturers.","[' When did some companies outsource the logistics aspect of supply-chain management?', ' What was the third-party logistics provider called?', ' Companies outsource production to who?']","['Starting in the 1990s', '3PL', 'contract manufacturers']"
2927,supply chain,Resilience,"Supply chain resilience is ""the capacity of a supply chain to persist, adapt, or transform in the face of change"". For a long time, the interpretation of resilience in the sense of engineering resilience (= robustness) prevailed in supply chain management, leading to the notion of persistence. A popular implementation of this idea is given by measuring the time-to-survive and the time-to-recover of the supply chain, allowing identification of weak points in the system. More recently, the interpretations of resilience in the sense of ecological resilience and social–ecological resilience have led to the notions of adaptation and transformation, respectively. A supply chain is thus interpreted as a social-ecological system that – similar to an ecosystem (e.g. forest) – is able to constantly adapt to external environmental conditions and – through the presence of social actors and their ability to foresight – also to transform itself into a fundamentally new system. This leads to a panarchical interpretation of a supply chain, embedding it into a system of systems, allowing to analyze the interactions of the supply chain with systems that operate at other levels (e.g. society, political economy, planet Earth). For example, these three components of resilience can be discussed for the 2021 Suez Canal obstruction, when a ship blocked the canal for several days. Persistence means to ""bounce back""; in our example it is about removing the ship as quickly as possible to allow ""normal"" operations. Adaptation means to accept that the system has reached to a ""new normal"" state and to act accordingly; here, this can be implemented by redirecting ships around the African cape or use alternative modes of transport. Finally, transformation means to question the assumptions of globalization, outsourcing, and linear supply chains and to envision alternatives; in this example this could lead to local and circular supply chains.
","Supply chain resilience is ""the capacity of a supply chain to persist, adapt, or transform in the face of change"". For a long time, the interpretation of resilience in the sense of engineering resilience (= robustness) prevailed in supply chain management, leading to the notion of persistence.","[' What is the term for the capacity of a supply chain to persist, adapt, or transform in the face of change?', ' For a long time, the interpretation of resilience in the sense of engineering resilience prevailed in what?', ' What led to the notion of persistence?']","['Supply chain resilience', 'supply chain management', 'engineering resilience']"
2928,supply chain,Role of the internet,"On the internet, customers can directly contact the distributors. This has reduced the length of the chain to some extent by cutting down on middlemen. Some of the benefits are cost reduction and greater collaboration. Social media now plays an important role in holding corporations accountable due to rapid spread of information that can sway purchasing decisions. Internet has allowed all types of transactions (economic and social) to be performed online, which in turn has enabled real-time digital capture of data; the use of big data analytics to drive supply chains has been reviewed in recent papers by Sanders and others.","On the internet, customers can directly contact the distributors. This has reduced the length of the chain to some extent by cutting down on middlemen.","[' On the internet, what can customers directly contact the distributors?', ' What has reduced the length of the chain?']","['middlemen', 'On the internet, customers can directly contact the distributors']"
2929,supply chain,Social responsibility,"Incidents like the 2013 Savar building collapse with more than 1,100 victims have led to widespread discussions about corporate social responsibility across global supply chains. Wieland and Handfield (2013) suggest that companies need to audit products and suppliers and that supplier auditing needs to go beyond direct relationships with first-tier suppliers (those who supply the main customer directly). They also demonstrate that visibility needs to be improved if the supply cannot be directly controlled and that smart and electronic technologies play a key role to improve visibility. Finally, they highlight that collaboration with local partners, across the industry and with universities is crucial to successfully manage social responsibility in supply chains.","Incidents like the 2013 Savar building collapse with more than 1,100 victims have led to widespread discussions about corporate social responsibility across global supply chains. Wieland and Handfield (2013) suggest that companies need to audit products and suppliers and that supplier auditing needs to go beyond direct relationships with first-tier suppliers (those who supply the main customer directly).","[' How many people died in the Savar building collapse in 2013?', ' What did Wieland and Handfield suggest that companies need to audit?', ' What does supplier auditing need to go beyond?', ' What are first-tier suppliers?']","['1,100', 'products and suppliers', 'direct relationships with first-tier suppliers', 'those who supply the main customer directly']"
2930,supply chain,Traceability in agricultural supply chains,"Many agribusinesses and food processors source raw materials from smallholder farmers. This is particularly true in certain sectors, such as coffee, cocoa and sugar. Over the past 20 years, there has been a shift towards more traceable supply chains. Rather than purchasing crops that have passed through several layers of collectors, firms are now sourcing directly from farmers or trusted aggregators. The drivers for this change include concerns about food safety, child labor and environmental sustainability as well as a desire to increase productivity and improve crop quality.","Many agribusinesses and food processors source raw materials from smallholder farmers. This is particularly true in certain sectors, such as coffee, cocoa and sugar.","[' Where do many agribusinesses and food processors source raw materials from?', ' Coffee, cocoa and sugar are examples of what?']","['smallholder farmers', 'Many agribusinesses and food processors source raw materials from smallholder farmers']"
2931,supply chain,Regulation,"Supply chain security has become particularly important in recent years. As a result, supply chains are often subject to global and local regulations. In the United States, several major regulations emerged in 2010 that have had a lasting impact on how global supply chains operate. These new regulations include the Importer Security Filing (ISF) and additional provisions of the Certified Cargo Screening Program. EU's draft supply chain law are due diligence requirements to protect human rights and the environment in the supply chain.
","Supply chain security has become particularly important in recent years. As a result, supply chains are often subject to global and local regulations.","[' What has become particularly important in recent years?', ' Supply chains are often subject to what?']","['Supply chain security', 'global and local regulations']"
2932,supply chain,Development and design,"With the increasing globalization and easier access to different kinds of alternative products in today's markets, the importance of product design to generating demand is more significant than ever. In addition, as supply, and therefore competition, among companies for the limited market demand increases and as pricing and other marketing elements become less distinguishing factors, product design likewise plays a different role by providing attractive features to generate demand. In this context, demand generation is used to define how attractive a product design is in terms of creating demand. In other words, it is the ability of a product's design to generate demand by satisfying customer expectations. But product design affects not only demand generation but also manufacturing processes, cost, quality, and lead time. The product design affects the associated supply chain and its requirements directly, including manufacturing, transportation, quality, quantity, production schedule, material selection, production technologies, production policies, regulations, and laws. Broadly, the success of the supply chain depends on the product design and the capabilities of the supply chain, but the reverse is also true: the success of the product depends on the supply chain that produces it.
","With the increasing globalization and easier access to different kinds of alternative products in today's markets, the importance of product design to generating demand is more significant than ever. In addition, as supply, and therefore competition, among companies for the limited market demand increases and as pricing and other marketing elements become less distinguishing factors, product design likewise plays a different role by providing attractive features to generate demand.","[' What is the importance of product design to generating demand more important than ever?', ' What increases as supply and competition among companies increases?', ' What factors become less distinguishing factors?', ' What does product design provide to generate demand?']","[""increasing globalization and easier access to different kinds of alternative products in today's markets"", 'limited market demand', 'pricing and other marketing elements', 'attractive features']"
2933,motion estimation,Summary,"Motion estimation is the process of determining motion vectors that describe the transformation from one 2D image to another; usually from adjacent frames in a video sequence. It is an ill-posed problem as the motion is in three dimensions but the images are a projection of the 3D scene onto a 2D plane. The motion vectors may relate to the whole image (global motion estimation) or specific parts, such as rectangular blocks, arbitrary shaped patches or even per pixel. The motion vectors may be represented by a translational model or many other models that can approximate the motion of a real video camera, such as rotation and translation in all three dimensions and zoom.
",Motion estimation is the process of determining motion vectors that describe the transformation from one 2D image to another; usually from adjacent frames in a video sequence. It is an ill-posed problem as the motion is in three dimensions but the images are a projection of the 3D scene onto a 2D plane.,"[' What is the process of determining motion vectors that describe the transformation from one 2D image to another?', ' What is an ill-posed problem as the motion is in three dimensions?', ' What are the images a projection of the 3D scene onto?']","['Motion estimation', 'Motion estimation', '2D plane']"
2934,motion estimation,Related terms,"More often than not, the term motion estimation and the term optical flow are used interchangeably. It is also related in concept to image registration and stereo correspondence. In fact all of these terms refer to the process of finding corresponding points between two images or video frames. The points that correspond to each other in two views (images or frames) of a real scene or object are ""usually"" the same point in that scene or on that object. Before we do motion estimation, we must define our measurement of correspondence, i.e., the matching metric, which is a measurement of how similar two image points are. There is no right or wrong here; the choice of matching metric is usually related to what the final estimated motion is used for as well as the optimisation strategy in the estimation process.
","More often than not, the term motion estimation and the term optical flow are used interchangeably. It is also related in concept to image registration and stereo correspondence.","[' What are the terms motion estimation and optical flow used for?', ' Motion estimation is related to image registration and what else?']","['image registration and stereo correspondence', 'stereo correspondence']"
2935,motion estimation,Algorithms,"The methods for finding motion vectors can be categorised into pixel based methods (""direct"") and feature based methods (""indirect""). A famous debate resulted in two papers from the opposing factions being produced to try to establish a conclusion.","The methods for finding motion vectors can be categorised into pixel based methods (""direct"") and feature based methods (""indirect""). A famous debate resulted in two papers from the opposing factions being produced to try to establish a conclusion.","[' What are the methods for finding motion vectors categorised into?', ' How many papers were produced to try to establish a conclusion?']","['pixel based methods (""direct"") and feature based methods (""indirect"").', 'two']"
2936,rough sets,Summary,"In computer science, a rough set, first described by Polish computer scientist Zdzisław I. Pawlak, is a formal approximation of a crisp set (i.e., conventional set) in terms of a pair of sets which give the lower and the upper approximation of the original set. In the standard version of rough set theory (Pawlak 1991), the lower- and upper-approximation sets are crisp sets, but in other variations, the approximating sets may be fuzzy sets.
","In computer science, a rough set, first described by Polish computer scientist Zdzisław I. Pawlak, is a formal approximation of a crisp set (i.e., conventional set) in terms of a pair of sets which give the lower and the upper approximation of the original set. In the standard version of rough set theory (Pawlak 1991), the lower- and upper-approximation sets are crisp sets, but in other variations, the approximating sets may be fuzzy sets.","[' Who first described a rough set?', ' What is a formal approximation of a crisp set in computer science?', ' What is the standard version of rough set theory?', ' What are the lower- and upper-approximation sets?', ' In other variations, the approximating sets may be what?']","['Zdzisław I. Pawlak', 'a rough set', 'Pawlak 1991', 'crisp sets', 'fuzzy sets']"
2937,rough sets,Definitions,"The following section contains an overview of the basic framework of rough set theory, as originally proposed by Zdzisław I. Pawlak, along with some of the key definitions. More formal properties and boundaries of rough sets can be found in Pawlak (1991) and cited references. The initial and basic theory of rough sets is sometimes referred to as ""Pawlak Rough Sets"" or ""classical rough sets"", as a means to distinguish from more recent extensions and generalizations.
","The following section contains an overview of the basic framework of rough set theory, as originally proposed by Zdzisław I. Pawlak, along with some of the key definitions. More formal properties and boundaries of rough sets can be found in Pawlak (1991) and cited references.","[' Who originally proposed the basic framework of rough set theory?', ' What is the main focus of this section?', ' Where can more formal properties and boundaries be found?']","['Zdzisław I. Pawlak', 'rough set theory', 'Pawlak (1991) and cited references']"
2938,rough sets,Rule extraction,"The category representations discussed above are all extensional in nature; that is, a category or complex class is simply the sum of all its members. To represent a category is, then, just to be able to list or identify all the objects belonging to that category.  However, extensional category representations have very limited practical use, because they provide no insight for deciding whether novel (never-before-seen) objects are members of the category.
","The category representations discussed above are all extensional in nature; that is, a category or complex class is simply the sum of all its members. To represent a category is, then, just to be able to list or identify all the objects belonging to that category.","[' What are the category representations discussed above?', ' What is the sum of all its members?', ' To represent a category is to be able to list or identify all the objects belonging to what?']","['extensional in nature', 'a category or complex class', 'that category']"
2939,rough sets,Rule extraction,"What is generally desired is an intentional description of the category, a representation of the category based on a set of rules that describe the scope of the category.  The choice of such rules is not unique, and therein lies the issue of inductive bias. See Version space and Model selection for more about this issue.
","What is generally desired is an intentional description of the category, a representation of the category based on a set of rules that describe the scope of the category. The choice of such rules is not unique, and therein lies the issue of inductive bias.","[' What is generally desired is an intentional description of a category?', ' What is the choice of rules that describe the scope of the category not unique?']","['a representation of the category based on a set of rules that describe the scope of the category', 'inductive bias']"
2940,rough sets,Rule extraction,"There are a few rule-extraction methods.  We will start from a rule-extraction procedure based on Ziarko & Shan (1995).
",There are a few rule-extraction methods. We will start from a rule-extraction procedure based on Ziarko & Shan (1995).,"[' How many rule-extraction methods are there?', ' What is the rule extraction procedure based on?']","['a few', 'Ziarko & Shan']"
2941,rough sets,Incomplete data,"Rough set theory is useful for rule induction from incomplete data sets. Using this approach we can distinguish between three types of missing attribute values: lost values (the values that were recorded but currently are unavailable), attribute-concept values (these missing attribute values may be replaced by any attribute value limited to the same concept), and ""do not care"" conditions  (the original values were irrelevant).  A  concept (class) is a set of all objects classified (or diagnosed) the same way.
","Rough set theory is useful for rule induction from incomplete data sets. Using this approach we can distinguish between three types of missing attribute values: lost values (the values that were recorded but currently are unavailable), attribute-concept values (these missing attribute values may be replaced by any attribute value limited to the same concept), and ""do not care"" conditions  (the original values were irrelevant).","[' What is useful for rule induction from incomplete data sets?', ' How many types of missing attribute values can we distinguish using rough set theory?', ' What are lost values?', ' What may be replaced by any attribute value limited to the same concept?', ' What are ""do not care"" conditions?']","['Rough set theory', 'three', 'the values that were recorded but currently are unavailable', 'attribute-concept values', 'the original values were irrelevant']"
2942,rough sets,Incomplete data,"In attribute-concept values interpretation of a missing attribute value, the missing attribute value may be replaced by any value of the attribute domain restricted to the concept to which the object with a missing attribute value belongs (Grzymala-Busse and Grzymala-Busse, 2007).  For example, if for a patient the value of an attribute Temperature is missing, this patient is sick with flu, and all remaining patients sick with flu have values high or very-high for  Temperature when using the interpretation of the missing attribute value as the  attribute-concept value, we will replace the missing attribute value with  high and very-high.  Additionally, the characteristic relation, (see, e.g., Grzymala-Busse and Grzymala-Busse, 2007) enables to process data sets with all three kind of missing attribute values at the same time: lost, ""do not care"" conditions, and attribute-concept values.
","In attribute-concept values interpretation of a missing attribute value, the missing attribute value may be replaced by any value of the attribute domain restricted to the concept to which the object with a missing attribute value belongs (Grzymala-Busse and Grzymala-Busse, 2007). For example, if for a patient the value of an attribute Temperature is missing, this patient is sick with flu, and all remaining patients sick with flu have values high or very-high for  Temperature when using the interpretation of the missing attribute value as the  attribute-concept value, we will replace the missing attribute value with  high and very-high.","[' What may be replaced by a value of the attribute domain restricted to the concept to which the object with a missing attribute value belongs?', ' What is the name of the attribute missing for a patient?', ' If the value of an attribute Temperature is missing, the patient is sick with what?', ' What is used to interpret the missing attribute value as the attribute-concept value?', ' What will we replace the missing attribute value with?']","['the missing attribute value', 'Temperature', 'flu', 'high and very-high', 'high and very-high']"
2943,rough sets,Applications,"Rough set methods can be applied as a component of hybrid solutions in machine learning and data mining. They have been found to be particularly useful for rule induction and feature selection (semantics-preserving dimensionality reduction). Rough set-based data analysis methods have been successfully applied in bioinformatics, economics and finance, medicine, multimedia, web and text mining, signal and image processing, software engineering, robotics, and engineering (e.g. power systems and control engineering). Recently the three regions of rough sets are interpreted as regions of acceptance, rejection and deferment. This leads to three-way decision making approach with the model which can potentially lead to interesting future applications.
",Rough set methods can be applied as a component of hybrid solutions in machine learning and data mining. They have been found to be particularly useful for rule induction and feature selection (semantics-preserving dimensionality reduction).,"[' What can be applied as a component of hybrid solutions in machine learning and data mining?', ' Rough set methods have been found to be particularly useful for what?']","['Rough set methods', 'rule induction and feature selection']"
2944,rough sets,History,"The idea of rough set was proposed by Pawlak (1981) as a new mathematical tool to deal with vague concepts. Comer, Grzymala-Busse, Iwinski, Nieminen, Novotny, Pawlak, Obtulowicz, and Pomykala have studied algebraic properties of rough sets. Different algebraic semantics have been developed by P. Pagliani, I. Duntsch, M. K. Chakraborty, M. Banerjee and A. Mani; these have been extended to more generalized rough sets by D. Cattaneo and A. Mani, in particular. Rough sets can be used to represent ambiguity, vagueness and general uncertainty.
","The idea of rough set was proposed by Pawlak (1981) as a new mathematical tool to deal with vague concepts. Comer, Grzymala-Busse, Iwinski, Nieminen, Novotny, Pawlak, Obtulowicz, and Pomykala have studied algebraic properties of rough sets.","[' In what year was the idea of rough set proposed by Pawlak?', ' What is a new mathematical tool to deal with vague concepts?', ' Who has studied algebraic properties of rough sets?']","['1981', 'rough set', 'Comer, Grzymala-Busse, Iwinski, Nieminen, Novotny, Pawlak, Obtulowicz, and Pomykala']"
2945,rough sets,Extensions and generalizations,"Since the development of rough sets, extensions and generalizations have continued to evolve. Initial developments focused on the relationship - both similarities and difference - with fuzzy sets. While some literature contends these concepts are different, other literature considers that rough sets are a generalization of fuzzy sets - as represented through either fuzzy rough sets or rough fuzzy sets.  Pawlak (1995) considered that fuzzy and rough sets should be treated as being complementary to each other, addressing different aspects of uncertainty and vagueness.
","Since the development of rough sets, extensions and generalizations have continued to evolve. Initial developments focused on the relationship - both similarities and difference - with fuzzy sets.","[' Since the development of rough sets, extensions and generalizations have continued to evolve what?', ' Initial developments focused on the relationship - both similarities and difference - with fuzzy sets?']","['fuzzy sets', 'development of rough sets']"
2946,linked data,Summary,"In computing, linked data (often capitalized as Linked Data) is structured data which is interlinked with other data so it becomes more useful through semantic queries. It builds upon standard Web technologies such as HTTP, RDF and URIs, but rather than using them to serve web pages only for human readers, it extends them to share information in a way that can be read automatically by computers. Part of the vision of linked data is for the Internet to become a global database.","In computing, linked data (often capitalized as Linked Data) is structured data which is interlinked with other data so it becomes more useful through semantic queries. It builds upon standard Web technologies such as HTTP, RDF and URIs, but rather than using them to serve web pages only for human readers, it extends them to share information in a way that can be read automatically by computers.","[' What is linked data often capitalized as?', ' What is structured data interlinked with other data so it becomes more useful through?', ' What does it extend to share information in a way that can read by computers?']","['Linked Data', 'semantic queries', 'linked data']"
2947,linked data,Linked open data,"Linked open data are linked data that are open data. Tim Berners-Lee gives the clearest definition of linked open data in differentiation with linked data.
",Linked open data are linked data that are open data. Tim Berners-Lee gives the clearest definition of linked open data in differentiation with linked data.,"["" What is Tim Berners-Lee's definition of linked open data?""]",['clearest']
2948,formal concept analysis,Summary,"In information science, formal concept analysis (FCA) is a principled way of deriving a concept hierarchy or formal ontology from a collection of objects and their properties. Each concept in the hierarchy represents the objects sharing some set of properties; and each sub-concept in the hierarchy represents a subset of the objects (as well as a superset of the properties) in the concepts above it. The term was introduced by Rudolf Wille in 1981, and builds on the mathematical theory of lattices and ordered sets that was developed by Garrett Birkhoff and others in the 1930s.
","In information science, formal concept analysis (FCA) is a principled way of deriving a concept hierarchy or formal ontology from a collection of objects and their properties. Each concept in the hierarchy represents the objects sharing some set of properties; and each sub-concept in the hierarchy represents a subset of the objects (as well as a superset of the properties) in the concepts above it.","[' What is FCA?', ' What is the principled way of deriving a concept hierarchy or formal ontology from a collection of objects and their properties?', ' Which sub-concept represents a subset of the objects in the concepts above it?']","['formal concept analysis', 'formal concept analysis', 'each sub-concept in the hierarchy']"
2949,formal concept analysis,Overview and history,"The original motivation of formal concept analysis was the search for real-world meaning of mathematical order theory. One such possibility of very general nature is that data tables can be transformed into algebraic structures called complete lattices, and that these can be utilized for data visualization and interpretation. A data table that represents a heterogeneous relation between objects and attributes, tabulating pairs of the form ""object g has attribute m"", is considered as a basic data type. It is referred to as a formal context. In this theory, a formal concept is defined to be a pair (A, B), where A is a set of objects (called the extent) and B is a set of attributes (the intent) such that
","The original motivation of formal concept analysis was the search for real-world meaning of mathematical order theory. One such possibility of very general nature is that data tables can be transformed into algebraic structures called complete lattices, and that these can be utilized for data visualization and interpretation.","[' What was the original motivation of formal concept analysis?', ' What can be transformed into algebraic structures called complete lattices?']","['search for real-world meaning of mathematical order theory', 'data tables']"
2950,formal concept analysis,Overview and history,"The formal concepts of any formal context can—as explained below—be ordered in a hierarchy called more formally the context's ""concept lattice."" The concept lattice can be graphically visualized as a ""line diagram"", which then may be helpful for understanding the data. Often however these lattices get too large for visualization. Then the mathematical theory of formal concept analysis may be helpful, e.g., for decomposing the lattice into smaller pieces without information loss, or for embedding it into another structure which is easier to interpret.
","The formal concepts of any formal context can—as explained below—be ordered in a hierarchy called more formally the context's ""concept lattice."" The concept lattice can be graphically visualized as a ""line diagram"", which then may be helpful for understanding the data.","[' What is the hierarchy in which the formal concepts of any formal context can be ordered?', ' What can be graphically visualized as a ""line diagram""?']","['concept lattice', 'The concept lattice']"
2951,formal concept analysis,Overview and history,"The theory in its present form goes back to the early 1980s and a research group led by Rudolf Wille, Bernhard Ganter and Peter Burmeister at the Technische Universität Darmstadt. Its basic mathematical definitions, however, were already introduced in the 1930s by Garrett Birkhoff as part of general lattice theory. Other previous approaches to the same idea arose from various French research groups, but the Darmstadt group normalised the field and systematically worked out both its mathematical theory and its philosophical foundations. The latter refer in particular to Charles S. Peirce, but also to the Port-Royal Logic.
","The theory in its present form goes back to the early 1980s and a research group led by Rudolf Wille, Bernhard Ganter and Peter Burmeister at the Technische Universität Darmstadt. Its basic mathematical definitions, however, were already introduced in the 1930s by Garrett Birkhoff as part of general lattice theory.","[' Who led the research group at the Technische Universität Darmstadt?', ' Who introduced the basic mathematical definitions of lattice theory in the 1930s?']","['Rudolf Wille, Bernhard Ganter and Peter Burmeister', 'Garrett Birkhoff']"
2952,formal concept analysis,Motivation and philosophical background,"This aim traces back to the educationalist Hartmut von Hentig, who in 1972 pleaded for restructuring sciences in view of better teaching and in order to make sciences mutually available and more generally (i.e. also without specialized knowledge) critiqueable. Hence, by its origins formal concept analysis aims at interdisciplinarity and democratic control of research.","This aim traces back to the educationalist Hartmut von Hentig, who in 1972 pleaded for restructuring sciences in view of better teaching and in order to make sciences mutually available and more generally (i.e. also without specialized knowledge) critiqueable.","[' Who was the educationalist Hartmut von Hentig?', ' In 1972, what did Hartmut Von Hengtig plead for?', ' What was the goal of restructuring sciences?']","['pleaded for restructuring sciences in view of better teaching', 'restructuring sciences', 'better teaching']"
2953,formal concept analysis,Motivation and philosophical background,"It corrects the starting point of lattice theory during the development of formal logic in the 19th century. Then—and later in model theory—a concept as unary predicate had been reduced to its extent. Now again, the philosophy of concepts should become less abstract by considering the intent. Hence, formal concept analysis is oriented towards the categories extension and intension of linguistics and classical conceptual logic.",It corrects the starting point of lattice theory during the development of formal logic in the 19th century. Then—and later in model theory—a concept as unary predicate had been reduced to its extent.,"[' In what century did formal logic begin to develop?', ' In model theory, a concept as unary predicate was reduced to what extent?']","['19th', 'its']"
2954,formal concept analysis,Motivation and philosophical background,"Formal concept analysis aims at the clarity of concepts according to Charles S. Peirce's pragmatic maxim by unfolding observable, elementary properties of the subsumed objects. In his late philosophy, Peirce assumed that logical thinking aims at perceiving reality, by the triade concept, judgement and conclusion. Mathematics is an abstraction of logic, develops patterns of possible realities and therefore may support rational communication. On this background, Wille defines:
","Formal concept analysis aims at the clarity of concepts according to Charles S. Peirce's pragmatic maxim by unfolding observable, elementary properties of the subsumed objects. In his late philosophy, Peirce assumed that logical thinking aims at perceiving reality, by the triade concept, judgement and conclusion.","["" What aims at the clarity of concepts according to Charles S. Peirce's pragmatic maxim?"", ' What does formal concept analysis aim at by unfolding observable, elementary properties of the subsumed objects?']","['Formal concept analysis', 'the clarity of concepts']"
2955,formal concept analysis,Example,"The data in the example is taken from a semantic field study, where different kinds of bodies of water were systematically categorized by their attributes. For the purpose here it has been simplified.
","The data in the example is taken from a semantic field study, where different kinds of bodies of water were systematically categorized by their attributes. For the purpose here it has been simplified.","[' What type of study was used to categorize different types of bodies of water?', ' What has been simplified for the purpose of this example?']","['semantic field study', 'The data in the example is taken from a semantic field study, where different kinds of bodies of water']"
2956,formal concept analysis,Example,"The data table represents a formal context, the line diagram next to it shows its concept lattice. Formal definitions follow below.
","The data table represents a formal context, the line diagram next to it shows its concept lattice. Formal definitions follow below.","[' What represents a formal context?', ' What shows its concept lattice?']","['The data table', 'the line diagram']"
2957,formal concept analysis,Example,"The above line diagram consists of circles, connecting line segments, and labels. Circles represent formal concepts. The lines allow to read off the subconcept-superconcept hierarchy. Each object and attribute name is used as a label exactly once in the diagram, with objects below and attributes above concept circles. This is done in a way that an attribute can be reached from an object via an ascending path if and only if the object has the attribute.
","The above line diagram consists of circles, connecting line segments, and labels. Circles represent formal concepts.","[' What represent formal concepts?', ' Circles represent what?']","['Circles', 'formal concepts']"
2958,formal concept analysis,Example,"In the diagram shown, e.g. the object reservoir has the attributes stagnant and constant, but not the attributes temporary, running, natural, maritime. Accordingly, puddle has exactly the characteristics temporary, stagnant and natural.
","In the diagram shown, e.g. the object reservoir has the attributes stagnant and constant, but not the attributes temporary, running, natural, maritime.","[' What attributes does the object reservoir have?', ' What attributes are not present in the diagram?']","['stagnant and constant', 'temporary, running, natural, maritime']"
2959,formal concept analysis,Example,"The original formal context can be reconstructed from the labelled diagram, as well as the formal concepts. The extent of a concept consists of those objects from which an ascending path leads to the circle representing the concept. The intent consists of those attributes to which there is an ascending path from that concept circle (in the diagram). In this diagram the concept immediately to the left of the label reservoir has the intent stagnant and natural and the extent puddle, maar, lake, pond, tarn, pool, lagoon, and sea.
","The original formal context can be reconstructed from the labelled diagram, as well as the formal concepts. The extent of a concept consists of those objects from which an ascending path leads to the circle representing the concept.","[' The original formal context can be reconstructed from what?', ' The extent of a concept consists of objects from which an ascending path leads to the circle representing the concept?']","['the labelled diagram', 'formal context']"
2960,formal concept analysis,Formal contexts and concepts,"A formal context is a triple K = (G, M, I), where G is a set of objects, M is a set of attributes, and I ⊆ G × M is a binary relation called incidence that expresses which objects have which attributes. For subsets A ⊆ G of objects and subsets B ⊆ M of attributes, one defines two derivation operators as follows:
","A formal context is a triple K = (G, M, I), where G is a set of objects, M is a set of attributes, and I ⊆ G × M is a binary relation called incidence that expresses which objects have which attributes. For subsets A ⊆ G of objects and subsets B ⊆ M of attributes, one defines two derivation operators as follows:","[' What is a formal context a triple K = (G, M, I)?', ' What is the binary relation that expresses which objects have which attributes?', ' How many derivation operators does one define for subsets A <unk> G of objects?']","['G is a set of objects', 'incidence', 'two']"
2961,formal concept analysis,Formal contexts and concepts,"The derivation operators define a Galois connection between sets of objects and of attributes. This is why in French a concept lattice is sometimes called a treillis de Galois (Galois lattice).
",The derivation operators define a Galois connection between sets of objects and of attributes. This is why in French a concept lattice is sometimes called a treillis de Galois (Galois lattice).,"[' What defines a Galois connection between sets of objects and of attributes?', ' What is a concept lattice sometimes called in French?']","['derivation operators', 'treillis de Galois']"
2962,formal concept analysis,Formal contexts and concepts,"For computing purposes, a formal context may be naturally represented as a (0,1)-matrix K in which the rows correspond to the objects, the columns correspond to the attributes, and each entry ki,j equals to 1 if ""object i has attribute j."" In this matrix representation, each formal concept corresponds to a maximal submatrix (not necessarily contiguous) all of whose elements equal 1. It is however misleading to consider a formal context as boolean, because the negated incidence (""object g does not have attribute m"") is not concept forming in the same way as defined above. For this reason, the values 1 and 0 or TRUE and FALSE are usually avoided when representing formal contexts, and a symbol like × is used to express incidence.
","For computing purposes, a formal context may be naturally represented as a (0,1)-matrix K in which the rows correspond to the objects, the columns correspond to the attributes, and each entry ki,j equals to 1 if ""object i has attribute j."" In this matrix representation, each formal concept corresponds to a maximal submatrix (not necessarily contiguous) all of whose elements equal 1.","[' What does each entry ki,j equal if ""object i has attribute j""?', ' For computing purposes, a formal context may be represented as what?', ' What does each formal concept correspond to in a matrix representation?', ' How many elements of the matrix representation equal 1?']","['1', 'a (0,1)-matrix K', 'a maximal submatrix', 'all']"
2963,formal concept analysis,Concept lattice of a formal context,"The concepts (Ai, Bi) of a context K can be (partially) ordered by the inclusion of extents, or, equivalently, by the dual inclusion of intents. An order ≤ on the concepts is defined as follows: for any two concepts (A1, B1) and (A2, B2) of K, we say that (A1, B1) ≤ (A2, B2) precisely when A1 ⊆ A2. Equivalently, (A1, B1) ≤ (A2, B2) whenever B1 ⊇ B2.
","The concepts (Ai, Bi) of a context K can be (partially) ordered by the inclusion of extents, or, equivalently, by the dual inclusion of intents. An order ≤ on the concepts is defined as follows: for any two concepts (A1, B1) and (A2, B2) of K, we say that (A1, B1) ≤ (A2, B2) precisely when A1 ⊆ A2.","[' How can the concepts (Ai, Bi) of a context K be ordered?', ' What can be ordered by the inclusion of extents or, equivalently, by the dual inclusion of intents?', ' An order <unk> on the concepts is defined as follows: for any two concepts of K, what do we say?', ' B1) and B2) of K, we say that (A1, B1) <unk> (A2, B2) precisely when A1 <unk> A2.']","['by the inclusion of extents, or, equivalently, by the dual inclusion of intents', 'The concepts (Ai, Bi) of a context K', '(A1, B1) ≤ (A2, B2)', '≤']"
2964,formal concept analysis,Concept lattice of a formal context,"In this order, every set of formal concepts has a greatest common subconcept, or meet. Its extent consists of those objects that are common to all extents of the set. Dually, every set of formal concepts has a least common superconcept, the intent of which comprises all attributes which all objects of that set of concepts have.
","In this order, every set of formal concepts has a greatest common subconcept, or meet. Its extent consists of those objects that are common to all extents of the set.",[' What is the most common subconcept in a set of formal concepts?'],['meet']
2965,formal concept analysis,Concept lattice of a formal context,"These meet and join operations satisfy the axioms defining a lattice, in fact a complete lattice. Conversely, it can be shown that every complete lattice is the concept lattice of some formal context (up to isomorphism).
","These meet and join operations satisfy the axioms defining a lattice, in fact a complete lattice. Conversely, it can be shown that every complete lattice is the concept lattice of some formal context (up to isomorphism).","[' Meet and join operations satisfy what axioms defining a lattice?', ' What is a complete lattce?', ' How can it be shown that every complete ltce is the concept latce of some formal context?']","['axioms defining a lattice, in fact a complete lattice', 'the concept lattice of some formal context', 'up to isomorphism']"
2966,formal concept analysis,Attribute values and negation,"Real-world data is often given in the form of an object-attribute table, where the attributes have ""values"". Formal concept analysis handles such data by transforming them into the basic type of a (""one-valued"") formal context. The method is called conceptual scaling.
","Real-world data is often given in the form of an object-attribute table, where the attributes have ""values"". Formal concept analysis handles such data by transforming them into the basic type of a (""one-valued"") formal context.","[' What is often given in the form of an object-attribute table?', ' What does formal concept analysis handle data in?']","['Real-world data', 'formal context']"
2967,formal concept analysis,Attribute values and negation,"The negation of an attribute m is an attribute ¬m, the extent of which is just the complement of the extent of m, i.e., with (¬m)′ = G \ m′. It is in general not assumed that negated attributes are available for concept formation. But pairs of attributes which are negations of each other often naturally occur, for example in contexts derived from conceptual scaling.
","The negation of an attribute m is an attribute ¬m, the extent of which is just the complement of the extent of m, i.e., with (¬m)′ = G \ m′. It is in general not assumed that negated attributes are available for concept formation.","[' The negation of an attribute m is an attribute what?', ' The extent of which is just the complement of the extent of m?', ' Negated attributes are not available for what purpose?']","['¬m', 'The negation of an attribute m', 'concept formation']"
2968,formal concept analysis,Implications,"An implication A → B relates two sets A and B of attributes and expresses that every object possessing each attribute from A also has each attribute from B. When (G,M,I) is a formal context and A, B are subsets of the set M of attributes (i.e., A,B ⊆ M), then the implication A → B is valid if A′ ⊆ B′. For each finite formal context, the set of all valid implications has a canonical basis, an irredundant set of implications from which all valid implications can be derived by the natural inference (Armstrong rules). This is used in attribute exploration, a knowledge acquisition method based on implications.","An implication A → B relates two sets A and B of attributes and expresses that every object possessing each attribute from A also has each attribute from B. When (G,M,I) is a formal context and A, B are subsets of the set M of attributes (i.e., A,B ⊆ M), then the implication A → B is valid if A′ ⊆ B′.","[' What relates two sets A and B of attributes?', ' What expresses that every object possessing each attribute from A also has every attribute from B?', ' When A, B are subsets of the set M of attributes, what are they?', ' What is the implication A <unk> B valid for?', ' What is a set of attributes?']","['An implication A → B', 'An implication A → B', 'A,B ⊆ M', 'if A′ ⊆ B′', 'M']"
2969,formal concept analysis,Arrow relations,"Formal concept analysis has elaborate mathematical foundations, making the field versatile. As a basic example we mention the arrow relations, which are simple and easy to compute, but very useful. They are defined as follows: For g ∈ G and m ∈ M let
","Formal concept analysis has elaborate mathematical foundations, making the field versatile. As a basic example we mention the arrow relations, which are simple and easy to compute, but very useful.","[' Formal concept analysis has elaborate mathematical foundations, making it what?', ' What are simple and easy to compute, but very useful?']","['the field versatile', 'arrow relations']"
2970,formal concept analysis,Arrow relations,"Since only non-incident object-attribute pairs can be related, these relations can conveniently be recorded in the table representing a formal context. Many lattice properties can be read off from the arrow relations, including distributivity and several of its generalizations. They also reveal structural information and can be used for determining, e.g., the congruence relations of the lattice.
","Since only non-incident object-attribute pairs can be related, these relations can conveniently be recorded in the table representing a formal context. Many lattice properties can be read off from the arrow relations, including distributivity and several of its generalizations.","[' What can be related to non-incident object-attribute pairs?', ' How can lattice properties be read off from the arrow relations?']","['these relations can conveniently be recorded in the table representing a formal context', 'distributivity and several of its generalizations']"
2971,formal concept analysis,Algorithms and tools,"There are a number of simple and fast algorithms for generating formal concepts and for constructing and navigating concept lattices. For a survey, see Kuznetsov and Obiedkov or the book by Ganter and Obiedkov, where also some pseudo-code can be found. Since the number of formal concepts may be exponential in the size of the formal context, the complexity of the algorithms usually is given with respect to the output size. Concept lattices with a few million elements can be handled without problems.
","There are a number of simple and fast algorithms for generating formal concepts and for constructing and navigating concept lattices. For a survey, see Kuznetsov and Obiedkov or the book by Ganter and Obiedkov, where also some pseudo-code can be found.","[' What are some of the algorithms used for generating formal concepts?', ' What can be found in the book by Ganter and Obiedkov?']","['simple and fast', 'pseudo-code']"
2972,formal concept analysis,Algorithms and tools,"Many FCA software applications are available today. The main purpose of these tools varies from formal context creation to formal concept mining and generating the concepts lattice of a given formal context and the corresponding implications and association rules. Most of these tools are academic open-source applications, such as:
",Many FCA software applications are available today. The main purpose of these tools varies from formal context creation to formal concept mining and generating the concepts lattice of a given formal context and the corresponding implications and association rules.,"[' Many FCA software applications are available today?', ' What is the main purpose of these tools?']","['formal context creation', 'formal context creation to formal concept mining and generating the concepts lattice of a given formal context and the corresponding implications and association rules']"
2973,formal concept analysis,Hands-on experience with formal concept analysis,"The formal concept analysis can be used as a qualitative method for data analysis. Since the early beginnings of FBA in the early 1980s, the FBA research group at TU Darmstadt has gained experience from more than 200 projects using the FBA (as of 2005). Including the fields of: medicine and cell biology, genetics, ecology, software engineering, ontology, information and library sciences, office administration, law, linguistics, political science.","The formal concept analysis can be used as a qualitative method for data analysis. Since the early beginnings of FBA in the early 1980s, the FBA research group at TU Darmstadt has gained experience from more than 200 projects using the FBA (as of 2005).","[' What can be used as a qualitative method for data analysis?', ' How many projects has the FBA research group at TU Darmstadt used?']","['formal concept analysis', 'more than 200']"
2974,formal concept analysis,Hands-on experience with formal concept analysis,"Many more examples are e.g. described in: Formal Concept Analysis. Foundations and Applications, conference papers at regular conferences such as: International Conference on Formal Concept Analysis (ICFCA), Concept Lattices and their Applications (CLA), or International Conference on Conceptual Structures (ICCS).",Many more examples are e.g. described in: Formal Concept Analysis.,[' How many examples are described in Formal Concept Analysis?'],['Many more examples']
2975,evolutionary computation,Summary,"
In computer science, evolutionary computation is a family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and soft computing studying these algorithms. In technical terms, they are a family of population-based trial and error problem solvers with a metaheuristic or stochastic optimization character.
","
In computer science, evolutionary computation is a family of algorithms for global optimization inspired by biological evolution, and the subfield of artificial intelligence and soft computing studying these algorithms. In technical terms, they are a family of population-based trial and error problem solvers with a metaheuristic or stochastic optimization character.","[' In computer science, evolutionary computation is inspired by what?', ' The subfield of artificial intelligence and soft computing study what algorithms?', ' Evolutionary computation is a family of what kind of problem solvers with a metaheuristic or stochastic optimization character?']","['biological evolution', 'evolutionary computation', 'population-based trial and error']"
2976,evolutionary computation,Summary,"In evolutionary computation, an initial set of candidate solutions is generated and iteratively updated. Each new generation is produced by stochastically removing less desired solutions, and introducing small random changes. In biological terminology, a population of solutions is subjected to natural selection (or artificial selection) and mutation. As a result, the population will gradually evolve to increase in fitness, in this case the chosen fitness function of the algorithm.
","In evolutionary computation, an initial set of candidate solutions is generated and iteratively updated. Each new generation is produced by stochastically removing less desired solutions, and introducing small random changes.","[' How is an initial set of candidate solutions generated and iteratively updated?', ' How is each new generation produced?', ' What is stochastically removing less desired solutions?']","['stochastically removing less desired solutions, and introducing small random changes', 'by stochastically removing less desired solutions, and introducing small random changes', 'introducing small random changes']"
2977,evolutionary computation,Summary,"Evolutionary computation techniques can produce highly optimized solutions in a wide range of problem settings, making them popular in computer science. Many variants and extensions exist, suited to more specific families of problems and data structures. Evolutionary computation is also sometimes used in evolutionary biology as an in silico experimental procedure to study common aspects of general evolutionary processes.
","Evolutionary computation techniques can produce highly optimized solutions in a wide range of problem settings, making them popular in computer science. Many variants and extensions exist, suited to more specific families of problems and data structures.","[' Evolutionary computation techniques can produce highly optimized solutions in a wide range of settings, making them popular in what field?', ' What are many variants and extensions suited to?']","['computer science', 'more specific families of problems and data structures']"
2978,evolutionary computation,History,"The use of evolutionary principles for automated problem solving originated in the 1950s. It was not until the 1960s that three distinct interpretations of this idea started to be developed in three different places.
",The use of evolutionary principles for automated problem solving originated in the 1950s. It was not until the 1960s that three distinct interpretations of this idea started to be developed in three different places.,"[' When did the use of evolutionary principles for automated problem solving originate?', ' How many different interpretations of the idea started to be developed in the 1960s?']","['1950s', 'three']"
2979,evolutionary computation,History,"Evolutionary programming was introduced by Lawrence J. Fogel in the US, while John Henry Holland called his method a genetic algorithm. In Germany Ingo Rechenberg and Hans-Paul Schwefel introduced evolution strategies. These areas developed separately for about 15 years. From the early nineties on they are unified as different representatives (""dialects"") of one technology, called evolutionary computing. Also in the early nineties, a fourth stream following the general ideas had emerged – genetic programming. Since the 1990s, nature-inspired algorithms are becoming an increasingly significant part of the evolutionary computation.
","Evolutionary programming was introduced by Lawrence J. Fogel in the US, while John Henry Holland called his method a genetic algorithm. In Germany Ingo Rechenberg and Hans-Paul Schwefel introduced evolution strategies.","[' Who introduced evolution programming in the US?', ' What did John Henry Holland call his method a genetic algorithm?', ' In what country did Ingo Rechenberg and Hans-Paul Schwefel introduce evolution strategies?']","['Lawrence J. Fogel', 'Evolutionary programming', 'Germany']"
2980,evolutionary computation,History,"The earliest computational simulations of evolution using evolutionary algorithms and artificial life techniques were performed by Nils Aall Barricelli in 1953, with first results published in 1954. Another pioneer in the 1950s was Alex Fraser, who published a series of papers on simulation of artificial selection. Artificial evolution became a widely recognised optimisation method as a result of the work of Ingo Rechenberg in the 1960s and early 1970s, who used evolution strategies to solve complex engineering problems. Genetic algorithms in particular became popular through the writing of John Holland. As academic interest grew, dramatic increases in the power of computers allowed practical applications, including the automatic evolution of computer programs. Evolutionary algorithms are now used to solve multi-dimensional problems more efficiently than software produced by human designers, and also to optimise the design of systems.","The earliest computational simulations of evolution using evolutionary algorithms and artificial life techniques were performed by Nils Aall Barricelli in 1953, with first results published in 1954. Another pioneer in the 1950s was Alex Fraser, who published a series of papers on simulation of artificial selection.","[' Who performed the earliest computational simulations of evolution using evolutionary algorithms and artificial life techniques?', ' When were the first results published?', ' Who published a series of papers on simulation of artificial selection?']","['Nils Aall Barricelli', '1954', 'Alex Fraser']"
2981,evolutionary computation,Techniques,"Evolutionary computing techniques mostly involve metaheuristic optimization algorithms. Broadly speaking, the field includes:
","Evolutionary computing techniques mostly involve metaheuristic optimization algorithms. Broadly speaking, the field includes:","[' What do evolutionary computing techniques mostly involve?', ' What are metaheuristic optimization algorithms?']","['metaheuristic optimization algorithms', 'Evolutionary computing techniques']"
2982,evolutionary computation,Evolutionary algorithms,"Evolutionary algorithms form a subset of evolutionary computation in that they generally only involve techniques implementing mechanisms inspired by biological evolution such as reproduction, mutation, recombination, natural selection and survival of the fittest. Candidate solutions to the optimization problem play the role of individuals in a population, and the cost function determines the environment within which the solutions ""live"" (see also fitness function). Evolution of the population then takes place after the repeated application of the above operators.
","Evolutionary algorithms form a subset of evolutionary computation in that they generally only involve techniques implementing mechanisms inspired by biological evolution such as reproduction, mutation, recombination, natural selection and survival of the fittest. Candidate solutions to the optimization problem play the role of individuals in a population, and the cost function determines the environment within which the solutions ""live"" (see also fitness function).","[' What is a subset of evolutionary computation?', ' Evolutionary algorithms only involve techniques implementing mechanisms inspired by what?', ' Candidate solutions to what problem play the role of individuals in a population?', ' What determines the environment within which solutions ""live""?', ' The role of individuals in a population determines what?']","['Evolutionary algorithms', 'biological evolution', 'optimization', 'the cost function', 'Candidate solutions to the optimization problem']"
2983,evolutionary computation,Evolutionary algorithms,"Many aspects of such an evolutionary process are stochastic. Changed pieces of information due to recombination and mutation are randomly chosen. On the other hand, selection operators can be either deterministic, or stochastic. In the latter case, individuals with a higher fitness have a higher chance to be selected than individuals with a lower fitness, but typically even the weak individuals have a chance to become a parent or to survive.
",Many aspects of such an evolutionary process are stochastic. Changed pieces of information due to recombination and mutation are randomly chosen.,"[' What are many aspects of an evolutionary process stochastic?', ' Changed pieces of information due to recombination and mutation are randomly chosen?']","['Changed pieces of information due to recombination and mutation are randomly chosen', 'stochastic']"
2984,evolutionary computation,Evolutionary algorithms and biology,"Genetic algorithms deliver methods to model biological systems and systems biology that are linked to the theory of dynamical systems, since they are used to predict the future states of the system. This is just a vivid (but perhaps misleading) way of drawing attention to the orderly, well-controlled and highly structured character of development in biology.
","Genetic algorithms deliver methods to model biological systems and systems biology that are linked to the theory of dynamical systems, since they are used to predict the future states of the system. This is just a vivid (but perhaps misleading) way of drawing attention to the orderly, well-controlled and highly structured character of development in biology.","[' Genetic algorithms deliver methods to model biological systems and systems biology that are linked to what theory?', ' Genetic algorithms are used to predict the future states of what system?', ' What is a way of drawing attention to the order, well-controlled and highly structured character of development in biology?']","['dynamical systems', 'dynamical systems', 'Genetic algorithms']"
2985,evolutionary computation,Evolutionary algorithms and biology,"This view has the merit of recognizing that there is no central control of development; organisms develop as a result of local interactions within and between cells. The most promising ideas about program-development parallels seem to us to be ones that point to an apparently close analogy between processes within cells, and the low-level operation of modern computers. Thus, biological systems are like computational machines that process input information to compute next states, such that biological systems are closer to a computation than classical dynamical system.","This view has the merit of recognizing that there is no central control of development; organisms develop as a result of local interactions within and between cells. The most promising ideas about program-development parallels seem to us to be ones that point to an apparently close analogy between processes within cells, and the low-level operation of modern computers.","[' What do organisms develop as a result of local interactions within and between cells?', ' What are the most promising ideas about program-development parallels?', ' What point to an apparent close analogy between processes within cells and the low-level operation of modern computers?']","['development', 'ones that point to an apparently close analogy between processes within cells, and the low-level operation of modern computers', 'The most promising ideas about program-development parallels']"
2986,evolutionary computation,Evolutionary algorithms and biology,"Evolutionary automata, a generalization of Evolutionary Turing machines, have been introduced in order to investigate more precisely properties of biological and evolutionary computation. In particular, they allow to obtain new results on expressiveness of evolutionary computation. This confirms the initial result about undecidability of natural evolution and evolutionary algorithms and processes. Evolutionary finite automata, the simplest subclass of Evolutionary automata working in terminal mode can accept arbitrary languages over a given alphabet, including non-recursively enumerable (e.g., diagonalization language) and recursively enumerable but not recursive languages (e.g., language of the universal Turing machine). 
","Evolutionary automata, a generalization of Evolutionary Turing machines, have been introduced in order to investigate more precisely properties of biological and evolutionary computation. In particular, they allow to obtain new results on expressiveness of evolutionary computation.","[' What is a generalization of Evolutionary Turing machines?', ' Evolutionary automata allow to obtain new results on what?']","['Evolutionary automata', 'expressiveness of evolutionary computation']"
2987,evolutionary computation,Notable practitioners,The list of active researchers is naturally dynamic and non-exhaustive. A network analysis of the community was published in 2007.,The list of active researchers is naturally dynamic and non-exhaustive. A network analysis of the community was published in 2007.,"[' What is the list of active researchers?', ' When was a network analysis published?']","['naturally dynamic and non-exhaustive', '2007']"
2988,greedy algorithm,Summary,"A greedy algorithm is any algorithm that follows the problem-solving heuristic of making the locally optimal choice at each stage. In many problems, a greedy strategy does not produce an optimal solution, but a greedy heuristic can yield locally optimal solutions that approximate a globally optimal solution in a reasonable amount of time.
","A greedy algorithm is any algorithm that follows the problem-solving heuristic of making the locally optimal choice at each stage. In many problems, a greedy strategy does not produce an optimal solution, but a greedy heuristic can yield locally optimal solutions that approximate a globally optimal solution in a reasonable amount of time.","[' What is an algorithm that follows the problem-solving heuristic of making the locally optimal choice at each stage?', ' In many problems, a greedy strategy does not produce what?', ' What can yield locally optimal solutions that approximate a globally optimal solution?', ' What are solutions that approximate a globally optimal solution in a reasonable amount of time?']","['A greedy algorithm', 'an optimal solution', 'a greedy heuristic', 'locally optimal solutions']"
2989,greedy algorithm,Summary,"For example, a greedy strategy for the travelling salesman problem (which is of high computational complexity) is the following heuristic: ""At each step of the journey, visit the nearest unvisited city."" This heuristic does not intend to find the best solution, but it terminates in a reasonable number of steps; finding an optimal solution to such a complex problem typically requires unreasonably many steps. In mathematical optimization, greedy algorithms optimally solve combinatorial problems having the properties of matroids and give constant-factor approximations to optimization problems with the submodular structure.
","For example, a greedy strategy for the travelling salesman problem (which is of high computational complexity) is the following heuristic: ""At each step of the journey, visit the nearest unvisited city."" This heuristic does not intend to find the best solution, but it terminates in a reasonable number of steps; finding an optimal solution to such a complex problem typically requires unreasonably many steps.","[' What is the greedy strategy for the travelling salesman problem of high computational complexity?', ' What does this heuristic not intend to find?', ' How many steps does finding an optimal solution to a complex problem typically require?']","['the following heuristic', 'the best solution', 'unreasonably']"
2990,greedy algorithm,Specifics,"Greedy algorithms produce good solutions on some mathematical problems, but not on others.  Most problems for which they work will  have two properties:
","Greedy algorithms produce good solutions on some mathematical problems, but not on others. Most problems for which they work will  have two properties:","[' Greedy algorithms produce good solutions on some mathematical problems, but not on others, what?', ' Most problems for which they work will have how many properties?']","['others', 'two']"
2991,greedy algorithm,Types,"Greedy algorithms can be characterized as being 'short sighted', and also as 'non-recoverable'. They are ideal only for problems that have an 'optimal substructure'. Despite this, for many simple problems, the best-suited algorithms are greedy. It is important, however, to note that the greedy algorithm can be used as a selection algorithm to prioritize options within a search, or branch-and-bound algorithm. There are a few variations to the greedy algorithm:
","Greedy algorithms can be characterized as being 'short sighted', and also as 'non-recoverable'. They are ideal only for problems that have an 'optimal substructure'.","[' Greedy algorithms can be characterized as being what?', ' What can be described as being short sighted and non-recoverable?']","['short sighted', 'Greedy algorithms']"
2992,greedy algorithm,Theory,"Greedy algorithms have a long history of study in combinatorial optimization and theoretical computer science. Greedy heuristics are known to produce suboptimal results on many problems, and so natural questions are:
","Greedy algorithms have a long history of study in combinatorial optimization and theoretical computer science. Greedy heuristics are known to produce suboptimal results on many problems, and so natural questions are:","[' Greedy algorithms have a long history of study in what two areas?', ' What are known to produce suboptimal results on many problems?']","['combinatorial optimization and theoretical computer science', 'Greedy heuristics']"
2993,greedy algorithm,Applications,"Greedy algorithms typically (but not always) fail to find the globally optimal solution because they usually do not operate exhaustively on all the data. They can make commitments to certain choices too early, preventing them from finding the best overall solution later. For example, all known greedy coloring algorithms for the graph coloring problem and all other NP-complete problems do not consistently find optimum solutions. Nevertheless, they are useful because they are quick to think up and often give good approximations to the optimum.
","Greedy algorithms typically (but not always) fail to find the globally optimal solution because they usually do not operate exhaustively on all the data. They can make commitments to certain choices too early, preventing them from finding the best overall solution later.","["" Greedy algorithms often fail to find the globally optimal solution because they don't operate exhaustively on what?"", ' Greey algorithms can make commitments to certain choices too early, preventing them from finding the best overall solution later?']","['all the data', 'Greedy']"
2994,greedy algorithm,Applications,"If a greedy algorithm can be proven to yield the global optimum for a given problem class, it typically becomes the method of choice because it is faster than other optimization methods like dynamic programming. Examples of such greedy algorithms are Kruskal's algorithm and Prim's algorithm for finding minimum spanning trees and the algorithm for finding optimum Huffman trees.
","If a greedy algorithm can be proven to yield the global optimum for a given problem class, it typically becomes the method of choice because it is faster than other optimization methods like dynamic programming. Examples of such greedy algorithms are Kruskal's algorithm and Prim's algorithm for finding minimum spanning trees and the algorithm for finding optimum Huffman trees.","[' What is a greedy algorithm faster than?', "" Kruskal's algorithm is faster than what?"", "" What are Kruskal's and Prim's algorithms for finding?"", ' What is the algorithm for finding optimum Huffman trees?']","['dynamic programming', 'dynamic programming', 'minimum spanning trees', ""Prim's algorithm""]"
2995,greedy algorithm,Applications,"Greedy algorithms appear in the network routing as well.  Using greedy routing, a message is forwarded to the neighbouring node which is ""closest"" to the destination. The notion of a node's location (and hence ""closeness"") may be determined by its physical location, as in geographic routing used by ad hoc networks.  Location may also be an entirely artificial construct as in small world routing and distributed hash table.
","Greedy algorithms appear in the network routing as well. Using greedy routing, a message is forwarded to the neighbouring node which is ""closest"" to the destination.","[' Greedy algorithms appear in the network routing as well as what?', ' What is used to forwarded a message to the neighboring node?']","['greedy routing, a message is forwarded to the neighbouring node which is ""closest"" to the destination', 'greedy routing']"
2996,stream cipher,Summary,"A stream cipher is a symmetric key cipher where plaintext digits are combined with a pseudorandom cipher digit stream (keystream). In a stream cipher, each plaintext digit is encrypted one at a time with the corresponding digit of the keystream, to give a digit of the ciphertext stream. Since encryption of each digit is dependent on the current state of the cipher, it is also known as state cipher. In practice, a digit is typically a bit and the combining operation is an exclusive-or (XOR).
","A stream cipher is a symmetric key cipher where plaintext digits are combined with a pseudorandom cipher digit stream (keystream). In a stream cipher, each plaintext digit is encrypted one at a time with the corresponding digit of the keystream, to give a digit of the ciphertext stream.","[' What is a stream cipher?', ' How are plaintext digits combined with a keystream?', ' What is the name of a pseudorandom digit stream?']","['a symmetric key cipher', 'A stream cipher', 'keystream']"
2997,stream cipher,Summary,"The pseudorandom keystream is typically generated serially from a random seed value using digital shift registers. The seed value serves as the cryptographic key for decrypting the ciphertext stream. Stream ciphers represent a different approach to symmetric encryption from block ciphers. Block ciphers operate on large blocks of digits with a fixed, unvarying transformation. This distinction is not always clear-cut: in some modes of operation, a block cipher primitive is used in such a way that it acts effectively as a stream cipher. Stream ciphers typically execute at a higher speed than block ciphers and have lower hardware complexity. However, stream ciphers can be susceptible to security breaches (see stream cipher attacks); for example, when the same starting state (seed) is used twice.
",The pseudorandom keystream is typically generated serially from a random seed value using digital shift registers. The seed value serves as the cryptographic key for decrypting the ciphertext stream.,"[' What is typically generated serially from a random seed value?', ' What serves as the cryptographic key for decrypting the ciphertext stream?']","['The pseudorandom keystream', 'The seed value']"
2998,stream cipher,Loose inspiration from the one-time pad,"Stream ciphers can be viewed as approximating the action of a proven unbreakable cipher, the one-time pad (OTP). A one-time pad uses a keystream of completely random digits. The keystream is combined with the plaintext digits one at a time to form the ciphertext. This system was proved to be secure by Claude E. Shannon in 1949. However, the keystream must be generated completely at random with at least the same length as the plaintext and cannot be used more than once. This makes the system cumbersome to implement in many practical applications, and as a result the one-time pad has not been widely used, except for the most critical applications. Key generation, distribution and management are critical for those applications.
","Stream ciphers can be viewed as approximating the action of a proven unbreakable cipher, the one-time pad (OTP). A one-time pad uses a keystream of completely random digits.","[' Stream ciphers can be viewed as approximating the action of what?', ' What is OTP?', ' A one-time pad uses a keystream of completely random digits?']","['a proven unbreakable cipher, the one-time pad (OTP).', 'one-time pad', 'one-time pad']"
2999,stream cipher,Loose inspiration from the one-time pad,"A stream cipher makes use of a much smaller and more convenient key such as 128 bits. Based on this key, it generates a pseudorandom keystream which can be combined with the plaintext digits in a similar fashion to the one-time pad. However, this comes at a cost. The keystream is now pseudorandom and so is not truly random. The proof of security associated with the one-time pad no longer holds. It is quite possible for a stream cipher to be completely insecure.","A stream cipher makes use of a much smaller and more convenient key such as 128 bits. Based on this key, it generates a pseudorandom keystream which can be combined with the plaintext digits in a similar fashion to the one-time pad.","[' What type of key does a stream cipher use?', ' What is the size of a key used in a crypther?', ' How many bits is a standard key?', ' In what way can a one-time pad be used?']","['128 bits', '128 bits', '128', 'similar fashion']"
3000,stream cipher,Types,"A stream cipher generates successive elements of the keystream based on an internal state. This state is updated in essentially two ways: if the state changes independently of the plaintext or ciphertext messages, the cipher is classified as a synchronous stream cipher. By contrast, self-synchronising stream ciphers update their state based on previous ciphertext digits.
","A stream cipher generates successive elements of the keystream based on an internal state. This state is updated in essentially two ways: if the state changes independently of the plaintext or ciphertext messages, the cipher is classified as a synchronous stream cipher.","[' A stream cipher generates successive elements of the keystream based on what?', ' What is updated in essentially two ways?', ' If the state changes independently of plaintext, what is it classified as?']","['an internal state', 'A stream cipher', 'synchronous stream cipher']"
3001,stream cipher,Based on linear-feedback shift registers,"Binary stream ciphers are often constructed using linear-feedback shift registers (LFSRs) because they can be easily implemented in hardware and can be readily analysed mathematically. The use of LFSRs on their own, however, is insufficient to provide good security. Various schemes have been proposed to increase the security of LFSRs.
","Binary stream ciphers are often constructed using linear-feedback shift registers (LFSRs) because they can be easily implemented in hardware and can be readily analysed mathematically. The use of LFSRs on their own, however, is insufficient to provide good security.","[' What are binary stream ciphers often constructed using?', ' What can be easily implemented in hardware and can be readily analysed mathematically?']","['linear-feedback shift registers', 'Binary stream ciphers are often constructed using linear-feedback shift registers']"
3002,stream cipher,Other designs,"Instead of a linear driving device, one may use a nonlinear update function. For example, Klimov and Shamir proposed triangular functions (T-functions) with a single cycle on n-bit words.
","Instead of a linear driving device, one may use a nonlinear update function. For example, Klimov and Shamir proposed triangular functions (T-functions) with a single cycle on n-bit words.","[' What could be used instead of a linear driving device?', ' Who proposed triangular functions?', ' What did Klimov and Shamir propose?']","['a nonlinear update function', 'Klimov and Shamir', 'triangular functions (T-functions) with a single cycle on n-bit words']"
3003,stream cipher,Security,"For a stream cipher to be secure, its keystream must have a large period, and it must be impossible to recover the cipher's key or internal state from the keystream. Cryptographers also demand that the keystream be free of even subtle biases that would let attackers distinguish a stream from random noise, and free of detectable relationships between keystreams that correspond to related keys or related cryptographic nonces. That should be true for all keys (there should be no weak keys), even if the attacker can know or choose some plaintext or ciphertext.
","For a stream cipher to be secure, its keystream must have a large period, and it must be impossible to recover the cipher's key or internal state from the keystream. Cryptographers also demand that the keystream be free of even subtle biases that would let attackers distinguish a stream from random noise, and free of detectable relationships between keystreams that correspond to related keys or related cryptographic nonces.","["" What must a cipher's keystream have for it to be secure?"", ' What must be impossible to recover from the keystream?', ' Cryptographers demand that a keystream be free of what?', ' What would let attackers distinguish a stream from random noise?', ' What would be free of detectable relationships between keystreams that correspond to related keys?']","['a large period', ""the cipher's key or internal state"", 'even subtle biases', 'subtle biases', 'the keystream']"
3004,stream cipher,Security,"Securely using a secure synchronous stream cipher requires that one never reuse the same keystream twice. That generally means a different nonce or key must be supplied to each invocation of the cipher. Application designers must also recognize that most stream ciphers provide not authenticity but privacy: encrypted messages may still have been modified in transit.
",Securely using a secure synchronous stream cipher requires that one never reuse the same keystream twice. That generally means a different nonce or key must be supplied to each invocation of the cipher.,"[' What does using a secure synchronous stream cipher require?', ' What must be supplied to each invocation of a crypther?']","['never reuse the same keystream twice', 'a different nonce or key']"
3005,stream cipher,Security,"Short periods for stream ciphers have been a practical concern. For example, 64-bit block ciphers like DES can be used to generate a keystream in output feedback (OFB) mode. However, when not using full feedback, the resulting stream has a period of around 232 blocks on average; for many applications, the period is far too low. For example, if encryption is being performed at a rate of 8 megabytes per second, a stream of period 232 blocks will repeat after about a half an hour.","Short periods for stream ciphers have been a practical concern. For example, 64-bit block ciphers like DES can be used to generate a keystream in output feedback (OFB) mode.","[' What has been a practical concern for stream ciphers?', ' What can be used to generate a keystream in output feedback mode?']","['Short periods', '64-bit block ciphers']"
3006,stream cipher,Usage,"Stream ciphers are often used for their speed and simplicity of implementation in hardware, and in applications where plaintext comes in quantities of unknowable length like a secure wireless connection. If a block cipher (not operating in a stream cipher mode) were to be used in this type of application, the designer would need to choose either transmission efficiency or implementation complexity, since block ciphers cannot directly work on blocks shorter than their block size. For example, if a 128-bit block cipher received separate 32-bit bursts of plaintext, three quarters of the data transmitted would be padding. Block ciphers must be used in ciphertext stealing or residual block termination mode to avoid padding, while stream ciphers eliminate this issue by naturally operating on the smallest unit that can be transmitted (usually bytes).
","Stream ciphers are often used for their speed and simplicity of implementation in hardware, and in applications where plaintext comes in quantities of unknowable length like a secure wireless connection. If a block cipher (not operating in a stream cipher mode) were to be used in this type of application, the designer would need to choose either transmission efficiency or implementation complexity, since block ciphers cannot directly work on blocks shorter than their block size.","[' Stream ciphers are often used for their speed and simplicity of implementation in what?', ' What is an example of an application where plaintext comes in quantities of unknowable length?', ' What would a designer need to choose to be used in a type of application?', ' What can block ciphers not work on?']","['hardware', 'a secure wireless connection', 'transmission efficiency or implementation complexity', 'blocks shorter than their block size']"
3007,stream cipher,Usage,"Another advantage of stream ciphers in military cryptography is that the cipher stream can be generated in a separate box that is subject to strict security measures and fed to other devices such as a radio set, which will perform the XOR operation as part of their function. The latter device can then be designed and used in less stringent environments.
","Another advantage of stream ciphers in military cryptography is that the cipher stream can be generated in a separate box that is subject to strict security measures and fed to other devices such as a radio set, which will perform the XOR operation as part of their function. The latter device can then be designed and used in less stringent environments.","[' What is another advantage of stream ciphers in military cryptography?', ' What can be generated in a separate box that is subject to strict security measures and fed to other devices?', ' What is the XOR operation part of?', ' What can be designed and used in less stringent environments?']","['the cipher stream can be generated in a separate box that is subject to strict security measures', 'the cipher stream', 'their function', 'stream ciphers']"
3008,tabu search,Summary,Tabu search is a metaheuristic search method employing local search methods used for mathematical optimization. It was created by Fred W. Glover in 1986 and formalized in 1989.,Tabu search is a metaheuristic search method employing local search methods used for mathematical optimization. It was created by Fred W. Glover in 1986 and formalized in 1989.,"[' What is Tabu search?', ' Who created Tabu?', ' When was Tabu searched formalized?', ' What is tabu search used for?']","['a metaheuristic search method', 'Fred W. Glover', '1989', 'mathematical optimization']"
3009,tabu search,Summary,"Local (neighborhood) searches take a potential solution to a problem and check its immediate neighbors (that is, solutions that are similar except for very few minor details) in the hope of finding an improved solution. Local search methods have a tendency to become stuck in suboptimal regions or on plateaus where many solutions are equally fit.
","Local (neighborhood) searches take a potential solution to a problem and check its immediate neighbors (that is, solutions that are similar except for very few minor details) in the hope of finding an improved solution. Local search methods have a tendency to become stuck in suboptimal regions or on plateaus where many solutions are equally fit.","[' What is another term for neighborhood searches?', ' What do local searches look for in the hope of finding an improved solution to a problem?', ' What is the tendency to become stuck in?']","['neighborhood', 'immediate neighbors', 'suboptimal regions']"
3010,tabu search,Summary,"Tabu search enhances the performance of local search by relaxing its basic rule. First, at each step  worsening moves can be accepted if no improving move is available (like when the search is stuck at a strict local minimum). In addition, prohibitions (henceforth the term tabu) are introduced to discourage the search from coming back to previously-visited solutions.
","Tabu search enhances the performance of local search by relaxing its basic rule. First, at each step  worsening moves can be accepted if no improving move is available (like when the search is stuck at a strict local minimum).","[' What enhances the performance of local search by relaxing its basic rule?', ' What can be accepted at each step if no improving move is available?']","['Tabu search', 'worsening moves']"
3011,tabu search,Summary,"The implementation of tabu search uses memory structures that describe the visited solutions or user-provided sets of rules. If a potential solution has been previously visited within a certain short-term period or if it has violated a rule, it is marked as ""tabu"" (forbidden) so that the algorithm does not consider that possibility repeatedly.
","The implementation of tabu search uses memory structures that describe the visited solutions or user-provided sets of rules. If a potential solution has been previously visited within a certain short-term period or if it has violated a rule, it is marked as ""tabu"" (forbidden) so that the algorithm does not consider that possibility repeatedly.","[' What does tabu search use to describe the visited solutions or user-provided sets of rules?', ' What is tabu marked if a potential solution has been previously visited within a certain short-term period or if it has violated a rule?', ' What does ""tabu"" mean?']","['memory structures', 'forbidden', 'forbidden']"
3012,tabu search,Background,"Current applications of TS span the areas of resource planning, telecommunications, VLSI design, financial analysis, scheduling, space planning, energy distribution, molecular engineering, logistics, pattern classification, flexible manufacturing, waste management, mineral exploration, biomedical analysis, environmental conservation and scores of others.  In recent years, journals in a wide variety of fields have published tutorial articles and computational studies documenting successes by tabu search in extending the frontier of problems that can be handled effectively — yielding solutions whose quality often significantly surpasses that obtained by methods previously applied. A comprehensive list of applications, including summary descriptions of gains achieved from practical implementations, can be found in ","Current applications of TS span the areas of resource planning, telecommunications, VLSI design, financial analysis, scheduling, space planning, energy distribution, molecular engineering, logistics, pattern classification, flexible manufacturing, waste management, mineral exploration, biomedical analysis, environmental conservation and scores of others. In recent years, journals in a wide variety of fields have published tutorial articles and computational studies documenting successes by tabu search in extending the frontier of problems that can be handled effectively — yielding solutions whose quality often significantly surpasses that obtained by methods previously applied.","[' What are some of the current applications of TS?', ' What have journals in a wide variety of fields done in recent years?', ' In recent years, journals in a wide variety of fields have published what?', ' What has tabu search done to extend the frontier of problems that can be handled effectively?']","['resource planning, telecommunications, VLSI design, financial analysis, scheduling, space planning, energy distribution, molecular engineering, logistics, pattern classification, flexible manufacturing, waste management, mineral exploration, biomedical analysis, environmental conservation', 'published tutorial articles and computational studies', 'tutorial articles and computational studies', 'tutorial articles and computational studies']"
3013,tabu search,Basic description,"Tabu search uses a local or neighborhood search procedure to iteratively move from one potential solution 



x


{\displaystyle x}
 to an improved solution 




x
′



{\displaystyle x'}
 in the neighborhood of 



x


{\displaystyle x}
, until some stopping criterion has been satisfied (generally, an attempt limit or a score threshold). Local search procedures often become stuck in poor-scoring areas or areas where scores plateau. In order to avoid these pitfalls and explore regions of the search space that would be left unexplored by other local search procedures, tabu search carefully explores the neighborhood of each solution as the search progresses. The solutions admitted to the new neighborhood, 




N

∗


(
x
)


{\displaystyle N^{*}(x)}
, are determined through the use of memory structures. Using these memory structures, the search progresses by iteratively moving from the current solution 



x


{\displaystyle x}
 to an improved solution 




x
′



{\displaystyle x'}
 in 




N

∗


(
x
)


{\displaystyle N^{*}(x)}
. 
","Tabu search uses a local or neighborhood search procedure to iteratively move from one potential solution 



x


{\displaystyle x}
 to an improved solution 




x
′



{\displaystyle x'}
 in the neighborhood of 



x


{\displaystyle x}
, until some stopping criterion has been satisfied (generally, an attempt limit or a score threshold). Local search procedures often become stuck in poor-scoring areas or areas where scores plateau.","[' Tabu search uses a local or neighborhood search procedure to iteratively move from one potential solution to an improved solution in the neighborhood of what?', ' What do local search procedures often become stuck in?', ' What are poor-scoring areas?']","['x', 'poor-scoring areas or areas where scores plateau', 'areas where scores plateau']"
3014,tabu search,Basic description,"Tabu search has several similarities with simulated annealing, as both involve possible down hills moves. In fact, simulated annealing could be viewed as a special form of TS, where by we use ""graduated tenure"", that is, a move becomes tabu with a specified probability.
","Tabu search has several similarities with simulated annealing, as both involve possible down hills moves. In fact, simulated annealing could be viewed as a special form of TS, where by we use ""graduated tenure"", that is, a move becomes tabu with a specified probability.","[' Tabu search has several similarities with what?', ' Both simulated annealing and tabu search involve possible down hills moves, what is it called?', ' What is the term for a move becoming tabu with a specified probability?']","['simulated annealing', 'Tabu search', 'graduated tenure']"
3015,tabu search,Basic description,"These memory structures form what is known as the tabu list, a set of rules and banned solutions used to filter which solutions will be admitted to the neighborhood 




N

∗


(
x
)


{\displaystyle N^{*}(x)}
 to be explored by the search. In its simplest form, a tabu list is a short-term set of the solutions that have been visited in the recent past (less than 



n


{\displaystyle n}
 iterations ago, where 



n


{\displaystyle n}
  is the number of previous solutions to be stored —  is also called the tabu tenure). More commonly, a tabu list consists of solutions that have changed by the process of moving from one solution to another. It is convenient, for ease of description, to understand a “solution” to be coded and represented by such attributes.
","These memory structures form what is known as the tabu list, a set of rules and banned solutions used to filter which solutions will be admitted to the neighborhood 




N

∗


(
x
)


{\displaystyle N^{*}(x)}
 to be explored by the search. In its simplest form, a tabu list is a short-term set of the solutions that have been visited in the recent past (less than 



n


{\displaystyle n}
 iterations ago, where 



n


{\displaystyle n}
  is the number of previous solutions to be stored —  is also called the tabu tenure).","[' What is known as the tabu list?', ' What is a set of rules and banned solutions used to filter?', ' What is a tabu list a short-term set of the solutions that have been visited in the recent past called?', ' What is also called the tabu tenure?']","['a set of rules and banned solutions', 'the tabu list', 'tabu tenure', 'n\n\n\n{\\displaystyle n}\n  is the number of previous solutions to be stored']"
3016,tabu search,Types of memory,"Short-term, intermediate-term and long-term memories can overlap in practice. Within these categories, memory can further be differentiated by measures such as frequency and impact of changes made. One example of an intermediate-term memory structure is one that prohibits or encourages solutions that contain certain attributes (e.g., solutions that include undesirable or desirable values for certain variables) or a memory structure that prevents or induces certain moves (e.g. based on frequency memory applied to solutions sharing features in common with unattractive or attractive solutions found in the past). In short-term memory, selected attributes in solutions recently visited are labelled ""tabu-active."" Solutions that contain tabu-active elements are banned. Aspiration criteria are employed to override a solution's tabu state, thereby including the otherwise-excluded solution in the allowed set (provided the solution is “good enough” according to a measure of quality or diversity). A simple and commonly used aspiration criterion is to allow solutions which are better than the currently-known best solution.
","Short-term, intermediate-term and long-term memories can overlap in practice. Within these categories, memory can further be differentiated by measures such as frequency and impact of changes made.","[' How can short-term, intermediate-term and long-term memories overlap in practice?', ' What can be further differentiated by measures such as frequency and impact of changes made?']","['memory can further be differentiated by measures such as frequency and impact of changes made', 'memory']"
3017,tabu search,Types of memory,"
Short-term memory alone may be enough to achieve solutions superior to those found by conventional local search methods, but intermediate and long-term structures are often necessary for solving harder problems.  Tabu search is often benchmarked against other metaheuristic methods — such as Simulated annealing, genetic algorithms, Ant colony optimization algorithms, Reactive search optimization, Guided Local Search, or greedy randomized adaptive search. In addition, tabu search is sometimes combined with other metaheuristics to create hybrid methods. The most common tabu search hybrid arises by joining TS with Scatter Search, a class of population-based procedures which has roots in common with tabu search, and is often employed in solving large non-linear optimization problems.
","
Short-term memory alone may be enough to achieve solutions superior to those found by conventional local search methods, but intermediate and long-term structures are often necessary for solving harder problems. Tabu search is often benchmarked against other metaheuristic methods — such as Simulated annealing, genetic algorithms, Ant colony optimization algorithms, Reactive search optimization, Guided Local Search, or greedy randomized adaptive search.","[' What may be enough to achieve solutions superior to those found by conventional local search methods?', ' What are often necessary for solving harder problems?', ' Tabu search is often benchmarked against other metaheuristic methods such as Simulated annealing, genetic algorithms, and what else?', ' What type of algorithms are used in Simulated annealing?', ' What kind of search algorithm is used in Guided Local Search?']","['Short-term memory', 'intermediate and long-term structures', 'Ant colony optimization algorithms', 'genetic algorithms', 'greedy randomized adaptive search']"
3018,tabu search,Pseudocode,"The following pseudocode presents a simplified version of the tabu search algorithm as described above. This implementation has a rudimentary short-term memory, but contains no intermediate or long-term memory structures. The term ""fitness"" refers to an evaluation of the candidate solution, as embodied in an objective function for mathematical optimization.
","The following pseudocode presents a simplified version of the tabu search algorithm as described above. This implementation has a rudimentary short-term memory, but contains no intermediate or long-term memory structures.","[' The following pseudocode presents a simplified version of what algorithm?', ' The implementation of the tabu search algorithm has what?']","['tabu search', 'a rudimentary short-term memory']"
3019,tabu search,Pseudocode,"Lines 1-4 represent some initial setup, respectively creating an initial solution (possibly chosen at random), setting that initial solution as the best seen to date, and initializing a tabu list with this initial solution. In this example, the tabu list is simply a short term memory structure that will contain a record of the elements of the states visited.
","Lines 1-4 represent some initial setup, respectively creating an initial solution (possibly chosen at random), setting that initial solution as the best seen to date, and initializing a tabu list with this initial solution. In this example, the tabu list is simply a short term memory structure that will contain a record of the elements of the states visited.","[' Lines 1-4 represent some initial setup, creating an initial solution, setting that initial solution as the best seen to date, and initializing a tabu list with what initial solution?', ' What type of memory structure will contain a record of the elements of the states visited?']","['this initial solution', 'the tabu list']"
3020,tabu search,Pseudocode,"The core algorithmic loop starts in line 5. This loop will continue searching for an optimal solution until a user-specified stopping condition is met (two examples of such conditions are a simple time limit or a threshold on the fitness score). The neighboring solutions are checked for tabu elements in line 9. Additionally, the algorithm keeps track of the best solution in the neighbourhood, that is not tabu.
",The core algorithmic loop starts in line 5. This loop will continue searching for an optimal solution until a user-specified stopping condition is met (two examples of such conditions are a simple time limit or a threshold on the fitness score).,"[' Where does the core algorithmic loop start?', ' What will the loop continue searching for until a user-specified stopping condition is met?']","['line 5', 'an optimal solution']"
3021,tabu search,Pseudocode,"The fitness function is generally a mathematical function, which returns a score or the aspiration criteria are satisfied — for example, an aspiration criterion could be considered as a new search space is found). If the best local candidate has a higher fitness value than the current best (line 13), it is set as the new best (line 14). The local best candidate is always added to the tabu list (line 16) and if the tabu list is full (line 17), some elements will be allowed to expire (line 18). Generally, elements expire from the list in the same order they are added. The procedure will select the best local candidate (although it has worse fitness than the sBest) in order to escape the local optimal.
","The fitness function is generally a mathematical function, which returns a score or the aspiration criteria are satisfied — for example, an aspiration criterion could be considered as a new search space is found). If the best local candidate has a higher fitness value than the current best (line 13), it is set as the new best (line 14).","[' What is the fitness function generally a mathematical function?', ' What could be considered as a new search space is found?', ' Which candidate has a higher fitness value than the current best?', ' What is a higher fitness value than the current best?', ' What is the new best set as?']","['returns a score or the aspiration criteria are satisfied', 'an aspiration criterion', 'the best local candidate', 'the best local candidate', 'If the best local candidate has a higher fitness value than the current best (line 13']"
3022,tabu search,Example: the traveling salesman problem,"The traveling salesman problem (TSP) is sometimes used to show the functionality of tabu search. This problem poses a straightforward question — given a list of cities, what is the shortest route that visits every city? For example, if city A and city B are next to each other, while city C is farther away, the total distance traveled will be shorter if cities A and B are visited one after the other before visiting city C. Since finding an optimal solution is NP-hard, heuristic-based approximation methods (such as local searches) are useful for devising close-to-optimal solutions. To obtain good TSP solutions, it is essential to exploit the graph structure. The value of exploiting problem structure is a recurring theme in metaheuristic methods, and tabu search is well-suited to this. A class of strategies associated with tabu search called ejection chain methods has made it possible to obtain high-quality TSP solutions efficiently ","The traveling salesman problem (TSP) is sometimes used to show the functionality of tabu search. This problem poses a straightforward question — given a list of cities, what is the shortest route that visits every city?","[' The traveling salesman problem is sometimes used to show the functionality of what?', ' What is the shortest route that visits every city?']","['tabu search', 'list of cities']"
3023,tabu search,Example: the traveling salesman problem,"On the other hand, a simple tabu search can be used to find a satisficing solution for the traveling salesman problem (that is, a solution that satisfies an adequacy criterion, although not with the high quality obtained by exploiting the graph structure). The search starts with an initial solution, which can be generated randomly or according to some sort of nearest neighbor algorithm. To create new solutions, the order that two cities are visited in a potential solution is swapped. The total traveling distance between all the cities is used to judge how ideal one solution is compared to another. To prevent cycles – i.e., repeatedly visiting a particular set of solutions – and to avoid becoming stuck in local optima, a solution is added to the tabu list if it is accepted into the solution neighborhood, 




N

∗


(
x
)


{\displaystyle N^{*}(x)}
.
","On the other hand, a simple tabu search can be used to find a satisficing solution for the traveling salesman problem (that is, a solution that satisfies an adequacy criterion, although not with the high quality obtained by exploiting the graph structure). The search starts with an initial solution, which can be generated randomly or according to some sort of nearest neighbor algorithm.","[' What can be used to find a satisficing solution for the traveling salesman problem?', ' What is a solution that meets an adequacy criterion but not the high quality obtained by exploiting the graph structure?', ' What does the search start with?', ' What can be generated randomly?']","['a simple tabu search', 'traveling salesman problem', 'an initial solution', 'an initial solution']"
3024,tabu search,Example: the traveling salesman problem,"New solutions are created until some stopping criterion, such as an arbitrary number of iterations, is met. Once the simple tabu search stops, it returns the best solution found during its execution.
","New solutions are created until some stopping criterion, such as an arbitrary number of iterations, is met. Once the simple tabu search stops, it returns the best solution found during its execution.","[' What is a stopping criterion?', ' What returns the best solution found during its execution?', ' When does tabu search stop?']","['an arbitrary number of iterations', 'tabu search', 'it returns the best solution found during its execution']"
3025,singular value decomposition,Summary,"In linear algebra, the singular value decomposition (SVD) is a factorization of a real or complex matrix. It generalizes the eigendecomposition of a square normal matrix with an orthonormal eigenbasis to any 



m
×
n


{\displaystyle m\times n}
 matrix. It is related to the polar decomposition.
","In linear algebra, the singular value decomposition (SVD) is a factorization of a real or complex matrix. It generalizes the eigendecomposition of a square normal matrix with an orthonormal eigenbasis to any 



m
×
n


{\displaystyle m\times n}
 matrix.","[' In linear algebra, what is SVD a factorization of a real or complex matrix?', ' What generalizes the eigendecomposition of square normal matrix with an orthonormal?']","['singular value decomposition', 'singular value decomposition (SVD)']"
3026,singular value decomposition,Summary,"Specifically, the singular value decomposition of an 



m
×
n


{\displaystyle m\times n}
 complex matrix M is a factorization of the form 




M

=

U
Σ

V

∗





{\displaystyle \mathbf {M} =\mathbf {U\Sigma V^{*}} }
, where U is an 



m
×
m


{\displaystyle m\times m}
 complex unitary matrix, 




Σ



{\displaystyle \mathbf {\Sigma } }
 is an 



m
×
n


{\displaystyle m\times n}
 rectangular diagonal matrix with non-negative real numbers on the diagonal, and V is an 



n
×
n


{\displaystyle n\times n}
 complex unitary matrix.  If M is real, U and V can also be guaranteed to be real orthogonal matrices. In such contexts, the SVD is often denoted 




U
Σ

V

T





{\displaystyle \mathbf {U\Sigma V^{T}} }
.
","Specifically, the singular value decomposition of an 



m
×
n


{\displaystyle m\times n}
 complex matrix M is a factorization of the form 




M

=

U
Σ

V

∗





{\displaystyle \mathbf {M} =\mathbf {U\Sigma V^{*}} }
, where U is an 



m
×
m


{\displaystyle m\times m}
 complex unitary matrix, 




Σ



{\displaystyle \mathbf {\Sigma } }
 is an 



m
×
n


{\displaystyle m\times n}
 rectangular diagonal matrix with non-negative real numbers on the diagonal, and V is an 



n
×
n


{\displaystyle n\times n}
 complex unitary matrix. If M is real, U and V can also be guaranteed to be real orthogonal matrices.","[' What is the singular value decomposition of an m <unk> n <unk>displaystyle m<unk>times n<unk> complex matrix M a factorization of the form M = U <unk> V?', ' What is U an an <unk>-m -display style m?', ' What is <unk> an m <unk> n <unk>displaystyle m<unk>times n<unk> rectangular diagonal matrix with non-negative real numbers on the diagonal?', ' What is V an n an a n and a complex unitary matrix?', ' If M is real, U and V can be guaranteed to be what?']","['m\n×\nn\n\n\n{\\displaystyle m\\times n}\n complex matrix M is a factorization of the form \n\n\n\n\nM\n\n=\n\nU\nΣ\n\nV\n\n∗\n\n\n\n\n\n{\\displaystyle \\mathbf {M} =\\mathbf {U\\Sigma V^{*}} }\n, where U is an \n\n\n\nm\n×\nm', 'm\n×\nm', 'V', 'n\n×\nn', 'real orthogonal matrices']"
3027,singular value decomposition,Summary,"The diagonal entries 




σ

i


=

Σ

i
i




{\displaystyle \sigma _{i}=\Sigma _{ii}}
 of 




Σ



{\displaystyle \mathbf {\Sigma } }
 are uniquely determined by M and are known as the singular values of M.  The number of non-zero singular values is equal to the rank of M.  The columns of U and the columns of V are called left-singular vectors and right-singular vectors of M, respectively. They form two sets of orthonormal bases u1, ..., um and v1, ..., vn , and the singular value decomposition can be written as 




M

=

∑

i
=
1


r



σ

i




u


i




v


i


∗




{\displaystyle \mathbf {M} =\sum _{i=1}^{r}\sigma _{i}\mathbf {u} _{i}\mathbf {v} _{i}^{*}}
, where 



r
≤
min
{
m
,
n
}


{\displaystyle r\leq \min\{m,n\}}
 is the rank of M.
","The diagonal entries 




σ

i


=

Σ

i
i




{\displaystyle \sigma _{i}=\Sigma _{ii}}
 of 




Σ



{\displaystyle \mathbf {\Sigma } }
 are uniquely determined by M and are known as the singular values of M.  The number of non-zero singular values is equal to the rank of M.  The columns of U and the columns of V are called left-singular vectors and right-singular vectors of M, respectively. They form two sets of orthonormal bases u1, ..., um and v1, ..., vn , and the singular value decomposition can be written as 




M

=

∑

i
=
1


r



σ

i




u


i




v


i


∗




{\displaystyle \mathbf {M} =\sum _{i=1}^{r}\sigma _{i}\mathbf {u} _{i}\mathbf {v} _{i}^{*}}
, where 



r
≤
min
{
m
,
n
}


{\displaystyle r\leq \min\{m,n\}}
 is the rank of M.","[' What are the diagonal entries uniquely determined by?', ' What are known as the singular values of M?', ' The number of non-zero singular values is equal to the rank of what?', ' What are the columns of U and V called?', ' How many sets of orthonormal bases form?', ' What can be written as M = <unk> i = 1 r?', ' What can be written as M = <unk> i = 1?', ' What is the rank of M?']","['M', 'diagonal entries \n\n\n\n\nσ\n\ni\n\n\n=\n\nΣ\n\ni\ni\n\n\n\n\n{\\displaystyle \\sigma _{i}=\\Sigma _{ii}}\n of \n\n\n\n\nΣ', 'M', 'left-singular vectors and right-singular vectors of M', 'two', 'i', 'i\n=\n1\n\n\nr\n\n\n\nσ\n\ni\n\n\n\n\nu\n\n\ni\n\n\n\n\nv\n\n\ni\n\n\n∗\n\n\n\n\n{\\displaystyle \\mathbf {M} =\\sum _{i=1}^{r}\\sigma', 'r\n≤\nmin\n{\nm\n,\nn\n}\n\n\n{\\displaystyle r\\leq \\min\\{m,n\\}}']"
3028,singular value decomposition,Summary,"The SVD is not unique. It is always possible to choose the decomposition so that the singular values 




Σ

i
i




{\displaystyle \Sigma _{ii}}
are in descending order.  In this case, 




Σ



{\displaystyle \mathbf {\Sigma } }
 (but not always U and V) is uniquely determined by M.  
","The SVD is not unique. It is always possible to choose the decomposition so that the singular values 




Σ

i
i




{\displaystyle \Sigma _{ii}}
are in descending order.","[' Is the SVD unique?', ' Is it always possible to choose the decomposition so that the singular values are in descending order?']","['not unique', 'It is always possible']"
3029,singular value decomposition,Summary,"The term sometimes refers to the compact SVD, a similar decomposition 




M

=

U
Σ

V

∗





{\displaystyle \mathbf {M} =\mathbf {U\Sigma V^{*}} }
 in which 




Σ



{\displaystyle \mathbf {\Sigma } }
 is square diagonal of size 



r
×
r


{\displaystyle r\times r}
, where 



r
≤
min
{
m
,
n
}


{\displaystyle r\leq \min\{m,n\}}
 is the rank of M, and has only the non-zero singular values. In this variant, U is an 



m
×
r


{\displaystyle m\times r}
  semi-unitary matrix and 




V



{\displaystyle \mathbf {V} }
 is an 



n
×
r


{\displaystyle n\times r}
  semi-unitary matrix, such that 





U

∗


U

=


V

∗


V

=


I


r




{\displaystyle \mathbf {U^{*}U} =\mathbf {V^{*}V} =\mathbf {I} _{r}}
.
","The term sometimes refers to the compact SVD, a similar decomposition 




M

=

U
Σ

V

∗





{\displaystyle \mathbf {M} =\mathbf {U\Sigma V^{*}} }
 in which 




Σ



{\displaystyle \mathbf {\Sigma } }
 is square diagonal of size 



r
×
r


{\displaystyle r\times r}
, where 



r
≤
min
{
m
,
n
}


{\displaystyle r\leq \min\{m,n\}}
 is the rank of M, and has only the non-zero singular values. In this variant, U is an 



m
×
r


{\displaystyle m\times r}
  semi-unitary matrix and 




V



{\displaystyle \mathbf {V} }
 is an 



n
×
r


{\displaystyle n\times r}
  semi-unitary matrix, such that 





U

∗


U

=


V

∗


V

=


I


r




{\displaystyle \mathbf {U^{*}U} =\mathbf {V^{*}V} =\mathbf {I} _{r}}
.","[' What term sometimes refers to the compact SVD?', ' What is square diagonal of size?', ' How many non-zero singular values does M have?', ' How many singular values does M have?', ' What is U an m <unk> r <unk>displaystyle m<unk>times r<unk> semi-unitary matrix?']","['M\n\n=\n\nU\nΣ\n\nV\n\n∗\n\n\n\n\n\n{\\displaystyle \\mathbf {M} =\\mathbf {U\\Sigma V^{*}} }\n in which \n\n\n\n\nΣ', 'r\n×\nr', 'r\n≤\nmin\n{\nm\n,\nn\n}\n\n\n{\\displaystyle r\\leq \\min\\{m,n\\}}\n is the rank of M, and has only the non-zero singular values. In this variant, U is an \n\n\n\nm\n×\nr\n\n\n{\\displaystyle m\\times r}\n  semi-unitary matrix and \n\n\n\n\nV\n\n\n\n{\\displaystyle \\mathbf {V} }\n is an \n\n\n\nn\n×\nr', 'non-zero', 'm\n×\nr']"
3030,singular value decomposition,Summary,"Mathematical applications of the SVD include computing the pseudoinverse, matrix approximation, and determining the rank, range, and null space of a matrix.  The SVD is also extremely useful in all areas of science, engineering, and statistics, such as signal processing, least squares fitting of data, and process control.
","Mathematical applications of the SVD include computing the pseudoinverse, matrix approximation, and determining the rank, range, and null space of a matrix. The SVD is also extremely useful in all areas of science, engineering, and statistics, such as signal processing, least squares fitting of data, and process control.","[' What are some of the mathematical applications of the SVD?', ' What is one of the areas of science, engineering, and statistics that uses the software?', ' How is the software used in signal processing?']","['computing the pseudoinverse, matrix approximation, and determining the rank, range, and null space of a matrix', 'signal processing', 'The SVD is also extremely useful']"
3031,singular value decomposition,Example,"The scaling matrix 




Σ



{\displaystyle \mathbf {\Sigma } }
 is zero outside of the diagonal (grey italics) and one diagonal element is zero (red bold). Furthermore, because the matrices U and V⁎ are unitary, multiplying by their respective conjugate transposes yields identity matrices, as shown below.  In this case, because U and V⁎ are real valued, each is an orthogonal matrix.
","The scaling matrix 




Σ



{\displaystyle \mathbf {\Sigma } }
 is zero outside of the diagonal (grey italics) and one diagonal element is zero (red bold). Furthermore, because the matrices U and V⁎ are unitary, multiplying by their respective conjugate transposes yields identity matrices, as shown below.","[' The scaling matrix <unk> <unk>displaystyle <unk>mathbf <unk>Sigma <unk> is zero outside of what?', ' One diagonal element is zero (what?', ' The matrices U and V<unk> are unitary because they are which?', ' Multiplying by their respective conjugate transposes yields what?<extra_id_51>']","['the diagonal', 'red bold', 'identity matrices', 'identity matrices']"
3032,singular value decomposition,Example,"This particular singular value decomposition is not unique.  Choosing 




V



{\displaystyle \mathbf {V} }
 such that
","This particular singular value decomposition is not unique. Choosing 




V



{\displaystyle \mathbf {V} }
 such that",[' What type of value decomposition is not unique?'],['singular']
3033,singular value decomposition,Existence proofs,"An eigenvalue λ of a matrix M is characterized by the algebraic relation Mu = λu. When M is Hermitian, a variational characterization is also available. Let M be a real n × n symmetric matrix. Define
","An eigenvalue λ of a matrix M is characterized by the algebraic relation Mu = λu. When M is Hermitian, a variational characterization is also available.","[' What is an eigenvalue <unk> of a matrix M characterized by?', ' What is a variational characterization available when M is Hermitian?']","['algebraic relation Mu = λu', 'eigenvalue λ of a matrix M']"
3034,singular value decomposition,Existence proofs,"By the extreme value theorem, this continuous function attains a maximum at some u when restricted to the unit sphere {||x|| = 1}. By the Lagrange multipliers theorem, u necessarily satisfies
","By the extreme value theorem, this continuous function attains a maximum at some u when restricted to the unit sphere {||x|| = 1}. By the Lagrange multipliers theorem, u necessarily satisfies","[' What theorem states that the continuous function attains a maximum at some u when restricted to the unit sphere <unk>||x|| = 1<unk>?', ' By the Lagrange multipliers theory, u necessarily satisfies what?']","['extreme value theorem', 'extreme value theorem']"
3035,singular value decomposition,Existence proofs,"for some real number λ. The nabla symbol, ∇, is the del operator (differentiation with respect to x). Using the symmetry of M we obtain 
","for some real number λ. The nabla symbol, ∇, is the del operator (differentiation with respect to x).","[' What is the nabla symbol?', ' What is <unk>?']","['the del operator', '∇, is the del operator']"
3036,singular value decomposition,Existence proofs,"Therefore Mu = λu, so u is a unit length eigenvector of M. For every unit length eigenvector v of M its eigenvalue is f(v), so λ is the largest eigenvalue of M. The same calculation performed on the orthogonal complement of u gives the next largest eigenvalue and so on. The complex Hermitian case is similar; there f(x) = x* M x is a real-valued function of 2n real variables.
","Therefore Mu = λu, so u is a unit length eigenvector of M. For every unit length eigenvector v of M its eigenvalue is f(v), so λ is the largest eigenvalue of M. The same calculation performed on the orthogonal complement of u gives the next largest eigenvalue and so on. The complex Hermitian case is similar; there f(x) = x* M x is a real-valued function of 2n real variables.","[' What is a unit length eigenvector of M?', ' What is the largest eigevalue of M for every unit length EIGnvector v?', ' The same calculation performed on the orthogonal complement of u gives what?', ' What gives the next largest eigenvalue?', ' The complex Hermitian case is similar to what?']","['Mu = λu, so u', 'λ', 'the next largest eigenvalue', 'orthogonal complement of u', 'a real-valued function of 2n real variables']"
3037,singular value decomposition,Existence proofs,"Singular values are similar in that they can be described algebraically or from variational principles. Although, unlike the eigenvalue case, Hermiticity, or symmetry, of M is no longer required.
","Singular values are similar in that they can be described algebraically or from variational principles. Although, unlike the eigenvalue case, Hermiticity, or symmetry, of M is no longer required.","[' Singular values can be described algebraically or from what?', ' What is no longer required in the eigenvalue case?']","['variational principles', 'Hermiticity']"
3038,singular value decomposition,Reduced SVDs,"In applications it is quite unusual for the full SVD, including a full unitary decomposition of the null-space of the matrix, to be required.  Instead, it is often sufficient (as well as faster, and more economical for storage) to compute a reduced version of the SVD.  The following can be distinguished for an m×n matrix M of rank r:
","In applications it is quite unusual for the full SVD, including a full unitary decomposition of the null-space of the matrix, to be required. Instead, it is often sufficient (as well as faster, and more economical for storage) to compute a reduced version of the SVD.","[' Is it unusual for a full SVD to be required in applications?', ' What is often sufficient to compute a reduced version of the SVd?']","['it is quite unusual', 'the full SVD']"
3039,singular value decomposition,History,"The singular value decomposition was originally developed by differential geometers, who wished to determine whether a real bilinear form could be made equal to another by independent orthogonal transformations of the two spaces it acts on. Eugenio Beltrami and Camille Jordan discovered independently, in 1873 and 1874 respectively, that the singular values of the bilinear forms, represented as a matrix, form a complete set of invariants for bilinear forms under orthogonal substitutions. James Joseph Sylvester also arrived at the singular value decomposition for real square matrices in 1889, apparently independently of both Beltrami and Jordan. Sylvester called the singular values the canonical multipliers of the matrix A. The fourth mathematician to discover the singular value decomposition independently is Autonne in 1915, who arrived at it via the polar decomposition. The first proof of the singular value decomposition for rectangular and complex matrices seems to be by Carl Eckart and Gale J. Young in 1936; they saw it as a generalization of the principal axis transformation for Hermitian matrices.
","The singular value decomposition was originally developed by differential geometers, who wished to determine whether a real bilinear form could be made equal to another by independent orthogonal transformations of the two spaces it acts on. Eugenio Beltrami and Camille Jordan discovered independently, in 1873 and 1874 respectively, that the singular values of the bilinear forms, represented as a matrix, form a complete set of invariants for bilinear forms under orthogonal substitutions.","[' Who originally developed the singular value decomposition?', ' What did differential geometers want to determine about a real bilinear form?', ' When did Eugenio Beltrami and Camille Jordan discover that the singular values decompose?', ' When did Jordan discover that the singular values of bilinear forms, represented as a matrix, form a complete set of invariants?']","['differential geometers', 'whether a real bilinear form could be made equal to another by independent orthogonal transformations of the two spaces it acts on', '1873 and 1874', '1873 and 1874']"
3040,singular value decomposition,History,"In 1907, Erhard Schmidt defined an analog of singular values for integral operators (which are compact, under some weak technical assumptions); it seems he was unaware of the parallel work on singular values of finite matrices. This theory was further developed by Émile Picard in 1910, who is the first to call the numbers 




σ

k




{\displaystyle \sigma _{k}}
 singular values (or in French, valeurs singulières).
","In 1907, Erhard Schmidt defined an analog of singular values for integral operators (which are compact, under some weak technical assumptions); it seems he was unaware of the parallel work on singular values of finite matrices. This theory was further developed by Émile Picard in 1910, who is the first to call the numbers 




σ

k




{\displaystyle \sigma _{k}}
 singular values (or in French, valeurs singulières).","[' In what year did Erhard Schmidt define an analog of singular values for integral operators?', ' What is compact under some weak technical assumptions?', ' Who was the first person to develop this theory?', ' When were the numbers <unk> k <unk>displaystyle <unk>sigma _<unk>k<unk> singular values developed?', ' Who is the first to call the numbers?', ' What is the French word for values singulières?']","['1907', 'integral operators', 'Émile Picard', '1910', 'Émile Picard', 'valeurs']"
3041,singular value decomposition,History,"Practical methods for computing the SVD date back to Kogbetliantz in 1954–1955 and Hestenes in 1958, resembling closely the Jacobi eigenvalue algorithm, which uses plane rotations or Givens rotations. However, these were replaced by the method of Gene Golub and William Kahan published in 1965, which uses Householder transformations or reflections. In 1970, Golub and Christian Reinsch published a variant of the Golub/Kahan algorithm that is still the one most-used today.
","Practical methods for computing the SVD date back to Kogbetliantz in 1954–1955 and Hestenes in 1958, resembling closely the Jacobi eigenvalue algorithm, which uses plane rotations or Givens rotations. However, these were replaced by the method of Gene Golub and William Kahan published in 1965, which uses Householder transformations or reflections.","[' When did Kogbetliantz and Hestenes come up with practical methods for computing the SVD?', ' When was the Jacobi eigenvalue algorithm published?', ' What did Gene Golub and William Kahan publish in 1965?', "" When was William Kahan's book published?"", ' What did Kahan use in his book?']","['1954–1955', '1965', 'which uses Householder transformations or reflections', '1965', 'Householder transformations or reflections']"
3042,robotics,Summary,"Robotics is an interdisciplinary branch of computer science and engineering. Robotics involves design, construction, operation, and use of robots. The goal of robotics is to design machines that can help and assist humans. Robotics integrates fields of mechanical engineering, electrical engineering, information engineering, mechatronics, electronics, bioengineering, computer engineering, control engineering, software engineering, mathematics, etc.
","Robotics is an interdisciplinary branch of computer science and engineering. Robotics involves design, construction, operation, and use of robots.","[' What is an interdisciplinary branch of computer science and engineering?', ' What involves design, construction, operation, and use of robots?']","['Robotics', 'Robotics']"
3043,robotics,Summary,"Robotics develops machines that can substitute for humans and replicate human actions. Robots can be used in many situations for many purposes, but today many are used in dangerous environments (including inspection of radioactive materials, bomb detection and deactivation), manufacturing processes, or where humans cannot survive (e.g. in space, underwater, in high heat, and clean up and containment of hazardous materials and radiation). Robots can take on any form, but some are made to resemble humans in appearance. This is claimed to help in the acceptance of robots in certain replicative behaviors which are usually performed by people. Such robots attempt to replicate walking, lifting, speech, cognition, or any other human activity. Many of today's robots are inspired by nature, contributing to the field of bio-inspired robotics.
","Robotics develops machines that can substitute for humans and replicate human actions. Robots can be used in many situations for many purposes, but today many are used in dangerous environments (including inspection of radioactive materials, bomb detection and deactivation), manufacturing processes, or where humans cannot survive (e.g.","[' Robotics develops machines that can substitute for humans and replicate what?', ' Robots can be used in many situations for many purposes, but today many are used where?']","['human actions', 'dangerous environments']"
3044,robotics,Summary,"Certain robots require user input to operate while other robots function autonomously. The concept of creating robots that can operate autonomously dates back to classical times, but research into the functionality and potential uses of robots did not grow substantially until the 20th century. Throughout history, it has been frequently assumed by various scholars, inventors, engineers, and technicians that robots will one day be able to mimic human behavior and manage tasks in a human-like fashion. Today, robotics is a rapidly growing field, as technological advances continue; researching, designing, and building new robots serve various practical purposes, whether domestically, commercially, or militarily. Many robots are built to do jobs that are hazardous to people, such as defusing bombs, finding survivors in unstable ruins, and exploring mines and shipwrecks. Robotics is also used in STEM (science, technology, engineering, and mathematics) as a teaching aid.","Certain robots require user input to operate while other robots function autonomously. The concept of creating robots that can operate autonomously dates back to classical times, but research into the functionality and potential uses of robots did not grow substantially until the 20th century.","[' What type of robots require user input to operate?', ' When did the concept of creating robots that can operate autonomously date back to?']","['Certain robots', 'classical times']"
3045,robotics,Etymology,"The word robotics was derived from the word robot, which was introduced to the public by Czech writer Karel Čapek in his play R.U.R. (Rossum's Universal Robots), which was published in 1920. The word robot comes from the Slavic word robota, which means work/job. The play begins in a factory that makes artificial people called robots, creatures who can be mistaken for humans – very similar to the modern ideas of androids. Karel Čapek himself did not coin the word. He wrote a short letter in reference to an etymology in the Oxford English Dictionary in which he named his brother Josef Čapek as its actual originator.","The word robotics was derived from the word robot, which was introduced to the public by Czech writer Karel Čapek in his play R.U.R. (Rossum's Universal Robots), which was published in 1920.","[' The word robotics was derived from what word?', ' Who introduced the word robot to the public?', ' When was the play R.U.R. published?']","['robot', 'Karel Čapek', '1920']"
3046,robotics,Etymology,"According to the Oxford English Dictionary, the word robotics was first used in print by Isaac Asimov, in his science fiction short story ""Liar!"", published in May 1941 in Astounding Science Fiction. Asimov was unaware that he was coining the term; since the science and technology of electrical devices is electronics, he assumed robotics already referred to the science and technology of robots. In some of Asimov's other works, he states that the first use of the word robotics was in his short story Runaround (Astounding Science Fiction, March 1942), where he introduced his concept of The Three Laws of Robotics. However, the original publication of ""Liar!"" predates that of ""Runaround"" by ten months, so the former is generally cited as the word's origin.
","According to the Oxford English Dictionary, the word robotics was first used in print by Isaac Asimov, in his science fiction short story ""Liar! "", published in May 1941 in Astounding Science Fiction.","[' Who first used the word robotics in print?', "" What was the name of Isaac Asimov's science fiction short story?"", ' When was ""Liar!"" published?']","['Isaac Asimov', 'Liar!', 'May 1941']"
3047,robotics,"History<span id=""History_of_robotics""></span>","Fully autonomous robots only appeared in the second half of the 20th century. The first digitally operated and programmable robot, the Unimate, was installed in 1961 to lift hot pieces of metal from a die casting machine and stack them. Commercial and industrial robots are widespread today and used to perform jobs more cheaply, more accurately and more reliably, than humans. They are also employed in some jobs which are too dirty, dangerous, or dull to be suitable for humans. Robots are widely used in manufacturing, assembly, packing and packaging, mining, transport, earth and space exploration, surgery, weaponry, laboratory research, safety, and the mass production of consumer and industrial goods.","Fully autonomous robots only appeared in the second half of the 20th century. The first digitally operated and programmable robot, the Unimate, was installed in 1961 to lift hot pieces of metal from a die casting machine and stack them.","[' When did fully autonomous robots first appear?', ' What was the first digitally operated and programmable robot?', ' When was the Unimate installed?']","['second half of the 20th century', 'the Unimate', '1961']"
3048,robotics,Robotic aspects,"There are many types of robots; they are used in many different environments and for many different uses. Although being very diverse in application and form, they all share three basic similarities when it comes to their construction:
","There are many types of robots; they are used in many different environments and for many different uses. Although being very diverse in application and form, they all share three basic similarities when it comes to their construction:","[' How many types of robots are there?', ' How many basic similarities do robots share?']","['many', 'three']"
3049,robotics,Applications,"As more and more robots are designed for specific tasks, this method of classification becomes more relevant. For example, many robots are designed for assembly work, which may not be readily adaptable for other applications. They are termed as ""assembly robots"". For seam welding, some suppliers provide complete welding systems with the robot i.e. the welding equipment along with other material handling facilities like turntables, etc. as an integrated unit. Such an integrated robotic system is called a ""welding robot"" even though its discrete manipulator unit could be adapted to a variety of tasks. Some robots are specifically designed for heavy load manipulation, and are labeled as ""heavy-duty robots"".","As more and more robots are designed for specific tasks, this method of classification becomes more relevant. For example, many robots are designed for assembly work, which may not be readily adaptable for other applications.","[' As more robots are designed for specific tasks, what method of classification becomes more relevant?', ' Many robots designed for assembly work may not be readily adaptable for what?']","['assembly', 'other applications']"
3050,robotics,Control,"The mechanical structure of a robot must be controlled to perform tasks. The control of a robot involves three distinct phases – perception, processing, and action (robotic paradigms). Sensors give information about the environment or the robot itself (e.g. the position of its joints or its end effector). This information is then processed to be stored or transmitted and to calculate the appropriate signals to the actuators (motors) which move the mechanical.
","The mechanical structure of a robot must be controlled to perform tasks. The control of a robot involves three distinct phases – perception, processing, and action (robotic paradigms).","[' What must be controlled to perform tasks?', ' How many distinct phases does the control of a robot involve?', ' What are robot paradigms?']","['The mechanical structure of a robot', 'three', 'perception, processing, and action']"
3051,robotics,Control,"The processing phase can range in complexity. At a reactive level, it may translate raw sensor information directly into actuator commands. Sensor fusion may first be used to estimate parameters of interest (e.g. the position of the robot's gripper) from noisy sensor data. An immediate task (such as moving the gripper in a certain direction) is inferred from these estimates. Techniques from control theory convert the task into commands that drive the actuators.
","The processing phase can range in complexity. At a reactive level, it may translate raw sensor information directly into actuator commands.","[' What can the processing phase range in complexity?', ' At a reactive level, what may translate raw sensor information directly into actuator commands?']","['translate raw sensor information directly into actuator commands', 'The processing phase']"
3052,robotics,Control,"At longer time scales or with more sophisticated tasks, the robot may need to build and reason with a ""cognitive"" model. Cognitive models try to represent the robot, the world, and how they interact. Pattern recognition and computer vision can be used to track objects. Mapping techniques can be used to build maps of the world. Finally, motion planning and other artificial intelligence techniques may be used to figure out how to act. For example, a planner may figure out how to achieve a task without hitting obstacles, falling over, etc.
","At longer time scales or with more sophisticated tasks, the robot may need to build and reason with a ""cognitive"" model. Cognitive models try to represent the robot, the world, and how they interact.","[' What model does a robot need to build and reason with?', ' What do cognitive models try to represent?']","['cognitive', 'the robot, the world, and how they interact']"
3053,robotics,Research,"Much of the research in robotics focuses not on specific industrial tasks, but on investigations into new types of robots, alternative ways to think about or design robots, and new ways to manufacture them. Other investigations, such as MIT's cyberflora project, are almost wholly academic.
","Much of the research in robotics focuses not on specific industrial tasks, but on investigations into new types of robots, alternative ways to think about or design robots, and new ways to manufacture them. Other investigations, such as MIT's cyberflora project, are almost wholly academic.","[' What is the focus of much of the research in robotics?', "" What is MIT's cyberflora project?""]","['investigations into new types of robots', 'almost wholly academic']"
3054,robotics,Research,"A first particular new innovation in robot design is the open sourcing of robot-projects. To describe the level of advancement of a robot, the term ""Generation Robots"" can be used. This term is coined by Professor Hans Moravec, Principal Research Scientist at the Carnegie Mellon University Robotics Institute in describing the near future evolution of robot technology. First generation robots, Moravec predicted in 1997, should have an intellectual capacity comparable to perhaps a lizard and should become available by 2010. Because the first generation robot would be incapable of learning, however, Moravec predicts that the second generation robot would be an improvement over the first and become available by 2020, with the intelligence maybe comparable to that of a mouse. The third generation robot should have the intelligence comparable to that of a monkey. Though fourth generation robots, robots with human intelligence, professor Moravec predicts, would become possible, he does not predict this happening before around 2040 or 2050.","A first particular new innovation in robot design is the open sourcing of robot-projects. To describe the level of advancement of a robot, the term ""Generation Robots"" can be used.","[' What is a first new innovation in robot design?', ' What is the term used to describe the level of advancement of a robot?']","['open sourcing of robot-projects', 'Generation Robots']"
3055,robotics,Research,"The second is evolutionary robots. This is a methodology that uses evolutionary computation to help design robots, especially the body form, or motion and behavior controllers. In a similar way to natural evolution, a large population of robots is allowed to compete in some way, or their ability to perform a task is measured using a fitness function. Those that perform worst are removed from the population and replaced by a new set, which have new behaviors based on those of the winners. Over time the population improves, and eventually a satisfactory robot may appear. This happens without any direct programming of the robots by the researchers. Researchers use this method both to create better robots, and to explore the nature of evolution. Because the process often requires many generations of robots to be simulated, this technique may be run entirely or mostly in simulation, using a robot simulator software package, then tested on real robots once the evolved algorithms are good enough. Currently, there are about 10 million industrial robots toiling around the world, and Japan is the top country having high density of utilizing robots in its manufacturing industry.","The second is evolutionary robots. This is a methodology that uses evolutionary computation to help design robots, especially the body form, or motion and behavior controllers.","[' What is the second type of robot?', ' What does evolutionary robots use to design robots?']","['evolutionary robots', 'evolutionary computation']"
3056,robotics,Education and training,"Robotics engineers design robots, maintain them, develop new applications for them, and conduct research to expand the potential of robotics. Robots have become a popular educational tool in some middle and high schools, particularly in parts of the USA, as well as in numerous youth summer camps, raising interest in programming, artificial intelligence, and robotics among students.
","Robotics engineers design robots, maintain them, develop new applications for them, and conduct research to expand the potential of robotics. Robots have become a popular educational tool in some middle and high schools, particularly in parts of the USA, as well as in numerous youth summer camps, raising interest in programming, artificial intelligence, and robotics among students.","[' What do engineers do with robots?', ' What are robots used for in schools?', ' In what countries have robots become a popular educational tool?', ' What are some of the topics that students are interested in learning about?']","['design robots, maintain them, develop new applications for them, and conduct research', 'educational tool', 'USA', 'programming, artificial intelligence, and robotics']"
3057,robotics,Employment,"Robotics is an essential component in many modern manufacturing environments. As factories increase their use of robots, the number of robotics–related jobs grow and have been observed to be steadily rising. The employment of robots in industries has increased productivity and efficiency savings and is typically seen as a long-term investment for benefactors. A paper by Michael Osborne and Carl Benedikt Frey found that 47 per cent of US jobs are at risk to automation ""over some unspecified number of years"". These claims have been criticized on the ground that social policy, not AI, causes unemployment. In a 2016 article in The Guardian, Stephen Hawking stated ""The automation of factories has already decimated jobs in traditional manufacturing, and the rise of artificial intelligence is likely to extend this job destruction deep into the middle classes, with only the most caring, creative or supervisory roles remaining"".","Robotics is an essential component in many modern manufacturing environments. As factories increase their use of robots, the number of robotics–related jobs grow and have been observed to be steadily rising.","[' What is an essential component in many modern manufacturing environments?', ' As factories increase their use of robots, the number of robotics-related jobs grow and have been observed to be what?']","['Robotics', 'steadily rising']"
3058,robotics,Occupational safety and health implications,"The greatest OSH benefits stemming from the wider use of robotics should be substitution for people working in unhealthy or dangerous environments. In space, defence, security, or the nuclear industry, but also in logistics, maintenance, and inspection, autonomous robots are particularly useful in replacing human workers performing dirty, dull or unsafe tasks, thus avoiding workers' exposures to hazardous agents and conditions and reducing physical, ergonomic and psychosocial risks. For example, robots are already used to perform repetitive and monotonous tasks, to handle radioactive material or to work in explosive atmospheres. In the future, many other highly repetitive, risky or unpleasant tasks will be performed by robots in a variety of sectors like agriculture, construction, transport, healthcare, firefighting or cleaning services.","The greatest OSH benefits stemming from the wider use of robotics should be substitution for people working in unhealthy or dangerous environments. In space, defence, security, or the nuclear industry, but also in logistics, maintenance, and inspection, autonomous robots are particularly useful in replacing human workers performing dirty, dull or unsafe tasks, thus avoiding workers' exposures to hazardous agents and conditions and reducing physical, ergonomic and psychosocial risks.","[' What are the greatest OSH benefits from the use of robotics?', ' What are autonomous robots particularly useful in replacing human workers performing?', ' What is particularly useful in replacing human workers performing dirty, dull or unsafe tasks?', "" What does this prevent workers' exposure to?""]","['substitution for people working in unhealthy or dangerous environments', 'dirty, dull or unsafe tasks', 'autonomous robots', 'hazardous agents and conditions']"
3059,robotics,Occupational safety and health implications,"Moreover, there are certain skills to which humans will be better suited than machines for some time to come and the question is how to achieve the best combination of human and robot skills. The advantages of robotics include heavy-duty jobs with precision and repeatability, whereas the advantages of humans include creativity, decision-making, flexibility, and adaptability. This need to combine optimal skills has resulted in collaborative robots and humans sharing a common workspace more closely and led to the development of new approaches and standards to guarantee the safety of the ""man-robot merger"". Some European countries are including robotics in their national programmes and trying to promote a safe and flexible co-operation between robots and operators to achieve better productivity. For example, the German Federal Institute for Occupational Safety and Health (BAuA) organises annual workshops on the topic ""human-robot collaboration"".
","Moreover, there are certain skills to which humans will be better suited than machines for some time to come and the question is how to achieve the best combination of human and robot skills. The advantages of robotics include heavy-duty jobs with precision and repeatability, whereas the advantages of humans include creativity, decision-making, flexibility, and adaptability.","[' What are some skills to which humans will be better suited than machines for some time to come?', ' The advantages of robotics include heavy-duty jobs with what?', ' What are the advantages of humans?', ' What are some of the advantages of humans?']","['creativity, decision-making, flexibility, and adaptability', 'precision and repeatability', 'creativity, decision-making, flexibility, and adaptability', 'creativity, decision-making, flexibility, and adaptability']"
3060,robotics,Occupational safety and health implications,"In the future, co-operation between robots and humans will be diversified, with robots increasing their autonomy and human-robot collaboration reaching completely new forms. Current approaches and technical standards aiming to protect employees from the risk of working with collaborative robots will have to be revised.
","In the future, co-operation between robots and humans will be diversified, with robots increasing their autonomy and human-robot collaboration reaching completely new forms. Current approaches and technical standards aiming to protect employees from the risk of working with collaborative robots will have to be revised.","[' In the future, how will robots and humans co-operate?', ' What will have to be revised to protect employees from the risk of working with collaborative robots?']","['diversified', 'Current approaches and technical standards']"
3061,face detection,Summary,Face detection is a computer technology being used in a variety of applications that identifies human faces in digital images. Face detection also refers to the psychological process by which humans locate and attend to faces in a visual scene.,Face detection is a computer technology being used in a variety of applications that identifies human faces in digital images. Face detection also refers to the psychological process by which humans locate and attend to faces in a visual scene.,"[' What is a computer technology being used in a variety of applications that identifies human faces in digital images?', ' What does face detection refer to?']","['Face detection', 'the psychological process by which humans locate and attend to faces in a visual scene']"
3062,face detection,Definition and related algorithms,"Face detection can be regarded as a specific case of object-class detection. In object-class detection, the task is to find the locations and sizes of all objects in an image that belong to a given class. Examples include upper torsos, pedestrians, and cars.
Face detection simply answers two question, 1. are there any human faces in the collected images or video? 2. where is the located? 
","Face detection can be regarded as a specific case of object-class detection. In object-class detection, the task is to find the locations and sizes of all objects in an image that belong to a given class.","[' Face detection can be regarded as a specific case of what?', ' In object-class detection, the task is to find the locations and sizes of all objects in what image?']","['object-class detection', 'an image that belong to a given class']"
3063,face detection,Definition and related algorithms,Face-detection algorithms focus on the detection of frontal human faces. It is analogous to image detection in which the image of a person is matched bit by bit. Image matches with the image stores in database. Any facial feature changes in the database will invalidate the matching process.,Face-detection algorithms focus on the detection of frontal human faces. It is analogous to image detection in which the image of a person is matched bit by bit.,"[' What do face-detection algorithms focus on?', ' What is the analogous to image detection in which the image of a person is matched bit by bit?']","['detection of frontal human faces', 'Face-detection algorithms']"
3064,face detection,Definition and related algorithms,"Firstly, the possible human eye regions are detected by testing all the valley regions in the gray-level image. Then the genetic algorithm is used to generate all the possible face regions which include the eyebrows, the iris, the nostril and the mouth corners.","Firstly, the possible human eye regions are detected by testing all the valley regions in the gray-level image. Then the genetic algorithm is used to generate all the possible face regions which include the eyebrows, the iris, the nostril and the mouth corners.","[' What are the possible human eye regions detected by testing?', ' What is used to generate all the possible face regions?']","['all the valley regions in the gray-level image', 'the genetic algorithm']"
3065,face detection,Definition and related algorithms,"Each possible face candidate is normalized to reduce both the lighting effect, which is caused by uneven illumination; and the shirring effect, which is due to head movement. The fitness value of each candidate is measured based on its projection on the eigen-faces.  After a number of iterations, all the face candidates with a high fitness value are selected for further verification. At this stage, the face symmetry is measured and the existence of the different facial features is verified for each face candidate.","Each possible face candidate is normalized to reduce both the lighting effect, which is caused by uneven illumination; and the shirring effect, which is due to head movement. The fitness value of each candidate is measured based on its projection on the eigen-faces.","[' What is caused by uneven illumination?', ' What is the cause of the shirring effect?', ' How is the fitness value of each candidate measured?']","['lighting effect', 'head movement', 'based on its projection on the eigen-faces']"
3066,heuristics,Summary,"A heuristic (; from Ancient Greek  εὑρίσκω (heurískō) 'I find, discover'), or heuristic technique, is any approach to problem solving or self-discovery that employs a practical method that is not guaranteed to be optimal, perfect, or rational, but is nevertheless sufficient for reaching an immediate, short-term goal or approximation. Where finding an optimal solution is impossible or impractical, heuristic methods can be used to speed up the process of finding a satisfactory solution. Heuristics can be mental shortcuts that ease the cognitive load of making a decision.","A heuristic (; from Ancient Greek  εὑρίσκω (heurískō) 'I find, discover'), or heuristic technique, is any approach to problem solving or self-discovery that employs a practical method that is not guaranteed to be optimal, perfect, or rational, but is nevertheless sufficient for reaching an immediate, short-term goal or approximation. Where finding an optimal solution is impossible or impractical, heuristic methods can be used to speed up the process of finding a satisfactory solution.","[' What is an approach to problem solving or self-discovery that employs a practical method that is not guaranteed to be optimal, perfect, or rational?', ' What can be used when finding an optimal solution is impossible or impractical?', ' What can speed up the process of finding a satisfactory solution?']","['heuristic', 'heuristic methods', 'heuristic methods']"
3067,heuristics,Summary,"Heuristics are the strategies derived from previous experiences with similar problems. These strategies depend on using readily accessible, though loosely applicable, information to control problem solving in human beings, machines and abstract issues. When an individual applies a heuristic in practice, it generally performs as expected. However it can alternatively create systematic errors.","Heuristics are the strategies derived from previous experiences with similar problems. These strategies depend on using readily accessible, though loosely applicable, information to control problem solving in human beings, machines and abstract issues.","[' What are the strategies derived from previous experiences with similar problems?', ' What do heuristics depend on using readily accessible, loosely applicable information to control?']","['Heuristics', 'problem solving']"
3068,heuristics,Summary,"
The most fundamental heuristic is trial and error, which can be used in everything from matching nuts and bolts to finding the values of variables in algebra problems. In mathematics, some common heuristics involve the use of visual representations, additional assumptions, forward/backward reasoning and simplification. Here are a few commonly used heuristics from George Pólya's 1945 book, How to Solve It:","
The most fundamental heuristic is trial and error, which can be used in everything from matching nuts and bolts to finding the values of variables in algebra problems. In mathematics, some common heuristics involve the use of visual representations, additional assumptions, forward/backward reasoning and simplification.","[' What is the most fundamental heuristic?', ' What can be used in everything from matching nuts and bolts to finding values of variables in algebra problems?']","['trial and error', 'trial and error']"
3069,heuristics,Summary,"In psychology, heuristics are simple, efficient rules, learned or inculcated by evolutionary processes, that have been proposed to explain how people make decisions, come to judgements, and solve problems typically when facing complex problems or incomplete information. Researchers test if people use those rules with various methods. These rules work well under most circumstances, but in certain cases can lead to systematic errors or cognitive biases.","In psychology, heuristics are simple, efficient rules, learned or inculcated by evolutionary processes, that have been proposed to explain how people make decisions, come to judgements, and solve problems typically when facing complex problems or incomplete information. Researchers test if people use those rules with various methods.","[' What are heuristics in psychology?', ' What are simple, efficient rules, learned or inculcated by evolutionary processes, that have been proposed to explain?']","['simple, efficient rules', 'heuristics']"
3070,heuristics,History,"The study of heuristics in human decision-making was developed in the 1970s and the 1980s by the psychologists Amos Tversky and Daniel Kahneman although the concept had been originally introduced by the Nobel laureate Herbert A. Simon, whose original, primary object of research was problem solving that showed that we operate within what he calls bounded rationality. He coined the term satisficing, which denotes a situation in which people seek solutions, or accept choices or judgements, that are ""good enough"" for their purposes although they could be optimised.","The study of heuristics in human decision-making was developed in the 1970s and the 1980s by the psychologists Amos Tversky and Daniel Kahneman although the concept had been originally introduced by the Nobel laureate Herbert A. Simon, whose original, primary object of research was problem solving that showed that we operate within what he calls bounded rationality. He coined the term satisficing, which denotes a situation in which people seek solutions, or accept choices or judgements, that are ""good enough"" for their purposes although they could be optimised.","[' Who developed the study of heuristics in human decision-making in the 1970s and 1980s?', ' Who was the first person to introduce the concept?', "" What was Simon's primary object of research?"", ' What was the object of his research?', ' What does satisficing denote?']","['Amos Tversky and Daniel Kahneman', 'Herbert A. Simon', 'problem solving', 'problem solving', 'a situation in which people seek solutions, or accept choices or judgements, that are ""good enough"" for their purposes although they could be optimised']"
3071,heuristics,Philosophy,"A good example is a model that, as it is never identical with what it models, is a heuristic device to enable understanding of what it models. Stories, metaphors, etc., can also be termed heuristic in this sense. A classic example is the notion of utopia as described in Plato's best-known work, The Republic. This means that the ""ideal city"" as depicted in The Republic is not given as something to be pursued, or to present an orientation-point for development. Rather, it shows how things would have to be connected, and how one thing would lead to another (often with highly problematic results), if one opted for certain principles and carried them through rigorously.
","A good example is a model that, as it is never identical with what it models, is a heuristic device to enable understanding of what it models. Stories, metaphors, etc., can also be termed heuristic in this sense.","[' What is a good example of a model that is never identical with what it models?', ' What can also be termed heuristic in this sense?']","['a heuristic device', 'Stories, metaphors']"
3072,heuristics,Philosophy,"Heuristic is also often used as a noun to describe a rule-of-thumb, procedure, or method. Philosophers of science have emphasised the importance of heuristics in creative thought and the construction of scientific theories. (See The Logic of Scientific Discovery by Karl Popper; and philosophers such as Imre Lakatos, Lindley Darden, William C. Wimsatt and others.)
","Heuristic is also often used as a noun to describe a rule-of-thumb, procedure, or method. Philosophers of science have emphasised the importance of heuristics in creative thought and the construction of scientific theories.","[' Heuristic is often used as a noun to describe what?', ' Philosophers of science have emphasised the importance of heuristics in creative thought and what else?']","['a rule-of-thumb, procedure, or method', 'the construction of scientific theories']"
3073,heuristics,Law,"The present securities regulation regime largely assumes that all investors act as perfectly rational persons.
In truth, actual investors face cognitive limitations from biases, heuristics, and framing effects. For instance, in all states in the United States the legal drinking age for unsupervised persons is 21 years, because it is argued that people need to be mature enough to make decisions involving the risks of alcohol consumption. However, assuming people mature at different rates, the specific age of 21 would be too late for some and too early for others. In this case, the somewhat arbitrary deadline is used because it is impossible or impractical to tell whether an individual is sufficiently mature for society to trust them with that kind of responsibility. Some proposed changes, however, have included the completion of an alcohol education course rather than the attainment of 21 years of age as the criterion for legal alcohol possession. This would put youth alcohol policy more on a case-by-case basis and less on a heuristic one, since the completion of such a course would presumably be voluntary and not uniform across the population.
","The present securities regulation regime largely assumes that all investors act as perfectly rational persons. In truth, actual investors face cognitive limitations from biases, heuristics, and framing effects.","[' What assumes that all investors act as perfectly rational persons?', ' What do actual investors face cognitive limitations from?']","['securities regulation regime', 'biases, heuristics, and framing effects']"
3074,heuristics,Law,"The same reasoning applies to patent law. Patents are justified on the grounds that inventors must be protected so they have incentive to invent. It is therefore argued that it is in society's best interest that inventors receive a temporary government-granted monopoly on their idea, so that they can recoup investment costs and make economic profit for a limited period. In the United States, the length of this temporary monopoly is 20 years from the date the patent application was filed, though the monopoly does not actually begin until the application has matured into a patent. However, like the drinking-age problem above, the specific length of time would need to be different for every product to be efficient. A 20-year term is used because it is difficult to tell what the number should be for any individual patent. More recently, some, including University of North Dakota law professor Eric E. Johnson, have argued that patents in different kinds of industries – such as software patents – should be protected for different lengths of time.",The same reasoning applies to patent law. Patents are justified on the grounds that inventors must be protected so they have incentive to invent.,[' What are patents justified on the grounds that inventors must be protected so they have incentive to invent?'],['patent law']
3075,heuristics,Stereotyping,"Stereotyping is a type of heuristic that people use to form opinions or make judgements about things they have never seen or experienced. They work as a mental shortcut to assess everything from the social status of a person (based on their actions), to whether a plant is a tree based on the assumption that it is tall, has a trunk and has leaves (even though the person making the evaluation might never have seen that particular type of tree before).
","Stereotyping is a type of heuristic that people use to form opinions or make judgements about things they have never seen or experienced. They work as a mental shortcut to assess everything from the social status of a person (based on their actions), to whether a plant is a tree based on the assumption that it is tall, has a trunk and has leaves (even though the person making the evaluation might never have seen that particular type of tree before).","[' What is a type of heuristic that people use to form opinions or make judgements about things they have never seen or experienced?', ' Stereotyping works as a mental shortcut to assess everything from the social status of a person to what?', ' What does an evaluation of a plant depend on?', ' What does the person making the evaluation assume a tree is?']","['Stereotyping', 'whether a plant is a tree', 'assumption that it is tall, has a trunk and has leaves', 'tall, has a trunk and has leaves']"
3076,heuristics,Artificial intelligence,"A heuristic can be used in artificial intelligence systems while searching a solution space. The heuristic is derived by using some function that is put into the system by the designer, or by adjusting the weight of branches based on how likely each branch is to lead to a goal node.
","A heuristic can be used in artificial intelligence systems while searching a solution space. The heuristic is derived by using some function that is put into the system by the designer, or by adjusting the weight of branches based on how likely each branch is to lead to a goal node.","[' What can be used in artificial intelligence systems while searching a solution space?', ' What is derived by using some function that is put into the system by the designer?', ' How is the heuristic derived?', ' How likely is each branch to lead to a goal node?']","['A heuristic', 'The heuristic', 'by using some function that is put into the system by the designer', 'weight']"
3077,smart grid,Summary,"Smart grid policy is organized in Europe as Smart Grid European Technology Platform. Policy in the United States is described in 42 U.S.C. ch. 152, subch. IX § 17381.
",Smart grid policy is organized in Europe as Smart Grid European Technology Platform. Policy in the United States is described in 42 U.S.C.,"[' What is Smart Grid European Technology Platform organized as?', ' What is the policy in the United States described in?']","['Smart grid policy', '42 U.S.C']"
3078,smart grid,Features,"The smart grid represents the full suite of current and proposed responses to the challenges of electricity supply. Because of the diverse range of factors there are numerous competing taxonomies and no agreement on a universal definition. Nevertheless, one possible categorization is given here.
",The smart grid represents the full suite of current and proposed responses to the challenges of electricity supply. Because of the diverse range of factors there are numerous competing taxonomies and no agreement on a universal definition.,"[' What represents the full suite of current and proposed responses to the challenges of electricity supply?', ' There are numerous competing taxonomies and no agreement on what?']","['The smart grid', 'a universal definition']"
3079,smart grid,Oppositions and concerns,"Most opposition and concerns have centered on smart meters and the items (such as remote control, remote disconnect, and variable rate pricing) enabled by them.  Where opposition to smart meters is encountered, they are often marketed as ""smart grid"" which connects smart grid to smart meters in the eyes of opponents.  Specific points of opposition or concern include:
","Most opposition and concerns have centered on smart meters and the items (such as remote control, remote disconnect, and variable rate pricing) enabled by them. Where opposition to smart meters is encountered, they are often marketed as ""smart grid"" which connects smart grid to smart meters in the eyes of opponents.","[' What do most opposition and concerns focus on?', ' What is the opposition to smart meters often marketed as?', ' What does smart grid connect to in the eyes of opponents?']","['smart meters', 'smart grid', 'smart meters']"
3080,smart grid,Other challenges to adoption,"Before a utility installs an advanced metering system, or any type of smart system, it must make a business case for the investment. Some components, like the power system stabilizers (PSS)  installed on generators are very expensive, require complex integration in the grid's control system, are needed only during emergencies, and are only effective if other suppliers on the network have them. Without any incentive to install them, power suppliers don't. Most utilities find it difficult to justify installing a communications infrastructure for a single application (e.g. meter reading). Because of this, a utility must typically identify several applications that will use the same communications infrastructure – for example, reading a meter, monitoring power quality, remote connection and disconnection of customers, enabling demand response, etc. Ideally, the communications infrastructure will not only support near-term applications, but unanticipated applications that will arise in the future. Regulatory or legislative actions can also drive utilities to implement pieces of a smart grid puzzle. Each utility has a unique set of business, regulatory, and legislative drivers that guide its investments. This means that each utility will take a different path to creating their smart grid and that different utilities will create smart grids at different adoption rates.","Before a utility installs an advanced metering system, or any type of smart system, it must make a business case for the investment. Some components, like the power system stabilizers (PSS)  installed on generators are very expensive, require complex integration in the grid's control system, are needed only during emergencies, and are only effective if other suppliers on the network have them.","[' What must a utility make before installing an advanced metering system?', ' What are power system stabilizers called?', "" What is the only time when the grid's control system is needed?""]","['a business case for the investment', 'PSS', 'emergencies']"
3081,smart grid,Other challenges to adoption,"Some features of smart grids draw opposition from industries that currently are, or hope to provide similar services. An example is competition with cable and DSL Internet providers from broadband over powerline internet access. Providers of SCADA control systems for grids have intentionally designed proprietary hardware, protocols and software so that they cannot inter-operate with other systems in order to tie its customers to the vendor.","Some features of smart grids draw opposition from industries that currently are, or hope to provide similar services. An example is competition with cable and DSL Internet providers from broadband over powerline internet access.","[' Some features of smart grids draw opposition from industries that currently are or hope to provide what services?', ' What is an example of competition with cable and DSL Internet providers from broadband over powerline internet access?']","['similar', 'smart grids']"
3082,smart grid,Other challenges to adoption,"The incorporation of digital communications and computer infrastructure with the grid's existing physical infrastructure poses challenges and inherent vulnerabilities. According to IEEE Security and Privacy Magazine, the smart grid will require that people develop and use large computer and communication infrastructure that supports a greater degree of situational awareness and that allows for more specific command and control operations. This process is necessary to support major systems such as demand-response wide-area measurement and control, storage and transportation of electricity, and the automation of electric distribution.","The incorporation of digital communications and computer infrastructure with the grid's existing physical infrastructure poses challenges and inherent vulnerabilities. According to IEEE Security and Privacy Magazine, the smart grid will require that people develop and use large computer and communication infrastructure that supports a greater degree of situational awareness and that allows for more specific command and control operations.","[' What will the smart grid require of people?', ' What does the IEEE Security and Privacy Magazine say the grid will require?', ' Infrastructure that supports a greater degree of situational awareness and that allows for more specific command and control operations?']","['develop and use large computer and communication infrastructure', 'people develop and use large computer and communication infrastructure', 'large computer and communication infrastructure']"
3083,smart grid,"Guidelines, standards and user groups","Part of the IEEE Smart Grid Initiative, IEEE 2030.2 represents an extension of the work aimed at utility storage systems for transmission and distribution networks. The IEEE P2030 group expects to deliver early 2011 an overarching set of guidelines on smart grid interfaces. The new guidelines will cover areas including batteries and supercapacitors as well as flywheels. The group has also spun out a 2030.1 effort drafting guidelines for integrating electric vehicles into the smart grid.
","Part of the IEEE Smart Grid Initiative, IEEE 2030.2 represents an extension of the work aimed at utility storage systems for transmission and distribution networks. The IEEE P2030 group expects to deliver early 2011 an overarching set of guidelines on smart grid interfaces.","[' What is part of the IEEE Smart Grid Initiative?', ' What is IEEE 2030.2 an extension of?', ' When does the IEEE P2030 group expect to deliver guidelines on smart grid interfaces?']","['IEEE 2030.2', 'IEEE Smart Grid Initiative', 'early 2011']"
3084,smart grid,"Guidelines, standards and user groups","IEC TC 57 has created a family of international standards that can be used as part of the smart grid. These standards include IEC 61850 which is an architecture for substation automation, and IEC 61970/61968 – the Common Information Model (CIM). The CIM provides for common semantics to be used for turning data into information.
","IEC TC 57 has created a family of international standards that can be used as part of the smart grid. These standards include IEC 61850 which is an architecture for substation automation, and IEC 61970/61968 – the Common Information Model (CIM).","[' IEC TC 57 has created a family of international standards that can be used as part of what?', ' What is an architecture for substation automation?', ' IEC 61970/61968 - the Common Information Model (CIM)?']","['the smart grid', 'IEC 61850', 'IEC 61850']"
3085,smart grid,"Guidelines, standards and user groups","OpenADR is an open-source smart grid communications standard used for demand response applications.
It is typically used to send information and signals to cause electrical power-using devices to be turned off during periods of higher demand.
",OpenADR is an open-source smart grid communications standard used for demand response applications. It is typically used to send information and signals to cause electrical power-using devices to be turned off during periods of higher demand.,"[' What is the name of the open-source smart grid communications standard?', ' What is OpenADR used for?']","['OpenADR', 'demand response applications']"
3086,smart grid,"Guidelines, standards and user groups","MultiSpeak has created a specification that supports distribution functionality of the smart grid. MultiSpeak has a robust set of integration definitions that supports nearly all of the software interfaces necessary for a distribution utility or for the distribution portion of a vertically integrated utility. MultiSpeak integration is defined using extensible markup language (XML) and web services.
",MultiSpeak has created a specification that supports distribution functionality of the smart grid. MultiSpeak has a robust set of integration definitions that supports nearly all of the software interfaces necessary for a distribution utility or for the distribution portion of a vertically integrated utility.,"[' MultiSpeak has created a specification that supports what kind of functionality?', "" What is supported by multiSpeak's set of integration definitions?""]","['distribution', 'nearly all of the software interfaces']"
3087,smart grid,"Guidelines, standards and user groups","NIST has included ITU-T G.hn as one of the ""Standards Identified for Implementation"" for the Smart Grid ""for which it believed there
was strong stakeholder consensus"". G.hn is standard for high-speed communications over power lines, phone lines and coaxial cables.
","NIST has included ITU-T G.hn as one of the ""Standards Identified for Implementation"" for the Smart Grid ""for which it believed there
was strong stakeholder consensus"". G.hn is standard for high-speed communications over power lines, phone lines and coaxial cables.","[' What is the standard for high-speed communications over power lines, phone lines and coaxial cables?', ' What is G.hn standard for?']","['ITU-T G.hn', 'high-speed communications over power lines, phone lines and coaxial cables']"
3088,smart grid,"Guidelines, standards and user groups","OASIS EnergyInterop' – An OASIS technical committee developing XML standards for energy interoperation. Its starting point is the California OpenADR standard.
",OASIS EnergyInterop' – An OASIS technical committee developing XML standards for energy interoperation. Its starting point is the California OpenADR standard.,[' What is the name of the OASIS technical committee that develops standards for energy interoperation?'],['OASIS EnergyInterop']
3089,smart grid,"Guidelines, standards and user groups","Under the Energy Independence and Security Act of 2007 (EISA), NIST is charged with overseeing the identification and selection of hundreds of standards that will be required to implement the Smart Grid in the U.S.  These standards will be referred by NIST to the Federal Energy Regulatory Commission (FERC).  This work has begun, and the first standards have already been selected for inclusion in NIST's Smart Grid catalog.  However, some commentators have suggested that the benefits that could be realized from Smart Grid standardization could be threatened by a growing number of patents that cover Smart Grid architecture and technologies.  If patents that cover standardized Smart Grid elements are not revealed until technology is broadly distributed throughout the network (""locked-in""), significant disruption could occur when patent holders seek to collect unanticipated rents from large segments of the market.
","Under the Energy Independence and Security Act of 2007 (EISA), NIST is charged with overseeing the identification and selection of hundreds of standards that will be required to implement the Smart Grid in the U.S. These standards will be referred by NIST to the Federal Energy Regulatory Commission (FERC).","[' Under what act is NIST charged with overseeing the identification and selection of hundreds of standards that will be required to implement the Smart Grid in the U.S.?', ' What agency will NIST refer the standards to?']","['Energy Independence and Security Act of 2007', 'Federal Energy Regulatory Commission']"
3090,smart grid,GridWise Alliance rankings,"In November 2017 the non-profit GridWise Alliance along with Clean Edge Inc., a clean energy group, released rankings for all 50 states in their efforts to modernize the electric grid. California was ranked number one. The other top states were Illinois, Texas, Maryland, Oregon, Arizona, the District of Columbia, New York, Nevada and Delaware. ""The 30-plus page report from the GridWise Alliance, which represents stakeholders that design, build and operate the electric grid, takes a deep dive into grid modernization efforts across the country and ranks them by state.""","In November 2017 the non-profit GridWise Alliance along with Clean Edge Inc., a clean energy group, released rankings for all 50 states in their efforts to modernize the electric grid. California was ranked number one.","[' When did GridWise Alliance and Clean Edge Inc. release rankings for all 50 states?', ' Which state was ranked number one in their efforts to modernize the electric grid?']","['November 2017', 'California']"
3091,multi-agent system,Summary,"A multi-agent system (MAS or ""self-organized system"") is a computerized system composed of multiple interacting intelligent agents. Multi-agent systems can solve problems that are difficult or impossible for an individual agent or a monolithic system to solve. Intelligence may include methodic, functional, procedural approaches, algorithmic search or reinforcement learning.","A multi-agent system (MAS or ""self-organized system"") is a computerized system composed of multiple interacting intelligent agents. Multi-agent systems can solve problems that are difficult or impossible for an individual agent or a monolithic system to solve.","[' What does MAS stand for?', ' What is a multi-agent system composed of?']","['multi-agent system', 'multiple interacting intelligent agents']"
3092,multi-agent system,Summary,"Despite considerable overlap, a multi-agent system is not always the same as an agent-based model (ABM).  The goal of an ABM is to search for explanatory insight into the collective behavior of agents (which don't necessarily need to be ""intelligent"") obeying simple rules, typically in natural systems, rather than in solving specific practical or engineering problems. The terminology of ABM tends to be used more often in the science, and MAS in engineering and technology. Applications where multi-agent systems research may deliver an appropriate approach include online trading, disaster response, target surveillance   and social structure modelling.","Despite considerable overlap, a multi-agent system is not always the same as an agent-based model (ABM). The goal of an ABM is to search for explanatory insight into the collective behavior of agents (which don't necessarily need to be ""intelligent"") obeying simple rules, typically in natural systems, rather than in solving specific practical or engineering problems.","[' A multi-agent system is not always the same as what?', ' What is the goal of an ABM?', ' ABM is to search for explanatory insight into the collective behavior of agents.', ' What is the most common way to obey simple rules?', ' What is more common in natural systems?']","['an agent-based model', 'to search for explanatory insight into the collective behavior of agents', 'agent-based model', 'natural systems', 'simple rules']"
3093,multi-agent system,Concept,"Multi-agent systems consist of agents and their environment. Typically multi-agent systems research refers to software agents. However, the agents in a multi-agent system could equally well be robots, humans or human teams. A multi-agent system may contain combined human-agent teams.
",Multi-agent systems consist of agents and their environment. Typically multi-agent systems research refers to software agents.,"[' What do multi-agent systems consist of?', ' What does research refer to?']","['agents and their environment', 'software agents']"
3094,multi-agent system,Concept,"Agents can be divided into types spanning simple to complex. Categories include:
",Agents can be divided into types spanning simple to complex. Categories include:,"[' What can agents be divided into?', ' What types of agents are there?']","['simple to complex', 'simple to complex']"
3095,multi-agent system,Concept,"Agent environments can also be organized according to properties such as accessibility (whether it is possible to gather complete information about the environment), determinism (whether an action causes a definite effect), dynamics (how many entities influence the environment in the moment), discreteness (whether the number of possible actions in the environment is finite), episodicity (whether agent actions in certain time periods influence other periods), and dimensionality (whether spatial characteristics are important factors of the environment and the agent considers space in its decision making). Agent actions are typically mediated via an appropriate middleware. This middleware offers a first-class design abstraction for multi-agent systems, providing means to govern resource access and agent coordination.","Agent environments can also be organized according to properties such as accessibility (whether it is possible to gather complete information about the environment), determinism (whether an action causes a definite effect), dynamics (how many entities influence the environment in the moment), discreteness (whether the number of possible actions in the environment is finite), episodicity (whether agent actions in certain time periods influence other periods), and dimensionality (whether spatial characteristics are important factors of the environment and the agent considers space in its decision making). Agent actions are typically mediated via an appropriate middleware.","[' What is accessibility?', ' What is determinism?', ' How many entities influence the environment in the moment?', ' What is discreteness?', ' What is episodicity?', ' How are agent actions typically mediated?', ' What type of middleware is used?']","['whether it is possible to gather complete information about the environment', 'whether an action causes a definite effect', 'dynamics (how many', 'whether the number of possible actions in the environment is finite', 'whether agent actions in certain time periods influence other periods', 'via an appropriate middleware', 'appropriate']"
3096,multi-agent system,Research,"The study of multi-agent systems is ""concerned with the development and analysis of sophisticated AI problem-solving and control architectures for both single-agent and multiple-agent systems."" Research topics include:
","The study of multi-agent systems is ""concerned with the development and analysis of sophisticated AI problem-solving and control architectures for both single-agent and multiple-agent systems."" Research topics include:","[' What is the study of multi-agent systems concerned with?', ' What is one of the research topics?']","['the development and analysis of sophisticated AI problem-solving and control architectures for both single-agent and multiple-agent systems', 'multi-agent systems']"
3097,multi-agent system,Frameworks,"Frameworks have emerged that implement common standards (such as the FIPA and OMG MASIF standards). These frameworks e.g. JADE, save time and aid in the standardization of MAS development.",Frameworks have emerged that implement common standards (such as the FIPA and OMG MASIF standards). These frameworks e.g.,"[' What are examples of common standards?', ' What are two common standards implemented by frameworks?']","['FIPA and OMG MASIF standards', 'FIPA and OMG MASIF standards']"
3098,multi-agent system,Frameworks,"Currently though, no standard is actively maintained from FIPA or OMG. Efforts for further development of software agents in industrial context are carried out in IEEE IES technical committee on Industrial Agents.","Currently though, no standard is actively maintained from FIPA or OMG. Efforts for further development of software agents in industrial context are carried out in IEEE IES technical committee on Industrial Agents.","[' What is the current status of FIPA or OMG?', ' Who is responsible for further development of software agents in industrial context?']","['no standard is actively maintained', 'IEEE IES technical committee on Industrial Agents']"
3099,multi-agent system,Applications,"MAS have not only been applied in academic research, but also in industry. MAS are applied in the real world to graphical applications such as computer games. Agent systems have been used in films. It is widely advocated for use in networking and mobile technologies, to achieve automatic and dynamic load balancing, high scalability and self-healing networks. They are being used for coordinated defence systems.
","MAS have not only been applied in academic research, but also in industry. MAS are applied in the real world to graphical applications such as computer games.","[' MAS have not only been applied in academic research but also in what industry?', ' MAS are applied in the real world to what kind of applications?']","['industry', 'graphical']"
3100,multi-agent system,Applications,"Also, Multi-agent Systems Artificial Intelligence (MAAI) are used for simulating societies, the purpose thereof being helpful in the fields of climate, energy, epidemiology, conflict management, child abuse, .... Some organisations working on using multi-agent system models include Center for Modelling Social Systems, Centre for Research in Social Simulation, Centre for Policy Modelling, Society for Modelling and Simulation International. Hallerbach et al. discussed the application of agent-based approaches for the development and validation of automated driving systems via a digital twin of the vehicle-under-test and microscopic traffic simulation based on independent agents. Waymo has created a multi-agent simulation environment Carcraft to test algorithms for self-driving cars. It simulates traffic interactions between human drivers, pedestrians and automated vehicles. People's behavior is imitated by artificial agents based on data of real human behavior.
","Also, Multi-agent Systems Artificial Intelligence (MAAI) are used for simulating societies, the purpose thereof being helpful in the fields of climate, energy, epidemiology, conflict management, child abuse, .... Some organisations working on using multi-agent system models include Center for Modelling Social Systems, Centre for Research in Social Simulation, Centre for Policy Modelling, Society for Modelling and Simulation International.","[' What are Multi-agent Systems Artificial Intelligence (MAAI) used for?', ' What is the purpose of MAAI?', ' Centre for Research in Social Simulation, Centre for Policy Modelling, Society for Modelling and Simulation International.']","['simulating societies', 'simulating societies', 'multi-agent system models']"
3101,optimal control,Summary,"Optimal control theory is a branch of mathematical optimization that deals with finding a control for a dynamical system over a period of time such that an objective function is optimized. It has numerous applications in science, engineering and operations research. For example, the dynamical system might be a spacecraft with controls corresponding to rocket thrusters, and the objective might be to reach the moon with minimum fuel expenditure. Or the dynamical system could be a nation's economy, with the objective to minimize unemployment; the controls in this case could be fiscal and monetary policy. A dynamical system may also be introduced to embed operations research problems within the framework of optimal control theory.","Optimal control theory is a branch of mathematical optimization that deals with finding a control for a dynamical system over a period of time such that an objective function is optimized. It has numerous applications in science, engineering and operations research.",[' Optimal control theory deals with finding a control for a dynamical system over a period of time such that an objective function is optimized?'],['optimization']
3102,optimal control,Summary,"Optimal control is an extension of the calculus of variations, and is a mathematical optimization method for deriving control policies. The method is largely due to the work of Lev Pontryagin and Richard Bellman in the 1950s, after contributions to calculus of variations by Edward J. McShane. Optimal control can be seen as a control strategy in control theory.","Optimal control is an extension of the calculus of variations, and is a mathematical optimization method for deriving control policies. The method is largely due to the work of Lev Pontryagin and Richard Bellman in the 1950s, after contributions to calculus of variations by Edward J. McShane.","[' What is an extension of the calculus of variations?', ' What is a mathematical optimization method for deriving control policies?', ' Who contributed to the method of optimal control in the 1950s?']","['Optimal control', 'Optimal control', 'Lev Pontryagin and Richard Bellman']"
3103,optimal control,General method,"Optimal control deals with the problem of finding a control law for a given system such that a certain optimality criterion is achieved. A control problem includes a cost functional that is a function of state and control variables. An optimal control is a set of differential equations describing the paths of the control variables that minimize the cost function. The optimal control can be derived using Pontryagin's maximum principle (a necessary condition also known as Pontryagin's minimum principle or simply Pontryagin's principle), or by solving the Hamilton–Jacobi–Bellman equation (a sufficient condition).
",Optimal control deals with the problem of finding a control law for a given system such that a certain optimality criterion is achieved. A control problem includes a cost functional that is a function of state and control variables.,"[' Optimal control deals with the problem of finding a control law for a given system such that a certain optimality criterion is achieved?', ' A control problem includes a cost functional that is a function of what?']","['Optimal control deals with the problem of finding a control law for a given system such that a certain optimality criterion is achieved.', 'state and control variables']"
3104,optimal control,General method,"We begin with a simple example. Consider a car traveling in a straight line on a hilly road. The question is, how should the driver press the accelerator pedal in order to minimize the total traveling time? In this example, the term control law refers specifically to the way in which the driver presses the accelerator and shifts the gears. The system consists of both the car and the road, and the optimality criterion is the minimization of the total traveling time. Control problems usually include ancillary constraints. For example, the amount of available fuel might be limited, the accelerator pedal cannot be pushed through the floor of the car, speed limits, etc.
",We begin with a simple example. Consider a car traveling in a straight line on a hilly road.,[' What is an example of a car traveling in a straight line on a hilly road?'],['simple']
3105,optimal control,General method,"A proper cost function will be a mathematical expression giving the traveling time as a function of the speed, geometrical considerations, and initial conditions of the system. Constraints are often interchangeable with the cost function.
","A proper cost function will be a mathematical expression giving the traveling time as a function of the speed, geometrical considerations, and initial conditions of the system. Constraints are often interchangeable with the cost function.","[' A proper cost function will be a mathematical expression giving the traveling time as a function of what?', ' Constraints are often interchangeable with the cost function.']","['the speed, geometrical considerations, and initial conditions of the system', 'A proper cost function']"
3106,optimal control,General method,"Another related optimal control problem may be to find the way to drive the car so as to minimize its fuel consumption, given that it must complete a given course in a time not exceeding some amount. Yet another related control problem may be to minimize the total monetary cost of completing the trip, given assumed monetary prices for time and fuel.
","Another related optimal control problem may be to find the way to drive the car so as to minimize its fuel consumption, given that it must complete a given course in a time not exceeding some amount. Yet another related control problem may be to minimize the total monetary cost of completing the trip, given assumed monetary prices for time and fuel.","[' What may be a related optimal control problem?', ' What must a car complete a given course in a time not exceeding some amount?', ' What may be a control problem?', ' What are assumed monetary prices for time and fuel?']","['to find the way to drive the car', 'fuel consumption', 'to minimize the total monetary cost of completing the trip', 'monetary cost of completing the trip']"
3107,optimal control,General method,"A more abstract framework goes as follows.  Minimize the continuous-time cost functional
",A more abstract framework goes as follows. Minimize the continuous-time cost functional,[' A more abstract framework goes as follows: Minimize the continuous-time cost functional?'],['.']
3108,optimal control,General method,"where 





x


(
t
)


{\displaystyle {\textbf {x}}(t)}
 is the state, 





u


(
t
)


{\displaystyle {\textbf {u}}(t)}
 is the control, 



t


{\displaystyle t}
 is the independent variable (generally speaking, time), 




t

0




{\displaystyle t_{0}}
 is the initial time, and 




t

f




{\displaystyle t_{f}}
 is the terminal time.  The terms 



E


{\displaystyle E}
 and 



F


{\displaystyle F}
 are called the endpoint cost  and the running cost respectively. In the calculus of variations, 



E


{\displaystyle E}
 and 



F


{\displaystyle F}
 are referred to as the Mayer term and the Lagrangian, respectively.  Furthermore, it is noted that the path constraints are in general inequality constraints and thus may not be active (i.e., equal to zero) at the optimal solution.  It is also noted that the optimal control problem as stated above may have multiple solutions (i.e., the solution may not be unique).  Thus, it is most often the case that any solution 



[



x



∗


(
t
)
,



u



∗


(
t
)
,

t

0


∗


,

t

f


∗


]


{\displaystyle [{\textbf {x}}^{*}(t),{\textbf {u}}^{*}(t),t_{0}^{*},t_{f}^{*}]}
 to the optimal control problem is locally minimizing.
","where 





x


(
t
)


{\displaystyle {\textbf {x}}(t)}
 is the state, 





u


(
t
)


{\displaystyle {\textbf {u}}(t)}
 is the control, 



t


{\displaystyle t}
 is the independent variable (generally speaking, time), 




t

0




{\displaystyle t_{0}}
 is the initial time, and 




t

f




{\displaystyle t_{f}}
 is the terminal time. The terms 



E


{\displaystyle E}
 and 



F


{\displaystyle F}
 are called the endpoint cost  and the running cost respectively.","[' What is the state of the display?', ' What is u ( t ) <unk>displaystyle <unk>textbf <unk>u<unk>(t)<unk>?', ' The term ""endpoint cost"" and ""f"" are called what?', ' What are the two terms for the endpoint cost and the running cost?']","['textbf', 'control', 'the running cost', 'E\n\n\n{\\displaystyle E}\n and \n\n\n\nF']"
3109,optimal control,Linear quadratic control,"A special case of the general nonlinear optimal control problem given in the previous section is the linear quadratic (LQ) optimal control problem.  The LQ problem is stated as follows.  Minimize the quadratic continuous-time cost functional
",A special case of the general nonlinear optimal control problem given in the previous section is the linear quadratic (LQ) optimal control problem. The LQ problem is stated as follows.,[' What is a special case of the general nonlinear optimal control problem?'],['linear quadratic (LQ) optimal control problem']
3110,optimal control,Linear quadratic control,"A particular form of the LQ problem that arises in many control system problems is that of the linear quadratic regulator (LQR) where all of the matrices (i.e., 




A



{\displaystyle \mathbf {A} }
, 




B



{\displaystyle \mathbf {B} }
, 




Q



{\displaystyle \mathbf {Q} }
, and 




R



{\displaystyle \mathbf {R} }
) are constant, the initial time is arbitrarily set to zero, and the terminal time is taken in the limit 




t

f


→
∞


{\displaystyle t_{f}\rightarrow \infty }
 (this last assumption is what is known as infinite horizon).  The LQR problem is stated as follows.  Minimize the infinite horizon quadratic continuous-time cost functional
","A particular form of the LQ problem that arises in many control system problems is that of the linear quadratic regulator (LQR) where all of the matrices (i.e., 




A



{\displaystyle \mathbf {A} }
, 




B



{\displaystyle \mathbf {B} }
, 




Q



{\displaystyle \mathbf {Q} }
, and 




R



{\displaystyle \mathbf {R} }
) are constant, the initial time is arbitrarily set to zero, and the terminal time is taken in the limit 




t

f


→
∞


{\displaystyle t_{f}\rightarrow \infty }
 (this last assumption is what is known as infinite horizon). The LQR problem is stated as follows.","[' What is a particular form of the LQ problem that arises in many control system problems?', ' Where all of the matrices are constant, what is the initial time constant?', ' The initial time is arbitrarily set to what?', ' The terminal time is taken in the limit t f <unk> <unk> what is known as infinite horizon?']","['linear quadratic regulator', 'arbitrarily set to zero', 'zero', '→\n∞\n\n\n{\\displaystyle t_{f}\\rightarrow \\infty']"
3111,optimal control,Linear quadratic control,"In the finite-horizon case the matrices are restricted in that 




Q



{\displaystyle \mathbf {Q} }
 and 




R



{\displaystyle \mathbf {R} }
 are positive semi-definite and positive definite, respectively.  In the infinite-horizon case, however, the matrices 




Q



{\displaystyle \mathbf {Q} }
 and 




R



{\displaystyle \mathbf {R} }
 are not only positive-semidefinite and positive-definite, respectively, but are also constant.  These additional restrictions on





Q



{\displaystyle \mathbf {Q} }
 and 




R



{\displaystyle \mathbf {R} }
 in the infinite-horizon case are enforced to ensure that the cost functional remains positive.  Furthermore, in order to ensure that the cost function is bounded, the additional restriction is imposed that the pair 



(

A

,

B

)


{\displaystyle (\mathbf {A} ,\mathbf {B} )}
 is controllable.  Note that the LQ or LQR cost functional can be thought of physically as attempting to minimize the control energy (measured as a quadratic form).
","In the finite-horizon case the matrices are restricted in that 




Q



{\displaystyle \mathbf {Q} }
 and 




R



{\displaystyle \mathbf {R} }
 are positive semi-definite and positive definite, respectively. In the infinite-horizon case, however, the matrices 




Q



{\displaystyle \mathbf {Q} }
 and 




R



{\displaystyle \mathbf {R} }
 are not only positive-semidefinite and positive-definite, respectively, but are also constant.","[' What are the matrices in the finite-horizon case?', ' What are Q <unk>displaystyle <unk>mathbf <unk>Q<unk> <unk> and R <unk>Displaystyle', ' What are <unk>R<unk> <unk> not only positive-semidefinite and positive-definite?']","['positive-semidefinite and positive-definite, respectively, but are also constant', 'Q', 'constant']"
3112,optimal control,Linear quadratic control,"The infinite horizon problem (i.e., LQR) may seem overly restrictive and essentially useless because it assumes that the operator is driving the system to zero-state and hence driving the output of the system to zero. This is indeed correct. However the problem of driving the output to a desired nonzero level can be solved after the zero output one is. In fact, it can be proved that this secondary LQR problem can be solved in a very straightforward manner.  It has been shown in classical optimal control theory that the LQ (or LQR) optimal control has the feedback form
","The infinite horizon problem (i.e., LQR) may seem overly restrictive and essentially useless because it assumes that the operator is driving the system to zero-state and hence driving the output of the system to zero. This is indeed correct.","[' What is the infinite horizon problem?', ' What assumes that the operator is driving the system to zero-state?']","['it assumes that the operator is driving the system to zero-state and hence driving the output of the system to zero', 'The infinite horizon problem']"
3113,optimal control,Linear quadratic control,"and 




S

(
t
)


{\displaystyle \mathbf {S} (t)}
 is the solution of the differential Riccati equation.  The differential Riccati equation is given as
","and 




S

(
t
)


{\displaystyle \mathbf {S} (t)}
 is the solution of the differential Riccati equation. The differential Riccati equation is given as",[' What is the solution of the differential Riccati equation?'],['\\mathbf']
3114,optimal control,Linear quadratic control,"Understanding that the ARE arises from infinite horizon problem, the matrices 




A



{\displaystyle \mathbf {A} }
, 




B



{\displaystyle \mathbf {B} }
, 




Q



{\displaystyle \mathbf {Q} }
, and 




R



{\displaystyle \mathbf {R} }
 are all constant.  It is noted that there are in general multiple solutions to the algebraic Riccati equation and the positive definite (or positive semi-definite) solution is the one that is used to compute the feedback gain.  The LQ (LQR) problem was elegantly solved by Rudolf E. Kálmán.","Understanding that the ARE arises from infinite horizon problem, the matrices 




A



{\displaystyle \mathbf {A} }
, 




B



{\displaystyle \mathbf {B} }
, 




Q



{\displaystyle \mathbf {Q} }
, and 




R



{\displaystyle \mathbf {R} }
 are all constant. It is noted that there are in general multiple solutions to the algebraic Riccati equation and the positive definite (or positive semi-definite) solution is the one that is used to compute the feedback gain.","[' What arises from infinite horizon problem?', ' What are the matrices A, B, Q, and R constant?', ' What is the solution to the algebraic Riccati equation used for?', ' What is used to compute the feedback gain?']","['the ARE', 'A\n\n\n\n{\\displaystyle \\mathbf {A} }\n, \n\n\n\n\nB\n\n\n\n{\\displaystyle \\mathbf {B} }\n, \n\n\n\n\nQ\n\n\n\n{\\displaystyle \\mathbf {Q} }\n, and \n\n\n\n\nR\n\n\n\n{\\displaystyle \\mathbf {R} }\n are all constant', 'compute the feedback gain', 'positive definite (or positive semi-definite) solution']"
3115,optimal control,Numerical methods for optimal control,"Optimal control problems are generally nonlinear and therefore, generally do not have analytic solutions (e.g., like the linear-quadratic optimal control problem).  As a result, it is necessary to employ numerical methods to solve optimal control problems.  In the early years of optimal control (c. 1950s to 1980s) the favored approach for solving optimal control problems was that of indirect methods.  In an indirect method, the calculus of variations is employed to obtain the first-order optimality conditions.  These conditions result in a two-point (or, in the case of a complex problem, a multi-point) boundary-value problem.  This boundary-value problem actually has a special structure because it arises from taking the derivative of a Hamiltonian.  Thus, the resulting dynamical system is a Hamiltonian system of the form","Optimal control problems are generally nonlinear and therefore, generally do not have analytic solutions (e.g., like the linear-quadratic optimal control problem). As a result, it is necessary to employ numerical methods to solve optimal control problems.","[' Optimal control problems are generally nonlinear and therefore do not have what?', ' What is necessary to employ numerical methods to solve optimal control problems?', ' The linear-quadratic optimal control problem is an example of what type of solution?']","['analytic solutions', 'Optimal control problems are generally nonlinear and therefore, generally do not have analytic solutions', 'analytic']"
3116,optimal control,Numerical methods for optimal control,"is the augmented Hamiltonian and in an indirect method, the boundary-value problem is solved (using the appropriate boundary or transversality conditions).  The beauty of using an indirect method is that the state and adjoint (i.e., 




λ



{\displaystyle {\boldsymbol {\lambda }}}
) are solved for and the resulting solution is readily verified to be an extremal trajectory.  The disadvantage of indirect methods is that the boundary-value problem is often extremely difficult to solve (particularly for problems that span large time intervals or problems with interior point constraints).   A well-known software program that implements indirect methods is BNDSCO.","is the augmented Hamiltonian and in an indirect method, the boundary-value problem is solved (using the appropriate boundary or transversality conditions). The beauty of using an indirect method is that the state and adjoint (i.e., 




λ



{\displaystyle {\boldsymbol {\lambda }}}
) are solved for and the resulting solution is readily verified to be an extremal trajectory.","[' What method solves the boundary-value problem?', ' What is the beauty of using an indirect method?', ' The state and adjoint are solved for and the solution is readily verified to what?', ' What is readily verified to be an extremal trajectory?']","['indirect method', 'the state and adjoint (i.e., \n\n\n\n\nλ\n\n\n\n{\\displaystyle {\\boldsymbol {\\lambda }}}\n) are solved for and the resulting solution is readily verified to be an extremal trajectory', 'an extremal trajectory', 'λ\n\n\n\n{\\displaystyle {\\boldsymbol {\\lambda }}}\n) are solved for and the resulting solution']"
3117,optimal control,Numerical methods for optimal control,"The approach that has risen to prominence in numerical optimal control since the 1980s is that of so-called direct methods.  In a direct method, the state or the control, or both, are approximated using an appropriate function approximation (e.g., polynomial approximation or piecewise constant parameterization).  Simultaneously, the cost functional is approximated as a cost function.  Then, the coefficients of the function approximations are treated as optimization variables and the problem is ""transcribed"" to a nonlinear optimization problem of the form:
","The approach that has risen to prominence in numerical optimal control since the 1980s is that of so-called direct methods. In a direct method, the state or the control, or both, are approximated using an appropriate function approximation (e.g., polynomial approximation or piecewise constant parameterization).","[' What is the approach that has risen to prominence in numerical optimal control since the 1980s?', ' In a direct method, the state or the control are approximated using what?']","['direct methods', 'an appropriate function approximation']"
3118,optimal control,Numerical methods for optimal control,"Depending upon the type of direct method employed, the size of the nonlinear optimization problem can be quite small (e.g., as in a direct shooting or quasilinearization method), moderate (e.g. pseudospectral optimal control) or may be quite large (e.g., a direct collocation method). In the latter case (i.e., a collocation method), the nonlinear optimization problem may be literally thousands to tens of thousands of variables and constraints. Given the size of many NLPs arising from a direct method, it may appear somewhat counter-intuitive that solving the nonlinear optimization problem is easier than solving the boundary-value problem. It is, however, the fact that the NLP is easier to solve than the boundary-value problem. The reason for the relative ease of computation, particularly of a direct collocation method, is that the NLP is sparse and many well-known software programs exist (e.g., SNOPT) to solve large sparse NLPs. As a result, the range of problems that can be solved via direct methods (particularly direct collocation methods which are very popular these days) is significantly larger than the range of problems that can be solved via indirect methods. In fact, direct methods have become so popular these days that many people have written elaborate software programs that employ these methods. In particular, many such programs include DIRCOL, SOCS, OTIS, GESOP/ASTOS, DITAN. and PyGMO/PyKEP. In recent years, due to the advent of the MATLAB programming language, optimal control software in MATLAB has become more common. Examples of academically developed MATLAB software tools implementing direct methods include RIOTS, DIDO, DIRECT, FALCON.m, and GPOPS, while an example of an industry developed MATLAB tool is PROPT. These software tools have increased significantly the opportunity for people to explore complex optimal control problems both for academic research and industrial problems. Finally, it is noted that general-purpose MATLAB optimization environments such as TOMLAB have made coding complex optimal control problems significantly easier than was previously possible in languages such as C and FORTRAN.
","Depending upon the type of direct method employed, the size of the nonlinear optimization problem can be quite small (e.g., as in a direct shooting or quasilinearization method), moderate (e.g. pseudospectral optimal control) or may be quite large (e.g., a direct collocation method).","[' What is the size of a nonlinear optimization problem?', ' What is a direct shooting method?']","['small', 'quasilinearization method']"
3119,optimal control,Discrete-time optimal control,"The examples thus far have shown continuous time systems and control solutions. In fact, as optimal control solutions are now often implemented digitally, contemporary control theory is now primarily concerned with discrete time systems and solutions.  The Theory of Consistent Approximations provides conditions under which solutions to a series of increasingly accurate discretized optimal control problem converge to the solution of the original, continuous-time problem.  Not all discretization methods have this property, even seemingly obvious ones.  For instance, using a variable step-size routine to integrate the problem's dynamic equations may generate a gradient which does not converge to zero (or point in the right direction) as the solution is approached.   The direct method RIOTS is based on the Theory of Consistent Approximation.
","The examples thus far have shown continuous time systems and control solutions. In fact, as optimal control solutions are now often implemented digitally, contemporary control theory is now primarily concerned with discrete time systems and solutions.","[' What type of time systems and control solutions have been shown in control theory?', ' What kind of control solutions are now often implemented digitally?', ' Contemporary control theory is now primarily concerned with what kind of system?']","['continuous', 'optimal', 'discrete time']"
3120,optimal control,Examples,"A common solution strategy in many optimal control problems is to solve for the costate (sometimes called the shadow price) 



λ
(
t
)


{\displaystyle \lambda (t)}
. The costate summarizes in one number the marginal value of expanding or contracting the state variable next turn. The marginal value is not only the gains accruing to it next turn but associated with the duration of the program. It is nice when 



λ
(
t
)


{\displaystyle \lambda (t)}
 can be solved analytically, but usually, the most one can do is describe it sufficiently well that the intuition can grasp the character of the solution and an equation solver can solve numerically for the values.
","A common solution strategy in many optimal control problems is to solve for the costate (sometimes called the shadow price) 



λ
(
t
)


{\displaystyle \lambda (t)}
. The costate summarizes in one number the marginal value of expanding or contracting the state variable next turn.","[' What is a common solution strategy in many optimal control problems?', ' The costate summarizes in one number the marginal value of expanding or contracting the state variable next turn?']","['λ\n(\nt\n)\n\n\n{\\displaystyle \\lambda (t)}', 'λ\n(\nt\n)\n\n\n{\\displaystyle \\lambda (t)}']"
3121,optimal control,Examples,"Having obtained 



λ
(
t
)


{\displaystyle \lambda (t)}
, the turn-t optimal value for the control can usually be solved as a differential equation conditional on knowledge of 



λ
(
t
)


{\displaystyle \lambda (t)}
. Again it is infrequent, especially in continuous-time problems, that one obtains the value of the control or the state explicitly. Usually, the strategy is to solve for thresholds and regions that characterize the optimal control and use a numerical solver to isolate the actual choice values in time.
","Having obtained 



λ
(
t
)


{\displaystyle \lambda (t)}
, the turn-t optimal value for the control can usually be solved as a differential equation conditional on knowledge of 



λ
(
t
)


{\displaystyle \lambda (t)}
. Again it is infrequent, especially in continuous-time problems, that one obtains the value of the control or the state explicitly.","[' What can be solved as a differential equation conditional on knowledge of?', ' What is infrequent, especially in continuous-time problems, that one obtains the value of the control or the state explicitly?']","['the turn-t optimal value for the control', 'λ']"
3122,scalability,Summary,"In an economic context, a scalable business model implies that a company can increase sales given increased resources. For example, a package delivery system is scalable because more packages can be delivered by adding more delivery vehicles. However, if all packages had to first pass through a single warehouse for sorting, the system would not be as scalable, because one warehouse can handle only a limited number of packages.","In an economic context, a scalable business model implies that a company can increase sales given increased resources. For example, a package delivery system is scalable because more packages can be delivered by adding more delivery vehicles.","[' What implies that a company can increase sales given increased resources?', ' What is a scalable business model?']","['a scalable business model', 'implies that a company can increase sales given increased resources']"
3123,scalability,Summary,"In computing, scalability is a characteristic of computers, networks, algorithms, networking protocols, programs and applications. An example is a search engine, which must support increasing numbers of users, and the number of topics it indexes. Webscale is a computer architectural approach that brings the capabilities of large-scale cloud computing companies into enterprise data centers.","In computing, scalability is a characteristic of computers, networks, algorithms, networking protocols, programs and applications. An example is a search engine, which must support increasing numbers of users, and the number of topics it indexes.","[' In computing, what is a characteristic of computers, networks, algorithms, networking protocols, programs and applications?', ' What must support increasing numbers of users and the number of topics it index?']","['scalability', 'search engine']"
3124,scalability,Examples,"The Incident Command System (ICS) is used by emergency response agencies in the United States. ICS can scale resource coordination from a single-engine roadside brushfire to an interstate wildfire. The first resource on scene establishes command, with authority to order resources and delegate responsibility (managing five to seven officers, who will again delegate to up to seven, and on as the incident grows).  As an incident expands, more senior officers assume command.",The Incident Command System (ICS) is used by emergency response agencies in the United States. ICS can scale resource coordination from a single-engine roadside brushfire to an interstate wildfire.,"[' What system is used by emergency response agencies in the United States?', ' What can ICS scale resource coordination from?']","['The Incident Command System (ICS)', 'a single-engine roadside brushfire to an interstate wildfire']"
3125,scalability,Database scalability,"Scalability for databases requires that the database system be able to perform additional work given greater hardware resources, such as additional servers, processors, memory and storage. Workloads have continued to grow and demands on databases have followed suit.
","Scalability for databases requires that the database system be able to perform additional work given greater hardware resources, such as additional servers, processors, memory and storage. Workloads have continued to grow and demands on databases have followed suit.","[' What requires that the database system be able to perform additional work given greater hardware resources?', ' Workloads have continued to grow and demands on databases have followed suit?']","['Scalability for databases', 'databases']"
3126,scalability,Database scalability,"Algorithmic innovations have include row-level locking and table and index partitioning. Architectural innovations include shared-nothing and shared-everything architectures for managing multi-server configurations.
",Algorithmic innovations have include row-level locking and table and index partitioning. Architectural innovations include shared-nothing and shared-everything architectures for managing multi-server configurations.,"[' What are two examples of algorithmic innovations?', ' What type of architectures are used for managing multi-server configurations?']","['row-level locking and table and index partitioning', 'shared-nothing and shared-everything']"
3127,scalability,Strong versus eventual consistency (storage),"In the context of scale-out data storage, scalability is defined as the maximum storage cluster size which guarantees full data consistency, meaning there is only ever one valid version of stored data in the whole cluster, independently from the number of redundant physical data copies. Clusters which provide ""lazy"" redundancy by updating copies in an asynchronous fashion are called 'eventually consistent'. This type of scale-out design is suitable when availability and responsiveness are rated higher than consistency, which is true for many web file-hosting services or web caches (if you want the latest version, wait some seconds for it to propagate). For all classical transaction-oriented applications, this design should be avoided.","In the context of scale-out data storage, scalability is defined as the maximum storage cluster size which guarantees full data consistency, meaning there is only ever one valid version of stored data in the whole cluster, independently from the number of redundant physical data copies. Clusters which provide ""lazy"" redundancy by updating copies in an asynchronous fashion are called 'eventually consistent'.","[' What is scalability defined as?', ' What guarantees full data consistency?', ' Clusters which provide ""lazy"" redundancy provide what?', ' Clusters that provide ""lazy"" redundancy by updating copies in an asynchronous fashion are called what?']","['the maximum storage cluster size', 'scalability is defined as the maximum storage cluster size', 'eventually consistent', 'eventually consistent']"
3128,scalability,Strong versus eventual consistency (storage),"Many open-source and even commercial scale-out storage clusters, especially those built on top of standard PC hardware and networks, provide eventual consistency only. Idem some NoSQL databases like CouchDB and others mentioned above. Write operations invalidate other copies, but often don't wait for their acknowledgements. Read operations typically don't check every redundant copy prior to answering, potentially missing the preceding write operation. The large amount of metadata signal traffic would require specialized hardware and short distances to be handled with acceptable performance (i.e., act like a non-clustered storage device or database).
","Many open-source and even commercial scale-out storage clusters, especially those built on top of standard PC hardware and networks, provide eventual consistency only. Idem some NoSQL databases like CouchDB and others mentioned above.","[' What do many open-source and commercial scale-out storage clusters provide?', ' What are some NoSQL databases like?']","['eventual consistency only', 'CouchDB']"
3129,scalability,Strong versus eventual consistency (storage),"Indicators for eventually consistent designs (not suitable for transactional applications!) are:
",Indicators for eventually consistent designs (not suitable for transactional applications!) are:,[' Indicators for eventually consistent designs (not suitable for transactional applications) are:'],['Indicators']
3130,scalability,Performance tuning versus hardware scalability,"It is often advised to focus system design on hardware scalability rather than on capacity. It is typically cheaper to add a new node to a system in order to achieve improved performance than to partake in performance tuning to improve the capacity that each node can handle. But this approach can have diminishing returns (as discussed in performance engineering). For example: suppose 70% of a program can be sped up if parallelized and run on multiple CPUs instead of one. If 



α


{\displaystyle \alpha }
 is the fraction of a calculation that is sequential, and 



1
−
α


{\displaystyle 1-\alpha }
 is the fraction that can be parallelized, the maximum speedup that can be achieved by using P processors is given according to Amdahl's Law:
",It is often advised to focus system design on hardware scalability rather than on capacity. It is typically cheaper to add a new node to a system in order to achieve improved performance than to partake in performance tuning to improve the capacity that each node can handle.,"[' What is often advised to focus system design on rather than capacity?', ' What is typically cheaper to add a new node to a system in order to achieve improved performance than to partake in performance tuning?']","['hardware scalability', 'It is typically cheaper to add a new node to a system in order to achieve improved performance than to partake in performance tuning to improve the capacity that each node can handle']"
3131,scalability,Performance tuning versus hardware scalability,"Doubling the processing power has only sped up the process by roughly one-fifth. If the whole problem was parallelizable, the speed would also double. Therefore, throwing in more hardware is not necessarily the optimal approach.
","Doubling the processing power has only sped up the process by roughly one-fifth. If the whole problem was parallelizable, the speed would also double.","[' How much has doubling the processing power sped up the process?', ' What would happen if the whole problem was parallelizable?']","['one-fifth', 'the speed would also double']"
3132,medium access control,Summary,"In IEEE 802 LAN/MAN standards, the medium access control (MAC, also called media access control) sublayer is the layer that controls the hardware responsible for interaction with the wired, optical or wireless transmission medium. The MAC sublayer and the logical link control (LLC) sublayer together make up the data link layer. Within the data link layer, the LLC provides flow control and multiplexing for the logical link (i.e. EtherType, 802.1Q VLAN tag etc), while the MAC provides flow control and multiplexing for the transmission medium.
","In IEEE 802 LAN/MAN standards, the medium access control (MAC, also called media access control) sublayer is the layer that controls the hardware responsible for interaction with the wired, optical or wireless transmission medium. The MAC sublayer and the logical link control (LLC) sublayer together make up the data link layer.","[' What is another name for medium access control?', ' What is the layer that controls the hardware responsible for interaction with the wired, optical or wireless transmission medium?', ' The MAC sublayer and what other sublayer together make up the data link?', ' What is the name of the sublayer that makes up the data link layer?']","['media access control', 'medium access control (MAC, also called media access control) sublayer', 'logical link control (LLC)', 'logical link control (LLC)']"
3133,medium access control,Summary,"These two sublayers together correspond to layer 2 of the OSI model. For compatibility reasons, LLC is optional for implementations of IEEE 802.3 (the frames are then ""raw""), but compulsory for implementations of other IEEE 802 physical layer standards. Within the hierarchy of the OSI model and IEEE 802 standards, the MAC sublayer provides a control abstraction of the physical layer such that the complexities of physical link control are invisible to the LLC and upper layers of the network stack. Thus any LLC sublayer (and higher layers) may be used with any MAC. In turn, the medium access control block is formally connected to the PHY via a media-independent interface. Although the MAC block is today typically integrated with the PHY within the same device package, historically any MAC could be used with any PHY, independent of the transmission medium.
","These two sublayers together correspond to layer 2 of the OSI model. For compatibility reasons, LLC is optional for implementations of IEEE 802.3 (the frames are then ""raw""), but compulsory for implementations of other IEEE 802 physical layer standards.","[' How many sublayers together correspond to layer 2 of the OSI model?', ' What is optional for implementations of IEEE 802.3 (the frames are then ""raw""?']","['two', 'LLC']"
3134,medium access control,Summary,"When sending data to another device on the network, the MAC sublayer encapsulates higher-level frames into frames appropriate for the transmission medium (i.e. the MAC adds a syncword preamble and also padding if necessary), adds a frame check sequence to identify transmission errors, and then forwards the data to the physical layer as soon as the appropriate channel access method permits it. For topologies with a collision domain (bus, ring, mesh, point-to-multipoint topologies), controlling when data is sent and when to wait is necessary to avoid collisions. Additionally, the MAC is also responsible for compensating for collisions by initiating retransmission if a jam signal is detected. When receiving data from the physical layer, the MAC block ensures data integrity by verifying the sender's frame check sequences, and strips off the sender's preamble and padding before passing the data up to the higher layers.
","When sending data to another device on the network, the MAC sublayer encapsulates higher-level frames into frames appropriate for the transmission medium (i.e. the MAC adds a syncword preamble and also padding if necessary), adds a frame check sequence to identify transmission errors, and then forwards the data to the physical layer as soon as the appropriate channel access method permits it.","[' The MAC sublayer encapsulates higher-level frames into frames appropriate for what medium?', ' What does the MAC add when sending data to another device on the network?', ' What is the name of the method used to identify transmission errors?', ' What is used to forward the data to the physical layer?']","['transmission', 'a syncword preamble and also padding', 'frame check sequence', 'the MAC sublayer encapsulates higher-level frames into frames appropriate for the transmission medium (i.e. the MAC adds a syncword preamble and also padding if necessary), adds a frame check sequence to identify transmission errors, and then forwards the data to the physical layer as soon as the appropriate channel access method']"
3135,medium access control,Addressing mechanism,"The local network addresses used in IEEE 802 networks and FDDI networks are called media access control addresses; they are based on the addressing scheme that was used in early Ethernet implementations. A MAC address is intended as a unique serial number. MAC addresses are typically assigned to network interface hardware at the time of manufacture. The most significant part of the address identifies the manufacturer, who assigns the remainder of the address, thus provide a potentially unique address. This makes it possible for frames to be delivered on a network link that interconnects hosts by some combination of repeaters, hubs, bridges and switches, but not by network layer  routers. Thus, for example, when an IP packet reaches its destination (sub)network, the destination IP address (a layer 3 or network layer concept) is resolved with the Address Resolution Protocol for IPv4, or by Neighbor Discovery Protocol (IPv6) into the MAC address (a layer 2 concept) of the destination host.
",The local network addresses used in IEEE 802 networks and FDDI networks are called media access control addresses; they are based on the addressing scheme that was used in early Ethernet implementations. A MAC address is intended as a unique serial number.,"[' What are the local network addresses used in IEEE 802 networks and FDDI networks called?', ' What is a MAC address intended as?']","['media access control addresses', 'a unique serial number']"
3136,medium access control,Channel access control mechanism,"The channel access control mechanisms provided by the MAC layer are also known as a multiple access method. This makes it possible for several stations connected to the same physical medium to share it. Examples of shared physical media are bus networks, ring networks, hub networks, wireless networks and half-duplex point-to-point links. The multiple access method may detect or avoid data packet collisions if a packet mode contention based channel access method is used, or reserve resources to establish a logical channel if a circuit-switched or channelization-based channel access method is used. The channel access control mechanism relies on a physical layer multiplex scheme.
",The channel access control mechanisms provided by the MAC layer are also known as a multiple access method. This makes it possible for several stations connected to the same physical medium to share it.,"[' What are the channel access control mechanisms provided by the MAC layer also known as?', ' What makes it possible for stations connected to the same physical medium to share it?']","['a multiple access method', 'multiple access method']"
3137,medium access control,Channel access control mechanism,"The most widespread multiple access method is the contention-based CSMA/CD used in Ethernet networks. This mechanism is only utilized within a network collision domain, for example an Ethernet bus network or a hub-based star topology network. An Ethernet network may be divided into several collision domains, interconnected by bridges and switches.
","The most widespread multiple access method is the contention-based CSMA/CD used in Ethernet networks. This mechanism is only utilized within a network collision domain, for example an Ethernet bus network or a hub-based star topology network.","[' What is the most widely used multiple access method?', ' What type of network is the contention-based CSMA/CD?']","['contention-based CSMA/CD', 'Ethernet']"
3138,medium access control,Cellular networks,"Cellular networks, such as GSM, UMTS or LTE networks, also use a MAC layer. The MAC protocol in cellular networks is designed to maximize the utilization of the expensive licensed spectrum. The air interface of a cellular network is at layers 1 and 2 of the OSI model; at layer 2, it is divided into multiple protocol layers. In UMTS and LTE, those protocols are the Packet Data Convergence Protocol (PDCP), the Radio Link Control (RLC) protocol, and the MAC protocol. The base station has absolute control over the air interface and schedules the downlink access as well as the uplink access of all devices. The MAC protocol is specified by 3GPP in TS 25.321 for UMTS, TS 36.321 for LTE and TS 38.321 for 5G.
","Cellular networks, such as GSM, UMTS or LTE networks, also use a MAC layer. The MAC protocol in cellular networks is designed to maximize the utilization of the expensive licensed spectrum.","[' What type of network uses a MAC layer?', ' What is the purpose of the MAC protocol in cellular networks?']","['Cellular', 'maximize the utilization of the expensive licensed spectrum']"
3139,distributed systems,Summary,"Distributed computing is a field of computer science that studies distributed systems. A distributed system is a system whose components are located on different networked computers, which communicate and coordinate their actions by passing messages to one another from any system. The components interact with one another in order to achieve a common goal. Three significant characteristics of distributed systems are: concurrency of components, lack of a global clock, and independent failure of components. It deals with a central challenge that, when components of a system fails, it doesn't imply the entire system fails. Examples of distributed systems vary from SOA-based systems to massively multiplayer online games to peer-to-peer applications.
","Distributed computing is a field of computer science that studies distributed systems. A distributed system is a system whose components are located on different networked computers, which communicate and coordinate their actions by passing messages to one another from any system.","[' What is the field of computer science that studies distributed systems?', ' What is a distributed system whose components are located on different networked computers?', ' How do distributed systems communicate and coordinate their actions?']","['Distributed computing', 'A distributed system', 'by passing messages to one another from any system']"
3140,distributed systems,Summary,"A computer program that runs within a distributed system is called  a distributed program (and distributed programming is the process of writing such programs). There are many different types of implementations for the message passing mechanism, including pure HTTP, RPC-like connectors and message queues.","A computer program that runs within a distributed system is called  a distributed program (and distributed programming is the process of writing such programs). There are many different types of implementations for the message passing mechanism, including pure HTTP, RPC-like connectors and message queues.","[' What is a computer program that runs within a distributed system called?', ' What is distributed programming?', ' There are many different types of implementations for what?']","['a distributed program', 'the process of writing such programs', 'the message passing mechanism']"
3141,distributed systems,Summary,"Distributed computing also refers to the use of distributed systems to solve computational problems. In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers, which communicate with each other via message passing.","Distributed computing also refers to the use of distributed systems to solve computational problems. In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers, which communicate with each other via message passing.","[' What does distributed computing refer to?', ' What is used to solve computational problems?', ' How do computers communicate with each other in distributed computing?']","['the use of distributed systems to solve computational problems', 'distributed systems', 'message passing']"
3142,distributed systems,Introduction,"The word distributed in terms such as ""distributed system"", ""distributed programming"", and ""distributed algorithm"" originally referred to computer networks where individual computers were physically distributed within some geographical area. The terms are nowadays used in a much wider sense, even referring to autonomous processes that run on the same physical computer and interact with each other by message passing.","The word distributed in terms such as ""distributed system"", ""distributed programming"", and ""distributed algorithm"" originally referred to computer networks where individual computers were physically distributed within some geographical area. The terms are nowadays used in a much wider sense, even referring to autonomous processes that run on the same physical computer and interact with each other by message passing.","[' What did the word distributed originally refer to?', "" What are the terms distributed in today's world?"", ' What are autonomous processes that run on the same physical computer and interact with each other?']","['computer networks', 'autonomous processes that run on the same physical computer and interact with each other by message passing', 'message passing']"
3143,distributed systems,Introduction,"A distributed system may have a common goal, such as solving a large computational problem; the user then perceives the collection of autonomous processors as a unit. Alternatively, each computer may have its own user with individual needs, and the purpose of the distributed system is to coordinate the use of shared resources or provide communication services to the users.","A distributed system may have a common goal, such as solving a large computational problem; the user then perceives the collection of autonomous processors as a unit. Alternatively, each computer may have its own user with individual needs, and the purpose of the distributed system is to coordinate the use of shared resources or provide communication services to the users.","[' What is a common goal of a distributed system?', ' Who perceives the collection of autonomous processors as a unit?', ' Each computer may have its own user with what?', ' What is the purpose of a distributed system?']","['solving a large computational problem', 'the user', 'individual needs', 'to coordinate the use of shared resources or provide communication services to the users']"
3144,distributed systems,Parallel and distributed computing,"Distributed systems are groups of networked computers which share a common goal for their work.
The terms ""concurrent computing"", ""parallel computing"", and ""distributed computing"" have much overlap, and no clear distinction exists between them. The same system may be characterized both as ""parallel"" and ""distributed""; the processors in a typical distributed system run concurrently in parallel. Parallel computing may be seen as a particular tightly coupled form of distributed computing, and distributed computing may be seen as a loosely coupled form of parallel computing. Nevertheless, it is possible to roughly classify concurrent systems as ""parallel"" or ""distributed"" using the following criteria:
","Distributed systems are groups of networked computers which share a common goal for their work. The terms ""concurrent computing"", ""parallel computing"", and ""distributed computing"" have much overlap, and no clear distinction exists between them.","[' What are distributed systems?', ' What are the terms ""concurrent computing"", ""parallel computing"" and ""distributed computing""?']","['groups of networked computers which share a common goal for their work', 'overlap']"
3145,distributed systems,Parallel and distributed computing,"The figure on the right illustrates the difference between distributed and parallel systems. Figure (a) is a schematic view of a typical distributed system; the system is represented as a network topology in which each node is a computer and each line connecting the nodes is a communication link. Figure (b) shows the same distributed system in more detail: each computer has its own local memory, and information can be exchanged only by passing messages from one node to another by using the available communication links. Figure (c) shows a parallel system in which each processor has a direct access to a shared memory.
",The figure on the right illustrates the difference between distributed and parallel systems. Figure (a) is a schematic view of a typical distributed system; the system is represented as a network topology in which each node is a computer and each line connecting the nodes is a communication link.,"[' The figure on the right illustrates the difference between what two types of systems?', ' What is a schematic view of a typical distributed system?', ' The system is represented as what?']","['distributed and parallel systems', 'Figure (a)', 'a network topology']"
3146,distributed systems,Parallel and distributed computing,"The situation is further complicated by the traditional uses of the terms parallel and distributed algorithm that do not quite match the above definitions of parallel and distributed systems (see below for more detailed discussion). Nevertheless, as a rule of thumb, high-performance parallel computation in a shared-memory multiprocessor uses parallel algorithms while the coordination of a large-scale distributed system uses distributed algorithms.","The situation is further complicated by the traditional uses of the terms parallel and distributed algorithm that do not quite match the above definitions of parallel and distributed systems (see below for more detailed discussion). Nevertheless, as a rule of thumb, high-performance parallel computation in a shared-memory multiprocessor uses parallel algorithms while the coordination of a large-scale distributed system uses distributed algorithms.","[' What do traditional uses of the terms parallel and distributed algorithm not match?', ' What does high-performance parallel computation in a shared-memory multiprocessor use as a rule of thumb?', ' What does a shared-memory multiprocessor use?', ' What does the coordination of a large-scale distributed system do?']","['the above definitions of parallel and distributed systems', 'parallel algorithms', 'parallel algorithms', 'uses distributed algorithms']"
3147,distributed systems,History,"The use of concurrent processes which communicate through message-passing has its roots in operating system architectures studied in the 1960s. The first widespread distributed systems were local-area networks such as Ethernet, which was invented in the 1970s.","The use of concurrent processes which communicate through message-passing has its roots in operating system architectures studied in the 1960s. The first widespread distributed systems were local-area networks such as Ethernet, which was invented in the 1970s.","[' In what decade were operating system architectures studied?', ' What was the first widespread distributed system?', ' When was Ethernet invented?']","['1960s', 'local-area networks such as Ethernet', '1970s']"
3148,distributed systems,History,"ARPANET, one of the predecessors of the Internet, was introduced in the late 1960s, and ARPANET e-mail was invented in the early 1970s. E-mail became the most successful application of ARPANET, and it is probably the earliest example of a large-scale distributed application. In addition to ARPANET (and its successor, the global Internet), other early worldwide computer networks included Usenet and FidoNet from the 1980s, both of which were used to support distributed discussion systems.","ARPANET, one of the predecessors of the Internet, was introduced in the late 1960s, and ARPANET e-mail was invented in the early 1970s. E-mail became the most successful application of ARPANET, and it is probably the earliest example of a large-scale distributed application.","[' When was ARPANET introduced?', ' When was e-mail invented?', ' E-mail is probably the earliest example of what type of application?']","['late 1960s', 'early 1970s', 'large-scale distributed']"
3149,distributed systems,History,"The study of distributed computing became its own branch of computer science in the late 1970s and early 1980s. The first conference in the field, Symposium on Principles of Distributed Computing (PODC), dates back to 1982, and its counterpart International Symposium on Distributed Computing (DISC) was first held in Ottawa in 1985 as the International Workshop on Distributed Algorithms on Graphs.","The study of distributed computing became its own branch of computer science in the late 1970s and early 1980s. The first conference in the field, Symposium on Principles of Distributed Computing (PODC), dates back to 1982, and its counterpart International Symposium on Distributed Computing (DISC) was first held in Ottawa in 1985 as the International Workshop on Distributed Algorithms on Graphs.","[' When did the study of distributed computing become its own branch of computer science?', ' What is the name of the first conference in the field?', ' Where was the first International Symposium on Distributed Computing held?', ' When was the Symposium on Distributed Computing (DISC) first held in Ottawa?', ' What was the name of the first symposium on distributed computing?', ' Where was the DISC first held?']","['late 1970s and early 1980s', 'Symposium on Principles of Distributed Computing', 'Ottawa', '1985', 'Symposium on Principles of Distributed Computing', 'Ottawa']"
3150,distributed systems,Architectures,"Various hardware and software architectures are used for distributed computing. At a lower level, it is necessary to interconnect multiple CPUs with some sort of network, regardless of whether that network is printed onto a circuit board or made up of loosely coupled devices and cables. At a higher level, it is necessary to interconnect processes running on those CPUs with some sort of communication system.","Various hardware and software architectures are used for distributed computing. At a lower level, it is necessary to interconnect multiple CPUs with some sort of network, regardless of whether that network is printed onto a circuit board or made up of loosely coupled devices and cables.","[' Various hardware and software architectures are used for what kind of computing?', ' At a lower level, it is necessary to interconnect multiple CPUs with some sort of what?']","['distributed', 'network']"
3151,distributed systems,Architectures,"Another basic aspect of distributed computing architecture is the method of communicating and coordinating work among concurrent processes. Through various message passing protocols, processes may communicate directly with one another, typically in a master/slave relationship. Alternatively, a ""database-centric"" architecture can enable distributed computing to be done without any form of direct inter-process communication, by utilizing a shared database. Database-centric architecture in particular provides relational processing analytics in a schematic architecture allowing for live environment relay. This enables distributed computing functions both within and beyond the parameters of a networked database.","Another basic aspect of distributed computing architecture is the method of communicating and coordinating work among concurrent processes. Through various message passing protocols, processes may communicate directly with one another, typically in a master/slave relationship.","[' What is the method of communicating and coordinating work among concurrent processes?', ' Through various message passing protocols, processes may communicate directly with one another, typically in a master/slave relationship?']","['distributed computing architecture', 'distributed computing architecture']"
3152,horn clause,Summary,"In mathematical logic and logic programming, a Horn clause is a logical formula of a particular rule-like form which gives it useful properties for use in logic programming, formal specification, and model theory. Horn clauses are named for the logician Alfred Horn, who first pointed out their significance in 1951.","In mathematical logic and logic programming, a Horn clause is a logical formula of a particular rule-like form which gives it useful properties for use in logic programming, formal specification, and model theory. Horn clauses are named for the logician Alfred Horn, who first pointed out their significance in 1951.","[' What is a logical formula of a particular rule-like form?', ' Who is the logician Alfred Horn?', ' When did Alfred Horn first point out the significance of Horn clauses?']","['a Horn clause', 'Horn clauses', '1951']"
3153,horn clause,Definition,"A Horn clause is a clause (a disjunction of literals) with at most one positive, i.e. unnegated, literal. 
","A Horn clause is a clause (a disjunction of literals) with at most one positive, i.e. unnegated, literal.","[' What is a Horn clause a disjunction of?', ' How many positives does a horn clause have?', ' What is an unnegated literal?']","['literals', 'at most one', 'Horn clause']"
3154,horn clause,Definition,"A Horn clause with exactly one positive literal is a definite clause or a strict Horn clause; a definite clause with no negative literals is a unit clause, and a unit clause without variables is a fact;.
A Horn clause without a positive literal is a goal clause.
Note that the empty clause, consisting of no literals (which is equivalent to false) is a goal clause.
These three kinds of Horn clauses are illustrated in the following propositional example:
","A Horn clause with exactly one positive literal is a definite clause or a strict Horn clause; a definite clause with no negative literals is a unit clause, and a unit clause without variables is a fact;. A Horn clause without a positive literal is a goal clause.","[' A Horn clause with exactly one positive literal is what?', ' A definite clause with no negative literals is a what clause?']","['a definite clause or a strict Horn clause', 'unit clause']"
3155,horn clause,Definition,"All variables in a clause are implicitly universally quantified with the scope being the entire clause. Thus, for example:
","All variables in a clause are implicitly universally quantified with the scope being the entire clause. Thus, for example:",[' All variables in a clause are implicitly universally quantified with the scope being what?'],['the entire clause']
3156,horn clause,Logic programming,"In logic programming, a definite clause behaves as a goal-reduction procedure. For example, the Horn clause written above behaves as the procedure:
","In logic programming, a definite clause behaves as a goal-reduction procedure. For example, the Horn clause written above behaves as the procedure:","[' What does a definite clause behave as in logic programming?', ' What is the Horn clause written above?']","['a goal-reduction procedure', 'procedure']"
3157,horn clause,Logic programming,"In logic programming, computation and query evaluation are performed by representing the negation of a problem to be solved as a goal clause. For example, the problem of solving the existentially quantified conjunction of positive literals:
","In logic programming, computation and query evaluation are performed by representing the negation of a problem to be solved as a goal clause. For example, the problem of solving the existentially quantified conjunction of positive literals:","[' What are computation and query evaluation performed by representing the negation of a problem to be solved as?', ' What is the problem of solving the existentially quantified conjunction of positive literals?']","['a goal clause', 'logic programming']"
3158,horn clause,Logic programming,"Solving the problem amounts to deriving a contradiction, which is represented by the empty clause (or ""false""). The solution of the problem is a substitution of terms for the variables in the goal clause, which can be extracted from the proof of contradiction. Used in this way, goal clauses are similar to conjunctive queries in relational databases, and Horn clause logic is equivalent in computational power to a universal Turing machine. 
","Solving the problem amounts to deriving a contradiction, which is represented by the empty clause (or ""false""). The solution of the problem is a substitution of terms for the variables in the goal clause, which can be extracted from the proof of contradiction.","[' What does the empty clause represent?', ' What is the solution of the problem?']","['a contradiction', 'a substitution of terms for the variables in the goal clause']"
3159,horn clause,Logic programming,"The Prolog notation is actually ambiguous, and the term “goal clause” is sometimes also used ambiguously.  The variables in a goal clause can be read as universally or existentially quantified, and deriving “false” can be interpreted either as deriving a contradiction or as deriving a successful solution of the problem to be solved.","The Prolog notation is actually ambiguous, and the term “goal clause” is sometimes also used ambiguously. The variables in a goal clause can be read as universally or existentially quantified, and deriving “false” can be interpreted either as deriving a contradiction or as deriving a successful solution of the problem to be solved.","[' What is ambiguous in the Prolog notation?', ' What term is sometimes used ambiguously?', ' The variables in a goal clause can be read as universal or what?', ' What is the term for a successful solution of a problem?']","['goal clause', 'goal clause', 'existentially quantified', 'deriving “false']"
3160,expert system,Summary,"In artificial intelligence, an expert system is a computer system emulating the decision-making ability of a human expert.
Expert systems are designed to solve complex problems by reasoning through bodies of knowledge, represented mainly as if–then rules rather than through conventional procedural code. The first expert systems were created in the 1970s and then proliferated in the 1980s. Expert systems were among the first truly successful forms of artificial intelligence (AI) software. 
An expert system is divided into two subsystems: the inference engine and the knowledge base. The knowledge base represents facts and rules. The inference engine applies the rules to the known facts to deduce new facts. Inference engines can also include explanation and debugging abilities.
","In artificial intelligence, an expert system is a computer system emulating the decision-making ability of a human expert. Expert systems are designed to solve complex problems by reasoning through bodies of knowledge, represented mainly as if–then rules rather than through conventional procedural code.","[' What is a computer system emulating the decision-making ability of a human expert?', ' Expert systems are designed to solve complex problems by reasoning through bodies of knowledge, represented mainly as what?']","['an expert system', 'if–then rules']"
3161,expert system,Software architecture,"An expert system is an example of a knowledge-based system. Expert systems were the first commercial systems to use a knowledge-based architecture. In general view, an expert system includes the following components: a knowledge base, an inference engine, an explanation facility, a knowledge acquisition facility, and a user interface. ",An expert system is an example of a knowledge-based system. Expert systems were the first commercial systems to use a knowledge-based architecture.,"[' What is an example of a knowledge-based system?', ' Expert systems were the first commercial systems to use what?']","['An expert system', 'a knowledge-based architecture']"
3162,expert system,Software architecture,"The knowledge base represents facts about the world. In early expert systems such as Mycin and Dendral, these facts were represented mainly as flat assertions about variables. In later expert systems developed with commercial shells, the knowledge base took on more structure and used concepts from object-oriented programming. The world was represented as classes, subclasses, and instances and assertions were replaced by values of object instances. The rules worked by querying and asserting values of the objects.
","The knowledge base represents facts about the world. In early expert systems such as Mycin and Dendral, these facts were represented mainly as flat assertions about variables.","[' What represents facts about the world?', ' In early systems such as Mycin and Dendral, facts were represented mainly as what?']","['The knowledge base', 'flat assertions about variables']"
3163,expert system,Software architecture,"The inference engine is an automated reasoning system that evaluates the current state of the knowledge-base, applies relevant rules, and then asserts new knowledge into the knowledge base. The inference engine may also include abilities for explanation, so that it can explain to a user the chain of reasoning used to arrive at a particular conclusion by tracing back over the firing of rules that resulted in the assertion.","The inference engine is an automated reasoning system that evaluates the current state of the knowledge-base, applies relevant rules, and then asserts new knowledge into the knowledge base. The inference engine may also include abilities for explanation, so that it can explain to a user the chain of reasoning used to arrive at a particular conclusion by tracing back over the firing of rules that resulted in the assertion.","[' What is an automated reasoning system that evaluates the current state of the knowledge-base?', ' What can the inference engine also include abilities for?', ' What can explain to a user the chain of reasoning used to arrive at a particular conclusion?']","['The inference engine', 'explanation', 'The inference engine']"
3164,expert system,Software architecture,"There are mainly two modes for an inference engine: forward chaining and backward chaining. The different approaches are dictated by whether the inference engine is being driven by the antecedent (left hand side) or the consequent (right hand side) of the rule. In forward chaining an antecedent fires and asserts the consequent. For example, consider the following rule:
",There are mainly two modes for an inference engine: forward chaining and backward chaining. The different approaches are dictated by whether the inference engine is being driven by the antecedent (left hand side) or the consequent (right hand side) of the rule.,"[' What are the two modes for an inference engine?', ' What is the antecedent of the rule?', ' How are the different approaches dictated?']","['forward chaining and backward chaining', 'left hand side', 'by whether the inference engine is being driven by the antecedent (left hand side) or the consequent (right hand side) of the rule']"
3165,expert system,Software architecture,"A simple example of forward chaining would be to assert Man(Socrates) to the system and then trigger the inference engine. It would match R1 and assert Mortal(Socrates) into the knowledge base.
",A simple example of forward chaining would be to assert Man(Socrates) to the system and then trigger the inference engine. It would match R1 and assert Mortal(Socrates) into the knowledge base.,"[' What would be a simple example of forward chaining?', ' What would happen if Man was asserted to the system and then trigger the inference engine?']","['to assert Man(Socrates)', 'It would match R1 and assert Mortal(Socrates) into the knowledge base']"
3166,expert system,Software architecture,"Backward chaining is a bit less straight forward. In backward chaining the system looks at possible conclusions and works backward to see if they might be true. So if the system was trying to determine if Mortal(Socrates) is true it would find R1 and query the knowledge base to see if Man(Socrates) is true. One of the early innovations of expert systems shells was to integrate inference engines with a user interface. This could be especially powerful with backward chaining. If the system needs to know a particular fact but does not, then it can simply generate an input screen and ask the user if the information is known. So in this example, it could use R1 to ask the user if Socrates was a Man and then use that new information accordingly.
",Backward chaining is a bit less straight forward. In backward chaining the system looks at possible conclusions and works backward to see if they might be true.,"[' What is a bit less straight forward in backward chaining?', ' What does the system look at and work backward to see if they might be true?']","['Backward chaining', 'possible conclusions']"
3167,expert system,Software architecture,"The use of rules to explicitly represent knowledge also enabled explanation abilities. In the simple example above if the system had used R1 to assert that Socrates was Mortal and a user wished to understand why Socrates was mortal they could query the system and the system would look back at the rules which fired to cause the assertion and present those rules to the user as an explanation. In English, if the user asked ""Why is Socrates Mortal?"" the system would reply ""Because all men are mortal and Socrates is a man"".  A significant area for research was the generation of explanations from the knowledge base in natural English rather than simply by showing the more formal but less intuitive rules.",The use of rules to explicitly represent knowledge also enabled explanation abilities. In the simple example above if the system had used R1 to assert that Socrates was Mortal and a user wished to understand why Socrates was mortal they could query the system and the system would look back at the rules which fired to cause the assertion and present those rules to the user as an explanation.,"[' The use of rules to explicitly represent knowledge enabled what?', ' What enabled explanation abilities?', ' Who could query the system and the system would look back at the rules which fired to cause the assertion and present the rules as an explanation to the user?']","['explanation abilities', 'The use of rules to explicitly represent knowledge', 'a user']"
3168,expert system,Software architecture,"As expert systems evolved, many new techniques were incorporated into various types of inference engines. Some of the most important of these were:
","As expert systems evolved, many new techniques were incorporated into various types of inference engines. Some of the most important of these were:","[' What new techniques were incorporated into various types of inference engines?', ' What were some of the most important?']","['expert systems evolved, many new techniques', 'inference engines']"
3169,expert system,Advantages,"The goal of knowledge-based systems is to make the critical information required for the system to work explicit rather than implicit.  In a traditional computer program the logic is embedded in code that can typically only be reviewed by an IT specialist. With an expert system the goal was to specify the rules in a format that was intuitive and easily understood, reviewed, and even edited by domain experts rather than IT experts. The benefits of this explicit knowledge representation were rapid development and ease of maintenance.
",The goal of knowledge-based systems is to make the critical information required for the system to work explicit rather than implicit. In a traditional computer program the logic is embedded in code that can typically only be reviewed by an IT specialist.,"[' What is the goal of knowledge-based systems?', ' What type of computer program is the logic embedded in?', ' Who can typically only review the logic of a traditional computer program?']","['to make the critical information required for the system to work explicit rather than implicit', 'traditional', 'an IT specialist']"
3170,expert system,Advantages,"Ease of maintenance is the most obvious benefit. This was achieved in two ways. First, by removing the need to write conventional code, many of the normal problems that can be caused by even small changes to a system could be avoided with expert systems.  Essentially, the logical flow of the program (at least at the highest level) was simply a given for the system, simply invoke the inference engine. This also was a reason for the second benefit: rapid prototyping. With an expert system shell it was possible to enter a few rules and have a prototype developed in days rather than the months or year typically associated with complex IT projects.
",Ease of maintenance is the most obvious benefit. This was achieved in two ways.,"[' What is the most obvious benefit of maintenance?', ' How many ways was maintenance achieved?']","['Ease', 'two']"
3171,expert system,Advantages,"A claim for expert system shells that was often made was that they removed the need for trained programmers and that experts could develop systems themselves. In reality, this was seldom if ever true. While the rules for an expert system were more comprehensible than typical computer code, they still had a formal syntax where a misplaced comma or other character could cause havoc as with any other computer language. Also, as expert systems moved from prototypes in the lab to deployment in the business world, issues of integration and maintenance became far more critical. Inevitably demands to integrate with, and take advantage of, large legacy databases and systems arose. To accomplish this, integration required the same skills as any other type of system.","A claim for expert system shells that was often made was that they removed the need for trained programmers and that experts could develop systems themselves. In reality, this was seldom if ever true.","[' What was a common claim for expert system shells?', ' What did experts use to develop systems?']","['they removed the need for trained programmers', 'expert system shells']"
3172,expert system,Disadvantages,"The most common disadvantage cited for expert systems in the academic literature is the knowledge acquisition problem. Obtaining the time of domain experts for any software application is always difficult, but for expert systems it was especially difficult because the experts were by definition highly valued and in constant demand by the organization. As a result of this problem, a great deal of research in the later years of expert systems was focused on tools for knowledge acquisition, to help automate the process of designing, debugging, and maintaining rules defined by experts. However, when looking at the life-cycle of expert systems in actual use, other problems – essentially the same problems as those of any other large system – seem at least as critical as knowledge acquisition: integration, access to large databases, and performance.","The most common disadvantage cited for expert systems in the academic literature is the knowledge acquisition problem. Obtaining the time of domain experts for any software application is always difficult, but for expert systems it was especially difficult because the experts were by definition highly valued and in constant demand by the organization.","[' What is the most common disadvantage cited for expert systems in the academic literature?', ' What is always difficult to obtain the time of domain experts for any software application?', ' Expert systems were by definition highly valued and in constant demand what?', ' What were experts by definition highly valued and in constant demand by the organization?']","['knowledge acquisition problem', 'expert systems', 'the organization', 'expert systems']"
3173,expert system,Disadvantages,"Performance could be especially problematic because early expert systems were built using tools (such as earlier Lisp versions) that interpreted code expressions without first compiling them. This provided a powerful development environment, but with the drawback that it was virtually impossible to match the efficiency of the fastest compiled languages (such as C). System and database integration were difficult for early expert systems because the tools were mostly in languages and platforms that were neither familiar to nor welcome in most corporate IT environments – programming languages such as Lisp and Prolog, and hardware platforms such as Lisp machines and personal computers. As a result, much effort in the later stages of expert system tool development was focused on integrating with legacy environments such as COBOL and large database systems, and on porting to more standard platforms. These issues were resolved mainly by the client–server paradigm shift, as PCs were gradually accepted in the IT environment as a legitimate platform for serious business system development and as affordable minicomputer servers provided the processing power needed for AI applications.","Performance could be especially problematic because early expert systems were built using tools (such as earlier Lisp versions) that interpreted code expressions without first compiling them. This provided a powerful development environment, but with the drawback that it was virtually impossible to match the efficiency of the fastest compiled languages (such as C).","[' Early expert systems were built using tools that interpreted code expressions without first compiling what?', ' What was a drawback of early expert systems?', ' Is it possible to match the efficiency of the fastest compiled languages?']","['them', 'it was virtually impossible to match the efficiency of the fastest compiled languages', 'it was virtually impossible']"
3174,expert system,Disadvantages,"Another major challenge of expert systems emerges when the size of the knowledge base increases. This causes the processing complexity to increase.  For instance, when an expert system with 100 million rules was envisioned as the ultimate expert system, it became obvious that such system would be too complex and it would face too many computational problems. An inference engine would have to be able to process huge numbers of rules to reach a decision.
",Another major challenge of expert systems emerges when the size of the knowledge base increases. This causes the processing complexity to increase.,"[' When does the size of the knowledge base increase?', ' What does this increase in?']","['processing complexity to increase', 'processing complexity']"
3175,expert system,Disadvantages,"How to verify that decision rules are consistent with each other is also a challenge when there are too many rules.  Usually such problem leads to a satisfiability (SAT) formulation. This is a well-known NP-complete problem Boolean satisfiability problem.  If we assume only binary variables, say n of them, and then the corresponding search space is of size 2






n




{\displaystyle ^{n}}
.  Thus, the search space can grow exponentially.
",How to verify that decision rules are consistent with each other is also a challenge when there are too many rules. Usually such problem leads to a satisfiability (SAT) formulation.,"[' How to verify that decision rules are consistent with each other is also a challenge when there are too many rules?', ' What usually leads to a satisfiability formulation?']","['satisfiability (SAT) formulation', 'there are too many rules']"
3176,expert system,Disadvantages,Other problems are related to the overfitting and overgeneralization effects when using known facts and trying to generalize to other cases not described explicitly in the knowledge base.  Such problems exist with methods that employ machine learning approaches too.,Other problems are related to the overfitting and overgeneralization effects when using known facts and trying to generalize to other cases not described explicitly in the knowledge base. Such problems exist with methods that employ machine learning approaches too.,[' What are some problems with using known facts and trying to generalize to other cases not described explicitly in the knowledge base?'],['overfitting and overgeneralization effects']
3177,expert system,Disadvantages,"Another problem related to the knowledge base is how to make updates of its knowledge quickly and effectively. Also how to add a new piece of knowledge (i.e., where to add it among many rules) is challenging.  Modern approaches that rely on machine learning methods are easier in this regard.
","Another problem related to the knowledge base is how to make updates of its knowledge quickly and effectively. Also how to add a new piece of knowledge (i.e., where to add it among many rules) is challenging.","[' What is another problem related to the knowledge base?', ' How to make updates of its knowledge quickly and effectively?']","['how to make updates of its knowledge quickly and effectively', 'knowledge base']"
3178,expert system,Disadvantages,"Because of the above challenges, it became clear that new approaches to AI were required instead of rule-based technologies.  These new approaches are based on the use of machine learning techniques, along with the use of feedback mechanisms.","Because of the above challenges, it became clear that new approaches to AI were required instead of rule-based technologies. These new approaches are based on the use of machine learning techniques, along with the use of feedback mechanisms.","[' Why were new approaches to AI needed instead of rule-based technologies?', ' What are these new approaches based on?']","['Because of the above challenges', 'machine learning techniques']"
3179,expert system,Applications,"Hayes-Roth divides expert systems applications into 10 categories illustrated in the following table. The example applications were not in the original Hayes-Roth table, and some of them arose well afterward. Any application that is not footnoted is described in the Hayes-Roth book. Also, while these categories provide an intuitive framework to describe the space of expert systems applications, they are not rigid categories, and in some cases an application may show traits of more than one category.
","Hayes-Roth divides expert systems applications into 10 categories illustrated in the following table. The example applications were not in the original Hayes-Roth table, and some of them arose well afterward.",[' How many categories does Hayes-Roth divide expert systems applications into?'],['10']
3180,expert system,Applications,"Hearsay was an early attempt at solving voice recognition through an expert systems approach. For the most part this category of expert systems was not all that successful. Hearsay and all interpretation systems are essentially pattern recognition systems—looking for patterns in noisy data. In the case of Hearsay recognizing phonemes in an audio stream. Other early examples were analyzing sonar data to detect Russian submarines. These kinds of systems proved much more amenable to a neural network AI solution than a rule-based approach.
",Hearsay was an early attempt at solving voice recognition through an expert systems approach. For the most part this category of expert systems was not all that successful.,"[' What was the name of the early attempt at solving voice recognition through expert systems?', ' What did Hearsay attempt to solve?']","['Hearsay', 'voice recognition']"
3181,expert system,Applications,"CADUCEUS and MYCIN were medical diagnosis systems. The user describes their symptoms to the computer as they would to a doctor and the computer returns a medical diagnosis.
",CADUCEUS and MYCIN were medical diagnosis systems. The user describes their symptoms to the computer as they would to a doctor and the computer returns a medical diagnosis.,"[' What were CADUCEUS and MYCIN?', ' What did the user describe their symptoms to the computer as they would to a doctor?']","['medical diagnosis systems', 'the computer returns a medical diagnosis']"
3182,expert system,Applications,"Dendral was a tool to study hypothesis formation in the identification of organic molecules. The general problem it solved—designing a solution given a set of constraints—was one of the most successful areas for early expert systems applied to business domains such as salespeople configuring Digital Equipment Corporation (DEC) VAX computers and mortgage loan application development.
",Dendral was a tool to study hypothesis formation in the identification of organic molecules. The general problem it solved—designing a solution given a set of constraints—was one of the most successful areas for early expert systems applied to business domains such as salespeople configuring Digital Equipment Corporation (DEC) VAX computers and mortgage loan application development.,"[' What was Dendral used to study?', ' What was one of the most successful areas for early expert systems applied to business domains?', ' Salespeople configuring Digital Equipment Corporation (DEC) VAX computers and what else?']","['hypothesis formation in the identification of organic molecules', 'Dendral', 'mortgage loan application development']"
3183,expert system,Applications,"
Mistral  is an expert system to monitor dam safety, developed in the 1990s by Ismes (Italy). It gets data from an automatic monitoring system and performs a diagnosis of the state of the dam. Its first copy, installed in 1992 on the Ridracoli Dam (Italy), is still operational 24/7/365. It has been installed on several dams in Italy and abroad (e.g., Itaipu Dam in Brazil), and on landslide sites under the name of Eydenet, and on monuments under the name of Kaleidos. Mistral is a registered trade mark of CESI.","
Mistral  is an expert system to monitor dam safety, developed in the 1990s by Ismes (Italy). It gets data from an automatic monitoring system and performs a diagnosis of the state of the dam.","[' What is the name of the expert system to monitor dam safety?', ' Who developed Mistral?', ' What does Mistral get data from?']","['Mistral', 'Ismes', 'an automatic monitoring system']"
3184,security policy,Summary,"Security policy is a definition of what it means to be secure for a system, organization or other entity.  For an organization, it addresses the constraints on behavior of its members as well as constraints imposed on adversaries by mechanisms such as doors, locks, keys and walls.  For systems, the security policy addresses constraints on functions and flow among them, constraints on access by external systems and adversaries including programs and access to data by people.  
","Security policy is a definition of what it means to be secure for a system, organization or other entity. For an organization, it addresses the constraints on behavior of its members as well as constraints imposed on adversaries by mechanisms such as doors, locks, keys and walls.","[' What is a definition of what it means to be secure for a system, organization or other entity?', ' For an organization, what addresses the constraints on behavior of its members?']","['Security policy', 'Security policy']"
3185,security policy,Significance,"If it is important to be secure, then it is important to be sure all of the security policy is enforced by mechanisms that are strong.  There are organized methodologies and risk assessment strategies to assure completeness of security policies and assure that they are completely enforced.  In complex systems, such as information systems, policies can be decomposed into sub-policies to facilitate the allocation of security mechanisms to enforce sub-policies.  However, this practice has pitfalls.  It is too easy to simply go directly to the sub-policies, which are essentially the rules of operation and dispense with the top level policy.  That gives the false sense that the rules of operation address some overall definition of security when they do not.  Because it is so difficult to think clearly with completeness about security, rules of operation stated as ""sub-policies"" with no ""super-policy"" usually turn out to be rambling rules that fail to enforce anything with completeness.  Consequently, a top-level security policy is essential to any serious security scheme and sub-policies and rules of operation are meaningless without it.","If it is important to be secure, then it is important to be sure all of the security policy is enforced by mechanisms that are strong. There are organized methodologies and risk assessment strategies to assure completeness of security policies and assure that they are completely enforced.","[' What is important to be secure?', ' What are organized methodologies and risk assessment strategies?']","['it is important to be sure all of the security policy is enforced by mechanisms that are strong', 'to assure completeness of security policies']"
3186,virtual environment,Summary,"A virtual environment is a networked application that allows a user to interact with both the computing environment and the work of other users. Email, chat, and web-based document sharing applications are all examples of virtual environments.  Simply put, it is a networked common operating space.  Once the fidelity of the virtual environment is such that it ""creates a psychological state in which the individual perceives himself or herself as existing within the virtual environment"" (Blascovich, 2002, p. 129) then the virtual environment (VE) has progressed into the realm of immersive virtual environments (IVEs).
","A virtual environment is a networked application that allows a user to interact with both the computing environment and the work of other users. Email, chat, and web-based document sharing applications are all examples of virtual environments.","[' What is a networked application that allows a user to interact with both the computing environment and the work of other users?', ' What are examples of virtual environments?']","['A virtual environment', 'Email, chat, and web-based document sharing applications']"
3187,integer programming,Summary,"An integer programming problem is a mathematical optimization or feasibility program in which some or all of the variables are restricted to be integers. In many settings the term refers to integer linear programming (ILP), in which the objective function and the constraints (other than the integer constraints) are linear.
","An integer programming problem is a mathematical optimization or feasibility program in which some or all of the variables are restricted to be integers. In many settings the term refers to integer linear programming (ILP), in which the objective function and the constraints (other than the integer constraints) are linear.","[' What is a mathematical optimization or feasibility program in which some or all variables are restricted to be integers?', ' In many settings, the term refers to what?']","['An integer programming problem', 'integer linear programming']"
3188,integer programming,Summary,"Integer programming is NP-complete. In particular, the special case of 0-1 integer linear programming, in which unknowns are binary, and only the restrictions must be satisfied, is one of Karp's 21 NP-complete problems.
","Integer programming is NP-complete. In particular, the special case of 0-1 integer linear programming, in which unknowns are binary, and only the restrictions must be satisfied, is one of Karp's 21 NP-complete problems.","[' Integer programming is what?', "" What is one of Karp's 21 NP-complete problems?""]","['NP-complete', '0-1 integer linear programming']"
3189,integer programming,Canonical and standard form for ILPs,"In integer linear programming, canonical form is distinct from standard form. An integer linear program in canonical form is expressed thus (note that it is the 




x



{\displaystyle \mathbf {x} }
 vector which is to be decided):","In integer linear programming, canonical form is distinct from standard form. An integer linear program in canonical form is expressed thus (note that it is the 




x



{\displaystyle \mathbf {x} }
 vector which is to be decided):","[' What is the difference between canonical and standard form in integer linear programming?', ' What is to be decided in an integer linear program?']","['distinct', 'the \n\n\n\n\nx\n\n\n\n{\\displaystyle \\mathbf {x} }\n vector']"
3190,integer programming,Canonical and standard form for ILPs,"where 




c

,

b



{\displaystyle \mathbf {c} ,\mathbf {b} }
  are vectors and 



A


{\displaystyle A}
 is a matrix, where all entries are integers. As with linear programs, ILPs not in standard form can be converted to standard form by eliminating inequalities, introducing slack variables (




s



{\displaystyle \mathbf {s} }
) and replacing variables that are not sign-constrained with the difference of two sign-constrained variables
","where 




c

,

b



{\displaystyle \mathbf {c} ,\mathbf {b} }
  are vectors and 



A


{\displaystyle A}
 is a matrix, where all entries are integers. As with linear programs, ILPs not in standard form can be converted to standard form by eliminating inequalities, introducing slack variables (




s



{\displaystyle \mathbf {s} }
) and replacing variables that are not sign-constrained with the difference of two sign-constrained variables","[' What are vectors?', ' What are all entries in A <unk>displaystyle A<unk>?', ' How can ILPs not in standard form be converted to standard form?', ' What is the difference between two sign-constrained variables?']","['mathbf {c} ,\\mathbf {b}', 'integers', 'by eliminating inequalities', 'mathbf {c} ,\\mathbf {b}']"
3191,integer programming,Example,"The feasible integer points are shown in red, and the red dashed lines indicate their convex hull, which is the smallest convex polyhedron that contains all of these points. The blue lines together with the coordinate axes define the polyhedron of the LP relaxation, which is given by the inequalities without the integrality constraint. The goal of the optimization is to move the black dotted line as far upward while still touching the polyhedron. The optimal solutions of the integer problem are the points 



(
1
,
2
)


{\displaystyle (1,2)}
 and 



(
2
,
2
)


{\displaystyle (2,2)}
 which both have an objective value of 2. The unique optimum of the relaxation is 



(
1.8
,
2.8
)


{\displaystyle (1.8,2.8)}
 with objective value of 2.8. If the solution of the relaxation is rounded to the nearest integers, it is not feasible for the ILP.
","The feasible integer points are shown in red, and the red dashed lines indicate their convex hull, which is the smallest convex polyhedron that contains all of these points. The blue lines together with the coordinate axes define the polyhedron of the LP relaxation, which is given by the inequalities without the integrality constraint.","[' What is the smallest convex polyhedron that contains all of the feasible integer points?', ' The blue lines together with the coordinate axes define what?', ' What is given by the inequalities?', ' What is given by the inequalities without the integrality constraint?']","['convex hull', 'the polyhedron of the LP relaxation', 'LP relaxation', 'the polyhedron of the LP relaxation']"
3192,integer programming,Proof of NP-hardness,"Let 



G
=
(
V
,
E
)


{\displaystyle G=(V,E)}
 be an undirected graph. Define a linear program as follows:
","Let 



G
=
(
V
,
E
)


{\displaystyle G=(V,E)}
 be an undirected graph. Define a linear program as follows:","[' What type of graph is G = (V, E )?']",['undirected']
3193,integer programming,Proof of NP-hardness,"Given that the constraints limit 




y

v




{\displaystyle y_{v}}
 to either 0 or 1, any feasible solution to the integer program is a subset of vertices. The first constraint implies that at least one end point of every edge is included in this subset. Therefore, the solution describes a vertex cover. Additionally given some vertex cover C, 




y

v




{\displaystyle y_{v}}
 can be set to 1 for any 



v
∈
C


{\displaystyle v\in C}
 and to 0 for any 



v
∉
C


{\displaystyle v\not \in C}
 thus giving us a feasible solution to the integer program. Thus we can conclude that if we minimize the sum of 




y

v




{\displaystyle y_{v}}
 we have also found the minimum vertex cover.","Given that the constraints limit 




y

v




{\displaystyle y_{v}}
 to either 0 or 1, any feasible solution to the integer program is a subset of vertices. The first constraint implies that at least one end point of every edge is included in this subset.","[' What is a feasible solution to the integer program?', ' What constraint implies that at least one end point of every edge is included in this subset?']","['a subset of vertices', 'first']"
3194,integer programming,Variants,"Zero-one linear programming (or binary integer programming) involves problems in which the variables are restricted to be either 0 or 1. Any bounded integer variable can be expressed as a combination of binary variables. For example, given an integer variable, 



0
≤
x
≤
U


{\displaystyle 0\leq x\leq U}
, the variable can be expressed using 



⌊

log

2


⁡
U
⌋
+
1


{\displaystyle \lfloor \log _{2}U\rfloor +1}
 binary variables:
",Zero-one linear programming (or binary integer programming) involves problems in which the variables are restricted to be either 0 or 1. Any bounded integer variable can be expressed as a combination of binary variables.,"[' What type of programming involves problems where variables are restricted to be either 0 or 1?', ' Any bounded integer variable can be expressed as a combination of what?']","['Zero-one linear programming', 'binary variables']"
3195,integer programming,Algorithms,"The naive way to solve an ILP is to simply remove the constraint that x is integer, solve the corresponding LP (called the LP relaxation of the ILP), and then round the entries of the solution to the LP relaxation.  But, not only may this solution not be optimal, it may not even be feasible; that is, it may violate some constraint.
","The naive way to solve an ILP is to simply remove the constraint that x is integer, solve the corresponding LP (called the LP relaxation of the ILP), and then round the entries of the solution to the LP relaxation. But, not only may this solution not be optimal, it may not even be feasible; that is, it may violate some constraint.","[' What is a naive way to solve an ILP?', ' What is the LP relaxation of the ILP called?', ' What may not be optimal?', ' What may violate some constraint?']","['to simply remove the constraint that x is integer', 'solve the corresponding LP', 'solution', 'this solution']"
3196,integer programming,Sparse integer programming,"It is often the case that the matrix 



A


{\displaystyle A}
 which defines the integer program is sparse. In particular, this occurs when the matrix has a block structure, which is the case in many applications. The sparsity of the matrix can be measured as follows. The graph of 



A


{\displaystyle A}
 has vertices corresponding to columns of 



A


{\displaystyle A}
, and two columns form an edge if 



A


{\displaystyle A}
 has a row where both columns have nonzero entries. Equivalently, the vertices correspond to variables, and two variables form an edge if they share an inequality. The sparsity measure 



d


{\displaystyle d}
 of 



A


{\displaystyle A}
 is the minimum between the tree-depth of the graph of 



A


{\displaystyle A}
 and the tree-depth of the graph of the transpose of 



A


{\displaystyle A}
. Let 



a


{\displaystyle a}
 be the numeric measure of 



A


{\displaystyle A}
 defined as the maximum absolute value of any entry of 



A


{\displaystyle A}
. Let 



n


{\displaystyle n}
 be the number of variables of the integer program. Then it was shown in 2018 that integer programming can be solved in strongly polynomial and fixed-parameter tractable time parameterized by 



a


{\displaystyle a}
 and 



d


{\displaystyle d}
. That is, for some computable function 



f


{\displaystyle f}
 and some constant 



k


{\displaystyle k}
, integer programming can be solved in time 



f
(
a
,
d
)

n

k




{\displaystyle f(a,d)n^{k}}
. In particular, the time is independent of the right-hand side 



b


{\displaystyle b}
 and objective function 



c


{\displaystyle c}
. Moreover, in contrast to the classical result of Lenstra, where the number 



n


{\displaystyle n}
 of variables is a parameter, here the number 



n


{\displaystyle n}
 of variables is a variable part of the input.
","It is often the case that the matrix 



A


{\displaystyle A}
 which defines the integer program is sparse. In particular, this occurs when the matrix has a block structure, which is the case in many applications.","[' What is often the case when the matrix A defines the integer program is sparse?', ' When the matrix has a block structure, what is the case?']","['A\n\n\n{\\displaystyle A}', 'sparse']"
3197,uml,Summary,"The creation of UML was originally motivated by the desire to standardize the disparate notational systems and approaches to software design. It was developed at Rational Software in 1994–1995, with further development led by them through 1996.","The creation of UML was originally motivated by the desire to standardize the disparate notational systems and approaches to software design. It was developed at Rational Software in 1994–1995, with further development led by them through 1996.","[' What motivated the creation of UML?', ' What company developed UML in 1994-1995?', ' Where was UML developed?']","['the desire to standardize the disparate notational systems and approaches to software design', 'Rational Software', 'Rational Software']"
3198,uml,Summary,"In 1997, UML was adopted as a standard by the Object Management Group (OMG), and has been managed by this organization ever since. In 2005, UML was also published by the International Organization for Standardization (ISO) as an approved ISO standard. Since then the standard has been periodically revised to cover the latest revision of UML. In software engineering, most practitioners do not use UML, but instead produce informal hand drawn diagrams; these diagrams, however, often include elements from UML.: 536 ","In 1997, UML was adopted as a standard by the Object Management Group (OMG), and has been managed by this organization ever since. In 2005, UML was also published by the International Organization for Standardization (ISO) as an approved ISO standard.","[' When was UML adopted as a standard by the Object Management Group (OMG)?', ' What organization published UML as an approved ISO standard?']","['1997', 'International Organization for Standardization']"
3199,uml,Diagrams,"UML 2 has many types of diagrams, which are divided into two categories. Some types represent structural information, and the rest represent general types of behavior, including a few that represent different aspects of interactions. These diagrams can be categorized hierarchically as shown in the following class diagram:","UML 2 has many types of diagrams, which are divided into two categories. Some types represent structural information, and the rest represent general types of behavior, including a few that represent different aspects of interactions.","[' How many types of diagrams does UML 2 have?', ' How many categories are there of UML diagrams?', ' Some diagram types represent structural information, while the rest represent what?']","['many', 'two', 'general types of behavior']"
3200,uml,Metamodeling,"The Object Management Group (OMG) has developed a metamodeling architecture to define the UML, called the Meta-Object Facility. MOF is designed as a four-layered architecture, as shown in the image at right. It provides a meta-meta model at the top, called the M3 layer. This M3-model is the language used by Meta-Object Facility to build metamodels, called M2-models.
","The Object Management Group (OMG) has developed a metamodeling architecture to define the UML, called the Meta-Object Facility. MOF is designed as a four-layered architecture, as shown in the image at right.","[' What is the Meta-Object Facility?', ' How many layers does the MOF have?']","['metamodeling architecture to define the UML', 'four']"
3201,uml,Metamodeling,"The most prominent example of a Layer 2 Meta-Object Facility model is the UML metamodel, which describes the UML itself. These M2-models describe elements of the M1-layer, and thus M1-models. These would be, for example, models written in UML. The last layer is the M0-layer or data layer. It is used to describe runtime instances of the system.","The most prominent example of a Layer 2 Meta-Object Facility model is the UML metamodel, which describes the UML itself. These M2-models describe elements of the M1-layer, and thus M1-models.","[' What is the most prominent example of a Layer 2 Meta-Object Facility model?', ' What describes the UML itself?']","['UML metamodel', 'UML metamodel']"
3202,uml,Metamodeling,"The meta-model can be extended using a mechanism called stereotyping. This has been criticized as being insufficient/untenable by Brian Henderson-Sellers and Cesar Gonzalez-Perez in ""Uses and Abuses of the Stereotype Mechanism in UML 1.x and 2.0"".","The meta-model can be extended using a mechanism called stereotyping. This has been criticized as being insufficient/untenable by Brian Henderson-Sellers and Cesar Gonzalez-Perez in ""Uses and Abuses of the Stereotype Mechanism in UML 1.x and 2.0"".","[' What can be extended using a mechanism called stereotyping?', ' What has been criticized as being insufficient/untenable by Brian Henderson-Sellers?']","['The meta-model', 'The meta-model can be extended using a mechanism called stereotyping']"
3203,uml,Adoption,"It has been treated, at times, as a design silver bullet, which leads to problems. UML misuse includes overuse (designing every part of the system with it, which is unnecessary) and assuming that novices can design with it.","It has been treated, at times, as a design silver bullet, which leads to problems. UML misuse includes overuse (designing every part of the system with it, which is unnecessary) and assuming that novices can design with it.","[' What has been treated as a design silver bullet?', ' What leads to problems with UML?', ' UML misuse includes what?']","['UML', 'It has been treated, at times, as a design silver bullet', 'overuse']"
3204,uml,Adoption,"It is considered a large language, with many constructs. Some people (including Jacobson) feel that UML's size hinders learning (and therefore, using) it.","It is considered a large language, with many constructs. Some people (including Jacobson) feel that UML's size hinders learning (and therefore, using) it.","[' What is UML considered to be?', ' What do some people feel hinders learning UML?']","['a large language', 'size']"
3205,efficiency,Summary,"Efficiency is the (often measurable) ability to avoid wasting materials, energy, efforts, money, and time in doing something or in producing a desired result. In a more general sense, it is the ability to do things well, successfully, and without waste. ""Efficiency is thus not a goal in itself. It is not something we want for its own sake, but rather because it helps us attain more of the things we value"". In more mathematical or scientific terms, it signifies the level of performance that uses the least amount of inputs to achieve the highest amount of output. It often specifically comprises the capability of a specific application of effort to produce a specific outcome with a minimum amount or quantity of waste, expense, or unnecessary effort. Efficiency refers to very different inputs and outputs in different fields and industries.
","Efficiency is the (often measurable) ability to avoid wasting materials, energy, efforts, money, and time in doing something or in producing a desired result. In a more general sense, it is the ability to do things well, successfully, and without waste.","[' What is the ability to avoid waste materials, energy, efforts, money, and time in doing something or in producing a desired result?', ' In a more general sense, what is efficiency?']","['Efficiency', 'the ability to do things well, successfully, and without waste']"
3206,efficiency,Efficiency and effectiveness,"Efficiency is very often confused with effectiveness. In general, efficiency is a measurable concept, quantitatively determined by the ratio of useful output to total useful input.  Effectiveness is the simpler concept of being able to achieve a desired result, which can be expressed quantitatively but does not usually require more complicated mathematics than addition. Efficiency can often be expressed as a percentage of the result that could ideally be expected, for example if no energy were lost due to friction or other causes, in which case 100% of fuel or other input would be used to produce the desired result. In some cases efficiency can be indirectly quantified with a non-percentage value, e.g. specific impulse.
","Efficiency is very often confused with effectiveness. In general, efficiency is a measurable concept, quantitatively determined by the ratio of useful output to total useful input.","[' How is efficiency often confused with effectiveness?', ' What is efficiency a measurable concept?', ' Efficiency is quantitatively determined by the ratio of what?']","['Efficiency is very often confused with effectiveness. In general, efficiency is a measurable concept', 'the ratio of useful output to total useful input', 'useful output to total useful input']"
3207,efficiency,Efficiency and effectiveness,"A common but confusing way of distinguishing between efficiency and effectiveness is the saying ""Efficiency is doing things right, while effectiveness is doing the right things."" This saying indirectly emphasizes that the selection of objectives of a production process is just as important as the quality of that process. This saying popular in business however obscures the more common sense of ""effectiveness"", which would/should produce the following mnemonic: ""Efficiency is doing things right; effectiveness is getting things done."" This makes it clear that effectiveness, for example large production numbers, can also be achieved through inefficient processes if, for example, workers are willing or used to working longer hours or with greater physical effort than in other companies or countries or if they can be forced to do so. Similarly, a company can achieve effectiveness, for example large production numbers, through inefficient processes if it can afford to use more energy per product, for example if energy prices or labor costs or both are lower than for its competitors.
","A common but confusing way of distinguishing between efficiency and effectiveness is the saying ""Efficiency is doing things right, while effectiveness is doing the right things."" This saying indirectly emphasizes that the selection of objectives of a production process is just as important as the quality of that process.","[' What is a common but confusing way of distinguishing between efficiency and effectiveness?', ' What does the saying ""Efficiency is doing things right, while effectiveness is doing the right things"" emphasize?']","['the saying ""Efficiency is doing things right, while effectiveness is doing the right things.""', 'the selection of objectives of a production process is just as important as the quality of that process']"
3208,efficiency,Mathematical expression,"Efficiency is often measured as the ratio of useful output to total input, which can be expressed with the mathematical formula r=P/C, where P is the amount of useful output (""product"") produced per the amount C (""cost"") of resources consumed. This may correspond to a percentage if products and consumables are quantified in compatible units, and if consumables are transformed into products via a conservative process. For example, in the analysis of the energy conversion efficiency of heat engines in thermodynamics, the product P may be the amount of useful work output, while the consumable C is the amount of high-temperature heat input.  Due to the conservation of energy, P can never be greater than C, and so the efficiency r is never greater than 100% (and in fact must be even less at finite temperatures).
","Efficiency is often measured as the ratio of useful output to total input, which can be expressed with the mathematical formula r=P/C, where P is the amount of useful output (""product"") produced per the amount C (""cost"") of resources consumed. This may correspond to a percentage if products and consumables are quantified in compatible units, and if consumables are transformed into products via a conservative process.","[' What is often measured as the ratio of useful output to total input?', ' What can be expressed with the mathematical formula r=P/C?', ' What may correspond to a percentage if products and consumables are quantified in compatible units?', ' Consumables can be transformed into products via what process?']","['Efficiency', 'Efficiency is often measured as the ratio of useful output to total input', 'Efficiency is often measured as the ratio of useful output to total input', 'conservative']"
3209,random oracle model,Summary,"In cryptography, a random oracle is an oracle (a theoretical black box) that responds to every unique query with a (truly) random response chosen uniformly from its output domain. If a query is repeated, it responds the same way every time that query is submitted.
","In cryptography, a random oracle is an oracle (a theoretical black box) that responds to every unique query with a (truly) random response chosen uniformly from its output domain. If a query is repeated, it responds the same way every time that query is submitted.","[' What is a random oracle in cryptography?', ' What is the name of the theoretical black box that responds to every unique query with a true random response?']","['a theoretical black box', 'a random oracle']"
3210,random oracle model,Summary,"Random oracles as a mathematical abstraction were first used in rigorous cryptographic proofs in the 1993 publication by Mihir Bellare and Phillip Rogaway (1993). They are typically used when the proof cannot be carried out using weaker assumptions on the cryptographic hash function. A system that is proven secure when every hash function is replaced by a random oracle is described as being secure in the random oracle model, as opposed to secure in the standard model of cryptography.
",Random oracles as a mathematical abstraction were first used in rigorous cryptographic proofs in the 1993 publication by Mihir Bellare and Phillip Rogaway (1993). They are typically used when the proof cannot be carried out using weaker assumptions on the cryptographic hash function.,"[' When were random oracles first used in rigorous cryptographic proofs?', ' Who published the 1993 publication?', ' What are typically used when the proof cannot be carried out using weaker assumptions on the cryptographic hash function?']","['1993', 'Mihir Bellare and Phillip Rogaway', 'Random oracles']"
3211,random oracle model,Applications,"Random oracles are typically used as an idealised replacement for cryptographic hash functions in schemes where strong randomness assumptions are needed of the hash function's output. Such a proof often shows that a system or a protocol is secure by showing that an attacker must require impossible behavior from the oracle, or solve some mathematical problem believed hard in order to break it. However, it only proves such properties in the random oracle model, making sure no major design flaws are present. It is in general not true that such a proof implies the same properties in the standard model. Still, a proof in the random oracle model is considered better than no formal security proof at all.","Random oracles are typically used as an idealised replacement for cryptographic hash functions in schemes where strong randomness assumptions are needed of the hash function's output. Such a proof often shows that a system or a protocol is secure by showing that an attacker must require impossible behavior from the oracle, or solve some mathematical problem believed hard in order to break it.","[' Random oracles are idealised replacement for what?', "" What is needed for strong randomness assumptions of the hash function's output?"", ' A proof often shows that a system or a protocol is secure by showing that an attacker must require impossible behavior from the system or protocol?', ' What must an attacker require from the oracle to break it?', ' What must a hacker have to do to break a computer?']","['cryptographic hash functions', 'Random oracles', 'Random oracles', 'impossible behavior', 'solve some mathematical problem believed hard']"
3212,random oracle model,Applications,"Not all uses of cryptographic hash functions require random oracles: schemes that require only one or more properties having a definition in the standard model (such as collision resistance, preimage resistance, second preimage resistance, etc.) can often be proven secure in the standard model (e.g., the Cramer–Shoup cryptosystem).
","Not all uses of cryptographic hash functions require random oracles: schemes that require only one or more properties having a definition in the standard model (such as collision resistance, preimage resistance, second preimage resistance, etc.) can often be proven secure in the standard model (e.g., the Cramer–Shoup cryptosystem).","[' What does not require random oracles?', ' Schemes that require only one or more properties having a definition in the standard model can be proven secure in what model?', ' What is the Cramer-Shoup cryptosystem?']","['cryptographic hash functions', 'standard model', 'secure in the standard model']"
3213,random oracle model,Applications,"Random oracles have long been considered in computational complexity theory, and many schemes have been proven secure in the random oracle model, for example Optimal Asymmetric Encryption Padding, RSA-FDH and Probabilistic Signature Scheme. In 1986, Amos Fiat and Adi Shamir showed a major application of random oracles – the removal of interaction from protocols for the creation of signatures.
","Random oracles have long been considered in computational complexity theory, and many schemes have been proven secure in the random oracle model, for example Optimal Asymmetric Encryption Padding, RSA-FDH and Probabilistic Signature Scheme. In 1986, Amos Fiat and Adi Shamir showed a major application of random oracles – the removal of interaction from protocols for the creation of signatures.","[' What has long been considered in computational complexity theory?', ' What have many schemes been proven secure in the random oracle model?', ' In what year did Amos Fiat and Adi Shamir show a major application of Random Oracles?', ' What did random oracles remove from protocols for the creation of signatures?']","['Random oracles', 'Optimal Asymmetric Encryption Padding, RSA-FDH and Probabilistic Signature Scheme', '1986', 'interaction']"
3214,random oracle model,Applications,"In 1993, Mihir Bellare and Phillip Rogaway were the first to advocate their use in cryptographic constructions. In their definition, the random oracle produces a bit-string of infinite length which can be truncated to the length desired.
","In 1993, Mihir Bellare and Phillip Rogaway were the first to advocate their use in cryptographic constructions. In their definition, the random oracle produces a bit-string of infinite length which can be truncated to the length desired.","[' Who were the first to advocate the use of random oracles in cryptographic constructions?', ' What did Mihir Bellare and Phillip Rogaway advocate in 1993?']","['Mihir Bellare and Phillip Rogaway', 'their use in cryptographic constructions']"
3215,random oracle model,Applications,"When a random oracle is used within a security proof, it is made available to all players, including the adversary or adversaries. A single oracle may be treated as multiple oracles by pre-pending a fixed bit-string to the beginning of each query (e.g., queries formatted as ""1|x"" or ""0|x"" can be considered as calls to two separate random oracles, similarly ""00|x"", ""01|x"", ""10|x"" and ""11|x"" can be used to represent calls to four separate random oracles).
","When a random oracle is used within a security proof, it is made available to all players, including the adversary or adversaries. A single oracle may be treated as multiple oracles by pre-pending a fixed bit-string to the beginning of each query (e.g., queries formatted as ""1|x"" or ""0|x"" can be considered as calls to two separate random oracles, similarly ""00|x"", ""01|x"", ""10|x"" and ""11|x"" can be used to represent calls to four separate random oracles).","[' What is made available to all players when a random oracle is used within a security proof?', ' What may be treated as multiple oracles by pre-pending a fixed bit-string to the beginning of each query?', ' What can be used to represent calls to four separate random oracles?']","['including the adversary or adversaries', 'A single oracle', '""00|x"", ""01|x"", ""10|x"" and ""11|x""']"
3216,random oracle model,Limitations,"In fact, certain artificial signature and encryption schemes are known which are proven secure in the random oracle model, but which are trivially insecure when any real function is substituted for the random oracle. Nonetheless, for any more natural protocol a proof of security in the random oracle model gives very strong evidence of the practical security of the protocol.","In fact, certain artificial signature and encryption schemes are known which are proven secure in the random oracle model, but which are trivially insecure when any real function is substituted for the random oracle. Nonetheless, for any more natural protocol a proof of security in the random oracle model gives very strong evidence of the practical security of the protocol.","[' What are known as being secure in the random oracle model but trivially insecure when a real function is substituted for it?', ' For any more natural protocol a proof of security in what model gives?', ' What model gives very strong evidence of the practical security of a protocol?']","['artificial signature and encryption schemes', 'random oracle', 'random oracle']"
3217,random oracle model,Limitations,"In general, if a protocol is proven secure, attacks to that protocol must either be outside what was proven, or break one of the assumptions in the proof; for instance if the proof relies on the hardness of integer factorization, to break this assumption one must discover a fast integer factorization algorithm. Instead, to break the random oracle assumption, one must discover some unknown and undesirable property of the actual hash function; for good hash functions where such properties are believed unlikely, the considered protocol can be considered secure.
","In general, if a protocol is proven secure, attacks to that protocol must either be outside what was proven, or break one of the assumptions in the proof; for instance if the proof relies on the hardness of integer factorization, to break this assumption one must discover a fast integer factorization algorithm. Instead, to break the random oracle assumption, one must discover some unknown and undesirable property of the actual hash function; for good hash functions where such properties are believed unlikely, the considered protocol can be considered secure.","[' If a protocol is proven secure, what must be outside of what was proven?', ' If the proof relies on the hardness of integer factorization, one must discover what to break this assumption?', ' To break the random oracle assumption, one must discover what?', ' What must one discover in order to break this assumption?', ' For good hash functions where such properties are believed unlikely, what can be considered secure?']","['attacks', 'a fast integer factorization algorithm', 'some unknown and undesirable property of the actual hash function', 'some unknown and undesirable property of the actual hash function', 'the considered protocol']"
3218,random oracle model,Random Oracle Hypothesis,"Although the Baker–Gill–Solovay theorem showed that there exists an oracle A such that PA = NPA, subsequent work by Bennett and Gill, showed that for a random oracle B (a function from {0,1}n to {0,1} such that each input element maps to each of 0 or 1 with probability 1/2, independently of the mapping of all other inputs), PB ⊊ NPB with probability 1. Similar separations, as well as the fact that random oracles separate classes with probability 0 or 1 (as a consequence of the Kolmogorov's zero–one law), led to the creation of the Random Oracle Hypothesis, that two ""acceptable"" complexity classes C1 and C2 are equal if and only if they are equal (with probability 1) under a random oracle (the acceptability of a complexity class is defined in BG81). This hypothesis was later shown to be false, as the two acceptable complexity classes IP and PSPACE were shown to be equal despite IPA ⊊ PSPACEA for a random oracle A with probability 1.","Although the Baker–Gill–Solovay theorem showed that there exists an oracle A such that PA = NPA, subsequent work by Bennett and Gill, showed that for a random oracle B (a function from {0,1}n to {0,1} such that each input element maps to each of 0 or 1 with probability 1/2, independently of the mapping of all other inputs), PB ⊊ NPB with probability 1. Similar separations, as well as the fact that random oracles separate classes with probability 0 or 1 (as a consequence of the Kolmogorov's zero–one law), led to the creation of the Random Oracle Hypothesis, that two ""acceptable"" complexity classes C1 and C2 are equal if and only if they are equal (with probability 1) under a random oracle (the acceptability of a complexity class is defined in BG81).","[' What theorem showed that there exists an oracle A such that PA = NPA?', ' What work by Bennett and Gill showed that for a random or acle B, each input element maps to each of 0 or 1 with probability 1/2?', "" What is a consequence of the Kolmogorov's zero-one law?"", ' What does PB <unk> NPB have in common?', ' What law led to the creation of the Random Oracle Hypothesis?', ' How many ""acceptable"" complexity classes C1 and C2 are equal if and only if they are equal under a random oracle?', ' What is the acceptability of a complexity class defined in?']","['Baker–Gill–Solovay', 'oracle B', 'random oracles separate classes with probability 0 or 1', 'separations', ""Kolmogorov's zero–one law"", 'two', 'BG81']"
3219,random oracle model,Ideal Cipher,"An ideal cipher is a random permutation oracle that is used to model an idealized block cipher. A random permutation decrypts each ciphertext block into one and only one plaintext block and vice versa, so there is a one-to-one correspondence. Some cryptographic proofs make not only the ""forward"" permutation available to all players, but also the ""reverse"" permutation.
","An ideal cipher is a random permutation oracle that is used to model an idealized block cipher. A random permutation decrypts each ciphertext block into one and only one plaintext block and vice versa, so there is a one-to-one correspondence.","[' What is an ideal cipher?', ' What is a random permutation oracle used to model?']","['a random permutation oracle', 'an idealized block cipher']"
3220,random oracle model,Ideal Permutation,"An ideal permutation is an idealized object sometimes used in cryptography to model the behaviour of a permutation whose outputs are indistinguishable from those of a random permutation. In the ideal permutation model, an additional oracle access is given to the ideal permutation and its inverse. The ideal permutation model can be seen as a special case of the ideal cipher model where access is given to only a single permutation, instead of a family of permutations as in the case of the ideal cipher model.
","An ideal permutation is an idealized object sometimes used in cryptography to model the behaviour of a permutation whose outputs are indistinguishable from those of a random permutation. In the ideal permutation model, an additional oracle access is given to the ideal permutation and its inverse.","[' What is an idealized object sometimes used in cryptography to model the behaviour of a permutation?', ' What outputs are indistinguishable from those of random permutations?', ' An additional oracle access is given to what?']","['An ideal permutation', 'ideal permutation', 'the ideal permutation and its inverse']"
3221,random oracle model,Quantum-accessible Random Oracles,"Post-quantum cryptography studies quantum attacks on classical cryptographic schemes. As a random oracle is an abstraction of a hash function, it makes sense to assume that a quantum attacker can access the random oracle in quantum superposition. Many of the classical security proofs break down in that quantum random oracle model and need to be revised.
","Post-quantum cryptography studies quantum attacks on classical cryptographic schemes. As a random oracle is an abstraction of a hash function, it makes sense to assume that a quantum attacker can access the random oracle in quantum superposition.","[' Post-quantum cryptography studies quantum attacks on what?', ' What is an abstraction of a hash function?', ' A quantum attacker can access the random oracle in quantum superposition?']","['classical cryptographic schemes', 'random oracle', 'random oracle is an abstraction of a hash function']"
3222,ambient intelligence,Summary,"In computing, ambient intelligence (AmI) refers to electronic environments that are sensitive and responsive to the presence of people. Ambient intelligence was a projection on the future of consumer electronics, telecommunications and computing that was originally developed in the late 1990s by Eli Zelkha and his team at Palo Alto Ventures for the time frame 2010–2020. Ambient intelligence would allow devices to work in concert to support people in carrying out their everyday life activities, tasks and rituals in an intuitive way using information and intelligence that is hidden in the network connecting these devices (for example: The Internet of Things). As these devices grew smaller, more connected and more integrated into our environment, the technological framework behind them would disappear into our surroundings until only the user interface remains perceivable by users.
","In computing, ambient intelligence (AmI) refers to electronic environments that are sensitive and responsive to the presence of people. Ambient intelligence was a projection on the future of consumer electronics, telecommunications and computing that was originally developed in the late 1990s by Eli Zelkha and his team at Palo Alto Ventures for the time frame 2010–2020.","[' What does AmI stand for?', ' What is the term for electronic environments that are sensitive and responsive to the presence of people?', ' When was Ambient Intelligence first developed?', ' Where was Eli Zelkha based?', ' Eli Zelkha and his team at Palo Alto Ventures in what decade?', ' What was the time frame between 2010 and 2020?']","['ambient intelligence', 'ambient intelligence', 'late 1990s', 'Palo Alto Ventures', '2010–2020', '2010–']"
3223,ambient intelligence,Overview,"Ambient intelligence is primarily of interest because of its relationship to user experience and the advancement in sensor technology and sensor networks. The interest in user experience grew in importance in the late 1990s as a result of the increasing volume and importance of digital products and services that were difficult to understand or use. In response, the user experience design emerged to create new technologies and media around the user's personal experience. Ambient intelligence is influenced by user-centered design where the user is placed in the center of the design activity and asked to give feedback through specific user evaluations and tests to improve the design or even co-create the design with the designer (participatory design) or with other users (end-user development).
",Ambient intelligence is primarily of interest because of its relationship to user experience and the advancement in sensor technology and sensor networks. The interest in user experience grew in importance in the late 1990s as a result of the increasing volume and importance of digital products and services that were difficult to understand or use.,"[' What is primarily of interest because of its relationship to user experience and the advancement in sensor technology and sensor networks?', ' What grew in importance in the late 1990s due to the increasing volume and importance of digital products and services?', ' What was the volume of digital products and services that were difficult to understand or use?']","['Ambient intelligence', 'The interest in user experience', 'increasing volume and importance']"
3224,ambient intelligence,Overview,"Ambient intelligence requires a number of key technologies to exist. These include unobtrusive, user-friendly hardware such as miniaturization, nanotechnology, and smart devices, as well as human-centric computer interfaces (intelligent agents, multimodal interaction, context awareness etc.) These systems and devices operate through a seamless mobile/fixed communication and computing infrastructure characterized by interoperability, wired and wireless networks, and service-oriented architecture.
","Ambient intelligence requires a number of key technologies to exist. These include unobtrusive, user-friendly hardware such as miniaturization, nanotechnology, and smart devices, as well as human-centric computer interfaces (intelligent agents, multimodal interaction, context awareness etc.)","[' What is a key technology for Ambient Intelligence to exist?', ' What are some examples of unobtrusive, user-friendly hardware?', ' Miniaturization, nanotechnology and smart devices are examples of what?']","['unobtrusive, user-friendly hardware such as miniaturization, nanotechnology, and smart devices', 'miniaturization, nanotechnology, and smart devices', 'unobtrusive, user-friendly hardware']"
3225,ambient intelligence,Overview,"To implement ambient intelligence dynamic and massively distributed device networks, which are easy to control and program (e.g. service discovery, auto-configuration, end-user programmable devices and systems, etc.) These systems and devices must also be dependable and secure, which may be achieved through self-testing and self-repairing software and privacy ensuring technology.
","To implement ambient intelligence dynamic and massively distributed device networks, which are easy to control and program (e.g. service discovery, auto-configuration, end-user programmable devices and systems, etc.)","[' To implement ambient intelligence dynamic and massively distributed device networks, which are easy to control and program?']","['service discovery, auto-configuration, end-user programmable devices and systems']"
3226,ambient intelligence,History and invention,"In 1998, the board of management of Philips commissioned a series of presentations and internal workshops, organized by Eli Zelkha and Brian Epstein of Palo Alto Ventures. Zelkha, together with Simon Birrell, coined the term 'ambient intelligence' to investigate different scenarios that would transform the high-volume consumer electronic industry of the 1990s, which they described as ""fragmented with features"", into an industry where user-friendly devices supported ubiquitous information, communication and entertainment by 2020. While developing the ambient intelligence concept, Palo Alto Ventures created the keynote address for Roel Pieper of Philips for the Digital Living Room Conference, 1998. The group included Eli Zelkha, Brian Epstein, Simon Birrell, Doug Randall, and Clark Dodsworth. These plans continued to develop throughout the 1990s, and in 2000, plans were made to construct a feasibility and usability facility dedicated to ambient intelligence. This HomeLab officially opened on 24 April 2002. In 2005, Philips joined the Oxygen alliance, an international consortium of industrial partners within the context of the MIT Oxygen project, aimed at developing technology for the computer of the 21st century.
","In 1998, the board of management of Philips commissioned a series of presentations and internal workshops, organized by Eli Zelkha and Brian Epstein of Palo Alto Ventures. Zelkha, together with Simon Birrell, coined the term 'ambient intelligence' to investigate different scenarios that would transform the high-volume consumer electronic industry of the 1990s, which they described as ""fragmented with features"", into an industry where user-friendly devices supported ubiquitous information, communication and entertainment by 2020.","[' Who organized a series of presentations and internal workshops in 1998?', ' What did Eli Zelkha and Brian Epstein call Ambient Intelligence?', ' What was the high-volume consumer electronic industry of the 1990s described as?', ' By what year would user-friendly devices support ubiquitous information, communication and entertainment?']","['Eli Zelkha and Brian Epstein', 'fragmented with features', 'fragmented with features', '2020']"
3227,ambient intelligence,History and invention,"Along with the development of the vision at Philips, a number of parallel initiatives started to explore ambient intelligence in more detail. Following the advice of the Information Society and Technology Advisory Group (ISTAG), the European Commission used the vision for the launch of their sixth framework (FP6) in Information, Society and Technology (IST), with a subsidiary budget of 3.7 billion euros. The European Commission played a crucial role in the further development of the AmI vision. As a result of many initiatives the AmI vision gained traction. During the past few years several major initiatives have been started. Fraunhofer Society started several activities in a variety of domains including multimedia, microsystems design and augmented spaces. Massachusetts Institute of Technology started an ambient intelligence research group at their Media Lab. Several more research projects started in a variety of countries such as the US, Canada, Spain, France and the Netherlands. Since 2004, the European Symposium on Ambient Intelligence (EUSAI) and many other conferences have been held that address special topics in AmI.
","Along with the development of the vision at Philips, a number of parallel initiatives started to explore ambient intelligence in more detail. Following the advice of the Information Society and Technology Advisory Group (ISTAG), the European Commission used the vision for the launch of their sixth framework (FP6) in Information, Society and Technology (IST), with a subsidiary budget of 3.7 billion euros.","[' What company developed the vision for ambient intelligence?', ' What group advised the European Commission on the development of their sixth framework in Information?', ' What is the name of the sixth framework?', ' What is IST?', ' How much money does IST have?']","['Philips', 'Information Society and Technology Advisory Group', 'FP6', 'Information, Society and Technology', '3.7 billion euros']"
3228,ambient intelligence,Criticism,"As far as dissemination of information on personal presence is out of control, ambient intelligence vision is subject of criticism (e.g. David Wright, Serge Gutwirth, Michael Friedewald et al., Safeguards in a World of Ambient Intelligence, Springer, Dordrecht, 2008). Any immersive, personalized, context-aware and anticipatory characteristics brings up societal, political and cultural concerns about the loss of privacy. The example scenario above shows both the positive and negative possibilities offered by ambient intelligence. Applications of ambient intelligence do not necessarily have to reduce privacy in order to work.","As far as dissemination of information on personal presence is out of control, ambient intelligence vision is subject of criticism (e.g. David Wright, Serge Gutwirth, Michael Friedewald et al., Safeguards in a World of Ambient Intelligence, Springer, Dordrecht, 2008).","[' What is the subject of criticism regarding ambient intelligence vision?', ' Who wrote Safeguards in a World of Ambient Intelligence?']","['dissemination of information on personal presence is out of control', 'David Wright, Serge Gutwirth, Michael Friedewald']"
3229,ambient intelligence,Criticism,"Power concentration in large organizations, a fragmented, decreasingly private society and hyperreal environments where the virtual is indistinguishable from the real are the main topics of critics. Several research groups and communities are investigating the socioeconomic, political and cultural aspects of ambient intelligence.
","Power concentration in large organizations, a fragmented, decreasingly private society and hyperreal environments where the virtual is indistinguishable from the real are the main topics of critics. Several research groups and communities are investigating the socioeconomic, political and cultural aspects of ambient intelligence.","[' What are the main topics of critics?', ' What is the virtual indistinguishable from the real?']","['Power concentration in large organizations, a fragmented, decreasingly private society and hyperreal environments', 'hyperreal environments']"
3230,ambient intelligence,Social and political aspects,"The ISTAG advisory group suggests that the following characteristics will permit the societal acceptance of ambient intelligence. Ambient intelligence should:
",The ISTAG advisory group suggests that the following characteristics will permit the societal acceptance of ambient intelligence. Ambient intelligence should:,[' What does the ISTAG advisory group suggest will permit the societal acceptance of ambient intelligence?'],['the following characteristics']
3231,ambient intelligence,Computing,This means of computing links all pieces of technology together. This also allows the device to have the capability to remember past requests.,This means of computing links all pieces of technology together. This also allows the device to have the capability to remember past requests.,"[' What does this means of computing link all pieces of technology together?', ' What does the device have the capability to remember?']","['allows the device to have the capability to remember past requests', 'past requests']"
3232,symbolic execution,Summary,"In computer science, symbolic execution (also symbolic evaluation or symbex) is a means of analyzing a program to determine what inputs cause each part of a program to execute.  An interpreter follows the program, assuming symbolic values for inputs rather than obtaining actual inputs as normal execution of the program would.  It thus arrives at expressions in terms of those symbols for expressions and variables in the program, and constraints in terms of those symbols for the possible outcomes of each conditional branch. Finally, the possible inputs that trigger a branch can be determined by solving the constraints.
","In computer science, symbolic execution (also symbolic evaluation or symbex) is a means of analyzing a program to determine what inputs cause each part of a program to execute. An interpreter follows the program, assuming symbolic values for inputs rather than obtaining actual inputs as normal execution of the program would.","[' What is symbolic execution also known as?', ' What is a means of analyzing a program to determine what inputs cause each part of the program to execute?', ' An interpreter follows the program, assuming what?', ' What type of inputs would a normal program obtain?']","['symbolic evaluation or symbex', 'symbolic execution', 'symbolic values for inputs', 'actual inputs']"
3233,symbolic execution,Summary,"The field of symbolic simulation applies the same concept to hardware. Symbolic computation applies the concept to the analysis of mathematical expressions.
",The field of symbolic simulation applies the same concept to hardware. Symbolic computation applies the concept to the analysis of mathematical expressions.,"[' What does symbolic simulation apply the same concept to?', ' What applies the concept to the analysis of mathematical expressions?']","['hardware', 'Symbolic computation']"
3234,symbolic execution,Example,"During a normal execution (""concrete"" execution), the program would read a concrete input value (e.g., 5) and assign it to y. Execution would then proceed with the multiplication and the conditional branch, which would evaluate to false and print OK.
","During a normal execution (""concrete"" execution), the program would read a concrete input value (e.g., 5) and assign it to y. Execution would then proceed with the multiplication and the conditional branch, which would evaluate to false and print OK.","[' During what type of execution would the program read a concrete input value and assign it to y?', ' The program would then proceed with the multiplication and the conditional branch which would evaluate to false and print OK?']","['normal', 'execution']"
3235,symbolic execution,Example,"During symbolic execution, the program reads a symbolic value (e.g., λ) and assigns it to y. The program would then proceed with the multiplication and assign λ * 2 to z. When reaching the if statement, it would evaluate λ * 2 == 12. At this point of the program, λ could take any value, and symbolic execution can therefore proceed along both branches, by ""forking"" two paths. Each path gets assigned a copy of the program state at the branch instruction as well as a path constraint. In this example, the path constraint is λ * 2 == 12 for the then branch and λ * 2 != 12 for the else branch. Both paths can be symbolically executed independently. When paths terminate (e.g., as a result of executing fail() or simply exiting), symbolic execution computes a concrete value for λ by solving the accumulated path constraints on each path. These concrete values can be thought of as concrete test cases that can, e.g., help developers reproduce bugs. In this example, the constraint solver would determine that in order to reach the fail() statement, λ would need to equal 6.
","During symbolic execution, the program reads a symbolic value (e.g., λ) and assigns it to y. The program would then proceed with the multiplication and assign λ * 2 to z.","[' During symbolic execution, the program reads a symbolic value and assigns it to what?', ' The program would then proceed with the multiplication and assign what to z?']","['y', 'λ * 2']"
3236,formal methods,Summary,"In computer science, specifically software engineering and hardware engineering, formal methods are a particular kind of mathematically rigorous techniques for the specification, development and verification of software and hardware systems. The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design.","In computer science, specifically software engineering and hardware engineering, formal methods are a particular kind of mathematically rigorous techniques for the specification, development and verification of software and hardware systems. The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design.","[' What are formal methods in computer science?', ' What is a particular kind of mathematical rigorous techniques for the specification, development and verification of software and hardware?', ' What is motivated by the expectation that performing appropriate mathematical analysis can contribute to the reliability and robustness of a design?']","['mathematically rigorous techniques for the specification, development and verification of software and hardware systems', 'formal methods', 'The use of formal methods for software and hardware design']"
3237,formal methods,Background,"Semi-Formal Methods are formalisms and languages that are not considered fully “formal”. It defers the task of completing the semantics to a later stage, which is then done either by human interpretation or by interpretation through software like code or test case generators.","Semi-Formal Methods are formalisms and languages that are not considered fully “formal”. It defers the task of completing the semantics to a later stage, which is then done either by human interpretation or by interpretation through software like code or test case generators.","[' What are formalisms and languages that are not considered fully formal?', ' Semi-Formal Methods defer the task of completing the semantics to a later stage which is done either by human interpretation or by what?']","['Semi-Formal Methods', 'interpretation through software']"
3238,formal methods,Taxonomy,"Level 0: Formal specification may be undertaken and then a program developed from this informally. This has been dubbed formal methods lite. This may be the most cost-effective option in many cases.
",Level 0: Formal specification may be undertaken and then a program developed from this informally. This has been dubbed formal methods lite.,"[' Level 0: Formal specification may be undertaken and then a program developed from this what?', ' What has been dubbed formal methods lite?']","['informally', 'Formal specification may be undertaken and then a program developed from this informally']"
3239,formal methods,Taxonomy,"Level 1: Formal development and formal verification may be used to produce a program in a more formal manner. For example, proofs of properties or refinement from the specification to a program may be undertaken. This may be most appropriate in high-integrity systems involving safety or security.
","Level 1: Formal development and formal verification may be used to produce a program in a more formal manner. For example, proofs of properties or refinement from the specification to a program may be undertaken.","[' Level 1: Formal development and formal verification may be used to produce a program in a more formal manner.', ' Proofs of properties or refinement from the specification may be undertaken what?']","['proofs of properties or refinement from the specification to a program may be undertaken.', 'a program']"
3240,formal methods,Taxonomy,"Level 2: Theorem provers may be used to undertake fully formal machine-checked proofs. Despite improving tools and declining costs, this can be very expensive and is only practically worthwhile if the cost of mistakes is very high (e.g., in critical parts of operating system or microprocessor design).
","Level 2: Theorem provers may be used to undertake fully formal machine-checked proofs. Despite improving tools and declining costs, this can be very expensive and is only practically worthwhile if the cost of mistakes is very high (e.g., in critical parts of operating system or microprocessor design).","[' Level 2: Theorem provers may be used to undertake what?', ' What can be very expensive?']","['fully formal machine-checked proofs', 'Theorem provers may be used to undertake fully formal machine-checked proofs']"
3241,formal methods,Applications,"Formal methods are applied in different areas of hardware and software, including routers, Ethernet switches, routing protocols, security applications, and operating system microkernels such as seL4. There are several examples in which they have been used to verify the functionality of the hardware and software used in DCs. IBM used ACL2, a theorem prover, in the AMD x86 processor development process. Intel uses such methods to verify its hardware and firmware (permanent software programmed into a read-only memory). Dansk Datamatik Center used formal methods in the 1980s to develop a compiler system for the Ada programming language that went on to become a long-lived commercial product.","Formal methods are applied in different areas of hardware and software, including routers, Ethernet switches, routing protocols, security applications, and operating system microkernels such as seL4. There are several examples in which they have been used to verify the functionality of the hardware and software used in DCs.","[' What type of methods are used in routers, Ethernet switches, routing protocols, security applications, and operating system microkernels such as seL4?']",['Formal']
3242,formal methods,Applications,"Formal verification has been frequently used in hardware by most of the well-known hardware vendors, such as IBM, Intel, and AMD. There are many areas of hardware, where Intel have used FMs to verify the working of the products, such as parameterized verification of cache-coherent protocol, Intel Core i7 processor execution engine validation  (using theorem proving, BDDs, and symbolic evaluation), optimization for Intel IA-64 architecture using HOL light theorem prover, and verification of high-performance dual-port gigabit Ethernet controller with support for PCI express protocol and Intel advance management technology using Cadence. Similarly, IBM has used formal methods in the verification of power gates, registers, and functional verification of the IBM Power7 microprocessor.","Formal verification has been frequently used in hardware by most of the well-known hardware vendors, such as IBM, Intel, and AMD. There are many areas of hardware, where Intel have used FMs to verify the working of the products, such as parameterized verification of cache-coherent protocol, Intel Core i7 processor execution engine validation  (using theorem proving, BDDs, and symbolic evaluation), optimization for Intel IA-64 architecture using HOL light theorem prover, and verification of high-performance dual-port gigabit Ethernet controller with support for PCI express protocol and Intel advance management technology using Cadence.","[' IBM, Intel, and AMD are some of the well-known hardware vendors using what?', ' What type of verification has been frequently used in hardware?', ' Which processors have used FMs to verify the working of?', ' What does theorem proving, BDDs, and symbolic evaluation do?', ' What is used to optimize for Intel IA-64 architecture?', ' How is the optimization of dual-port gigabit Ethernet controller verified?', ' Support for what protocol?', ' Support for Intel advance management technology using Cadence?']","['Formal verification', 'Formal verification', 'Intel', 'Intel Core i7 processor execution engine validation', 'HOL light theorem prover', 'HOL light theorem prover', 'PCI express protocol', 'PCI express protocol']"
3243,formal methods,In software development,"In software development, formal methods are mathematical approaches to solving software (and hardware) problems at the requirements, specification, and design levels. Formal methods are most likely to be applied to safety-critical or security-critical software and systems, such as avionics software. Software safety assurance standards, such as DO-178C allows the usage of formal methods through supplementation, and Common Criteria mandates formal methods at the highest levels of categorization.
","In software development, formal methods are mathematical approaches to solving software (and hardware) problems at the requirements, specification, and design levels. Formal methods are most likely to be applied to safety-critical or security-critical software and systems, such as avionics software.","[' What are formal methods used for in software development?', ' Formal methods are most likely to be applied to what type of software and systems?']","['solving software (and hardware) problems at the requirements, specification, and design levels', 'safety-critical or security-critical']"
3244,formal methods,In software development,"Another approach to formal methods in software development is to write a specification in some form of logic—usually a variation of first-order logic (FOL)—and then to directly execute the logic as though it were a program. The OWL language, based on Description Logic (DL), is an example. There is also work on mapping some version of English (or another natural language) automatically to and from logic, as well as executing the logic directly. Examples are Attempto Controlled English, and Internet Business Logic, which do not seek to control the vocabulary or syntax. A feature of systems that support bidirectional English-logic mapping and direct execution of the logic is that they can be made to explain their results, in English, at the business or scientific level.","Another approach to formal methods in software development is to write a specification in some form of logic—usually a variation of first-order logic (FOL)—and then to directly execute the logic as though it were a program. The OWL language, based on Description Logic (DL), is an example.","[' What is another approach to formal methods in software development?', ' What is the OWL language based on?']","['to write a specification in some form of logic', 'Description Logic']"
3245,crowdsourcing,Summary,"As of 2021, crowdsourcing typically involves using the internet to attract and divide work between participants to achieve a cumulative result, however, it may not always be an online activity. The word ""crowdsourcing"" itself—a portmanteau of ""crowd"" and ""outsourcing""—was allegedly coined in 2005. In contrast to outsourcing, however, crowdsourcing usually involves a less-specific, more public group.","As of 2021, crowdsourcing typically involves using the internet to attract and divide work between participants to achieve a cumulative result, however, it may not always be an online activity. The word ""crowdsourcing"" itself—a portmanteau of ""crowd"" and ""outsourcing""—was allegedly coined in 2005.","[' What does crowdsourcing typically involve using the internet to attract and divide work between participants to achieve a cumulative result?', ' When was the word ""crowdsourcing"" allegedly coined?']","['As of 2021', '2005']"
3246,crowdsourcing,Summary,"Advantages of using crowdsourcing may include improved costs, speed, quality, flexibility, scalability, or diversity. Common crowdsourcing methods include competitions, virtual labour markets, and open online collaboration. Some forms of crowdsourcing, such as in ""idea competitions"" or ""innovation contests"" provide ways for organizations to learn beyond the ""base of minds"" provided by their employees (e.g. LEGO Ideas). Monotonous ""microtasks"" performed in parallel by large, paid crowds (e.g. Amazon Mechanical Turk) are another form of crowdsourcing. Not-for-profit organizations have used crowdsourcing to develop common goods (e.g. Wikipedia).","Advantages of using crowdsourcing may include improved costs, speed, quality, flexibility, scalability, or diversity. Common crowdsourcing methods include competitions, virtual labour markets, and open online collaboration.","[' What are some of the advantages of using crowdsourcing?', ' What is one of the common crowdsourcing methods?', ' Competitions, virtual labour markets, and open online collaboration are examples of what?']","['improved costs, speed, quality, flexibility, scalability, or diversity', 'competitions', 'crowdsourcing methods']"
3247,crowdsourcing,Definitions,"The term crowdsourcing was coined in 2005 by Jeff Howe and Mark Robinson, editors at Wired, to describe how businesses were using the Internet to ""outsource work to the crowd,"" which quickly led to the portmanteau ""crowdsourcing"". Howe first published a definition for the term in a companion blog post to his June 2006 Wired article, ""The Rise of Crowdsourcing"", which came out in print just days later:","The term crowdsourcing was coined in 2005 by Jeff Howe and Mark Robinson, editors at Wired, to describe how businesses were using the Internet to ""outsource work to the crowd,"" which quickly led to the portmanteau ""crowdsourcing"". Howe first published a definition for the term in a companion blog post to his June 2006 Wired article, ""The Rise of Crowdsourcing"", which came out in print just days later:","[' Who coined the term crowdsourcing in 2005?', ' What did Jeff Howe and Mark Robinson describe businesses using the internet to do?', ' When was the term ""crowdsourcing"" first used?', ' How did Howe first publish a definition for the term?', "" What was Crowdsourcing's 2006 Wired article called?"", ' When did Wired publish the article?', "" What was the title of Wired's June 2006 article?<extra_id_51>""]","['Jeff Howe and Mark Robinson', 'outsource work to the crowd', '2005', 'a companion blog post', 'The Rise of Crowdsourcing', 'June 2006', 'The Rise of Crowdsourcing']"
3248,crowdsourcing,Definitions,"Simply defined, crowdsourcing represents the act of a company or institution taking a function once performed by employees and outsourcing it to an undefined (and generally large) network of people in the form of an open call. This can take the form of peer-production (when the job is performed collaboratively), but is also often undertaken by sole individuals. The crucial prerequisite is the use of the open call format and the large network of potential laborers.","Simply defined, crowdsourcing represents the act of a company or institution taking a function once performed by employees and outsourcing it to an undefined (and generally large) network of people in the form of an open call. This can take the form of peer-production (when the job is performed collaboratively), but is also often undertaken by sole individuals.","[' What is the act of a company or institution taking a function once performed by employees and outsourcing it to an undefined network of people?', ' What is peer-production when the job is performed collaboratively?', ' What is the form of peer-production when the job is performed collaboratively?']","['crowdsourcing', 'crowdsourcing', 'crowdsourcing']"
3249,crowdsourcing,Definitions,"In an article from 1 February 2008, Daren C. Brabham, ""the first [person] to publish scholarly research using the word crowdsourcing"" and writer of the 2013 book Crowdsourcing, defined it as an ""online, distributed problem-solving and production model."" Kristen L. Guth and Brabham found that the performance of ideas offered in crowdsourcing platforms are affected not only by their quality, but also by the communication among users about the ideas, and presentation in the platform itself.","In an article from 1 February 2008, Daren C. Brabham, ""the first [person] to publish scholarly research using the word crowdsourcing"" and writer of the 2013 book Crowdsourcing, defined it as an ""online, distributed problem-solving and production model."" Kristen L. Guth and Brabham found that the performance of ideas offered in crowdsourcing platforms are affected not only by their quality, but also by the communication among users about the ideas, and presentation in the platform itself.","[' Who was the first person to publish scholarly research using the word crowdsourcing?', ' What book did Daren C. Brabham write in 2013?', ' Who found that the performance of ideas offered by Crowdsourcing was a problem-solving and production model?', ' Guth and Brabham found that the performance of ideas offered in crowdsourcing platforms are affected by what?', ' What is the communication among users about the ideas affected by?']","['Daren C. Brabham', 'Crowdsourcing', 'Kristen L. Guth and Brabham', 'the communication among users about the ideas', 'presentation in the platform itself']"
3250,crowdsourcing,Definitions,"Crowdsourcing can either take an explicit or an implicit route. Explicit crowdsourcing lets users work together to evaluate, share, and build different specific tasks, while implicit crowdsourcing means that users solve a problem as a side effect of something else they are doing. With explicit crowdsourcing, users can evaluate particular items like books or webpages, or share by posting products or items. Users can also build artifacts by providing information and editing other people's work. Implicit crowdsourcing can take two forms: standalone and piggyback. Standalone allows people to solve problems as a side effect of the task they are doing, whereas piggyback takes users' information from a third-party website to gather information.","Crowdsourcing can either take an explicit or an implicit route. Explicit crowdsourcing lets users work together to evaluate, share, and build different specific tasks, while implicit crowdsourcing means that users solve a problem as a side effect of something else they are doing.","[' What kind of route can crowdsourcing take?', ' What type of crowdsourcing lets users work together to evaluate, share, and build different specific tasks?']","['explicit or an implicit', 'Explicit']"
3251,crowdsourcing,Definitions,"As mentioned by the definitions of Brabham and Estellés-Arolas and Ladrón-de-Guevara above, crowdsourcing in the modern conception is an IT-mediated phenomenon, meaning that a form of IT is always used to create and access crowds of people. In this respect, crowdsourcing has been considered to encompass three separate, but stable techniques; competition crowdsourcing, virtual labor market crowdsourcing, and open collaboration crowdsourcing.","As mentioned by the definitions of Brabham and Estellés-Arolas and Ladrón-de-Guevara above, crowdsourcing in the modern conception is an IT-mediated phenomenon, meaning that a form of IT is always used to create and access crowds of people. In this respect, crowdsourcing has been considered to encompass three separate, but stable techniques; competition crowdsourcing, virtual labor market crowdsourcing, and open collaboration crowdsourcing.","[' What is crowdsourcing in the modern conception an IT-mediated phenomenon?', ' How many separate but stable types of people has crowdsourcing been considered to encompass?', ' What is a form of IT used to create and access crowds of people?', ' How many separate but stable techniques has crowdsourcing been considered to encompass?', ' What are competition crowdsourcing, virtual labor market crowdsourcing and open collaboration crowdsourcing?']","['a form of IT is always used to create and access crowds of people', 'three', 'crowdsourcing', 'three', 'crowdsourcing has been considered to encompass three separate, but stable techniques']"
3252,crowdsourcing,Definitions,"Despite the multiplicity of definitions for crowdsourcing, one constant has been the broadcasting of problems to the public, and an open call for contributions to help solve the problem. Members of the public submit solutions that are then owned by the entity, which originally broadcast the problem. In some cases, the contributor of the solution is compensated monetarily with prizes or with recognition. In other cases, the only rewards may be kudos or intellectual satisfaction. Crowdsourcing may produce solutions from amateurs or volunteers working in their spare time or from experts or small businesses, which were previously unknown to the initiating organization.","Despite the multiplicity of definitions for crowdsourcing, one constant has been the broadcasting of problems to the public, and an open call for contributions to help solve the problem. Members of the public submit solutions that are then owned by the entity, which originally broadcast the problem.","[' What is one constant of crowdsourcing?', ' What is the name of the call for contributions to help solve a problem?', ' Members of the public submit solutions that are then owned by what entity?']","['broadcasting of problems to the public', 'open', 'the entity, which originally broadcast the problem']"
3253,crowdsourcing,Modern methods,"Currently, crowdsourcing has transferred mainly to the Internet, which provides a particularly beneficial venue for crowdsourcing since individuals tend to be more open in web-based projects where they are not being physically judged or scrutinized, and thus can feel more comfortable sharing. This approach ultimately allows for well-designed artistic projects because individuals are less conscious, or maybe even less aware, of scrutiny towards their work. In an online atmosphere, more attention can be given to the specific needs of a project, rather than spending as much time in communication with other individuals. The effect of user communication and the platform presentation should be taken into account when evaluating the performance of ideas in crowdsourcing contexts.","Currently, crowdsourcing has transferred mainly to the Internet, which provides a particularly beneficial venue for crowdsourcing since individuals tend to be more open in web-based projects where they are not being physically judged or scrutinized, and thus can feel more comfortable sharing. This approach ultimately allows for well-designed artistic projects because individuals are less conscious, or maybe even less aware, of scrutiny towards their work.","[' Where has crowdsourcing mainly moved?', ' What provides a particularly beneficial venue for crowdsourcing?', ' People tend to be more open in web-based projects where they are not being what?', ' What type of sharing allows for a well-designed artistic project?', ' What is the main benefit of sharing?']","['the Internet', 'the Internet', 'physically judged or scrutinized', 'web-based projects where they are not being physically judged or scrutinized', 'individuals tend to be more open in web-based projects where they are not being physically judged or scrutinized, and thus can feel more comfortable sharing']"
3254,crowdsourcing,Modern methods,"The crowdsourced problem can be huge (epic tasks like finding alien life or mapping earthquake zones) or very small ('where can I skate safely?'). Some examples of successful crowdsourcing themes are problems that bug people, things that make people feel good about themselves, projects that tap into niche knowledge of proud experts, subjects that people find sympathetic or any form of injustice.","The crowdsourced problem can be huge (epic tasks like finding alien life or mapping earthquake zones) or very small ('where can I skate safely?'). Some examples of successful crowdsourcing themes are problems that bug people, things that make people feel good about themselves, projects that tap into niche knowledge of proud experts, subjects that people find sympathetic or any form of injustice.","[' What is one example of a large crowdsourced problem?', ' What is another example of small crowdsourced problems?', ' What do projects that tap into niche knowledge of proud experts tap into?', ' What do people find sympathetic about?']","['finding alien life', ""('where can I skate safely?'"", 'subjects that people find sympathetic or any form of injustice', 'injustice']"
3255,crowdsourcing,Modern methods,"Crowdsourcing can either take an explicit or an implicit route. Explicit crowdsourcing lets users work together to evaluate, share, and build different specific tasks, while implicit crowdsourcing means that users solve a problem as a side effect of something else they are doing. With explicit crowdsourcing, users can evaluate particular items like books or webpages, or share by posting products or items. Users can also build artifacts by providing information and editing other people's work. Implicit crowdsourcing can take two forms: standalone and piggyback. Standalone allows people to solve problems as a side effect of the task they are actually doing, whereas piggyback takes users' information from a third-party website to gather information.","Crowdsourcing can either take an explicit or an implicit route. Explicit crowdsourcing lets users work together to evaluate, share, and build different specific tasks, while implicit crowdsourcing means that users solve a problem as a side effect of something else they are doing.","[' What kind of route can crowdsourcing take?', ' What type of crowdsourcing lets users work together to evaluate, share, and build different specific tasks?']","['explicit or an implicit', 'Explicit']"
3256,crowdsourcing,Modern methods,"Crowdsourcing often allows participants to rank each other's contributions, e.g. in answer to the question ""What is one thing we can do to make Acme a great company?""  One common method for ranking is ""like"" counting, where the contribution with the most likes ranks first. This method is simple and easy to understand, but it privileges early contributions, which have more time to accumulate likes. In recent years several crowdsourcing companies have begun to use pairwise comparisons, backed by ranking algorithms. Ranking algorithms do not penalize late contributions. They also produce results faster.  Ranking algorithms have proven to be at least 10 times faster than manual stack ranking. One drawback, however, is that ranking algorithms are more difficult to understand than like counting.
","Crowdsourcing often allows participants to rank each other's contributions, e.g. in answer to the question ""What is one thing we can do to make Acme a great company?""","["" What allows participants to rank each other's contributions?"", ' What is one thing participants can do to make Acme great?']","['Crowdsourcing', 'Crowdsourcing']"
3257,crowdsourcing,Modern methods,"In ""How to Manage Crowdsourcing Platforms Effectively"", Ivo Blohm states that there are four types of Crowdsourcing Platforms: Microtasking, Information Pooling, Broadcast Search, and Open Collaboration. They differ in the diversity and aggregation of contributions that are created. The diversity of information collected can either be homogenous or heterogenous. The aggregation of information can either be selective or integrative. Some common categories of crowdsourcing have been used effectively in the commercial world, including crowdvoting, crowdsolving, crowdfunding, microwork, creative crowdsourcing, crowdsource workforce management, and inducement prize contests. Although this may not be an exhaustive list, the items cover the current major ways in which people use crowds to perform tasks.","In ""How to Manage Crowdsourcing Platforms Effectively"", Ivo Blohm states that there are four types of Crowdsourcing Platforms: Microtasking, Information Pooling, Broadcast Search, and Open Collaboration. They differ in the diversity and aggregation of contributions that are created.","[' How many types of Crowdsourcing Platforms is Ivo Blohm referring to?', ' What type of platform is Microtasking, Information Pooling, Broadcast Search, and Open Collaboration?']","['four', 'Crowdsourcing']"
3258,crowdsourcing,Crowdsourcers,"A number of motivations exist for businesses to use crowdsourcing to accomplish their tasks, find solutions for problems, or to gather information. These include the ability to offload peak demand, access cheap labor and information, generate better results, access a wider array of talent than might be present in one organization, and undertake problems that would have been too difficult to solve internally. Crowdsourcing allows businesses to submit problems on which contributors can work, on topics such as science, manufacturing, biotech, and medicine, with monetary rewards for successful solutions. Although crowdsourcing complicated tasks can be difficult, simple work tasks can be crowdsourced cheaply and effectively.","A number of motivations exist for businesses to use crowdsourcing to accomplish their tasks, find solutions for problems, or to gather information. These include the ability to offload peak demand, access cheap labor and information, generate better results, access a wider array of talent than might be present in one organization, and undertake problems that would have been too difficult to solve internally.","[' What motivations exist for businesses to use crowdsourcing to accomplish their tasks?', ' What are some of the motivations for businesses using crowdsourcing?', ' How can businesses access cheap labor and information?', ' What would have been too difficult to solve internally?']","['the ability to offload peak demand, access cheap labor and information', 'the ability to offload peak demand, access cheap labor and information', 'offload peak demand', 'problems']"
3259,crowdsourcing,Crowdsourcers,"Crowdsourcing also has the potential to be a problem-solving mechanism for government and nonprofit use. Urban and transit planning are prime areas for crowdsourcing. One project to test crowdsourcing's public participation process for transit planning in Salt Lake City was carried out from 2008 to 2009, funded by a U.S. Federal Transit Administration grant. Another notable application of crowdsourcing to government problem-solving is the Peer to Patent Community Patent Review project for the U.S. Patent and Trademark Office.",Crowdsourcing also has the potential to be a problem-solving mechanism for government and nonprofit use. Urban and transit planning are prime areas for crowdsourcing.,"[' Crowdsourcing has the potential to be a problem-solving mechanism for what?', ' What are prime areas for crowdsourcing?']","['government and nonprofit use', 'Urban and transit planning']"
3260,crowdsourcing,Crowdsourcers,"Researchers have used crowdsourcing systems like the Mechanical Turk to aid their research projects by crowdsourcing some aspects of the research process, such as data collection, parsing, and evaluation. Notable examples include using the crowd to create speech and language databases, and using the crowd to conduct user studies. Crowdsourcing systems provide these researchers with the ability to gather large amounts of data. Additionally, using crowdsourcing, researchers can collect data from populations and demographics they may not have had access to locally, but that improve the validity and value of their work.","Researchers have used crowdsourcing systems like the Mechanical Turk to aid their research projects by crowdsourcing some aspects of the research process, such as data collection, parsing, and evaluation. Notable examples include using the crowd to create speech and language databases, and using the crowd to conduct user studies.","[' What crowdsourcing system has researchers used to aid their research projects?', ' What are some aspects of the research process that researchers have used the Mechanical Turk to aid?']","['Mechanical Turk', 'data collection, parsing, and evaluation']"
3261,crowdsourcing,Crowdsourcers,"Artists have also used crowdsourcing systems. In his project called the Sheep Market, Aaron Koblin used Mechanical Turk to collect 10,000 drawings of sheep from contributors around the world. Artist Sam Brown leverages the crowd by asking visitors of his website explodingdog to send him sentences that he uses as inspirations for paintings. Art curator Andrea Grover argues that individuals tend to be more open in crowdsourced projects because they are not being physically judged or scrutinized. As with other crowdsourcers, artists use crowdsourcing systems to generate and collect data. The crowd also can be used to provide inspiration and to collect financial support for an artist's work.","Artists have also used crowdsourcing systems. In his project called the Sheep Market, Aaron Koblin used Mechanical Turk to collect 10,000 drawings of sheep from contributors around the world.","["" What was Aaron Koblin's project called?"", ' How many sheep drawings did Aaron collect from contributors?']","['the Sheep Market', '10,000']"
3262,mapreduce,Summary,"A MapReduce program is composed of a map procedure, which performs filtering and sorting (such as sorting students by first name into queues, one queue for each name), and a reduce method, which performs a summary operation (such as counting the number of students in each queue, yielding name frequencies). The ""MapReduce System"" (also called ""infrastructure"" or ""framework"") orchestrates the processing by marshalling the distributed servers, running the various tasks in parallel, managing all communications and data transfers between the various parts of the system, and providing for redundancy and fault tolerance.
","A MapReduce program is composed of a map procedure, which performs filtering and sorting (such as sorting students by first name into queues, one queue for each name), and a reduce method, which performs a summary operation (such as counting the number of students in each queue, yielding name frequencies). The ""MapReduce System"" (also called ""infrastructure"" or ""framework"") orchestrates the processing by marshalling the distributed servers, running the various tasks in parallel, managing all communications and data transfers between the various parts of the system, and providing for redundancy and fault tolerance.","[' What is a MapReduce program composed of?', ' What performs filtering and sorting?', ' Which performs a summary operation?', ' What is another name for the ""MapReduce System""?', ' How does the MapReduce system orchestrate the processing?', ' What is the name of the system that orchestrates the processing by marshalling the servers?', ' What is a part of a system that provides for redundancy and fault tolerance?']","['a map procedure', 'a map procedure', 'reduce method', 'infrastructure', 'by marshalling the distributed servers', 'MapReduce System', 'MapReduce System']"
3263,mapreduce,Summary,"The model is a specialization of the split-apply-combine strategy for data analysis.
It is inspired by the map and reduce functions commonly used in functional programming, although their purpose in the MapReduce framework is not the same as in their original forms. The key contributions of the MapReduce framework are not the actual map and reduce functions (which, for example, resemble the 1995 Message Passing Interface standard's reduce and scatter operations), but the scalability and fault-tolerance achieved for a variety of applications by optimizing the execution engine. As such, a single-threaded implementation of MapReduce is usually not faster than a traditional (non-MapReduce) implementation; any gains are usually only seen with multi-threaded implementations on multi-processor hardware. The use of this model is beneficial only when the optimized distributed shuffle operation (which reduces network communication cost) and fault tolerance features of the MapReduce framework come into play. Optimizing the communication cost is essential to a good MapReduce algorithm.","The model is a specialization of the split-apply-combine strategy for data analysis. It is inspired by the map and reduce functions commonly used in functional programming, although their purpose in the MapReduce framework is not the same as in their original forms.","[' What is the model a specialization of?', ' What is inspired by the map and reduce functions commonly used in functional programming?']","['split-apply-combine strategy for data analysis', 'The model']"
3264,mapreduce,Summary,"MapReduce libraries have been written in many programming languages, with different levels of optimization. A popular open-source implementation that has support for distributed shuffles is part of Apache Hadoop. The name MapReduce originally referred to the proprietary Google technology, but has since been genericized. By 2014, Google was no longer using MapReduce as their primary big data processing model, and development on Apache Mahout had moved on to more capable and less disk-oriented mechanisms that incorporated full map and reduce capabilities.","MapReduce libraries have been written in many programming languages, with different levels of optimization. A popular open-source implementation that has support for distributed shuffles is part of Apache Hadoop.","[' What are MapReduce libraries written in?', ' What is a popular open-source implementation that has support for distributed shuffles?']","['many programming languages', 'Apache Hadoop']"
3265,mapreduce,Overview,"MapReduce is a framework for processing parallelizable problems across large datasets using a large number of computers (nodes), collectively referred to as a cluster (if all nodes are on the same local network and use similar hardware) or a grid (if the nodes are shared across geographically and administratively distributed systems, and use more heterogeneous hardware). Processing can occur on data stored either in a filesystem (unstructured) or in a database (structured). MapReduce can take advantage of the locality of data, processing it near the place it is stored in order to minimize communication overhead.
","MapReduce is a framework for processing parallelizable problems across large datasets using a large number of computers (nodes), collectively referred to as a cluster (if all nodes are on the same local network and use similar hardware) or a grid (if the nodes are shared across geographically and administratively distributed systems, and use more heterogeneous hardware). Processing can occur on data stored either in a filesystem (unstructured) or in a database (structured).","[' MapReduce is a framework for processing parallelizable problems across large datasets using what?', ' What is collectively referred to as a cluster if all nodes are on the same local network?', ' What type of hardware does a distributed system use?', ' What is a filesystem called?', ' In what type of system is data stored?']","['a large number of computers', 'MapReduce', 'heterogeneous', 'unstructured', 'filesystem']"
3266,mapreduce,Overview,"MapReduce allows for the distributed processing of the map and reduction operations. Maps can be performed in parallel, provided that each mapping operation is independent of the others; in practice, this is limited by the number of independent data sources and/or the number of CPUs near each source. Similarly, a set of 'reducers' can perform the reduction phase, provided that all outputs of the map operation that share the same key are presented to the same reducer at the same time, or that the reduction function is associative. While this process often appears inefficient compared to algorithms that are more sequential (because multiple instances of the reduction process must be run), MapReduce can be applied to significantly larger datasets than a single ""commodity"" server can handle – a large server farm can use MapReduce to sort a petabyte of data in only a few hours. The parallelism also offers some possibility of recovering from partial failure of servers or storage during the operation: if one mapper or reducer fails, the work can be rescheduled – assuming the input data are still available.
","MapReduce allows for the distributed processing of the map and reduction operations. Maps can be performed in parallel, provided that each mapping operation is independent of the others; in practice, this is limited by the number of independent data sources and/or the number of CPUs near each source.","[' What allows for the distributed processing of the map and reduction operations?', ' Maps can be performed in parallel, provided that each mapping operation is independent of what?']","['MapReduce', 'the others']"
3267,mapreduce,Overview,"In many situations, the input data might have already been distributed (""sharded"") among many different servers, in which case step 1 could sometimes be greatly simplified by assigning Map servers that would process the locally present input data.  Similarly, step 3 could sometimes be sped up by assigning Reduce processors that are as close as possible to the Map-generated data they need to process.
","In many situations, the input data might have already been distributed (""sharded"") among many different servers, in which case step 1 could sometimes be greatly simplified by assigning Map servers that would process the locally present input data. Similarly, step 3 could sometimes be sped up by assigning Reduce processors that are as close as possible to the Map-generated data they need to process.","[' How could step 1 be greatly simplified?', ' What could step 3 be sped up by assigning Reduce processors?', ' What could be sped up by assigning Reduce processors that are as close as possible to the Map-generated data?']","['by assigning Map servers that would process the locally present input data', 'Map-generated data they need to process', 'step 3']"
3268,mapreduce,Logical view,"The  Map and Reduce functions of MapReduce are both defined with respect to data structured in (key, value) pairs. Map takes one pair of data with a type in one data domain, and returns a list of pairs in a different domain:
","The  Map and Reduce functions of MapReduce are both defined with respect to data structured in (key, value) pairs. Map takes one pair of data with a type in one data domain, and returns a list of pairs in a different domain:","[' What are the Map and Reduce functions of MapReduce defined with respect to?', ' Map takes one pair of data with a type in one data domain and returns a list of pairs in a different domain?']","['data structured in (key, value) pairs', 'Map']"
3269,mapreduce,Logical view,"The Map function is applied in parallel to every pair (keyed by k1) in the input dataset. This produces a list of pairs (keyed by k2) for each call.
After that, the MapReduce framework collects all pairs with the same key (k2) from all lists and groups them together, creating one group for each key.
",The Map function is applied in parallel to every pair (keyed by k1) in the input dataset. This produces a list of pairs (keyed by k2) for each call.,"[' What is applied in parallel to every pair in the input dataset?', ' What produces a list of pairs for each call?']","['The Map function', 'The Map function']"
3270,mapreduce,Logical view,"Each Reduce call typically produces either one key value pair or an empty return, though one call is allowed to return more than one key value pair. The returns of all calls are collected as the desired result list.
","Each Reduce call typically produces either one key value pair or an empty return, though one call is allowed to return more than one key value pair. The returns of all calls are collected as the desired result list.","[' How many key value pairs does each Reduce call typically produce?', ' How many keys can a Reduce call return?', ' What is collected as the desired result list?']","['one', 'one', 'The returns of all calls']"
3271,mapreduce,Logical view,"Thus the MapReduce framework transforms a list of (key, value) pairs into another list of (key, value) pairs. This behavior is different from the typical functional programming map and reduce combination, which accepts a list of arbitrary values and returns one single value that combines all the values returned by map.
","Thus the MapReduce framework transforms a list of (key, value) pairs into another list of (key, value) pairs. This behavior is different from the typical functional programming map and reduce combination, which accepts a list of arbitrary values and returns one single value that combines all the values returned by map.","[' What does the MapReduce framework transform a list of into another list of?', ' What is different from the typical functional programming map and reduce combination?', ' What is one single value that combines all the values returned by map?']","['(key, value) pairs', 'transforms a list of (key, value) pairs into another list of (key, value) pairs. This behavior', 'reduce combination']"
3272,mapreduce,Logical view,"It is necessary but not sufficient to have implementations of the map and reduce abstractions in order to implement MapReduce. Distributed implementations of MapReduce require a means of connecting the processes performing the Map and Reduce phases. This may be a distributed file system. Other options are possible, such as direct streaming from mappers to reducers, or for the mapping processors to serve up their results to reducers that query them.
",It is necessary but not sufficient to have implementations of the map and reduce abstractions in order to implement MapReduce. Distributed implementations of MapReduce require a means of connecting the processes performing the Map and Reduce phases.,"[' What is required but not sufficient to implement MapReduce?', ' What require a means of connecting the processes performing the Map and Reduce phases?']","['implementations of the map and reduce abstractions', 'Distributed implementations of MapReduce']"
3273,mapreduce,Dataflow,"Software framework architecture adheres to open-closed principle where code is effectively divided into unmodifiable frozen spots and extensible hot spots. The frozen spot of the MapReduce framework is a large distributed sort. The hot spots, which the application defines, are:
",Software framework architecture adheres to open-closed principle where code is effectively divided into unmodifiable frozen spots and extensible hot spots. The frozen spot of the MapReduce framework is a large distributed sort.,"[' What principle does Software framework architecture adhere to?', ' What is effectively divided into unmodifiable frozen spots and extensible hot spots?']","['open-closed', 'code']"
3274,mapreduce,Performance considerations,"MapReduce programs are not guaranteed to be fast. The main benefit of this programming model is to exploit the optimized shuffle operation of the platform, and only having to write the Map and Reduce parts of the program.
In practice, the author of a MapReduce program however has to take the shuffle step into consideration; in particular the partition function and the amount of data written by the Map function can have a large impact on the performance and scalability. Additional modules such as the Combiner function can help to reduce the amount of data written to disk, and transmitted over the network. MapReduce applications can achieve sub-linear speedups under specific circumstances.","MapReduce programs are not guaranteed to be fast. The main benefit of this programming model is to exploit the optimized shuffle operation of the platform, and only having to write the Map and Reduce parts of the program.","[' MapReduce programs are not guaranteed to be what?', ' The main benefit of this programming model is to exploit the optimized shuffle operation of what platform?']","['fast', 'MapReduce']"
3275,mapreduce,Performance considerations,"When designing a MapReduce algorithm, the author needs to choose a good tradeoff between the computation and the communication costs. Communication cost often dominates the computation cost, and many MapReduce implementations are designed to write all communication to distributed storage for crash recovery.
","When designing a MapReduce algorithm, the author needs to choose a good tradeoff between the computation and the communication costs. Communication cost often dominates the computation cost, and many MapReduce implementations are designed to write all communication to distributed storage for crash recovery.","[' When designing a MapReduce algorithm, the author needs to choose a good tradeoff between the computation and what?', ' What often dominates the computation cost?', ' Many Mapreduce implementations are designed to write all communication to where?']","['communication costs', 'Communication cost', 'distributed storage']"
3276,mapreduce,Performance considerations,"In tuning performance of MapReduce, the complexity of mapping, shuffle, sorting (grouping by the key), and reducing has to be taken into account. The amount of data produced by the mappers is a key parameter that shifts the bulk of the computation cost between mapping and reducing. Reducing includes sorting (grouping of the keys) which has nonlinear complexity. Hence, small partition sizes reduce sorting time, but there is a trade-off because having a large number of reducers may be impractical. The influence of split unit size is marginal (unless chosen particularly badly, say <1MB). The gains from some mappers reading load from local disks, on average, is minor.","In tuning performance of MapReduce, the complexity of mapping, shuffle, sorting (grouping by the key), and reducing has to be taken into account. The amount of data produced by the mappers is a key parameter that shifts the bulk of the computation cost between mapping and reducing.",[' What is a key parameter that shifts the bulk of computation cost between mapping and reducing?'],['The amount of data produced by the mappers']
3277,mapreduce,Performance considerations,"For processes that complete quickly, and where the data fits into main memory of a single machine or a small cluster, using a MapReduce framework usually is not effective. Since these frameworks are designed to recover from the loss of whole nodes during the computation, they write interim results to distributed storage. This crash recovery is expensive, and only pays off when the computation involves many computers and a long runtime of the computation. A task that completes in seconds can just be restarted in the case of an error, and the likelihood of at least one machine failing grows quickly with the cluster size. On such problems, implementations keeping all data in memory and simply restarting a computation on node failures or —when the data is small enough— non-distributed solutions will often be faster than a MapReduce system.
","For processes that complete quickly, and where the data fits into main memory of a single machine or a small cluster, using a MapReduce framework usually is not effective. Since these frameworks are designed to recover from the loss of whole nodes during the computation, they write interim results to distributed storage.","[' What is not effective for processes that complete quickly?', ' What are MapReduce frameworks designed to recover from?', ' During computation, what do interim results write to?']","['using a MapReduce framework', 'the loss of whole nodes during the computation', 'distributed storage']"
3278,mapreduce,Distribution and reliability,"MapReduce achieves reliability by parceling out a number of operations on the set of data to each node in the network. Each node is expected to report back periodically with completed work and status updates. If a node falls silent for longer than that interval, the master node (similar to the master server in the Google File System) records the node as dead and sends out the node's assigned work to other nodes. Individual operations use atomic operations for naming file outputs as a check to ensure that there are not parallel conflicting threads running. When files are renamed, it is possible to also copy them to another name in addition to the name of the task (allowing for side-effects).
",MapReduce achieves reliability by parceling out a number of operations on the set of data to each node in the network. Each node is expected to report back periodically with completed work and status updates.,"[' What does MapReduce achieve by parceling out a number of operations on the set of data to each node in the network?', ' Each node is expected to report back periodically with completed work and status updates?']","['reliability', 'MapReduce']"
3279,mapreduce,Distribution and reliability,"The reduce operations operate much the same way. Because of their inferior properties with regard to parallel operations, the master node attempts to schedule reduce operations on the same node, or in the same rack as the node holding the data being operated on. This property is desirable as it conserves bandwidth across the backbone network of the datacenter.
","The reduce operations operate much the same way. Because of their inferior properties with regard to parallel operations, the master node attempts to schedule reduce operations on the same node, or in the same rack as the node holding the data being operated on.","[' How do the reduce operations operate?', ' Why do the master nodes try to schedule reduce operations on the same node or in the same rack?']","['much the same way', 'Because of their inferior properties with regard to parallel operations']"
3280,mapreduce,Distribution and reliability,"Implementations are not necessarily highly reliable. For example, in older versions of Hadoop the NameNode was a single point of failure for the distributed filesystem.  Later versions of Hadoop have high availability with an active/passive failover for the ""NameNode.""
","Implementations are not necessarily highly reliable. For example, in older versions of Hadoop the NameNode was a single point of failure for the distributed filesystem.","[' What type of implementations are not necessarily highly reliable?', ' What was a single point of failure for the distributed filesystem?']","['Implementations', 'the NameNode']"
3281,mapreduce,Uses,"MapReduce is useful in a wide range of applications, including distributed pattern-based searching, distributed sorting, web link-graph reversal, Singular Value Decomposition, web access log stats, inverted index construction, document clustering, machine learning, and statistical machine translation. Moreover, the MapReduce model has been adapted to several computing environments like multi-core and many-core systems, desktop grids,
multi-cluster, volunteer computing environments, dynamic cloud environments, mobile environments, and high-performance computing environments.","MapReduce is useful in a wide range of applications, including distributed pattern-based searching, distributed sorting, web link-graph reversal, Singular Value Decomposition, web access log stats, inverted index construction, document clustering, machine learning, and statistical machine translation. Moreover, the MapReduce model has been adapted to several computing environments like multi-core and many-core systems, desktop grids,
multi-cluster, volunteer computing environments, dynamic cloud environments, mobile environments, and high-performance computing environments.","[' MapReduce is useful in a wide range of applications, including distributed pattern-based searching, distributed sorting, web link-graph reversal, Singular Value Decomposition, web access log stats, inverted index construction, document clustering, machine learning, and statistical machine translation?', ' The Mapreduce model has been adapted to several computing environments, including multi-core and what else?', ' What has been adapted to several computing environments like multi-core and many-core systems, desktop grids, multi-cluster, volunteer computing environments?']","['multi-cluster, volunteer computing environments, dynamic cloud environments, mobile environments, and high-performance computing environments', 'many-core systems', 'MapReduce model']"
3282,mapreduce,Uses,"At Google, MapReduce was used to completely regenerate Google's index of the World Wide Web. It replaced the old ad hoc programs that updated the index and ran the various analyses. Development at Google has since moved on to technologies such as Percolator, FlumeJava and MillWheel that offer streaming operation and updates instead of batch processing, to allow integrating ""live"" search results without rebuilding the complete index.","At Google, MapReduce was used to completely regenerate Google's index of the World Wide Web. It replaced the old ad hoc programs that updated the index and ran the various analyses.","[' What was MapReduce used for at Google?', ' What replaced the old ad hoc programs that updated the index?']","[""completely regenerate Google's index of the World Wide Web"", 'MapReduce']"
3283,mapreduce,Uses,"MapReduce's stable inputs and outputs are usually stored in a distributed file system. The transient data are usually stored on local disk and fetched remotely by the reducers.
",MapReduce's stable inputs and outputs are usually stored in a distributed file system. The transient data are usually stored on local disk and fetched remotely by the reducers.,"["" Where are MapReduce's stable inputs and outputs usually stored?"", ' Where are the transient data usually stored on?', ' What are the reducers fetched remotely?']","['distributed file system', 'local disk', 'transient data']"
3284,tree automaton,Summary,"A tree automaton is a type of state machine. Tree automata deal with tree structures, rather than the strings of more conventional state machines.
","A tree automaton is a type of state machine. Tree automata deal with tree structures, rather than the strings of more conventional state machines.","[' A tree automaton is a type of what?', ' Tree automatons deal with what rather than strings?']","['state machine', 'tree structures']"
3285,tree automaton,Summary,"As with classical automata, finite tree automata (FTA) can be either a deterministic automaton or not. According to how the automaton processes the input tree, finite tree automata can be of two types: (a) bottom up, (b) top down. This is an important issue, as although non-deterministic (ND) top-down and ND bottom-up tree automata are equivalent in expressive power, deterministic top-down automata are strictly less powerful than their deterministic bottom-up counterparts, because tree properties specified by deterministic top-down tree automata can only depend on path properties. (Deterministic bottom-up tree automata are as powerful as ND tree automata.)
","As with classical automata, finite tree automata (FTA) can be either a deterministic automaton or not. According to how the automaton processes the input tree, finite tree automata can be of two types: (a) bottom up, (b) top down.","[' What is a finite tree automata?', ' What type of automaton can be a deterministic automaton or not?', ' How many types of automata are there?']","['bottom up, (b) top down', 'finite tree automata', 'two']"
3286,tree automaton,Definitions,"A bottom-up finite tree automaton over F is defined as a tuple
(Q, F, Qf, Δ),
where Q is a set of states, F is a ranked alphabet (i.e., an alphabet whose symbols have an associated arity), Qf ⊆ Q is a set of final states, and Δ is a set of transition rules of the form f(q1(x1),...,qn(xn)) → q(f(x1,...,xn)), for an n-ary f ∈ F, q, qi ∈ Q, and xi variables denoting subtrees. That is, members of Δ are rewrite rules from nodes whose childs' roots are states, to nodes whose roots are states. Thus the state of a node is deduced from the states of its children.
","A bottom-up finite tree automaton over F is defined as a tuple
(Q, F, Qf, Δ),
where Q is a set of states, F is a ranked alphabet (i.e., an alphabet whose symbols have an associated arity), Qf ⊆ Q is a set of final states, and Δ is a set of transition rules of the form f(q1(x1),...,qn(xn)) → q(f(x1,...,xn)), for an n-ary f ∈ F, q, qi ∈ Q, and xi variables denoting subtrees. That is, members of Δ are rewrite rules from nodes whose childs' roots are states, to nodes whose roots are states.","[' What is a bottom-up finite tree automaton over F defined as?', ' What is Q a set of states, F a ranked alphabet?', ' Qf <unk> Q is what?', ' What is <unk> a set of transition rules of the form f(q1(x1),...,qn(xn))?', ' For an n-ary f <unk> F, Q, qi <unk> Q, and xi variables denoting subtrees, what are the members of <unk>?', "" What are rewrite rules from nodes whose childs' roots are states?"", "" What are the rules from nodes whose child's roots are states?""]","['a tuple\n(Q, F, Qf, Δ),', 'Q', 'a set of final states', 'Δ', ""rewrite rules from nodes whose childs' roots are states, to nodes whose roots are states"", 'members of Δ', 'rewrite rules']"
3287,tree automaton,Definitions,"For n=0, that is, for a constant symbol f, the above transition rule definition reads f() → q(f()); often the empty parentheses are omitted for convenience: f → q(f).
Since these transition rules for constant symbols (leaves) do not require a state, no explicitly defined initial states are needed.
A bottom-up tree automaton is run on a ground term over F, starting at all its leaves simultaneously and moving upwards, associating a run state from Q with each subterm.
The term is accepted if its root is associated to an accepting state from Qf.","For n=0, that is, for a constant symbol f, the above transition rule definition reads f() → q(f()); often the empty parentheses are omitted for convenience: f → q(f). Since these transition rules for constant symbols (leaves) do not require a state, no explicitly defined initial states are needed.","[' What does the above transition rule definition read for n=0?', ' Why are empty parentheses omitted?', ' What do transition rules for constant symbols (leaves) not require?']","['f() → q(f());', 'for convenience', 'a state']"
3288,tree automaton,Definitions,"A top-down finite tree automaton over F is defined as a tuple
(Q, F, Qi, Δ),
with two differences with bottom-up tree automata. First, Qi ⊆ Q, the set of its initial states, replaces Qf; second, its transition rules are oriented conversely:
q(f(x1,...,xn))  → f(q1(x1),...,qn(xn)), for an n-ary f ∈ F, q, qi ∈ Q, and xi variables denoting subtrees.
That is, members of Δ are here rewrite rules from nodes whose roots are states to nodes whose children's roots are states.
A top-down automaton starts in some of its initial states at the root and moves downward along branches of the tree, associating along a run a state with each subterm inductively.
A tree is accepted if every branch can be gone through this way.","A top-down finite tree automaton over F is defined as a tuple
(Q, F, Qi, Δ),
with two differences with bottom-up tree automata. First, Qi ⊆ Q, the set of its initial states, replaces Qf; second, its transition rules are oriented conversely:
q(f(x1,...,xn))  → f(q1(x1),...,qn(xn)), for an n-ary f ∈ F, q, qi ∈ Q, and xi variables denoting subtrees.","[' What is a top-down finite tree automaton over F defined as?', ' What are the two differences with bottom-up tree automata?', ' Qi <unk> Q replaces what?', ' What does f(q1(x1),...,qn(xn)) do for an n-ary f <unk> F?', ' What variables denote subtrees?']","['a tuple\n(Q, F, Qi, Δ),', 'Qi ⊆ Q, the set of its initial states, replaces Qf', 'Qf', '→', 'xi']"
3289,tree automaton,Definitions,"A tree automaton is called deterministic (abbreviated DFTA) if no two rules from Δ have the same left hand side; otherwise it is called nondeterministic (NFTA). Non-deterministic top-down tree automata have the same expressive power as non-deterministic bottom-up ones; the transition rules are simply reversed, and the final states become the initial states.
","A tree automaton is called deterministic (abbreviated DFTA) if no two rules from Δ have the same left hand side; otherwise it is called nondeterministic (NFTA). Non-deterministic top-down tree automata have the same expressive power as non-deterministic bottom-up ones; the transition rules are simply reversed, and the final states become the initial states.","[' What is a tree automaton called if no two rules from <unk> have the same left hand side?', ' What is another name for nondeterministic?', ' Non-deterministic top-down tree automata have what expressive power?', ' How are transition rules reversed?', ' What are the final states?']","['deterministic', 'NFTA', 'the same', 'simply reversed, and the final states become the initial states', 'initial states']"
3290,tree automaton,Definitions,"In contrast, deterministic top-down tree automata are less powerful than their bottom-up counterparts, because in a deterministic tree automaton no two transition rules have the same left-hand side. For tree automata, transition rules are rewrite rules; and for top-down ones, the left-hand side will be parent nodes. Consequently, a deterministic top-down tree automaton will only be able to test for tree properties that are true in all branches, because the choice of the state to write into each child branch is determined at the parent node, without knowing the child branches contents.
","In contrast, deterministic top-down tree automata are less powerful than their bottom-up counterparts, because in a deterministic tree automaton no two transition rules have the same left-hand side. For tree automata, transition rules are rewrite rules; and for top-down ones, the left-hand side will be parent nodes.","[' Why are top-down tree automata less powerful than bottom-up ones?', ' What are rewrite rules?', ' Which side of transition rules will be the parent nodes in a deterministic tree automaton?']","['no two transition rules have the same left-hand side', 'transition rules', 'left-hand']"
3291,vertex cover,Summary,"In graph theory, a vertex cover (sometimes node cover) of a graph is a set of vertices that includes at least one endpoint of every edge  of the graph. In computer science, the problem of finding a minimum vertex cover is a classical optimization problem. It is NP-hard, so it cannot be solved by a polynomial-time algorithm if P ≠ NP. Moreover, it is hard to approximate - it cannot be approximated up to a factor smaller than 2 if the unique games conjecture is true. On the other hand, it has several simple 2-factor approximations. It is a typical example of an NP-hard optimization problem that has an approximation algorithm. Its decision version, the vertex cover problem, was one of Karp's 21 NP-complete problems and is therefore a classical NP-complete problem in computational complexity theory. Furthermore, the vertex cover problem is fixed-parameter tractable and a central problem in parameterized complexity theory.
","In graph theory, a vertex cover (sometimes node cover) of a graph is a set of vertices that includes at least one endpoint of every edge  of the graph. In computer science, the problem of finding a minimum vertex cover is a classical optimization problem.","[' In graph theory, what is a set of vertices that includes at least one endpoint of every edge of a graph?', ' In computer science, the problem of finding a minimum vertex cover is what?']","['vertex cover', 'classical optimization problem']"
3292,vertex cover,Definition,"Formally, a vertex cover 




V
′



{\displaystyle V'}
 of an undirected graph 



G
=
(
V
,
E
)


{\displaystyle G=(V,E)}
 is a subset of 



V


{\displaystyle V}
 such that 



u
v
∈
E
⇒
u
∈

V
′

∨
v
∈

V
′



{\displaystyle uv\in E\Rightarrow u\in V'\lor v\in V'}
, that is to say it is a set of vertices 




V
′



{\displaystyle V'}
 where every edge has at least one endpoint in the vertex cover 




V
′



{\displaystyle V'}
. Such a set is said to cover the edges of 



G


{\displaystyle G}
. The upper figure shows two examples of vertex covers, with some vertex cover 




V
′



{\displaystyle V'}
 marked in red.
","Formally, a vertex cover 




V
′



{\displaystyle V'}
 of an undirected graph 



G
=
(
V
,
E
)


{\displaystyle G=(V,E)}
 is a subset of 



V


{\displaystyle V}
 such that 



u
v
∈
E
⇒
u
∈

V
′

∨
v
∈

V
′



{\displaystyle uv\in E\Rightarrow u\in V'\lor v\in V'}
, that is to say it is a set of vertices 




V
′



{\displaystyle V'}
 where every edge has at least one endpoint in the vertex cover 




V
′



{\displaystyle V'}
. Such a set is said to cover the edges of 



G


{\displaystyle G}
.","["" What is a vertex cover V ′ <unk>displaystyle V'<unk> of an undirected graph G = (V, E )?"", ' How many edge has at least one endpoint in the vertex?', "" Every edge has at least one endpoint in the vertex cover V ′ 'displaystyle V'?"", ' A set of endpoints is said to cover the edges of what?']","['V', 'every edge has at least one endpoint in the vertex cover \n\n\n\n\nV\n′', 'V', 'G']"
3293,vertex cover,Definition,"A minimum vertex cover is a vertex cover of smallest possible size.  The vertex cover number 



τ


{\displaystyle \tau }
 is the size of a minimum vertex cover, i.e. 



τ
=

|


V
′


|



{\displaystyle \tau =|V'|}
. The lower figure shows examples of minimum vertex covers in the previous graphs.
","A minimum vertex cover is a vertex cover of smallest possible size. The vertex cover number 



τ


{\displaystyle \tau }
 is the size of a minimum vertex cover, i.e.",[' What is the size of a minimum vertex cover?'],['τ']
3294,vertex cover,Computational problem,"The vertex cover problem is an NP-complete problem: it was one of Karp's 21 NP-complete problems. It is often used in computational complexity theory as a starting point for NP-hardness proofs.
",The vertex cover problem is an NP-complete problem: it was one of Karp's 21 NP-complete problems. It is often used in computational complexity theory as a starting point for NP-hardness proofs.,"[' What type of problem is the vertex cover problem?', ' How many NP-complete problems did Karp have?', ' What is often used in computational complexity theory?']","['NP-complete', '21', 'The vertex cover problem']"
3295,vertex cover,Applications,"Vertex cover optimization serves as a model for many real-world and theoretical problems. For example, a commercial establishment interested in installing the fewest possible closed circuit cameras covering all hallways (edges) connecting all rooms (nodes) on a floor might model the objective as a vertex cover minimization problem. The problem has also been used to model the elimination of repetitive DNA sequences for synthetic biology and metabolic engineering applications.","Vertex cover optimization serves as a model for many real-world and theoretical problems. For example, a commercial establishment interested in installing the fewest possible closed circuit cameras covering all hallways (edges) connecting all rooms (nodes) on a floor might model the objective as a vertex cover minimization problem.","[' What serves as a model for many real-world and theoretical problems?', ' What would a commercial establishment interested in installing the fewest possible closed circuit cameras cover?']","['Vertex cover optimization', 'all hallways']"
3296,case study,Summary,"A case study is an in-depth, detailed examination of a particular case (or cases) within a real-world context. For example, case studies in medicine may focus on an individual patient or ailment; case studies in business might cover a particular firm's strategy or a broader market; similarly, case studies in politics can range from a narrow happening over time (e.g., a specific political campaign) to an enormous undertaking (e.g., a world war).
","A case study is an in-depth, detailed examination of a particular case (or cases) within a real-world context. For example, case studies in medicine may focus on an individual patient or ailment; case studies in business might cover a particular firm's strategy or a broader market; similarly, case studies in politics can range from a narrow happening over time (e.g., a specific political campaign) to an enormous undertaking (e.g., a world war).","[' What is an in-depth, detailed examination of a particular case within a real-world context?', ' A case study in medicine may focus on an individual patient or ailment, what else might a case study cover?', ' What can be a case study in politics?', ' What is an example of a narrow happening over time?']","['A case study', ""a particular firm's strategy or a broader market"", 'a specific political campaign) to an enormous undertaking', 'a specific political campaign']"
3297,case study,Summary,"Generally, a case study can highlight nearly any individual, group, organization, event, belief system, or action. A case study does not necessarily have to be one observation (N=1), but may include many observations (one or multiple individuals and entities across multiple time periods, all within the same case study). Research projects involving numerous cases are frequently called cross-case research, whereas a study of a single case is called within-case research.","Generally, a case study can highlight nearly any individual, group, organization, event, belief system, or action. A case study does not necessarily have to be one observation (N=1), but may include many observations (one or multiple individuals and entities across multiple time periods, all within the same case study).","[' What can highlight nearly any individual, group, organization, event, belief system, or action?', ' A case study does not necessarily have to be one observation (N=1) but may include what?']","['a case study', 'many observations']"
3298,case study,Summary,Case study research has been extensively practiced in both the social and natural sciences.: 5–6 ,Case study research has been extensively practiced in both the social and natural sciences. : 5–6,[' What type of research has been extensively practiced in both the social and natural sciences?'],['Case study']
3299,case study,Definition,"There are multiple definitions of case studies, which may emphasize the number of observations (a small N), the method (qualitative), the thickness of the research (a comprehensive examination of a phenomenon and its context), and the naturalism (a ""real-life context"" is being examined) involved in the research. There is general agreement among scholars that a case study does not necessarily have to entail one observation (N=1), but can include many observations within a single case or across numerous cases. For example, a case study of the French Revolution would at the bare minimum be an observation of two observations: France before and after a revolution. John Gerring writes that the N=1 research design is so rare in practice that it amounts to a ""myth.""","There are multiple definitions of case studies, which may emphasize the number of observations (a small N), the method (qualitative), the thickness of the research (a comprehensive examination of a phenomenon and its context), and the naturalism (a ""real-life context"" is being examined) involved in the research. There is general agreement among scholars that a case study does not necessarily have to entail one observation (N=1), but can include many observations within a single case or across numerous cases.","[' How many definitions of case studies are there?', ' How many observations are there in a case study?', ' What is the term for a comprehensive examination of a phenomenon and its context?', ' What type of study can include many observations within a single case or across numerous cases?', ' What is the general agreement among scholars that a case study does not necessarily have to entail one observation?']","['multiple', 'a small N', 'thickness of the research', 'case study', 'can include many observations within a single case or across numerous cases']"
3300,case study,Definition,"John Gerring defines the case study approach as an ""intensive study of a single unit or a small number of units (the cases), for the purpose of understanding a larger class of similar units (a population of cases)."" According to Gerring, case studies lend themselves to an idiographic style of analysis, whereas quantitative work lends itself to a nomothetic style of analysis. He adds that ""the defining feature of qualitative work is its use of noncomparable observations—observations that pertain to different aspects of a causal or descriptive question"", whereas quantitative observations are comparable.","John Gerring defines the case study approach as an ""intensive study of a single unit or a small number of units (the cases), for the purpose of understanding a larger class of similar units (a population of cases)."" According to Gerring, case studies lend themselves to an idiographic style of analysis, whereas quantitative work lends itself to a nomothetic style of analysis.","[' Who defines the case study approach as an intensive study of a single unit or a small number of units for the purpose of understanding a larger class of similar units?', ' According to Gerring, case studies lend themselves to what style of study?', ' What type of style of analysis do case studies lend themselves to?', ' Quantitative work lends itself to what type of analysis?']","['John Gerring', 'idiographic', 'idiographic', 'nomothetic']"
3301,case study,Definition,"According to John Gerring, the key characteristic that distinguishes case studies from all other methods is the ""reliance on evidence drawn from a single case and its attempts, at the same time, to illuminate features of a broader set of cases."" Scholars use case studies to shed light on a ""class"" of phenomena.
","According to John Gerring, the key characteristic that distinguishes case studies from all other methods is the ""reliance on evidence drawn from a single case and its attempts, at the same time, to illuminate features of a broader set of cases."" Scholars use case studies to shed light on a ""class"" of phenomena.","[' What is the key characteristic that distinguishes case studies from all other methods?', ' What do scholars use case studies to shed light on?', ' Who said that case studies rely on evidence drawn from a single case and attempts to illuminate features of a broader set of cases?', ' What do scholars use to shed light on a class of phenomena?']","['reliance on evidence drawn from a single case', 'a ""class"" of phenomena', 'John Gerring', 'case studies']"
3302,case study,Research designs,"As with other social science methods, no single research design dominates case study research. Case studies can use at least four types of designs. First, there may be a ""no theory first"" type of case study design, which is closely connected to Kathleen M. Eisenhardt's methodological work. A second type of research design highlights the distinction between single- and multiple-case studies, following Robert K. Yin's guidelines and extensive examples. A third design deals with a ""social construction of reality"", represented by the work of Robert E. Stake. Finally, the design rationale for a case study may be to identify ""anomalies"". A representative scholar of this design is Michael Burawoy. Each of these four designs may lead to different applications, and understanding their sometimes unique ontological and epistemological assumptions becomes important. However, although the designs can have substantial methodological differences, the designs also can be used in explicitly acknowledged combinations with each other.
","As with other social science methods, no single research design dominates case study research. Case studies can use at least four types of designs.","[' No single research design dominates what type of research?', ' How many types of designs can case studies use?']","['case study', 'four']"
3303,case study,Uses,"Case studies have commonly been seen as a fruitful way to come up with hypotheses and generate theories. Classic examples of case studies that generated theories includes Darwin's theory of evolution (derived from his travels to the Easter Island), and Douglass North's theories of economic development (derived from case studies of early developing states, such as England).","Case studies have commonly been seen as a fruitful way to come up with hypotheses and generate theories. Classic examples of case studies that generated theories includes Darwin's theory of evolution (derived from his travels to the Easter Island), and Douglass North's theories of economic development (derived from case studies of early developing states, such as England).","[' What is a fruitful way to come up with hypotheses and generate theories?', "" What is Darwin's theory of evolution derived from?"", "" Who's theories of economic development were derived by case studies?"", ' Whose theories of economic development were derived from case studies of early developing states?']","['Case studies', 'his travels to the Easter Island', 'Douglass North', 'Douglass North']"
3304,case study,Uses,"Case studies are also useful for formulating concepts, which are an important aspect of theory construction. The concepts used in qualitative research will tend to have higher conceptual validity than concepts used in quantitative research (due to conceptual stretching: the unintentional comparison of dissimilar cases). Case studies add descriptive richness, and can have greater internal validity than quantitative studies. Case studies are suited to explain outcomes in individual cases, which is something that quantitative methods are less equipped to do.","Case studies are also useful for formulating concepts, which are an important aspect of theory construction. The concepts used in qualitative research will tend to have higher conceptual validity than concepts used in quantitative research (due to conceptual stretching: the unintentional comparison of dissimilar cases).","[' What is an important aspect of theory construction?', ' What will have higher conceptual validity than concepts used in quantitative research?', ' The unintentional comparison of dissimilar cases is what?']","['Case studies are also useful for formulating concepts', 'concepts used in qualitative research', 'conceptual stretching']"
3305,case study,Uses,"Through fine-gained knowledge and description, case studies can fully specify the causal mechanisms in a way that may be harder in a large-N study. In terms of identifying ""causal mechanisms"", some scholars distinguish between ""weak"" and ""strong chains"". Strong chains actively connect elements of the causal chain to produce an outcome whereas weak chains are just intervening variables.","Through fine-gained knowledge and description, case studies can fully specify the causal mechanisms in a way that may be harder in a large-N study. In terms of identifying ""causal mechanisms"", some scholars distinguish between ""weak"" and ""strong chains"".","[' Through fine-gained knowledge and description, case studies can fully specify the causal mechanisms in a way that may be harder in what study?', ' Some scholars distinguish between what two types of chains?']","['large-N', 'weak"" and ""strong chains']"
3306,case study,Uses,"Case studies of cases that defy existing theoretical expectations may contribute knowledge by delineating why the cases violate theoretical predictions and specifying the scope conditions of the theory. Case studies are useful in situations of causal complexity where there may be equifinality, complex interaction effects and path dependency. They may also be more appropriate for empirical verifications of strategic interactions in rationalist scholarship than quantitative methods. Case studies can identify necessary and insufficient conditions, as well as complex combinations of necessary and sufficient conditions. They argue that case studies may also be useful in identifying the scope conditions of a theory: whether variables are sufficient or necessary to bring about an outcome.","Case studies of cases that defy existing theoretical expectations may contribute knowledge by delineating why the cases violate theoretical predictions and specifying the scope conditions of the theory. Case studies are useful in situations of causal complexity where there may be equifinality, complex interaction effects and path dependency.","[' Case studies of cases that defy existing theoretical expectations may contribute what?', ' Case studies can contribute knowledge by delineating why the cases violate theoretical predictions and specifying the scope conditions of what theory?']","['knowledge', 'the theory']"
3307,case study,Uses,"Qualitative research may be necessary to determine whether a treatment is as-if random or not. As a consequence, good quantitative observational research often entails a qualitative component.","Qualitative research may be necessary to determine whether a treatment is as-if random or not. As a consequence, good quantitative observational research often entails a qualitative component.","[' Qualitative research may be necessary to determine whether a treatment is as-if random or not?', ' Good quantitative observational research often entails what?']","['quantitative observational research often entails a qualitative component.', 'a qualitative component']"
3308,case study,Limitations,"Designing Social Inquiry (also called ""KKV""), an influential 1994 book written by Gary King, Robert Keohane, and Sidney Verba, primarily applies lessons from regression-oriented analysis to qualitative research, arguing that the same logics of causal inference can be used in both types of research. The authors' recommendation is to increase the number of observations (a recommendation that Barbara Geddes also makes in Paradigms and Sand Castles), because few observations make it harder to estimate multiple causal effects, as well as increase the risk that there is measurement error, and that an event in a single case was caused by random error or unobservable factors. KKV sees process-tracing and qualitative research as being ""unable to yield strong causal inference"" due to the fact that qualitative scholars would struggle with determining which of many intervening variables truly links the independent variable with a dependent variable. The primary problem is that qualitative research lacks a sufficient number of observations to properly estimate the effects of an independent variable. They write that the number of observations could be increased through various means, but that would simultaneously lead to another problem: that the number of variables would increase and thus reduce degrees of freedom. Christopher H. Achen and Duncan Snidal similarly argue that case studies are not useful for theory construction and theory testing.","Designing Social Inquiry (also called ""KKV""), an influential 1994 book written by Gary King, Robert Keohane, and Sidney Verba, primarily applies lessons from regression-oriented analysis to qualitative research, arguing that the same logics of causal inference can be used in both types of research. The authors' recommendation is to increase the number of observations (a recommendation that Barbara Geddes also makes in Paradigms and Sand Castles), because few observations make it harder to estimate multiple causal effects, as well as increase the risk that there is measurement error, and that an event in a single case was caused by random error or unobservable factors.","[' What is another name for Designing Social Inquiry?', ' What is the name of the influential 1994 book written by Gary King, Robert Keohana, and Sidney Verba?', "" What is Barbara Geddes' recommendation for increasing the number of observations?"", ' Why is it harder to estimate multiple causal effects?', ' What is the risk of measurement error?', ' What increases the risk that there is measurement error?', ' What causes an event in a single case?']","['KKV', 'Designing Social Inquiry', 'few observations make it harder to estimate multiple causal effects', 'few observations', 'an event in a single case was caused by random error or unobservable factors', 'increase the number of observations', 'random error or unobservable factors']"
3309,case study,Limitations,"The purported ""degrees of freedom"" problem that KKV identify is widely considered flawed; while quantitative scholars try to aggregate variables to reduce the number of variables and thus increase the degrees of freedom, qualitative scholars intentionally want their variables to have many different attributes and complexity. For example, James Mahoney writes, ""the Bayesian nature of process tracing explains why it is inappropriate to view qualitative research as suffering from a small-N problem and certain standard causal identification problems."" By using Bayesian probability, it may be possible to makes strong causal inferences from a small sliver of data.","The purported ""degrees of freedom"" problem that KKV identify is widely considered flawed; while quantitative scholars try to aggregate variables to reduce the number of variables and thus increase the degrees of freedom, qualitative scholars intentionally want their variables to have many different attributes and complexity. For example, James Mahoney writes, ""the Bayesian nature of process tracing explains why it is inappropriate to view qualitative research as suffering from a small-N problem and certain standard causal identification problems.""","[' What is the purported ""degrees of freedom"" problem that KKV identify?', ' What do quantitative scholars try to reduce the number of variables and thus increase the degrees of freedom?', ' Who intentionally wants their variables to have many different attributes and complexity?', ' How many different attributes and complexity do qualitative research have?', ' Who writes that the Bayesian nature of process tracing explains why it is inappropriate to view qualitative research as suffering from a small-N problem?']","['flawed', 'aggregate variables', 'qualitative scholars', 'many', 'James Mahoney']"
3310,case study,Limitations,"KKV also identify inductive reasoning in qualitative research as a problem, arguing that scholars should not revise hypotheses during or after data has been collected because it allows for ad hoc theoretical adjustments to fit the collected data. However, scholars have pushed back on this claim, noting that inductive reasoning is a legitimate practice (both in qualitative and quantitative research).","KKV also identify inductive reasoning in qualitative research as a problem, arguing that scholars should not revise hypotheses during or after data has been collected because it allows for ad hoc theoretical adjustments to fit the collected data. However, scholars have pushed back on this claim, noting that inductive reasoning is a legitimate practice (both in qualitative and quantitative research).","[' Inductive reasoning in qualitative research is a problem, according to KKV?', ' Why should scholars not revise hypotheses during or after data has been collected?', ' What allows for ad hoc theoretical adjustments to fit collected data?', ' What is a legitimate practice in both qualitative and quantitative research?']","['scholars should not revise hypotheses during or after data has been collected because it allows for ad hoc theoretical adjustments to fit the collected data', 'because it allows for ad hoc theoretical adjustments to fit the collected data', 'inductive reasoning', 'inductive reasoning']"
3311,case study,Limitations,"A commonly described limit of case studies is that they do not lend themselves to generalizability. Due to the small number of cases, it may be harder to ensure that the chosen cases are representative of the larger population. Some scholars, such as Bent Flyvbjerg, have pushed back on that notion.","A commonly described limit of case studies is that they do not lend themselves to generalizability. Due to the small number of cases, it may be harder to ensure that the chosen cases are representative of the larger population.","[' What is a common limitation of case studies?', ' Due to the small number of cases, it may be harder to ensure that the chosen cases are representative of what?']","['they do not lend themselves to generalizability', 'the larger population']"
3312,case study,Limitations,"As small-N research should not rely on random sampling, scholars must be careful in avoiding selection bias when picking suitable cases. A common criticism of qualitative scholarship is that cases are chosen because they are consistent with the scholar's preconceived notions, resulting in biased research.","As small-N research should not rely on random sampling, scholars must be careful in avoiding selection bias when picking suitable cases. A common criticism of qualitative scholarship is that cases are chosen because they are consistent with the scholar's preconceived notions, resulting in biased research.","[' What should small-N research not rely on?', ' What must scholars be careful in avoiding when picking suitable cases?', ' A common criticism of qualitative scholarship is that cases are chosen because they are consistent with what?']","['random sampling', 'selection bias', ""the scholar's preconceived notions""]"
3313,case study,Teaching case studies,"Teachers may prepare a case study that will then be used in classrooms in the form of a ""teaching"" case study (also see case method and casebook method). For instance, as early as 1870 at Harvard Law School, Christopher Langdell departed from the traditional lecture-and-notes approach to teaching contract law and began using cases pled before courts as the basis for class discussions. By 1920, this practice had become the dominant pedagogical approach used by law schools in the United States.","Teachers may prepare a case study that will then be used in classrooms in the form of a ""teaching"" case study (also see case method and casebook method). For instance, as early as 1870 at Harvard Law School, Christopher Langdell departed from the traditional lecture-and-notes approach to teaching contract law and began using cases pled before courts as the basis for class discussions.","[' What is a ""teaching"" case study?', ' When did Christopher Langdell depart from the traditional lecture-and-notes approach to teaching contract law?', ' What was the traditional approach to teaching contract law?', ' What was used as the basis for class discussions?']","['Teachers may prepare a case study that will then be used in classrooms', '1870', 'lecture-and-notes', 'cases pled before courts']"
3314,case study,Teaching case studies,"Outside of law, teaching case studies have become popular in many different fields and professions, ranging from business education to science education. The Harvard Business School has been among the most prominent developers and users of teaching case studies. Teachers develop case studies with particular learning objectives in mind. Additional relevant documentation, such as financial statements, time-lines, short biographies, and multimedia supplements (such as video-recordings of interviews) often accompany the case studies. Similarly, teaching case studies have become increasingly popular in science education, covering different biological and physical sciences. The National Center for Case Studies in Teaching Science has made a growing body of teaching case studies available for classroom use, for university as well as secondary school coursework.","Outside of law, teaching case studies have become popular in many different fields and professions, ranging from business education to science education. The Harvard Business School has been among the most prominent developers and users of teaching case studies.",[' What school has been one of the most prominent developers and users of teaching case studies?'],['Harvard Business School']
3315,markov chain,Summary,"A Markov chain or Markov process is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. A countably infinite sequence, in which the chain moves state at discrete time steps, gives a discrete-time Markov chain (DTMC). A continuous-time process is called a continuous-time Markov chain (CTMC). It is named after the Russian mathematician Andrey Markov.
","A Markov chain or Markov process is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. A countably infinite sequence, in which the chain moves state at discrete time steps, gives a discrete-time Markov chain (DTMC).","[' What is a Markov chain or Markov process?', ' What model describes a sequence of possible events in which the probability of each event depends only on the state attained in the previous event?', ' A countably infinite sequence moves state at what steps?', ' Chain moves state at discrete time steps, gives what?']","['a stochastic model', 'Markov chain', 'discrete time steps', 'discrete-time Markov chain']"
3316,markov chain,History,"Markov studied Markov processes in the early 20th century, publishing his first paper on the topic in 1906. Markov processes in continuous time were discovered long before Andrey Markov's work in the early 20th century in the form of the Poisson process. Markov was interested in studying an extension of independent random sequences, motivated by a disagreement with Pavel Nekrasov who claimed independence was necessary for the weak law of large numbers to hold. In his first paper on Markov chains, published in 1906, Markov showed that under certain conditions the average outcomes of the Markov chain would converge to a fixed vector of values, so proving a weak law of large numbers without the independence assumption, which had been commonly regarded as a requirement for such mathematical laws to hold. Markov later used Markov chains to study the distribution of vowels in Eugene Onegin, written by Alexander Pushkin, and proved a central limit theorem for such chains.","Markov studied Markov processes in the early 20th century, publishing his first paper on the topic in 1906. Markov processes in continuous time were discovered long before Andrey Markov's work in the early 20th century in the form of the Poisson process.","[' Who studied Markov processes in the early 20th century?', ' When did Markov publish his first paper on the topic?', "" What process was discovered long before Andrey Markov's work?""]","['Markov', '1906', 'Poisson process']"
3317,markov chain,History,"In 1912 Henri Poincaré studied Markov chains on finite groups with an aim to study card shuffling. Other early uses of Markov chains include a diffusion model, introduced by Paul and Tatyana Ehrenfest in 1907, and a branching process, introduced by Francis Galton and Henry William Watson in 1873, preceding the work of Markov. After the work of Galton and Watson, it was later revealed that their branching process had been independently discovered and studied around three decades earlier by Irénée-Jules Bienaymé. Starting in 1928, Maurice Fréchet became interested in Markov chains, eventually resulting in him publishing in 1938 a detailed study on Markov chains.","In 1912 Henri Poincaré studied Markov chains on finite groups with an aim to study card shuffling. Other early uses of Markov chains include a diffusion model, introduced by Paul and Tatyana Ehrenfest in 1907, and a branching process, introduced by Francis Galton and Henry William Watson in 1873, preceding the work of Markov.","[' In what year did Henri Poincaré study Markov chains on finite groups?', ' Who introduced a diffusion model in 1907?', ' What process was introduced by Francis Galton and Henry William Watson in 1873?', ' When was the work of Francis Galton and Henry William Watson published?', "" Who was the author of Markov's work?""]","['1912', 'Paul and Tatyana Ehrenfest', 'branching', '1873', 'Henri Poincaré']"
3318,markov chain,History,"Andrei Kolmogorov developed in a 1931 paper a large part of the early theory of continuous-time Markov processes. Kolmogorov was partly inspired by Louis Bachelier's 1900 work on fluctuations in the stock market as well as Norbert Wiener's work on Einstein's model of Brownian movement. He introduced and studied a particular set of Markov processes known as diffusion processes, where he derived a set of differential equations describing the processes. Independent of Kolmogorov's work, Sydney Chapman derived in a 1928 paper an equation, now called the Chapman–Kolmogorov equation, in a less mathematically rigorous way than Kolmogorov, while studying Brownian movement. The differential equations are now called the Kolmogorov equations or the Kolmogorov–Chapman equations. Other mathematicians who contributed significantly to the foundations of Markov processes include William Feller, starting in 1930s, and then later Eugene Dynkin, starting in the 1950s.",Andrei Kolmogorov developed in a 1931 paper a large part of the early theory of continuous-time Markov processes. Kolmogorov was partly inspired by Louis Bachelier's 1900 work on fluctuations in the stock market as well as Norbert Wiener's work on Einstein's model of Brownian movement.,"[' Who developed a large part of the early theory of continuous-time Markov processes?', ' Whose work on fluctuations in the stock market inspired Andrei Kolmogorov?']","['Andrei Kolmogorov', 'Louis Bachelier']"
3319,markov chain,Examples,"Random walks based on integers and the gambler's ruin problem are examples of Markov processes. Some variations of these processes were studied hundreds of years earlier in the context of independent variables. Two important examples of Markov processes are the Wiener process, also known as the Brownian motion process, and the Poisson process, which are considered the most important and central stochastic processes in the theory of stochastic processes. These two processes are Markov processes in continuous time, while random walks on the integers and the gambler's ruin problem are examples of Markov processes in discrete time.",Random walks based on integers and the gambler's ruin problem are examples of Markov processes. Some variations of these processes were studied hundreds of years earlier in the context of independent variables.,"["" Random walks based on integers and the gambler's ruin problem are examples of what?"", ' Some variations of Markov processes were studied hundreds of years earlier in the context of what variables?']","['Markov processes', 'independent variables']"
3320,markov chain,Examples,"A famous Markov chain is the so-called ""drunkard's walk"", a random walk on the number line where, at each step, the position may change by +1 or −1 with equal probability. From any position there are two possible transitions, to the next or previous integer. The transition probabilities depend only on the current position, not on the manner in which the position was reached. For example, the transition probabilities from 5 to 4 and 5 to 6 are both 0.5, and all other transition probabilities from 5 are 0. These probabilities are independent of whether the system was previously in 4 or 6.
","A famous Markov chain is the so-called ""drunkard's walk"", a random walk on the number line where, at each step, the position may change by +1 or −1 with equal probability. From any position there are two possible transitions, to the next or previous integer.","[' What is a famous Markov chain?', "" What is drunkard's walk?"", ' How many possible transitions are there from any position?']","[""drunkard's walk"", 'a random walk on the number line', 'two']"
3321,markov chain,Examples,"This creature's eating habits can be modeled with a Markov chain since its choice tomorrow depends solely on what it ate today, not what it ate yesterday or any other time in the past. One statistical property that could be calculated is the expected percentage, over a long period, of the days on which the creature will eat grapes.
","This creature's eating habits can be modeled with a Markov chain since its choice tomorrow depends solely on what it ate today, not what it ate yesterday or any other time in the past. One statistical property that could be calculated is the expected percentage, over a long period, of the days on which the creature will eat grapes.","["" What can a creature's eating habits be modeled with?"", ' What depends solely on what it ate today?', ' How could one calculate the expected percentage, over a long period?', ' What is the expected percentage of days on which the creature will eat grapes?']","['a Markov chain', 'its choice tomorrow', 'statistical', 'over a long period']"
3322,markov chain,Examples,"A series of independent events (for example, a series of coin flips) satisfies the formal definition of a Markov chain. However, the theory is usually applied only when the probability distribution of the next step depends non-trivially on the current state.
","A series of independent events (for example, a series of coin flips) satisfies the formal definition of a Markov chain. However, the theory is usually applied only when the probability distribution of the next step depends non-trivially on the current state.","[' A series of independent events satisfies what formal definition of a Markov chain?', ' The theory is usually applied only when the probability distribution of the next step depends non-trivially on what?']","['coin flips', 'the current state']"
3323,markov chain,Properties,"Two states communicate with each other if both are reachable from one another by a sequence of transitions that have positive probability. This is an equivalence relation which yields a set of communicating classes. A class is closed if the probability of leaving the class is zero. A Markov chain is irreducible if there is one communicating class, the state space.
",Two states communicate with each other if both are reachable from one another by a sequence of transitions that have positive probability. This is an equivalence relation which yields a set of communicating classes.,"[' Two states communicate with each other if both are reachable from one another by a sequence of transitions that have what?', ' What yields a set of communicating classes?']","['positive probability', 'equivalence relation']"
3324,markov chain,Properties,"A state i has period 



k


{\displaystyle k}
 if 



k


{\displaystyle k}
 is the greatest common divisor of the number of transitions by which i can be reached, starting from i. That is:
","A state i has period 



k


{\displaystyle k}
 if 



k


{\displaystyle k}
 is the greatest common divisor of the number of transitions by which i can be reached, starting from i. That is:",[' What is the most common divisor of the number of transitions by which i can be reached?'],['k']
3325,markov chain,Properties,"A state i is said to be transient if, starting from i, there is a non-zero probability that the chain will never return to i. It is recurrent otherwise. For a recurrent state i, the mean hitting time is defined as:
","A state i is said to be transient if, starting from i, there is a non-zero probability that the chain will never return to i. It is recurrent otherwise.","[' What is said to be transient if, starting from i, there is a non-zero probability that the chain will never return to i?']",['A state i']
3326,markov chain,Properties,"State i is positive recurrent if 




M

i




{\displaystyle M_{i}}
 is finite and null recurrent otherwise. Periodicity, transience, recurrence and positive and null recurrence are class properties—that is, if one state has the property then all states in its communicating class have the property.
","State i is positive recurrent if 




M

i




{\displaystyle M_{i}}
 is finite and null recurrent otherwise. Periodicity, transience, recurrence and positive and null recurrence are class properties—that is, if one state has the property then all states in its communicating class have the property.","[' What is positive recurrent if M i <unk>displaystyle M_<unk>i<unk> is finite?', ' What are periodicity, transience, recurrence, and positive and null are class properties?', ' If one state has the property then all states in its communicating class have what?']","['State i', 'if one state has the property then all states in its communicating class have the property', 'class properties']"
3327,relevance feedback,Summary,"Relevance feedback is a feature of some information retrieval systems.  The idea behind relevance feedback is to take the results that are initially returned from a given query, to gather user feedback, and to use information about whether or not those results are relevant to perform a new query.  We can usefully distinguish between three types of feedback: explicit feedback, implicit feedback, and blind or ""pseudo"" feedback.
","Relevance feedback is a feature of some information retrieval systems. The idea behind relevance feedback is to take the results that are initially returned from a given query, to gather user feedback, and to use information about whether or not those results are relevant to perform a new query.","[' What is a feature of some information retrieval systems?', ' The idea behind relevance feedback is to take the results that are initially returned from a given query, to gather user feedback and to use information about whether or not the results are relevant to perform a new query?']","['Relevance feedback', 'Relevance feedback']"
3328,relevance feedback,Explicit feedback,"Explicit feedback is obtained from assessors of relevance indicating the relevance of a document retrieved for a query. This type of feedback is defined as explicit only when the assessors (or other users of a system) know that the feedback provided is interpreted as relevance judgments.
",Explicit feedback is obtained from assessors of relevance indicating the relevance of a document retrieved for a query. This type of feedback is defined as explicit only when the assessors (or other users of a system) know that the feedback provided is interpreted as relevance judgments.,"[' What type of feedback is obtained from assessors of relevance indicating the relevance of a document retrieved for a query?', ' When is explicit feedback defined as explicit?']","['Explicit', 'when the assessors (or other users of a system) know that the feedback provided is interpreted as relevance judgments']"
3329,relevance feedback,Explicit feedback,"Users may indicate relevance explicitly using a binary or graded relevance system. Binary relevance feedback indicates that a document is either relevant or irrelevant for a given query. Graded relevance feedback indicates the relevance of a document to a query on a scale using numbers, letters, or descriptions (such as ""not relevant"", ""somewhat relevant"", ""relevant"", or ""very relevant""). Graded relevance may also take the form of a cardinal ordering of documents created by an assessor; that is, the assessor places documents of a result set in order of (usually descending) relevance.  An example of this would be the SearchWiki feature implemented by Google on their search website.
",Users may indicate relevance explicitly using a binary or graded relevance system. Binary relevance feedback indicates that a document is either relevant or irrelevant for a given query.,"[' What type of system can users use to indicate relevance?', ' What indicates that a document is either relevant or irrelevant for a given query?']","['binary or graded relevance system', 'Binary relevance feedback']"
3330,relevance feedback,Explicit feedback,"A performance metric which became popular around 2005 to measure the usefulness of a ranking algorithm based on the explicit relevance feedback is NDCG. Other measures include precision at k and mean average precision.
",A performance metric which became popular around 2005 to measure the usefulness of a ranking algorithm based on the explicit relevance feedback is NDCG. Other measures include precision at k and mean average precision.,"[' When did performance metric NDCG become popular?', ' What is another measure that became popular around 2005?']","['around 2005', 'NDCG. Other measures include precision at k and mean average precision']"
3331,relevance feedback,Implicit feedback,"Implicit feedback is inferred from user behavior, such as noting which documents they do and do not select for viewing, the duration of time spent viewing a document, or page browsing or scrolling actions. There are many signals during the search process that one can use for implicit feedback and the types of information to provide in response.","Implicit feedback is inferred from user behavior, such as noting which documents they do and do not select for viewing, the duration of time spent viewing a document, or page browsing or scrolling actions. There are many signals during the search process that one can use for implicit feedback and the types of information to provide in response.","[' What is implicit feedback inferred from?', ' What are some examples of user behavior?', ' What type of feedback can one use for implicit feedback?', ' What types of information can one provide in response to a search?']","['user behavior', 'noting which documents they do and do not select for viewing, the duration of time spent viewing a document, or page browsing or scrolling actions', 'signals', 'Implicit feedback']"
3332,relevance feedback,Implicit feedback,"An example of this is dwell time, which is a measure of how long a user spends viewing the page linked to in a search result. It is an indicator of how well the search result met the query intent of the user, and is used as a feedback mechanism to improve search results.
","An example of this is dwell time, which is a measure of how long a user spends viewing the page linked to in a search result. It is an indicator of how well the search result met the query intent of the user, and is used as a feedback mechanism to improve search results.","[' What is a measure of how long a user spends viewing a page linked to in a search result?', ' What is an indicator of how well the search result met the query intent of the user?', ' What is used as a feedback mechanism to improve search results?']","['dwell time', 'dwell time', 'dwell time']"
3333,relevance feedback,Blind feedback,"Pseudo relevance feedback, also known as  blind relevance feedback, provides a method for automatic local analysis. It automates the manual part of relevance feedback, so that the user gets improved retrieval performance without an extended interaction. The method is to do normal retrieval to find an initial set of most relevant documents, to then assume that the top ""k"" ranked documents are relevant, and finally to do relevance feedback as before under this assumption. The procedure is:
","Pseudo relevance feedback, also known as  blind relevance feedback, provides a method for automatic local analysis. It automates the manual part of relevance feedback, so that the user gets improved retrieval performance without an extended interaction.","[' What is Pseudo relevance feedback also known as?', ' What provides a method for automatic local analysis?']","['blind relevance feedback', 'Pseudo relevance feedback']"
3334,relevance feedback,Blind feedback,"This automatic technique mostly works. Evidence suggests that it tends to work better than global analysis. Through a query expansion, some relevant documents missed in the initial round can then be retrieved to improve the overall performance. Clearly, the effect of this method strongly relies on the quality of selected expansion terms. It has been found to improve performance in the TREC ad hoc task. But it is not without the dangers of an automatic process. For example, if the query is about copper mines and the top several documents are all about mines in Chile, then there may be query drift in the direction of documents on Chile. In addition, if the words added to the original query are unrelated to the query topic, the quality of the retrieval is likely to be degraded, especially in Web search, where web documents often cover multiple different topics. To improve the quality of expansion words in pseudo-relevance feedback, a positional relevance feedback for pseudo-relevance feedback has been proposed to select from feedback documents those words that are focused on the query topic based on positions of words in feedback documents. Specifically, the positional relevance model assigns more weights to words occurring closer to query words based on the intuition that words closer to query words are more likely to be related to the query topic.
",This automatic technique mostly works. Evidence suggests that it tends to work better than global analysis.,"[' What kind of analysis does automatic analysis generally work better than?', ' What type of analysis works better than global analysis?']","['global analysis', 'automatic']"
3335,relevance feedback,Using relevance information,"Relevance information is utilized by using the contents of the relevant documents to either adjust the weights of terms in the original query, or by using those contents to add words to the query.  Relevance feedback is often implemented using the Rocchio algorithm.
","Relevance information is utilized by using the contents of the relevant documents to either adjust the weights of terms in the original query, or by using those contents to add words to the query. Relevance feedback is often implemented using the Rocchio algorithm.","[' What is used to adjust the weights of terms in the original query?', ' What is often implemented using the Rocchio algorithm?']","['the contents of the relevant documents', 'Relevance feedback']"
3336,decision procedure,Summary,"In computability theory and computational complexity theory, a decision problem is a problem that can be posed as a yes–no question of the input values. An example of a decision problem is deciding whether a given natural number is prime. Another is the problem ""given two numbers x and y, does x evenly divide y?"".  The answer is either 'yes' or 'no' depending upon the values of x and y. A method for solving a decision problem, given in the form of an algorithm, is called a decision procedure for that problem. A decision procedure for the decision problem ""given two numbers x and y, does x evenly divide y?"" would give the steps for determining whether x evenly divides y. One such algorithm is long division. If the remainder is zero the answer is 'yes', otherwise it is 'no'. A decision problem which can be solved by an algorithm is called decidable.
","In computability theory and computational complexity theory, a decision problem is a problem that can be posed as a yes–no question of the input values. An example of a decision problem is deciding whether a given natural number is prime.","[' In computability theory and computational complexity theory, a decision problem is a problem that can be posed as a yes-no question of the input values?', ' What is an example decision problem?']","['yes', 'deciding whether a given natural number is prime']"
3337,decision procedure,Summary,"The field of computational complexity categorizes decidable decision problems by how difficult they are to solve. ""Difficult"", in this sense, is described in terms of the computational resources needed by the most efficient algorithm for a certain problem. The field of recursion theory, meanwhile, categorizes undecidable decision problems by Turing degree, which is a measure of the noncomputability inherent in any solution.
","The field of computational complexity categorizes decidable decision problems by how difficult they are to solve. ""Difficult"", in this sense, is described in terms of the computational resources needed by the most efficient algorithm for a certain problem.","[' What field categorizes decidable decision problems by how difficult they are to solve?', ' What is difficult described in terms of the computational resources needed by the most efficient algorithm for a certain problem?']","['computational complexity', 'Difficult']"
3338,decision procedure,Definition,A decision problem is a yes-or-no question on an infinite set of inputs. It is traditional to define the decision problem as the set of possible inputs together with the set of inputs for which the answer is yes.,A decision problem is a yes-or-no question on an infinite set of inputs. It is traditional to define the decision problem as the set of possible inputs together with the set of inputs for which the answer is yes.,"[' A decision problem is a yes-or-no question on what?', ' What is the traditional definition of a decision problem?']","['an infinite set of inputs', 'the set of possible inputs together with the set of inputs for which the answer is yes']"
3339,decision procedure,Definition,"These inputs can be natural numbers, but can also be values of some other kind, like binary strings or strings over some other alphabet. The subset of strings for which the problem returns ""yes"" is a formal language, and often decision problems are defined as formal languages.
","These inputs can be natural numbers, but can also be values of some other kind, like binary strings or strings over some other alphabet. The subset of strings for which the problem returns ""yes"" is a formal language, and often decision problems are defined as formal languages.","[' What can inputs be?', ' What is a formal language?', ' Decision problems are often defined as what?']","['natural numbers', 'The subset of strings', 'formal languages']"
3340,decision procedure,Definition,"Using an encoding such as Gödel numbering, any string can be encoded as a natural number, via which a decision problem can be defined as a subset of the natural numbers. Therefore, the algorithm of a decision problem is to compute the characteristic function of a subset of the natural numbers.
","Using an encoding such as Gödel numbering, any string can be encoded as a natural number, via which a decision problem can be defined as a subset of the natural numbers. Therefore, the algorithm of a decision problem is to compute the characteristic function of a subset of the natural numbers.","[' What can be encoded as a natural number using Gödel numbering?', ' A decision problem can be defined as what?', ' What is the algorithm of a decision problem?', ' How do you compute the characteristic function of a subset of the natural numbers?']","['any string', 'a subset of the natural numbers', 'to compute the characteristic function of a subset of the natural numbers', 'the algorithm of a decision problem']"
3341,decision procedure,Examples,"A classic example of a decidable decision problem is the set of prime numbers. It is possible to effectively decide whether a given natural number is prime by testing every possible nontrivial factor. Although much more efficient methods of primality testing are known, the existence of any effective method is enough to establish decidability.
",A classic example of a decidable decision problem is the set of prime numbers. It is possible to effectively decide whether a given natural number is prime by testing every possible nontrivial factor.,"[' What is a classic example of a decidable decision problem?', ' How is it possible to effectively decide whether a given natural number is prime?']","['the set of prime numbers', 'by testing every possible nontrivial factor']"
3342,decision procedure,Decidability,"A decision problem is decidable or effectively solvable if the set of inputs (or natural numbers) for which the answer is yes is a recursive set. A problem is partially decidable, semidecidable, solvable, or provable if the set of inputs (or natural numbers) for which the answer is yes a recursively enumerable set. Problems that are not decidable are undecidable. For those it is not possible to create an algorithm, efficient or otherwise, that solves them.
","A decision problem is decidable or effectively solvable if the set of inputs (or natural numbers) for which the answer is yes is a recursive set. A problem is partially decidable, semidecidable, solvable, or provable if the set of inputs (or natural numbers) for which the answer is yes a recursively enumerable set.","[' What is decidable or effectively solvable if the set of inputs for which the answer is yes is a recursive set?', ' What is the answer to a recursively enumerable set?']","['A decision problem', 'yes']"
3343,decision procedure,Complete problems,"Decision problems can be ordered according to many-one reducibility and related to feasible reductions such as polynomial-time reductions. A decision problem P is said to be complete for a set of decision problems S if P is a member of S and every problem in S can be reduced to P. Complete decision problems are used in computational complexity theory to characterize complexity classes of decision problems. For example, the Boolean satisfiability problem is complete for the class NP of decision problems under polynomial-time reducibility.
",Decision problems can be ordered according to many-one reducibility and related to feasible reductions such as polynomial-time reductions. A decision problem P is said to be complete for a set of decision problems S if P is a member of S and every problem in S can be reduced to P. Complete decision problems are used in computational complexity theory to characterize complexity classes of decision problems.,"[' What can be ordered according to many-one reducibility and related to feasible reductions such as polynomial-time reductions?', ' A decision problem P is said to be complete for a set of decision problems S if P is a member of what?', ' How can every problem in S be reduced to P?', ' Complete decision problems are used to characterize what classes of decision problems?']","['Decision problems', 'S', 'P is a member of S', 'complexity']"
3344,decision procedure,Function problems,"Decision problems are closely related to function problems, which can have answers that are more complex than a simple 'yes' or 'no'.  A corresponding function problem is ""given two numbers x and y, what is x divided by y?"".
","Decision problems are closely related to function problems, which can have answers that are more complex than a simple 'yes' or 'no'. A corresponding function problem is ""given two numbers x and y, what is x divided by y?","[' Decision problems are closely related to what?', "" Decision problems can have answers that are more complex than a simple 'yes' or 'no'?"", ' What is a corresponding function problem?']","['function problems', 'function problems', 'given two numbers x and y']"
3345,decision procedure,Function problems,"Every function problem can be turned into a decision problem; the decision problem is just the graph of the associated function.   (The graph of a function f is the set of pairs (x,y) such that f(x) = y.)  If this decision problem were effectively solvable then the function problem would be as well.  This reduction does not respect computational complexity, however.  For example, it is possible for the graph of a function to be decidable in polynomial time (in which case running time is computed as a function of the pair (x,y)) when the function is not computable in polynomial time (in which case running time is computed as a function of x alone).  The function f(x) = 2x has this property.
","Every function problem can be turned into a decision problem; the decision problem is just the graph of the associated function. (The graph of a function f is the set of pairs (x,y) such that f(x) = y.)","[' Every function problem can be turned into what?', ' What is the decision problem?', ' The graph of a function f is the set of pairs such that f(x) = y?']","['a decision problem', 'just the graph of the associated function', 'function problem can be turned into a decision problem']"
3346,decision procedure,Function problems,"Every decision problem can be converted into the function problem of computing the characteristic function of the set associated to the decision problem.   If this function is computable then the associated decision problem is decidable. However, this reduction is more liberal than the standard reduction used in computational complexity (sometimes called polynomial-time many-one reduction); for example, the complexity of the characteristic functions of an NP-complete problem and its co-NP-complete complement is exactly the same even though the underlying decision problems may not be considered equivalent in some typical models of computation.
",Every decision problem can be converted into the function problem of computing the characteristic function of the set associated to the decision problem. If this function is computable then the associated decision problem is decidable.,"[' What can every decision problem be converted into?', ' What is the function problem of computing the characteristic function of the set associated to the decision problem?', ' If the function is computable then the associated decision problem is what?']","['the function problem', 'decidable', 'decidable']"
3347,decision procedure,Optimization problems,"Unlike decision problems, for which there is only one correct answer for each input, optimization problems are concerned with finding the best answer to a particular input.  Optimization problems arise naturally in many applications, such as the traveling salesman problem and many questions in linear programming.
","Unlike decision problems, for which there is only one correct answer for each input, optimization problems are concerned with finding the best answer to a particular input. Optimization problems arise naturally in many applications, such as the traveling salesman problem and many questions in linear programming.","[' What are optimization problems concerned with finding the best answer to?', ' What is the traveling salesman problem?']","['a particular input', 'Optimization problems']"
3348,decision procedure,Optimization problems,"There are standard techniques for transforming function and optimization problems into decision problems. For example, in the traveling salesman problem, the optimization problem is to produce a tour with minimal weight. The associated decision problem is: for each N, to decide whether the graph has any tour with weight less than N.  By repeatedly answering the decision problem, it is possible to find the minimal weight of a tour.
","There are standard techniques for transforming function and optimization problems into decision problems. For example, in the traveling salesman problem, the optimization problem is to produce a tour with minimal weight.","[' What are standard techniques for transforming function and optimization problems into decision problems?', ' What is the optimization problem in the traveling salesman problem?']","['traveling salesman problem, the optimization problem is to produce a tour with minimal weight', 'to produce a tour with minimal weight']"
3349,decision procedure,Optimization problems,"Because the theory of decision problems is very well developed, research in complexity theory has typically focused on decision problems. Optimization problems themselves are still of interest in computability theory, as well as in fields such as operations research.
","Because the theory of decision problems is very well developed, research in complexity theory has typically focused on decision problems. Optimization problems themselves are still of interest in computability theory, as well as in fields such as operations research.","[' Why is complexity theory so well developed?', ' Optimization problems are still of interest in computability theory as well as what other field?']","['Because the theory of decision problems', 'operations research']"
3350,blind source separation,Summary,"Source separation, blind signal separation (BSS) or blind source separation, is the separation of a set of source signals from a set of mixed signals, without the aid of information (or with very little information) about the source signals or the mixing process. It is most commonly applied in digital signal processing and involves the analysis of mixtures of signals; the objective is to recover the original component signals from a mixture signal. The classical example of a source separation problem is the cocktail party problem, where a number of people are talking simultaneously in a room (for example, at a cocktail party), and a listener is trying to follow one of the discussions. The human brain can handle this sort of auditory source separation problem, but it is a difficult problem in digital signal processing.
","Source separation, blind signal separation (BSS) or blind source separation, is the separation of a set of source signals from a set of mixed signals, without the aid of information (or with very little information) about the source signals or the mixing process. It is most commonly applied in digital signal processing and involves the analysis of mixtures of signals; the objective is to recover the original component signals from a mixture signal.","[' What is another name for blind signal separation?', ' What is the term for the separation of source signals from mixed signals without the aid of information?', ' When is blind source separation most commonly applied?', ' What is the mixing process most commonly used for?', ' The mixing process involves the analysis of what types of signals?']","['blind source separation', 'Source separation', 'digital signal processing', 'digital signal processing', 'mixtures']"
3351,blind source separation,Summary,"This problem is in general highly underdetermined, but useful solutions can be derived under a surprising variety of conditions. Much of the early literature in this field focuses on the separation of temporal signals such as audio. However, blind signal separation is now routinely performed on multidimensional data, such as images and tensors, which may involve no time dimension whatsoever.
","This problem is in general highly underdetermined, but useful solutions can be derived under a surprising variety of conditions. Much of the early literature in this field focuses on the separation of temporal signals such as audio.","[' What is highly underdetermined?', ' What can be derived under a surprising variety of conditions?', ' Much of the early literature in this field focuses on what?']","['This problem', 'useful solutions', 'separation of temporal signals']"
3352,blind source separation,Summary,"Several approaches have been proposed for the solution of this problem but development is currently still very much in progress. Some of the more successful approaches are principal components analysis and independent component analysis, which work well when there are no delays or echoes present; that is, the problem is simplified a great deal. The field of computational auditory scene analysis attempts to achieve auditory source separation using an approach that is based on human hearing.
","Several approaches have been proposed for the solution of this problem but development is currently still very much in progress. Some of the more successful approaches are principal components analysis and independent component analysis, which work well when there are no delays or echoes present; that is, the problem is simplified a great deal.","[' Several approaches have been proposed for the solution of what problem?', ' Principal components analysis and independent component analysis work well when there are no delays or what?', ' No delays or echoes present?', ' The problem is simplified a great deal what?']","['principal components analysis and independent component analysis', 'echoes', 'principal components analysis and independent component analysis', 'principal components analysis and independent component analysis']"
3353,blind source separation,Summary,"The human brain must also solve this problem in real time. In human perception this ability is commonly referred to as auditory scene analysis or the cocktail party effect.
",The human brain must also solve this problem in real time. In human perception this ability is commonly referred to as auditory scene analysis or the cocktail party effect.,"[' What does the human brain have to do to solve a problem in real time?', ' What is the human perception referred to as in human perception?']","['auditory scene analysis', 'auditory scene analysis']"
3354,blind source separation,Mathematical representation,"The set of individual source signals, 



s
(
t
)
=
(

s

1


(
t
)
,
…
,

s

n


(
t
)

)

T




{\displaystyle s(t)=(s_{1}(t),\dots ,s_{n}(t))^{T}}
, is 'mixed' using a matrix, 



A
=
[

a

i
j


]
∈


R


m
×
n




{\displaystyle A=[a_{ij}]\in \mathbb {R} ^{m\times n}}
, to produce a set of 'mixed' signals, 



x
(
t
)
=
(

x

1


(
t
)
,
…
,

x

m


(
t
)

)

T




{\displaystyle x(t)=(x_{1}(t),\dots ,x_{m}(t))^{T}}
, as follows.  Usually, 



n


{\displaystyle n}
 is equal to 



m


{\displaystyle m}
. If 



m
>
n


{\displaystyle m>n}
, then the system of equations is overdetermined and thus can be unmixed using a conventional linear method. If 



n
>
m


{\displaystyle n>m}
, the system is underdetermined and a non-linear method must be employed to recover the unmixed signals. The signals themselves can be multidimensional.
","The set of individual source signals, 



s
(
t
)
=
(

s

1


(
t
)
,
…
,

s

n


(
t
)

)

T




{\displaystyle s(t)=(s_{1}(t),\dots ,s_{n}(t))^{T}}
, is 'mixed' using a matrix, 



A
=
[

a

i
j


]
∈


R


m
×
n




{\displaystyle A=[a_{ij}]\in \mathbb {R} ^{m\times n}}
, to produce a set of 'mixed' signals, 



x
(
t
)
=
(

x

1


(
t
)
,
…
,

x

m


(
t
)

)

T




{\displaystyle x(t)=(x_{1}(t),\dots ,x_{m}(t))^{T}}
, as follows. Usually, 



n


{\displaystyle n}
 is equal to 



m


{\displaystyle m}
.","["" What is used to produce a set of'mixed' signals?"", ' What is n <unk>displaystyle equal to?']","['a matrix', 'm\n\n\n{\\displaystyle m}']"
3355,blind source separation,Mathematical representation,"The above equation is effectively 'inverted' as follows. Blind source separation separates the set of mixed signals, 



x
(
t
)


{\displaystyle x(t)}
, through the determination of an 'unmixing' matrix, 



B
=
[

B

i
j


]
∈


R


n
×
m




{\displaystyle B=[B_{ij}]\in \mathbb {R} ^{n\times m}}
, to 'recover' an approximation of the original signals, 



y
(
t
)
=
(

y

1


(
t
)
,
…
,

y

n


(
t
)

)

T




{\displaystyle y(t)=(y_{1}(t),\dots ,y_{n}(t))^{T}}
.","The above equation is effectively 'inverted' as follows. Blind source separation separates the set of mixed signals, 



x
(
t
)


{\displaystyle x(t)}
, through the determination of an 'unmixing' matrix, 



B
=
[

B

i
j


]
∈


R


n
×
m




{\displaystyle B=[B_{ij}]\in \mathbb {R} ^{n\times m}}
, to 'recover' an approximation of the original signals, 



y
(
t
)
=
(

y

1


(
t
)
,
…
,

y

n


(
t
)

)

T




{\displaystyle y(t)=(y_{1}(t),\dots ,y_{n}(t))^{T}}
.","["" How is the above equation effectively 'inverted'?"", ' Blind source separation separates the set of mixed signals?']","['as follows', 'x']"
3356,blind source separation,Approaches,"Since the chief difficulty of the problem is its underdetermination, methods for blind source separation generally seek to narrow the set of possible solutions in a way that is unlikely to exclude the desired solution. In one approach, exemplified by principal and independent component analysis, one seeks source signals that are minimally correlated or maximally independent in a probabilistic or information-theoretic sense. A second approach, exemplified by nonnegative matrix factorization, is to impose structural constraints on the source signals. These structural constraints may be derived from a generative model of the signal, but are more commonly heuristics justified by good empirical performance. A common theme in the second approach is to impose some kind of low-complexity constraint on the signal, such as sparsity in some basis for the signal space. This approach can be particularly effective if one requires not the whole signal, but merely its most salient features.
","Since the chief difficulty of the problem is its underdetermination, methods for blind source separation generally seek to narrow the set of possible solutions in a way that is unlikely to exclude the desired solution. In one approach, exemplified by principal and independent component analysis, one seeks source signals that are minimally correlated or maximally independent in a probabilistic or information-theoretic sense.","[' What is the chief difficulty of a problem?', ' Methods for blind source separation aim to narrow the set of possible solutions in a way that is unlikely to exclude the desired solution?', ' Principal and independent component analysis seeks source signals that are what?', ' What type of analysis seeks source signals that are minimally correlated or maximally independent in a probabilistic sense?']","['its underdetermination', 'Since the chief difficulty of the problem is its underdetermination', 'minimally correlated or maximally independent', 'principal and independent component analysis']"
3357,xml,Summary,"Extensible Markup Language (XML) is a markup language and file format for storing, transmitting, and reconstructing arbitrary data. It defines a set of rules for encoding documents in a format that is both human-readable and machine-readable. The World Wide Web Consortium's XML 1.0 Specification of 1998 and several other related specifications—all of them free open standards—define XML.","Extensible Markup Language (XML) is a markup language and file format for storing, transmitting, and reconstructing arbitrary data. It defines a set of rules for encoding documents in a format that is both human-readable and machine-readable.","[' What does XML stand for?', ' What is the purpose of Extensible Markup Language?']","['Extensible Markup Language', 'storing, transmitting, and reconstructing arbitrary data']"
3358,xml,Summary,"The design goals of XML emphasize simplicity, generality, and usability across the Internet. It is a textual data format with strong support via Unicode for different human languages. Although the design of XML focuses on documents, the language is widely used for the representation of arbitrary data structures such as those used in web services.
","The design goals of XML emphasize simplicity, generality, and usability across the Internet. It is a textual data format with strong support via Unicode for different human languages.","[' What are the design goals of XML?', ' What is a textual data format with strong support via Unicode?']","['simplicity, generality, and usability', 'XML']"
3359,xml,Overview,"The main purpose of XML is serialization, i.e. storing, transmitting, and reconstructing arbitrary data. For two disparate systems to exchange information, they need to agree upon a file format. XML standardizes this process. XML is analogous to a lingua franca for representing information.: 1 ","The main purpose of XML is serialization, i.e. storing, transmitting, and reconstructing arbitrary data.","[' What is the main purpose of XML?', ' What is serialization?']","['serialization', 'storing, transmitting, and reconstructing arbitrary data']"
3360,xml,Overview,"As a markup language, XML labels, categorizes, and structurally organizes information.: 11  XML tags represent the data structure and contain metadata. What's within the tags is data, encoded in the way the XML standard specifies.: 11  An additional XML schema (XSD) defines the necessary metadata for interpreting and validating XML. (This is also referred to as the canonical schema.): 135  An XML document that adheres to basic XML rules is ""well-formed""; one that adheres to its schema is ""valid."": 135 ","As a markup language, XML labels, categorizes, and structurally organizes information. : 11  XML tags represent the data structure and contain metadata.","[' XML labels, categorizes, and structurally organizes information as what?', ' How many xml tags represent the data structure?']","['markup language', ':\u200a11']"
3361,xml,Overview,"IETF RFC 7303 (which supersedes the older RFC 3023), provides rules for the construction of media types for use in XML message. It defines two base media types: application/xml and text/xml. They are be used for transmitting raw XML files without exposing their internal semantics. RFC 7303 further recommends that XML-based languages be given media types ending in +xml, for example, image/svg+xml for SVG.
","IETF RFC 7303 (which supersedes the older RFC 3023), provides rules for the construction of media types for use in XML message. It defines two base media types: application/xml and text/xml.","[' What does IETF RFC 7303 supersede?', ' What defines two base media types?']","['RFC 3023', 'IETF RFC 7303']"
3362,xml,Applications,"XML has come into common use for the interchange of data over the Internet. Hundreds of document formats using XML syntax have been developed, including RSS, Atom, Office Open XML, OpenDocument, SVG, and XHTML. XML has also provides the base language for communication protocols such as SOAP and XMPP. It is the message exchange format for the Asynchronous JavaScript and XML (AJAX) programming technique.
","XML has come into common use for the interchange of data over the Internet. Hundreds of document formats using XML syntax have been developed, including RSS, Atom, Office Open XML, OpenDocument, SVG, and XHTML.","[' What has come into common use for the interchange of data over the internet?', ' How many document formats using XML syntax have been developed?']","['XML', 'Hundreds']"
3363,xml,Applications,"Many industry data standards, such as Health Level 7, OpenTravel Alliance, FpML, MISMO, and National Information Exchange Model are based on XML and the rich features of the XML schema specification. In publishing, Darwin Information Typing Architecture is an XML industry data standard. XML is used extensively to underpin various publishing formats.
","Many industry data standards, such as Health Level 7, OpenTravel Alliance, FpML, MISMO, and National Information Exchange Model are based on XML and the rich features of the XML schema specification. In publishing, Darwin Information Typing Architecture is an XML industry data standard.","[' What are some industry data standards based on?', ' What is an XML industry data standard?']","['XML', 'Darwin Information Typing Architecture']"
3364,xml,Key terminology,"The material in this section is based on the XML Specification. This is not an exhaustive list of all the constructs that appear in XML; it provides an introduction to the key constructs most often encountered in day-to-day use.
",The material in this section is based on the XML Specification. This is not an exhaustive list of all the constructs that appear in XML; it provides an introduction to the key constructs most often encountered in day-to-day use.,"[' The material in this section is based on what?', ' What is not an exhaustive list of all the constructs that appear in XML?']","['XML Specification', 'XML Specification']"
3365,xml,Characters and escaping,"XML documents consist entirely of characters from the Unicode repertoire. Except for a small number of specifically excluded control characters, any character defined by Unicode may appear within the content of an XML document.
","XML documents consist entirely of characters from the Unicode repertoire. Except for a small number of specifically excluded control characters, any character defined by Unicode may appear within the content of an XML document.","[' XML documents consist entirely of characters from what?', ' Except for a small number of specifically excluded control characters, any character defined by Unicode may appear within what content?']","['Unicode', 'XML document']"
3366,xml,Syntactical correctness and error-handling,"The XML specification defines an XML document as a well-formed text, meaning that it satisfies a list of syntax rules provided in the specification. Some key points in the fairly lengthy list include:
","The XML specification defines an XML document as a well-formed text, meaning that it satisfies a list of syntax rules provided in the specification. Some key points in the fairly lengthy list include:",[' What defines an XML document as a well-formed text?'],['The XML specification']
3367,xml,Syntactical correctness and error-handling,"The definition of an XML document excludes texts that contain violations of well-formedness rules; they are simply not XML. An XML processor that encounters such a violation is required to report such errors and to cease normal processing. This policy, occasionally referred to as ""draconian error handling,"" stands in notable contrast to the behavior of programs that process HTML, which are designed to produce a reasonable result even in the presence of severe markup errors. XML's policy in this area has been criticized as a violation of Postel's law (""Be conservative in what you send; be liberal in what you accept"").",The definition of an XML document excludes texts that contain violations of well-formedness rules; they are simply not XML. An XML processor that encounters such a violation is required to report such errors and to cease normal processing.,"[' What excludes texts that contain violations of well-formedness rules?', ' What is an XML processor required to report if it encounters such a violation?']","['The definition of an XML document', 'such errors and to cease normal processing']"
3368,xml,Schemas and validation,"In addition to being well-formed, an XML document may be valid. This means that it contains a reference to a Document Type Definition (DTD), and that its elements and attributes are declared in that DTD and follow the grammatical rules for them that the DTD specifies.
","In addition to being well-formed, an XML document may be valid. This means that it contains a reference to a Document Type Definition (DTD), and that its elements and attributes are declared in that DTD and follow the grammatical rules for them that the DTD specifies.","[' What does DTD stand for?', ' In addition to being well-formed, an XML document may be what?']","['Document Type Definition', 'valid']"
3369,xml,Schemas and validation,"XML processors are classified as validating or non-validating depending on whether or not they check XML documents for validity. A processor that discovers a validity error must be able to report it, but may continue normal processing.
","XML processors are classified as validating or non-validating depending on whether or not they check XML documents for validity. A processor that discovers a validity error must be able to report it, but may continue normal processing.","[' XML processors are classified as validating or what?', ' A processor that discovers a validity error must be able to report it, but may continue normal processing?']","['non-validating', 'XML processors']"
3370,xml,Schemas and validation,"A DTD is an example of a schema or grammar. Since the initial publication of XML 1.0, there has been substantial work in the area of schema languages for XML. Such schema languages typically constrain the set of elements that may be used in a document, which attributes may be applied to them, the order in which they may appear, and the allowable parent/child relationships.
","A DTD is an example of a schema or grammar. Since the initial publication of XML 1.0, there has been substantial work in the area of schema languages for XML.","[' What is an example of a schema or grammar?', ' Since the initial publication of XML 1.0, what has been substantial work in the area of?']","['A DTD', 'schema languages']"
3371,xml,Related specifications,"A cluster of specifications closely related to XML have been developed, starting soon after the initial publication of XML 1.0. It is frequently the case that the term ""XML"" is used to refer to XML together with one or more of these other technologies that have come to be seen as part of the XML core.
","A cluster of specifications closely related to XML have been developed, starting soon after the initial publication of XML 1.0. It is frequently the case that the term ""XML"" is used to refer to XML together with one or more of these other technologies that have come to be seen as part of the XML core.","[' What term is often used to refer to XML together with other technologies that have come to be seen?', ' What is a cluster of specifications closely related to?', ' What other technologies have come to be seen as part of the XML core?']","['XML', 'XML', 'one or more']"
3372,xml,Programming interfaces,"The design goals of XML include, ""It shall be easy to write programs which process XML documents."" Despite this, the XML specification contains almost no information about how programmers might go about doing such processing. The XML Infoset specification provides a vocabulary to refer to the constructs within an XML document, but does not provide any guidance on how to access this information. A variety of APIs for accessing XML have been developed and used, and some have been standardized.
","The design goals of XML include, ""It shall be easy to write programs which process XML documents."" Despite this, the XML specification contains almost no information about how programmers might go about doing such processing.","[' What are the design goals of XML?', ' What does the specification contain almost no information about?']","['""It shall be easy to write programs which process XML documents.""', 'how programmers might go about doing such processing']"
3373,xml,Programming interfaces,"Stream-oriented facilities require less memory and, for certain tasks based on a linear traversal of an XML document, are faster and simpler than other alternatives. Tree-traversal and data-binding APIs typically require the use of much more memory, but are often found more convenient for use by programmers; some include declarative retrieval of document components via the use of XPath expressions.
","Stream-oriented facilities require less memory and, for certain tasks based on a linear traversal of an XML document, are faster and simpler than other alternatives. Tree-traversal and data-binding APIs typically require the use of much more memory, but are often found more convenient for use by programmers; some include declarative retrieval of document components via the use of XPath expressions.","[' What do stream-oriented facilities require less of?', ' What are faster and simpler than other alternatives?', ' Tree-traversal and data-binding APIs typically require what?', ' XPath expressions can be used to retrieve document components via what?']","['memory', 'Stream-oriented facilities require less memory and, for certain tasks based on a linear traversal of an XML document', 'the use of much more memory', 'declarative']"
3374,xml,Programming interfaces,"XSLT is designed for declarative description of XML document transformations, and has been widely implemented both in server-side packages and Web browsers. XQuery overlaps XSLT in its functionality, but is designed more for searching of large XML databases.
","XSLT is designed for declarative description of XML document transformations, and has been widely implemented both in server-side packages and Web browsers. XQuery overlaps XSLT in its functionality, but is designed more for searching of large XML databases.","[' What is XSLT designed for?', ' What has been widely implemented both in server-side packages and Web browsers?', ' XQuery overlaps what?']","['declarative description of XML document transformations', 'XSLT', 'XSLT']"
3375,xml,History,"The versatility of SGML for dynamic information display was understood by early digital media publishers in the late 1980s prior to the rise of the Internet. By the mid-1990s some practitioners of SGML had gained experience with the then-new World Wide Web, and believed that SGML offered solutions to some of the problems the Web was likely to face as it grew. Dan Connolly added SGML to the list of W3C's activities when he joined the staff in 1995; work began in mid-1996 when Sun Microsystems engineer Jon Bosak developed a charter and recruited collaborators. Bosak was well connected in the small community of people who had experience both in SGML and the Web.","The versatility of SGML for dynamic information display was understood by early digital media publishers in the late 1980s prior to the rise of the Internet. By the mid-1990s some practitioners of SGML had gained experience with the then-new World Wide Web, and believed that SGML offered solutions to some of the problems the Web was likely to face as it grew.","[' When did early digital media publishers understand the versatility of SGML for dynamic information display?', ' When did some practitioners gain experience with the then-new World Wide Web?', ' What did SGML offer to the Web?', ' What was the Web likely to face in the future?']","['late 1980s', 'mid-1990s', 'solutions to some of the problems', 'problems']"
3376,xml,History,"XML was compiled by a working group of eleven members, supported by a (roughly) 150-member Interest Group. Technical debate took place on the Interest Group mailing list and issues were resolved by consensus or, when that failed, majority vote of the Working Group. A record of design decisions and their rationales was compiled by Michael Sperberg-McQueen on December 4, 1997. James Clark served as Technical Lead of the Working Group, notably contributing the empty-element <empty /> syntax and the name ""XML"". Other names that had been put forward for consideration included ""MAGMA"" (Minimal Architecture for Generalized Markup Applications), ""SLIM"" (Structured Language for Internet Markup) and ""MGML"" (Minimal Generalized Markup Language). The co-editors of the specification were originally Tim Bray and Michael Sperberg-McQueen. Halfway through the project Bray accepted a consulting engagement with Netscape, provoking vociferous protests from Microsoft. Bray was temporarily asked to resign the editorship. This led to intense dispute in the Working Group, eventually solved by the appointment of Microsoft's Jean Paoli as a third co-editor.
","XML was compiled by a working group of eleven members, supported by a (roughly) 150-member Interest Group. Technical debate took place on the Interest Group mailing list and issues were resolved by consensus or, when that failed, majority vote of the Working Group.","[' How many members made up the XML working group?', ' What group supported the xml compiled by the Working Group?', ' Where did technical debate take place?']","['eleven', 'Interest Group', 'on the Interest Group mailing list']"
3377,xml,History,"The XML Working Group never met face-to-face; the design was accomplished using a combination of email and weekly teleconferences. The major design decisions were reached in a short burst of intense work between August and November 1996, when the first Working Draft of an XML specification was published. Further design work continued through 1997, and XML 1.0 became a W3C Recommendation on February 10, 1998.
","The XML Working Group never met face-to-face; the design was accomplished using a combination of email and weekly teleconferences. The major design decisions were reached in a short burst of intense work between August and November 1996, when the first Working Draft of an XML specification was published.","[' What did the XML Working Group never meet face-to-face?', ' What was accomplished using a combination of email and weekly teleconferences?', ' Between what months were the major design decisions reached in a short burst of intense work?']","['design was accomplished using a combination of email and weekly teleconferences', 'the design', 'August and November 1996']"
3378,xml,Criticism,"Mapping the basic tree model of XML to type systems of programming languages or databases can be difficult, especially when XML is used for exchanging highly structured data between applications, which was not its primary design goal. However, XML data binding systems allow applications to access XML data directly from objects representing a data structure of the data in the programming language used, which ensures type safety, rather than using the DOM or SAX to retrieve data from a direct representation of the XML itself. This is accomplished by automatically creating a mapping between elements of the XML schema XSD of the document and members of a class to be represented in memory.
","Mapping the basic tree model of XML to type systems of programming languages or databases can be difficult, especially when XML is used for exchanging highly structured data between applications, which was not its primary design goal. However, XML data binding systems allow applications to access XML data directly from objects representing a data structure of the data in the programming language used, which ensures type safety, rather than using the DOM or SAX to retrieve data from a direct representation of the XML itself.","[' What can be difficult when XML is used for exchanging highly structured data between applications?', ' What allows applications to access xML data directly from a database?', ' What allows applications to access XML data directly from objects representing a data structure of the data in the programming language used?', ' What ensures type safety?']","['Mapping the basic tree model of XML to type systems of programming languages or databases', 'XML data binding systems', 'XML data binding systems', 'XML data binding systems']"
3379,xml,Criticism,"JSON, YAML, and S-Expressions are frequently proposed as simpler alternatives (see Comparison of data serialization formats) that focus on representing highly structured data rather than documents, which may contain both highly structured and relatively unstructured content. However, W3C standardized XML schema specifications offer a broader range of structured XSD data types compared to simpler serialization formats and offer modularity and reuse through XML namespaces.
","JSON, YAML, and S-Expressions are frequently proposed as simpler alternatives (see Comparison of data serialization formats) that focus on representing highly structured data rather than documents, which may contain both highly structured and relatively unstructured content. However, W3C standardized XML schema specifications offer a broader range of structured XSD data types compared to simpler serialization formats and offer modularity and reuse through XML namespaces.","[' JSON, YAML, and S-Expressions are often proposed as what?', ' What do W3C standardized XML schema specifications offer?', ' What type of data types do schema specifications offer compared to simpler serialization formats?', ' What kind of modularity and reuse are schema specifications able to offer?']","['simpler alternatives', 'a broader range of structured XSD data types', 'structured XSD', 'XML namespaces']"
3380,edit distance,Summary,"In computational linguistics and computer science, edit distance is a way of quantifying how dissimilar two strings (e.g., words) are to one another by counting the minimum number of operations required to transform one string into the other. Edit distances find applications in natural language processing, where automatic spelling correction can determine candidate corrections for a misspelled word by selecting words from a dictionary that have a low distance to the word in question. In bioinformatics, it can be used to quantify the similarity of DNA sequences, which can be viewed as strings of the letters A, C, G and T.
","In computational linguistics and computer science, edit distance is a way of quantifying how dissimilar two strings (e.g., words) are to one another by counting the minimum number of operations required to transform one string into the other. Edit distances find applications in natural language processing, where automatic spelling correction can determine candidate corrections for a misspelled word by selecting words from a dictionary that have a low distance to the word in question.","[' Edit distance is a way of quantifying how dissimilar two strings are to one another?', ' Edit distances find applications in what kind of processing?', ' How can automatic spelling correction determine candidate corrections for a misspelled word?']","['edit distance', 'natural language processing', 'by selecting words from a dictionary that have a low distance to the word in question']"
3381,edit distance,Summary,"Different definitions of an edit distance use different sets of string operations. Levenshtein distance operations are the removal, insertion, or substitution of a character in the string. Being the most common metric, the term Levenshtein distance is often used interchangeably with edit distance.","Different definitions of an edit distance use different sets of string operations. Levenshtein distance operations are the removal, insertion, or substitution of a character in the string.","[' What do different definitions of edit distance use different sets of?', ' What are the removal, insertion, or substitution of a character in a string?']","['string operations', 'Levenshtein distance operations']"
3382,edit distance,Types of edit distance,"Different types of edit distance allow different sets of string operations. For instance:
",Different types of edit distance allow different sets of string operations. For instance:,[' What types of edit distance allow different sets of string operations?'],['Different types']
3383,edit distance,Types of edit distance,"Some edit distances are defined as a parameterizable metric calculated with a specific set of allowed edit operations, and each operation is assigned a cost (possibly infinite). This is further generalized by DNA sequence alignment algorithms such as the Smith–Waterman algorithm, which make an operation's cost depend on where it is applied.
","Some edit distances are defined as a parameterizable metric calculated with a specific set of allowed edit operations, and each operation is assigned a cost (possibly infinite). This is further generalized by DNA sequence alignment algorithms such as the Smith–Waterman algorithm, which make an operation's cost depend on where it is applied.","[' What is a parameterizable metric calculated with a specific set of allowed edit operations?', ' How is each operation assigned a cost?', "" What algorithm makes an operation's cost depend on where it is located?"", "" What makes an operation's cost depend on where it is applied?""]","['edit distances', 'possibly infinite', 'Smith–Waterman algorithm', 'DNA sequence alignment algorithms such as the Smith–Waterman algorithm']"
3384,edit distance,Formal definition and properties,"Given two strings a and b on an alphabet Σ (e.g. the set of ASCII characters, the set of bytes [0..255], etc.), the edit distance d(a, b) is the minimum-weight series of edit operations that transforms a into b. One of the simplest sets of edit operations is that defined by Levenshtein in 1966:","Given two strings a and b on an alphabet Σ (e.g. the set of ASCII characters, the set of bytes [0..255], etc.","[' What are the strings a and b on an alphabet <unk>?', ' The set of ASCII characters, the set of bytes [0..255], etc.)?']","['the set of ASCII characters', 'Given two strings a and b on an alphabet Σ']"
3385,edit distance,Formal definition and properties,"In Levenshtein's original definition, each of these operations has unit cost (except that substitution of a character by itself has zero cost), so the Levenshtein distance is equal to the minimum number of operations required to transform a to b. A more general definition associates non-negative weight functions wins(x), wdel(x) and wsub(x, y) with the operations.","In Levenshtein's original definition, each of these operations has unit cost (except that substitution of a character by itself has zero cost), so the Levenshtein distance is equal to the minimum number of operations required to transform a to b. A more general definition associates non-negative weight functions wins(x), wdel(x) and wsub(x, y) with the operations.","[' What is the Levenshtein distance equal to the minimum number of operations required to transform a to b?', ' What non-negative weight functions are associated with wins(x), wdel(x)?', ' What non-negative weight functions are associated with wins(x), wdel(x) and wsub(x, y)?']","['unit cost', 'wsub(x,\xa0y)', 'operations']"
3386,edit distance,Formal definition and properties,"Additional primitive operations have been suggested. Damerau–Levenshtein distance counts as a single edit a common mistake: transposition of two adjacent characters, formally characterized by an operation that changes uxyv into uyxv.
For the task of correcting OCR output, merge and split operations have been used which replace a single character into a pair of them or vice versa.","Additional primitive operations have been suggested. Damerau–Levenshtein distance counts as a single edit a common mistake: transposition of two adjacent characters, formally characterized by an operation that changes uxyv into uyxv.","[' What has been suggested for primitive operations?', ' What counts as a single edit a common mistake?']","['Damerau–Levenshtein distance', 'Damerau–Levenshtein distance']"
3387,edit distance,Formal definition and properties,"Other variants of edit distance are obtained by restricting the set of operations. Longest common subsequence (LCS) distance is edit distance with insertion and deletion as the only two edit operations, both at unit cost.: 37  Similarly, by only allowing substitutions (again at unit cost), Hamming distance is obtained; this must be restricted to equal-length strings.Jaro–Winkler distance can be obtained from an edit distance where only transpositions are allowed.
","Other variants of edit distance are obtained by restricting the set of operations. Longest common subsequence (LCS) distance is edit distance with insertion and deletion as the only two edit operations, both at unit cost.","[' How are other variants of edit distance obtained?', ' What is the longest common subsequence (LCS) distance?', ' How many edit operations are there?']","['by restricting the set of operations', 'edit distance', 'two']"
3388,edit distance,Applications,"Edit distance finds applications in computational biology and natural language processing, e.g. the correction of spelling mistakes or OCR errors, and approximate string matching, where the objective is to find matches for short strings in many longer texts, in situations where a small number of differences is to be expected.
","Edit distance finds applications in computational biology and natural language processing, e.g. the correction of spelling mistakes or OCR errors, and approximate string matching, where the objective is to find matches for short strings in many longer texts, in situations where a small number of differences is to be expected.","[' Edit distance finds applications in computational biology and natural language processing, e.g. the correction of spelling mistakes or OCR errors, and approximate string matching where the objective is to find matches for short strings in many longer texts?']",['Edit distance']
3389,edit distance,Language edit distance,"A generalization of the edit distance between strings is the language edit distance between a string and a language, usually a formal language. Instead of considering the edit distance between one string and another, the language edit distance is the minimum edit distance that can be attained between a fixed string and any string taken from a set of strings. More formally, for any language L and string x over an alphabet Σ, the language edit distance d(L, x) is given by



d
(
L
,
x
)
=

min

y
∈
L


d
(
x
,
y
)


{\displaystyle d(L,x)=\min _{y\in L}d(x,y)}
, where 



d
(
x
,
y
)


{\displaystyle d(x,y)}
 is the string edit distance. When the language L is context free, there is a cubic time dynamic programming algorithm proposed by Aho and Peterson in 1972 which computes the language edit distance. For less expressive families of grammars, such as the regular grammars, faster algorithms exist for computing the edit distance.","A generalization of the edit distance between strings is the language edit distance between a string and a language, usually a formal language. Instead of considering the edit distance between one string and another, the language edit distance is the minimum edit distance that can be attained between a fixed string and any string taken from a set of strings.","[' What is a generalization of the edit distance between strings?', ' What is the minimum edit distance that can be attained between a fixed or fixed language?', ' What is the minimum edit distance between a fixed string and any string taken from a set of strings?']","['the language edit distance between a string and a language', 'language edit distance', 'language edit distance']"
3390,synchronization,Summary,"Synchronization is the coordination of events to operate a system in unison. For example, the conductor of an orchestra keeps the orchestra synchronized or in time. Systems that operate with all parts in synchrony are said to be synchronous or in sync—and those that are not are asynchronous.
","Synchronization is the coordination of events to operate a system in unison. For example, the conductor of an orchestra keeps the orchestra synchronized or in time.","[' Synchronization is the coordination of events to operate a system in what?', ' What is the conductor of an orchestra keeping the orchestra synchronized or in time?']","['unison', 'Synchronization']"
3391,synchronization,Navigation and railways,"Time-keeping and synchronization of clocks is a critical problem in long-distance ocean navigation.  Before radio navigation and satellite-based navigation, navigators required accurate time in conjunction with astronomical observations to determine how far east or west their vessel traveled. The invention of an accurate marine chronometer revolutionized marine navigation. By the end of the 19th century, important ports provided  time signals in the form of a signal gun, flag, or dropping time ball so that mariners could check and correct their chronometers for error.
","Time-keeping and synchronization of clocks is a critical problem in long-distance ocean navigation. Before radio navigation and satellite-based navigation, navigators required accurate time in conjunction with astronomical observations to determine how far east or west their vessel traveled.","[' What is a critical problem in long-distance ocean navigation?', ' Before radio navigation and satellite-based navigation, what did navigators require to determine how far east or west their vessel traveled?']","['Time-keeping and synchronization of clocks', 'accurate time in conjunction with astronomical observations']"
3392,synchronization,Navigation and railways,"Synchronization was important in the operation of 19th-century railways, these being the first major means of transport fast enough for differences in local mean time between nearby towns to be noticeable. Each line handled the problem by synchronizing all its stations to headquarters as a standard railway time. In some territories, companies shared a single railroad track and needed to avoid collisions. The need for strict timekeeping led the companies to settle on one standard, and civil authorities eventually abandoned local mean time in favor of railway time.
","Synchronization was important in the operation of 19th-century railways, these being the first major means of transport fast enough for differences in local mean time between nearby towns to be noticeable. Each line handled the problem by synchronizing all its stations to headquarters as a standard railway time.","[' What was important in the operation of 19th century railways?', ' What was the first major means of transport fast enough for differences in local mean time between nearby towns to be noticeable?', ' Each line handled the problem by synchronizing all its stations to headquarters as a standard railway time?']","['Synchronization', 'Synchronization', 'Synchronization']"
3393,synchronization,Communication,"In electrical engineering terms, for digital logic and data transfer, a synchronous circuit requires a clock signal.  A clock signal simply signals the start or end of some time period, often measured in microseconds or nanoseconds, that has an arbitrary relationship to any other system of measurement of the passage of minutes, hours, and days.
","In electrical engineering terms, for digital logic and data transfer, a synchronous circuit requires a clock signal. A clock signal simply signals the start or end of some time period, often measured in microseconds or nanoseconds, that has an arbitrary relationship to any other system of measurement of the passage of minutes, hours, and days.","[' In electrical engineering terms, a synchronous circuit requires what?', ' A clock signal signals the start or end of some time period?', ' What is a clock signal usually measured in?', ' What is the relationship to any other system of measurement of the passage of minutes, hours and days?']","['a clock signal', 'arbitrary relationship to any other system of measurement of the passage of minutes, hours, and days', 'microseconds or nanoseconds', 'arbitrary']"
3394,synchronization,Communication,"In a different sense, electronic systems are sometimes synchronized to make events at points far apart appear simultaneous or near-simultaneous from a certain perspective. Timekeeping technologies such as the GPS satellites and Network Time Protocol (NTP) provide real-time access to a close approximation to the UTC timescale and are used for many terrestrial synchronization applications of this kind.
","In a different sense, electronic systems are sometimes synchronized to make events at points far apart appear simultaneous or near-simultaneous from a certain perspective. Timekeeping technologies such as the GPS satellites and Network Time Protocol (NTP) provide real-time access to a close approximation to the UTC timescale and are used for many terrestrial synchronization applications of this kind.","[' What are electronic systems sometimes synchronized to make events at points far apart appear simultaneous or near-simultaneous from a certain perspective?', ' GPS satellites and what other timekeeping technology provide real-time access to a close approximation to the UTC timescale?', ' What is a close approximation to the UTC timescale and are used for many terrestrial synchronization applications?']","['In a different sense, electronic systems are sometimes synchronized to make events at points far apart appear simultaneous or near-simultaneous from a certain perspective. Timekeeping technologies', 'Network Time Protocol', 'Timekeeping technologies']"
3395,synchronization,Dynamical systems,"Synchronization of multiple interacting dynamical systems can occur when the systems are autonomous oscillators. Poincaré phase oscillators are model systems that can interact and partially synchronize within random or regular networks. In the case of global synchronization of phase oscillators, an abrupt transition from unsynchronized to full synchronization takes place when the coupling strength exceeds a critical threshold. This is known as the Kuramoto model phase transition. Synchronization is an emergent property that occurs in a broad range of dynamical systems, including neural signaling, the beating of the heart and the synchronization of fire-fly light waves.
",Synchronization of multiple interacting dynamical systems can occur when the systems are autonomous oscillators. Poincaré phase oscillators are model systems that can interact and partially synchronize within random or regular networks.,"[' When can synchronization of multiple interacting dynamical systems occur?', ' What are model systems that can interact and partially synchronize within random or regular networks?']","['when the systems are autonomous oscillators', 'Poincaré phase oscillators']"
3396,synchronization,Human movement,"Synchronization of movement is defined as similar movements between two or more people who are temporally aligned. This is different from mimicry, which occurs after a short delay. Line dance and military step are examples.
","Synchronization of movement is defined as similar movements between two or more people who are temporally aligned. This is different from mimicry, which occurs after a short delay.","[' Synchronization of movement is defined as similar movements between two or more people who are temporally aligned.', ' What occurs after a short delay?']","['Synchronization of movement is defined as similar movements between two or more people who are temporally aligned.', 'mimicry']"
3397,synchronization,Human movement,"Muscular bonding is the idea that moving in time evokes particular emotions. This sparked some of the first research into movement synchronization and its effects on human emotion. In groups, synchronization of movement has been shown to increase conformity, cooperation and trust.",Muscular bonding is the idea that moving in time evokes particular emotions. This sparked some of the first research into movement synchronization and its effects on human emotion.,"[' What is the idea that moving in time evokes particular emotions?', ' What sparked some of the first research into movement synchronization and its effects on human emotion?']","['Muscular bonding', 'Muscular bonding']"
3398,synchronization,Human movement,"In dyads, groups of two people, synchronization has been demonstrated to increase affiliation, self-esteem, compassion and altruistic behaviour and increase rapport. During arguments, synchrony between the arguing pair has been noted to decrease, however it is not clear whether this is due to the change in emotion or other factors. There is evidence to show that movement synchronization requires other people to cause its beneficial effects, as the effect on affiliation does not occur when one of the dyad is synchronizing their movements to something outside the dyad. This is known as interpersonal synchrony.
","In dyads, groups of two people, synchronization has been demonstrated to increase affiliation, self-esteem, compassion and altruistic behaviour and increase rapport. During arguments, synchrony between the arguing pair has been noted to decrease, however it is not clear whether this is due to the change in emotion or other factors.","[' What has been shown to increase affiliation, self-esteem, compassion, altruistic behaviour and increase rapport in dyads?', ' During arguments, synchrony between the arguing pair has been noted to decrease what?']","['synchronization', 'change in emotion']"
3399,synchronization,Human movement,"There has been dispute regarding the true effect of synchrony in these studies. Research in this area detailing the positive effects of synchrony, have attributed this to synchrony alone; however, many of the experiments incorporate a shared intention to achieve synchrony. Indeed, the Reinforcement of Cooperation Model suggests that perception of synchrony leads to reinforcement that cooperation is occurring, which leads to the pro-social effects of synchrony. More research is required to separate the effect of intentionality from the beneficial effect of synchrony.","There has been dispute regarding the true effect of synchrony in these studies. Research in this area detailing the positive effects of synchrony, have attributed this to synchrony alone; however, many of the experiments incorporate a shared intention to achieve synchrony.","[' What has been disputed regarding the true effect of synchrony in these studies?', ' What have researchers in this area detailed the positive effects of?', ' Many of the experiments incorporate a shared intention to achieve what?']","['positive effects of synchrony, have attributed this to synchrony alone', 'synchrony', 'synchrony']"
3400,synchronization,Uses,"Synchronization is important in digital telephony, video and digital audio where streams of sampled data are manipulated. Synchronization of image and sound was an important technical problem in sound film. More sophisticated film, video, and audio applications use time code to synchronize audio and video. In movie and television production it is necessary to synchronize video frames from multiple cameras. In addition to enabling basic editing, synchronization can also be used for 3D reconstruction","Synchronization is important in digital telephony, video and digital audio where streams of sampled data are manipulated. Synchronization of image and sound was an important technical problem in sound film.","[' What is important in digital telephony, video and digital audio?', ' What was an important technical problem in sound film?']","['Synchronization', 'Synchronization of image and sound']"
3401,synchronization,Uses,"Arbiters are needed in digital electronic systems such as microprocessors to deal with asynchronous inputs.  There are also electronic digital circuits called synchronizers that attempt to perform arbitration in one clock cycle.  Synchronizers, unlike arbiters, are prone to failure.  (See metastability in electronics).
",Arbiters are needed in digital electronic systems such as microprocessors to deal with asynchronous inputs. There are also electronic digital circuits called synchronizers that attempt to perform arbitration in one clock cycle.,"[' What are needed in microprocessors to deal with asynchronous inputs?', ' What are electronic digital circuits called that attempt to perform arbitration in one clock cycle?']","['Arbiters', 'synchronizers']"
3402,synchronization,Uses,"Some systems may be only approximately synchronized, or plesiochronous. Some applications require that relative offsets between events be determined. For others, only the order of the event is important.","Some systems may be only approximately synchronized, or plesiochronous. Some applications require that relative offsets between events be determined.","[' Some systems may be what?', ' Some applications require that relative offsets between events be determined?']","['approximately synchronized, or plesiochronous', 'Some systems may be only approximately synchronized, or plesiochronous']"
3403,range query,Summary,"A range query is a common database operation that retrieves all records where some value is between an upper and lower boundary. For example, list all employees with 3 to 5 years' experience. Range queries are unusual because it is not generally known in advance how many entries a range query will return, or if it will return any at all. Many other queries, such as the top ten most senior employees, or the newest employee, can be done more efficiently because there is an upper bound to the number of results they will return. A query that returns exactly one result is sometimes called a singleton.
","A range query is a common database operation that retrieves all records where some value is between an upper and lower boundary. For example, list all employees with 3 to 5 years' experience.",[' What is a common database operation that retrieves all records where some value is between an upper and lower boundary?'],['A range query']
3404,feature vector,Summary,"In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon. Choosing informative, discriminating and independent features is a crucial element of effective algorithms in pattern recognition,  classification and regression. Features are usually numeric, but structural features such as strings and graphs are used in syntactic pattern recognition. The concept of ""feature"" is related to that of explanatory variable used in statistical techniques such as linear regression.
","In machine learning and pattern recognition, a feature is an individual measurable property or characteristic of a phenomenon. Choosing informative, discriminating and independent features is a crucial element of effective algorithms in pattern recognition,  classification and regression.","[' In machine learning and pattern recognition, what is a feature an individual measurable property or characteristic of a phenomenon?', ' Choosing informative, discriminating and independent features is crucial element of what?']","['feature', 'effective algorithms']"
3405,feature vector,Classification,"A numeric feature can be conveniently described by a feature vector. One way to achieve binary classification is using a linear predictor function (related to the perceptron) with a feature vector as input. The method consists of calculating the scalar product between the feature vector and a vector of weights, qualifying those observations whose result exceeds a threshold.
",A numeric feature can be conveniently described by a feature vector. One way to achieve binary classification is using a linear predictor function (related to the perceptron) with a feature vector as input.,"[' What can be described by a feature vector?', ' What is one way to achieve binary classification?']","['A numeric feature', 'using a linear predictor function']"
3406,feature vector,Extensions,"In pattern recognition and machine learning, a feature vector is an n-dimensional vector of numerical features that represent some object. Many algorithms in machine learning require a numerical representation of objects, since such representations facilitate processing and statistical analysis. When representing images, the feature values might correspond to the pixels of an image, while when representing texts the features might be the frequencies of occurrence of textual terms. Feature vectors are equivalent to the vectors of explanatory variables used in statistical procedures such as linear regression.  Feature vectors are often combined with weights using a dot product in order to construct a linear predictor function that is used to determine a score for making a prediction.
","In pattern recognition and machine learning, a feature vector is an n-dimensional vector of numerical features that represent some object. Many algorithms in machine learning require a numerical representation of objects, since such representations facilitate processing and statistical analysis.","[' What is an n-dimensional vector of numerical features that represent some object?', ' Many algorithms in machine learning require a numerical representation of objects?']","['a feature vector', 'processing and statistical analysis']"
3407,feature vector,Extensions,"The vector space associated with these vectors is often called the feature space. In order to reduce the dimensionality of the feature space, a number of dimensionality reduction techniques can be employed.
","The vector space associated with these vectors is often called the feature space. In order to reduce the dimensionality of the feature space, a number of dimensionality reduction techniques can be employed.","[' What is the vector space associated with vectors often called?', ' What can be employed to reduce the dimensionality of the feature space?']","['feature space', 'dimensionality reduction techniques']"
3408,feature vector,Extensions,"Higher-level features can be obtained from already available features and added to the feature vector; for example, for the study of diseases the feature 'Age' is useful and is defined as Age = 'Year of death' minus 'Year of birth' . This process is referred to as feature construction. Feature construction is the application of a set of constructive operators to a set of existing features resulting in construction of new features. Examples of such constructive operators include checking for the equality conditions {=, ≠}, the arithmetic operators {+,−,×, /}, the array operators {max(S), min(S), average(S)} as well as other more sophisticated operators, for example count(S,C) that counts the number of features in the feature vector S satisfying some condition C or, for example, distances to other recognition classes generalized by some accepting device. Feature construction has long been considered a powerful tool for increasing both accuracy and understanding of structure, particularly in high-dimensional problems. Applications include studies of disease and emotion recognition from speech.","Higher-level features can be obtained from already available features and added to the feature vector; for example, for the study of diseases the feature 'Age' is useful and is defined as Age = 'Year of death' minus 'Year of birth' . This process is referred to as feature construction.","[' What can be obtained from already available features and added to the feature vector?', ' For the study of diseases, what is useful?', ' What is this process referred to as?']","['Higher-level features', ""Age'"", 'feature construction']"
3409,feature vector,Selection and extraction,"The initial set of raw features can be redundant and too large to be managed. Therefore, a preliminary step in many applications of machine learning and pattern recognition consists of selecting a subset of features, or constructing a new and reduced set of features to facilitate learning, and to improve generalization and interpretability.
","The initial set of raw features can be redundant and too large to be managed. Therefore, a preliminary step in many applications of machine learning and pattern recognition consists of selecting a subset of features, or constructing a new and reduced set of features to facilitate learning, and to improve generalization and interpretability.","[' What can be too large to be managed?', ' What is a preliminary step in many applications of machine learning and pattern recognition?', ' Reduced set of features to facilitate learning, and to improve generalization and what else?']","['The initial set of raw features', 'selecting a subset of features', 'interpretability']"
3410,feature vector,Selection and extraction,"Extracting or selecting features is a combination of art and science; developing systems to do so is known as feature engineering. It requires the experimentation of multiple possibilities and the combination of automated techniques with the intuition and knowledge of the domain expert. Automating this process is feature learning, where a machine not only uses features for learning, but learns the features itself.
",Extracting or selecting features is a combination of art and science; developing systems to do so is known as feature engineering. It requires the experimentation of multiple possibilities and the combination of automated techniques with the intuition and knowledge of the domain expert.,"[' What is a combination of art and science?', ' What is developing systems to do so called?', ' The combination of automated techniques with the intuition and knowledge of the domain expert is called what?']","['Extracting or selecting features', 'feature engineering', 'feature engineering']"
3411,domain ontology,Summary,"In computer science and information science, an ontology encompasses a representation, formal naming, and definition of the categories, properties, and relations between the concepts, data, and entities that substantiate one, many, or all domains of discourse. More simply, an ontology is a way of showing the properties of a subject area and how they are related, by defining a set of concepts and categories that represent the subject.
","In computer science and information science, an ontology encompasses a representation, formal naming, and definition of the categories, properties, and relations between the concepts, data, and entities that substantiate one, many, or all domains of discourse. More simply, an ontology is a way of showing the properties of a subject area and how they are related, by defining a set of concepts and categories that represent the subject.","[' What is an ontology in computer science and information science?', ' What are the categories, properties, and relations between concepts, data, and entities that substantiate one, many, or all domains of discourse?', ' What is a way of showing the properties of a subject area and how they are related?', ' What are concepts and categories that represent the subject area?']","['a representation, formal naming, and definition of the categories, properties, and relations between the concepts, data, and entities', 'an ontology', 'an ontology', 'an ontology']"
3412,domain ontology,Summary,"Every academic discipline or field creates ontologies to limit complexity and organize data into information and knowledge. 
Each uses ontological assumptions to frame explicit theories, research and applications. New ontologies may improve problem solving within that domain. Translating research papers within every field is a problem made easier when experts from different countries maintain a controlled vocabulary of jargon between each of their languages.","Every academic discipline or field creates ontologies to limit complexity and organize data into information and knowledge. Each uses ontological assumptions to frame explicit theories, research and applications.","[' What does each academic discipline or field create to limit complexity and organize data into information and knowledge?', ' What uses ontological assumptions to frame explicit theories, research and applications?']","['ontologies', 'Every academic discipline or field creates ontologies']"
3413,domain ontology,Summary,"For instance, the definition and ontology of economics is a primary concern in Marxist economics, but also in other subfields of economics. An example of economics relying on information science occurs in cases where a simulation or model is intended to enable economic decisions, such as determining what capital assets are at risk and by how much (see risk management). 
","For instance, the definition and ontology of economics is a primary concern in Marxist economics, but also in other subfields of economics. An example of economics relying on information science occurs in cases where a simulation or model is intended to enable economic decisions, such as determining what capital assets are at risk and by how much (see risk management).","[' What is a primary concern in Marxist economics?', ' What is an example of economics relying on information science?', ' To enable economic decisions, such as determining what is at risk and by how much?']","['the definition and ontology of economics', 'in cases where a simulation or model is intended to enable economic decisions', 'a simulation or model']"
3414,domain ontology,Summary,"What ontologies in both information science and philosophy have in common is the attempt to represent entities, ideas and events, with all their interdependent properties and relations, according to a system of categories. In both fields, there is considerable work on problems of ontology engineering (e.g., Quine and Kripke in philosophy, Sowa and Guarino in computer science), and debates concerning to what extent normative ontology is possible (e.g., foundationalism and coherentism in philosophy, BFO and Cyc in artificial intelligence).
","What ontologies in both information science and philosophy have in common is the attempt to represent entities, ideas and events, with all their interdependent properties and relations, according to a system of categories. In both fields, there is considerable work on problems of ontology engineering (e.g., Quine and Kripke in philosophy, Sowa and Guarino in computer science), and debates concerning to what extent normative ontology is possible (e.g., foundationalism and coherentism in philosophy, BFO and Cyc in artificial intelligence).","[' Ontologies in information science and philosophy attempt to represent entities, ideas and events according to a system of what?', ' In both fields, there is considerable work on problems of ontology engineering.', ' What are two examples of problems of ontology engineering?', ' What is possible to what extent in philosophy?', ' BFO and Cyc are examples of what?']","['categories', 'Quine and Kripke in philosophy, Sowa and Guarino in computer science', 'Quine and Kripke in philosophy, Sowa and Guarino in computer science', 'normative ontology', 'artificial intelligence']"
3415,domain ontology,Summary,"Applied ontology is considered a spiritual successor to prior work in philosophy, however many current efforts are more concerned with establishing controlled vocabularies of narrow domains than first principles, the existence of fixed essences or whether enduring objects (e.g., perdurantism and endurantism) may be ontologically more primary than processes. Artificial intelligence has retained the most attention regarding applied ontology in subfields like natural language processing within machine translation and knowledge representation, but ontology editors are being used often in a range of fields like education without the intent to contribute to AI.","Applied ontology is considered a spiritual successor to prior work in philosophy, however many current efforts are more concerned with establishing controlled vocabularies of narrow domains than first principles, the existence of fixed essences or whether enduring objects (e.g., perdurantism and endurantism) may be ontologically more primary than processes. Artificial intelligence has retained the most attention regarding applied ontology in subfields like natural language processing within machine translation and knowledge representation, but ontology editors are being used often in a range of fields like education without the intent to contribute to AI.","[' What is considered a spiritual successor to prior work in philosophy?', ' What are many current efforts more concerned with establishing controlled vocabularies of?', ' What may be ontological more primary than processes?', ' Artificial intelligence has retained the most attention regarding applied ontology in subfields like natural language processing?', ' Ontology editors are being used often in a range of fields without the intent to contribute what?', ' How many fields are there without the intent to contribute to AI?']","['Applied ontology', 'narrow domains', 'enduring objects', 'machine translation and knowledge representation', 'AI', 'education']"
3416,domain ontology,Etymology,"The compound word ontology combines onto-, from the Greek ὄν, on (gen. ὄντος, ontos), i.e. ""being; that which is"", which is the present participle of the verb εἰμί, eimí, i.e. ""to be, I am"", and -λογία, -logia, i.e. ""logical discourse"", see classical compounds for this type of word formation.","The compound word ontology combines onto-, from the Greek ὄν, on (gen. ὄντος, ontos), i.e. ""being; that which is"", which is the present participle of the verb εἰμί, eimí, i.e.","[' The compound word ontology combines onto-, from what Greek word?', ' What is the present participle of the verb <unk>, eim<unk>?']","['ὄν', 'εἰμί']"
3417,domain ontology,History,"Ontologies arise out of the branch of philosophy known as metaphysics, which deals with questions like ""what exists?"" and ""what is the nature of reality?"". One of five traditional branches of philosophy, metaphysics is concerned with exploring existence through properties, entities and relations such as those between particulars and universals, intrinsic and extrinsic properties, or essence and existence. Metaphysics has been an ongoing topic of discussion since recorded history.
","Ontologies arise out of the branch of philosophy known as metaphysics, which deals with questions like ""what exists?"" and ""what is the nature of reality?"".","[' What branch of philosophy does ontologies come from?', ' Metaphysics deals with questions like ""what exists?""']","['metaphysics', 'what exists?"" and ""what is the nature of reality']"
3418,domain ontology,History,"Since the mid-1970s, researchers in the field of artificial intelligence (AI) have recognized that knowledge engineering is the key to building large and powerful AI systems. AI researchers argued that they could create new ontologies as computational models that enable certain kinds of automated reasoning, which was only marginally successful. In the 1980s, the AI community began to use the term ontology to refer to both a theory of a modeled world and a component of knowledge-based systems. In particular, David Powers introduced the word ontology to AI to refer to real world or robotic grounding, publishing in 1990 literature reviews emphasizing grounded ontology in association with the call for papers for a AAAI Summer Symposium Machine Learning of Natural Language and Ontology, with an expanded version published in SIGART Bulletin and included as a preface to the proceedings.  Some researchers, drawing inspiration from philosophical ontologies, viewed computational ontology as a kind of applied philosophy.","Since the mid-1970s, researchers in the field of artificial intelligence (AI) have recognized that knowledge engineering is the key to building large and powerful AI systems. AI researchers argued that they could create new ontologies as computational models that enable certain kinds of automated reasoning, which was only marginally successful.","[' Since when have researchers in the field of artificial intelligence recognized that knowledge engineering is the key to building large and powerful AI systems?', ' What did researchers argue they could create as computational models that enable certain kinds of automated reasoning?']","['mid-1970s', 'new ontologies']"
3419,domain ontology,History,"
In 1993, the widely cited web page and paper ""Toward Principles for the Design of Ontologies Used for Knowledge Sharing"" by Tom Gruber used ontology as a technical term in computer science closely related to earlier idea of semantic networks and taxonomies. Gruber introduced the term as a specification of a conceptualization: ","
In 1993, the widely cited web page and paper ""Toward Principles for the Design of Ontologies Used for Knowledge Sharing"" by Tom Gruber used ontology as a technical term in computer science closely related to earlier idea of semantic networks and taxonomies. Gruber introduced the term as a specification of a conceptualization:","[' In what year was the paper ""Toward Principles for the Design of Ontologies Used for Knowledge Sharing"" published?', ' What was the technical term for ontology in computer science closely related to?', ' What did Gruber introduce the term as a specification of?']","['1993', 'earlier idea of semantic networks and taxonomies', 'a conceptualization']"
3420,domain ontology,History,"An ontology is a description (like a formal specification of a program) of the concepts and relationships that can formally exist for an agent or a community of agents. This definition is consistent with the usage of ontology as set of concept definitions, but more general. And it is a different sense of the word than its use in philosophy.","An ontology is a description (like a formal specification of a program) of the concepts and relationships that can formally exist for an agent or a community of agents. This definition is consistent with the usage of ontology as set of concept definitions, but more general.","[' What is an ontology?', ' What is a description of concepts and relationships that can formally exist for an agent or a community of agents?']","['a description (like a formal specification of a program) of the concepts and relationships that can formally exist for an agent or a community of agents', 'An ontology']"
3421,domain ontology,History,"Ontologies are often equated with taxonomic hierarchies of classes, class definitions, and the subsumption relation, but ontologies need not be limited to these forms. Ontologies are also not limited to conservative definitions — that is, definitions in the traditional logic sense that only introduce terminology and do not add any knowledge about the world. To specify a conceptualization, one needs to state axioms that do constrain the possible interpretations for the defined terms.","Ontologies are often equated with taxonomic hierarchies of classes, class definitions, and the subsumption relation, but ontologies need not be limited to these forms. Ontologies are also not limited to conservative definitions — that is, definitions in the traditional logic sense that only introduce terminology and do not add any knowledge about the world.","[' Ontologies are often equated with taxonomic hierarchies of classes, class definitions, and the subsumption relation, but they need not be limited to what forms?', ' Ontology definitions in the traditional logic sense introduce terminology and do not add what?', ' What do they introduce that only introduce terminology and do not add any knowledge about?']","['conservative definitions', 'knowledge about the world', 'conservative definitions']"
3422,domain ontology,Components,"Contemporary ontologies share many structural similarities, regardless of the language in which they are expressed.  Most ontologies describe individuals (instances), classes (concepts), attributes and relations.  In this section each of these components is discussed in turn.
","Contemporary ontologies share many structural similarities, regardless of the language in which they are expressed. Most ontologies describe individuals (instances), classes (concepts), attributes and relations.","[' Contemporary ontologies share many structural similarities regardless of what language?', ' What describe individuals?']","['the language in which they are expressed', 'instances']"
3423,domain ontology,Visualization,"A survey of ontology visualization methods is presented by Katifori et al. An updated survey of ontology visualization methods and tools was published by Dudás et al. The most established ontology visualization methods, namely indented tree and graph visualization are evaluated by Fu et al. A visual language for ontologies represented in OWL is specified by the Visual Notation for OWL Ontologies (VOWL).",A survey of ontology visualization methods is presented by Katifori et al. An updated survey of ontology visualization methods and tools was published by Dudás et al.,"[' Who presented a survey of ontology visualization methods and tools?', ' Who published an updated survey on ontological visualization tools and methods?']","['Katifori et al', 'Dudás et al']"
3424,domain ontology,Engineering,"Ontology engineering (also called ontology building)  is a set of tasks related to the development of ontologies for a particular domain. It is a subfield of knowledge engineering that studies the ontology development process, the ontology life cycle, the methods and methodologies for building ontologies, and the tools and languages that support them.","Ontology engineering (also called ontology building)  is a set of tasks related to the development of ontologies for a particular domain. It is a subfield of knowledge engineering that studies the ontology development process, the ontology life cycle, the methods and methodologies for building ontologies, and the tools and languages that support them.","[' What is another name for ontology engineering?', ' What is a set of tasks related to the development of ontologies for a particular domain called?', ' What are methods and methodologies for building?', ' What are tools and languages that support ontologies?']","['ontology building', 'Ontology engineering', 'ontologies', 'methods and methodologies']"
3425,domain ontology,Engineering,"Ontology engineering aims to make explicit the knowledge contained in software applications, and organizational procedures for a particular domain. Ontology engineering offers a direction for overcoming semantic obstacles, such as those related to the definitions of business terms and software classes. Known challenges with ontology engineering include:
","Ontology engineering aims to make explicit the knowledge contained in software applications, and organizational procedures for a particular domain. Ontology engineering offers a direction for overcoming semantic obstacles, such as those related to the definitions of business terms and software classes.","[' Ontology engineering aims to make explicit the knowledge contained in what?', ' What offers a direction for overcoming semantic obstacles?']","['software applications', 'Ontology engineering']"
3426,domain ontology,Languages,"An ontology language is a formal language used to encode an ontology. There are a number of such languages for ontologies, both proprietary and standards-based:
","An ontology language is a formal language used to encode an ontology. There are a number of such languages for ontologies, both proprietary and standards-based:","[' What is a formal language used to encode an ontology?', ' What are there a number of for ontologies?']","['ontology language', 'ontology language']"
3427,knowledge discovery,Summary,"Knowledge extraction is the creation of knowledge from structured (relational databases, XML) and unstructured (text, documents, images) sources. The resulting knowledge needs to be in a machine-readable and machine-interpretable format and must represent knowledge in a manner that facilitates inferencing. Although it is methodically similar to information extraction (NLP) and ETL (data warehouse), the main criterion is that the extraction result goes beyond the creation of structured information or the transformation into a relational schema. It requires either the reuse of existing formal knowledge (reusing identifiers or ontologies) or the generation of a schema based on the source data.
","Knowledge extraction is the creation of knowledge from structured (relational databases, XML) and unstructured (text, documents, images) sources. The resulting knowledge needs to be in a machine-readable and machine-interpretable format and must represent knowledge in a manner that facilitates inferencing.","[' What is the term for the creation of knowledge from structured sources and unstructured sources?', ' What are examples of structured sources of knowledge?', ' In what format is knowledge extracted?']","['Knowledge extraction', 'relational databases, XML', 'machine-readable and machine-interpretable']"
3428,knowledge discovery,Summary,"The RDB2RDF W3C group  is currently standardizing a language for extraction of resource description frameworks (RDF) from relational databases. Another popular example for knowledge extraction is the transformation of Wikipedia into structured data and also the mapping to existing knowledge (see DBpedia and Freebase).
",The RDB2RDF W3C group  is currently standardizing a language for extraction of resource description frameworks (RDF) from relational databases. Another popular example for knowledge extraction is the transformation of Wikipedia into structured data and also the mapping to existing knowledge (see DBpedia and Freebase).,"[' What group is standardizing a language for extraction of resource description frameworks from relational databases?', ' What is another popular example for knowledge extraction?', ' How is Wikipedia transformed into structured data?']","['RDB2RDF W3C', 'the transformation of Wikipedia into structured data', 'knowledge extraction']"
3429,knowledge discovery,Overview,"After the standardization of knowledge representation languages such as RDF and OWL, much research has been conducted in the area, especially regarding transforming relational databases into RDF, identity resolution, knowledge discovery and ontology learning. The general process uses traditional methods from information extraction and extract, transform, and load (ETL), which transform the data from the sources into structured formats.
","After the standardization of knowledge representation languages such as RDF and OWL, much research has been conducted in the area, especially regarding transforming relational databases into RDF, identity resolution, knowledge discovery and ontology learning. The general process uses traditional methods from information extraction and extract, transform, and load (ETL), which transform the data from the sources into structured formats.","[' What are two examples of knowledge representation languages?', ' What has been done to transform relational databases into RDF, identity resolution and knowledge discovery?', ' What does ETL stand for?', ' What transforms data from sources into structured formats?']","['RDF and OWL', 'research', 'information extraction and extract, transform, and load', 'extract, transform, and load (ETL),']"
3430,knowledge discovery,Extraction from natural language sources,"The largest portion of information contained in business documents (about 80%) is encoded in natural language and therefore unstructured. Because unstructured data is rather a challenge for knowledge extraction, more sophisticated methods are required, which generally tend to supply worse results compared to structured data. The potential for a massive acquisition of extracted knowledge, however, should compensate the increased complexity and decreased quality of extraction. In the following, natural language sources are understood as sources of information, where the data is given in an unstructured fashion as plain text.  If the given text is additionally embedded in a markup document (e. g. HTML document), the mentioned systems normally remove the markup elements automatically.
","The largest portion of information contained in business documents (about 80%) is encoded in natural language and therefore unstructured. Because unstructured data is rather a challenge for knowledge extraction, more sophisticated methods are required, which generally tend to supply worse results compared to structured data.","[' What percentage of information in business documents is encoded in natural language?', ' Why is unstructured data a challenge for knowledge extraction?', ' What are more sophisticated methods required for?']","['80%)', 'more sophisticated methods are required', 'knowledge extraction']"
3431,knowledge discovery,Knowledge discovery,"Knowledge discovery describes the process of automatically searching large volumes of data for patterns that can be considered knowledge about the data.  It is often described as deriving knowledge from the input data.  Knowledge discovery developed out of the data mining domain, and is closely related to it both in terms of methodology and terminology.",Knowledge discovery describes the process of automatically searching large volumes of data for patterns that can be considered knowledge about the data. It is often described as deriving knowledge from the input data.,"[' What is the process of automatically searching large volumes of data for?', ' Knowledge discovery is often described as deriving knowledge from what?']","['Knowledge discovery', 'the input data']"
3432,knowledge discovery,Knowledge discovery,"The most well-known branch of data mining is knowledge discovery, also known as knowledge discovery in databases (KDD). Just as many other forms of knowledge discovery it creates abstractions of the input data. The knowledge obtained through the process may become additional data that can be used for further usage and discovery. Often the outcomes from knowledge discovery are not actionable, actionable knowledge discovery, also known as domain driven data mining, aims to discover and deliver actionable knowledge and insights.
","The most well-known branch of data mining is knowledge discovery, also known as knowledge discovery in databases (KDD). Just as many other forms of knowledge discovery it creates abstractions of the input data.","[' What is the most well-known branch of data mining?', ' What is knowledge discovery also known as?']","['knowledge discovery', 'knowledge discovery in databases']"
3433,knowledge discovery,Knowledge discovery,"Another promising application of knowledge discovery is in the area of software modernization, weakness discovery and compliance which involves understanding existing software artifacts. This process is related to a concept of reverse engineering. Usually the knowledge obtained from existing software is presented in the form of models to which specific queries can be made when necessary. An entity relationship is a frequent format of representing knowledge obtained from existing software. Object Management Group (OMG) developed the specification Knowledge Discovery Metamodel (KDM) which defines an ontology for the software assets and their relationships for the purpose of performing knowledge discovery in existing code. Knowledge discovery from existing software systems, also known as software mining is closely related to data mining, since existing software artifacts contain enormous value for risk management and business value, key for the evaluation and evolution of software systems. Instead of mining individual data sets, software mining focuses on metadata, such as process flows (e.g. data flows, control flows, & call maps), architecture, database schemas, and business rules/terms/process.
","Another promising application of knowledge discovery is in the area of software modernization, weakness discovery and compliance which involves understanding existing software artifacts. This process is related to a concept of reverse engineering.","[' What is another promising application of knowledge discovery?', ' What is the process of understanding existing software artifacts related to?']","['software modernization', 'reverse engineering']"
3434,humanoid robot,Summary,"A humanoid robot is a robot resembling the human body in shape. The design may be for functional purposes, such as interacting with human tools and environments, for experimental purposes, such as the study of bipedal locomotion, or for other purposes. In general, humanoid robots have a torso, a head, two arms, and two legs, though some humanoid robots may replicate only part of the body, for example, from the waist up. Some humanoid robots also have heads designed to replicate human facial features such as eyes and mouths. Androids are humanoid robots built to aesthetically resemble humans.
","A humanoid robot is a robot resembling the human body in shape. The design may be for functional purposes, such as interacting with human tools and environments, for experimental purposes, such as the study of bipedal locomotion, or for other purposes.","[' A humanoid robot is a robot resembling what in shape?', ' What is the purpose of a humanoided robot?', ' A robot designed for experimental purposes is called what?']","['the human body', 'interacting with human tools and environments', 'humanoid robot']"
3435,humanoid robot,History,"The concept of a humanoid robot originated in many different cultures around the world. Some of the earliest accounts of the idea of humanoid automata date to the 4th century BCE in Greek mythologies and various religious and philosophical texts from China. Physical prototypes of humanoid automata were later created in the Middle East, Italy, Japan, and France. 
",The concept of a humanoid robot originated in many different cultures around the world. Some of the earliest accounts of the idea of humanoid automata date to the 4th century BCE in Greek mythologies and various religious and philosophical texts from China.,"[' Where did the concept of a humanoid robot originate?', ' When do some of the earliest accounts of the idea of humanoided automata date to?', ' What is the name of the ancient Greek mythologies and religious and philosophical texts?']","['many different cultures around the world', '4th century BCE', 'China']"
3436,humanoid robot,Applications,"Humanoid robots are now used as research tools in several scientific areas. Researchers study the human body structure and behavior (biomechanics) to build humanoid robots. On the other side, the attempt to simulate the human body leads to a better understanding of it. Human cognition is a field of study which is focused on how humans learn from sensory information in order to acquire perceptual and motor skills. This knowledge is used to develop computational models of human behavior, and it has been improving over time.
",Humanoid robots are now used as research tools in several scientific areas. Researchers study the human body structure and behavior (biomechanics) to build humanoid robots.,"[' What are humanoid robots used for?', ' What do researchers study to build robots?']","['research tools', 'human body structure and behavior']"
3437,humanoid robot,Applications,"It has been suggested that very advanced robotics will facilitate the enhancement of ordinary humans. See transhumanism.
",It has been suggested that very advanced robotics will facilitate the enhancement of ordinary humans. See transhumanism.,[' What technology has been suggested to facilitate the enhancement of ordinary humans?'],['robotics']
3438,humanoid robot,Sensors,"A sensor is a device that measures some attribute of the world. Being one of the three primitives of robotics (besides planning and control), sensing plays an important role in robotic paradigms.
","A sensor is a device that measures some attribute of the world. Being one of the three primitives of robotics (besides planning and control), sensing plays an important role in robotic paradigms.","[' What is a device that measures some attribute of the world?', ' What is one of the three primitives of robotics?']","['sensor', 'A sensor']"
3439,humanoid robot,Sensors,"Sensors can be classified according to the physical process with which they work or according to the type of measurement information that they give as output. In this case, the second approach was used.","Sensors can be classified according to the physical process with which they work or according to the type of measurement information that they give as output. In this case, the second approach was used.","[' Sensors can be classified according to the physical process with which they work or according to what?', ' The second approach was used in what case?']","['the type of measurement information that they give as output', 'Sensors']"
3440,humanoid robot,Actuators,"Humanoid robots are constructed in such a way that they mimic the human body. They use actuators that perform like muscles and joints, though with a different structure. The actuators of humanoid robots can be either electric, pneumatic, or hydraulic. It is ideal for these actuators to have high power, low mass, and small dimensions.","Humanoid robots are constructed in such a way that they mimic the human body. They use actuators that perform like muscles and joints, though with a different structure.","[' How are humanoid robots constructed?', ' What do robots use to perform like muscles and joints?']","['in such a way that they mimic the human body', 'actuators']"
3441,humanoid robot,Planning and control,"Planning in robots is the process of planning out motions and trajectories for the robot to carry out. Control is the actual execution of these planned motions and trajectories. In humanoid robots, the planning must carry out biped motions, meaning that robots should plan motions similar to a human. Since one of the main uses of humanoid robots is to interact with humans, it is important for the planning and control mechanisms of humanoid robots to work in a variety of terrain and environments.",Planning in robots is the process of planning out motions and trajectories for the robot to carry out. Control is the actual execution of these planned motions and trajectories.,"[' What is planning in robots?', ' What is the actual execution of planned motions and trajectories?']","['planning out motions and trajectories for the robot to carry out', 'Control']"
3442,humanoid robot,Planning and control,The question of walking biped robots stabilization on the surface is of great importance. Maintenance of the robot's gravity center over the center of bearing area for providing a stable position can be chosen as a goal of control.,The question of walking biped robots stabilization on the surface is of great importance. Maintenance of the robot's gravity center over the center of bearing area for providing a stable position can be chosen as a goal of control.,"[' What is of great importance to walking biped robots?', ' What can be chosen as a goal of control?']","['stabilization on the surface', ""Maintenance of the robot's gravity center""]"
3443,humanoid robot,Planning and control,"To maintain dynamic balance during the walk, a robot needs information about contact force and its current and desired motion. The solution to this problem relies on a major concept, the Zero Moment Point (ZMP).","To maintain dynamic balance during the walk, a robot needs information about contact force and its current and desired motion. The solution to this problem relies on a major concept, the Zero Moment Point (ZMP).","[' How does a robot maintain dynamic balance during a walk?', ' What is the ZMP?']","['information about contact force and its current and desired motion', 'Zero Moment Point']"
3444,humanoid robot,Planning and control,"Another characteristic of humanoid robots is that they move, gather information (using sensors) on the ""real world"", and interact with it. They do not stay still like factory manipulators and other robots that work in highly structured environments. To allow humanoids to move in complex environments, planning and control must focus on self-collision detection, path planning and obstacle avoidance.","Another characteristic of humanoid robots is that they move, gather information (using sensors) on the ""real world"", and interact with it. They do not stay still like factory manipulators and other robots that work in highly structured environments.","[' What do humanoid robots gather information on?', ' What do robots do not stay still like?']","['real world', 'factory manipulators']"
3445,humanoid robot,Planning and control,"Humanoid robots do not yet have some features of the human body. They include structures with variable flexibility, which provide safety (to the robot itself and to the people), and redundancy of movements, i.e. more degrees of freedom and therefore wide task availability. Although these characteristics are desirable to humanoid robots, they will bring more complexity and new problems to planning and control. The field of whole-body control deals with these issues and addresses the proper coordination of numerous degrees of freedom, e.g. to realize several control tasks simultaneously while following a given order of priority.","Humanoid robots do not yet have some features of the human body. They include structures with variable flexibility, which provide safety (to the robot itself and to the people), and redundancy of movements, i.e.","[' What are humanoid robots not yet equipped with?', ' What provide safety to the robot and to humans?']","['features of the human body', 'structures with variable flexibility']"
3446,humanoid robot,In science fiction,A common theme for the depiction of humanoid robots in science fiction pertains to how they can help humans in society or serve as threats to humanity. This theme essentially questions whether artificial intelligence is a force of good or bad for mankind. Humanoid robots that are depicted as good for society and benefit humans are Commander Data in Star Trek and C-3PO in Star Wars. Opposite portrayals where humanoid robots are shown as scary and threatening to humans are the T-800 in Terminator and Megatron in Transformers.,A common theme for the depiction of humanoid robots in science fiction pertains to how they can help humans in society or serve as threats to humanity. This theme essentially questions whether artificial intelligence is a force of good or bad for mankind.,"[' What is a common theme for the depiction of humanoid robots in science fiction?', ' How can robots help humans in society or serve as threats to humanity?']","['how they can help humans in society or serve as threats to humanity', 'how they can help humans in society or serve as threats to humanity. This theme essentially questions whether artificial intelligence is a force of good or bad for mankind']"
3447,humanoid robot,In science fiction,"Another prominent theme found in science fiction regarding humanoid robots focuses on personhood. Certain films, particularly Blade Runner and Blade Runner 2049, explore whether or not a constructed, synthetic being should be considered a person. In the films, androids called ""replicants"" are created indistinguishably from human beings, yet they are shunned and do not possess the same rights as humans. This theme incites audience sympathy while also sparking unease at the idea of humanoid robots mimicking humans too closely.","Another prominent theme found in science fiction regarding humanoid robots focuses on personhood. Certain films, particularly Blade Runner and Blade Runner 2049, explore whether or not a constructed, synthetic being should be considered a person.","[' What is another prominent theme found in science fiction regarding humanoid robots?', ' What films explore whether or not a constructed, synthetic being should be considered a person?']","['personhood', 'Blade Runner and Blade Runner 2049']"
3448,dynamic time warping,Summary,"In time series analysis, dynamic time warping (DTW) is an algorithm for measuring similarity between two temporal sequences, which may vary in speed.  For instance, similarities in walking could be detected using DTW, even if one person was walking faster than the other, or if there were accelerations and decelerations during the course of an observation. DTW has been applied to temporal sequences of video, audio, and graphics data — indeed, any data that can be turned into a linear sequence can be analyzed with DTW. A well-known application has been automatic speech recognition, to cope with different speaking speeds. Other applications include speaker recognition and online signature recognition. It can also be used in partial shape matching applications.
","In time series analysis, dynamic time warping (DTW) is an algorithm for measuring similarity between two temporal sequences, which may vary in speed. For instance, similarities in walking could be detected using DTW, even if one person was walking faster than the other, or if there were accelerations and decelerations during the course of an observation.","[' What is dynamic time warping?', ' What is DTW an algorithm for measuring?', ' How could similarities in walking be detected?', ' What are accelerations and decelerations during the course of an observation?']","['an algorithm for measuring similarity between two temporal sequences', 'similarity between two temporal sequences', 'using DTW', 'similarities']"
3449,dynamic time warping,Summary,"In general, DTW is a method that calculates an optimal match between two given sequences (e.g. time series) with certain restriction and rules:
","In general, DTW is a method that calculates an optimal match between two given sequences (e.g. time series) with certain restriction and rules:",[' What is a DTW method that calculates an optimal match between two given sequences?'],['DTW']
3450,dynamic time warping,Summary,"The sequences are ""warped"" non-linearly in the time dimension to determine a measure of their similarity independent of certain non-linear variations in the time dimension. This sequence alignment method is often used in time series classification. Although DTW measures a distance-like quantity between two given sequences, it doesn't guarantee the triangle inequality to hold.
","The sequences are ""warped"" non-linearly in the time dimension to determine a measure of their similarity independent of certain non-linear variations in the time dimension. This sequence alignment method is often used in time series classification.","[' Why are sequences ""warped"" non-linearly in the time dimension?', ' What is often used in time series classification?']","['to determine a measure of their similarity independent of certain non-linear variations in the time dimension', 'sequence alignment method']"
3451,dynamic time warping,Summary,"In addition to a similarity measure between the two sequences, a so called ""warping path"" is produced, by warping according to this path the two signals may be aligned in time. The signal with an original set of points X(original), Y(original) is transformed to X(warped), Y(warped). This finds applications in genetic sequence and audio synchronisation. In a related technique sequences of varying speed may be averaged using this technique see the average sequence section.
","In addition to a similarity measure between the two sequences, a so called ""warping path"" is produced, by warping according to this path the two signals may be aligned in time. The signal with an original set of points X(original), Y(original) is transformed to X(warped), Y(warped).","[' What is produced in addition to a similarity measure between the two sequences?', ' By warping according to this path the two signals may be aligned in time?', ' The signal with an original set of points X(original), Y(original) is transformed to what?']","['a so called ""warping path', 'warping path', 'X(warped), Y(warped).']"
3452,dynamic time warping,Implementation,"This example illustrates the implementation of the dynamic time warping algorithm when the two sequences s and t are strings of discrete symbols. For two symbols x and y, d(x, y) is a distance between the symbols, e.g. d(x, y) = 




|

x
−
y

|



{\displaystyle |x-y|}
.
","This example illustrates the implementation of the dynamic time warping algorithm when the two sequences s and t are strings of discrete symbols. For two symbols x and y, d(x, y) is a distance between the symbols, e.g.","[' The dynamic time warping algorithm is implemented when the two sequences s and t are strings of what?', ' For two symbols x and y, what is a distance between the symbols?']","['discrete symbols', 'd(x, y)']"
3453,dynamic time warping,Implementation,"We sometimes want to add a locality constraint. That is, we require that if s[i] is matched with t[j], then 




|

i
−
j

|



{\displaystyle |i-j|}
 is no larger than w, a window parameter.
","We sometimes want to add a locality constraint. That is, we require that if s[i] is matched with t[j], then 




|

i
−
j

|



{\displaystyle |i-j|}
 is no larger than w, a window parameter.","[' What do we sometimes want to add to a locality constraint?', ' If s[i] is matched with t[j], what is no larger than w?']","['if s[i] is matched with t[j], then \n\n\n\n\n|\n\ni\n−\nj\n\n|\n\n\n\n{\\displaystyle |i-j|}\n is no larger than w, a window parameter', 'i-j|}\n is no larger than w, a window parameter']"
3454,dynamic time warping,Implementation,"We can easily modify the above algorithm to add a locality constraint (differences marked).
However, the above given modification works only if 




|

n
−
m

|



{\displaystyle |n-m|}
 is no larger than w, i.e. the end point is within the window length from diagonal. In order to make the algorithm work, the window parameter w must be adapted so that 




|

n
−
m

|

≤
w


{\displaystyle |n-m|\leq w}
 (see the line marked with (*) in the code).
","We can easily modify the above algorithm to add a locality constraint (differences marked). However, the above given modification works only if 




|

n
−
m

|



{\displaystyle |n-m|}
 is no larger than w, i.e.","[' What can be easily modified to add a locality constraint?', ' What is the only condition that the above given modification works with?']","['the above algorithm', 'no larger than w']"
3455,dynamic time warping,Warping properties,"The DTW algorithm produces a discrete matching between existing elements of one series to another. In other words, it does not allow time-scaling of segments within the sequence. Other methods allow continuous warping. For example, Correlation Optimized Warping (COW) divides the sequence into uniform segments that are scaled in time using linear interpolation, to produce the best matching warping. The segment scaling causes potential creation of new elements, by time-scaling segments either down or up, and thus produces a more sensitive warping than DTW's discrete matching of raw elements.
","The DTW algorithm produces a discrete matching between existing elements of one series to another. In other words, it does not allow time-scaling of segments within the sequence.","[' What algorithm produces a discrete matching between existing elements of one series to another?', ' What does the DTW algorithm not allow?']","['DTW', 'time-scaling of segments within the sequence']"
3456,dynamic time warping,Complexity,"The time complexity of the DTW algorithm is 



O
(
N
M
)


{\displaystyle O(NM)}
, where 



N


{\displaystyle N}
 and 



M


{\displaystyle M}
 are the lengths of the two input sequences. The 50 years old quadratic time bound was recently broken, an algorithm due to Gold and Sharir enables computing DTW in 



O
(


N

2




/

log
⁡
log
⁡
(
N
)
)


{\displaystyle O({N^{2}}/\log \log(N))}
 time and space for two input sequences of length 



N


{\displaystyle N}
. 
This algorithm can also be adapted to sequences of different lengths. Despite this improvement, it was shown that a strongly subquadratic running time of the form 



O
(

N

2
−
ϵ


)


{\displaystyle O(N^{2-\epsilon })}
 for some 



ϵ
>
0


{\displaystyle \epsilon >0}
 cannot exist unless the Strong exponential time hypothesis fails.","The time complexity of the DTW algorithm is 



O
(
N
M
)


{\displaystyle O(NM)}
, where 



N


{\displaystyle N}
 and 



M


{\displaystyle M}
 are the lengths of the two input sequences. The 50 years old quadratic time bound was recently broken, an algorithm due to Gold and Sharir enables computing DTW in 



O
(


N

2




/

log
⁡
log
⁡
(
N
)
)


{\displaystyle O({N^{2}}/\log \log(N))}
 time and space for two input sequences of length 



N


{\displaystyle N}
.","[' What is the time complexity of the DTW algorithm?', ' What are the lengths of the two input sequences?', ' The 50 years old quadratic time bound was recently broken, an algorithm due to who?', ' Sharir enables computing DTW in O ( N 2 / log <unk> log ( N )?']","['O\n(\nN\nM\n)\n\n\n{\\displaystyle O(NM)}\n, where \n\n\n\nN\n\n\n{\\displaystyle N}\n and \n\n\n\nM\n\n\n{\\displaystyle M}\n are the lengths of the two input sequences. The 50 years old quadratic time bound was recently broken, an algorithm due to Gold and Sharir enables computing DTW in \n\n\n\nO\n(\n\n\nN\n\n2\n\n\n\n\n/\n\nlog\n\u2061\nlog\n\u2061\n(\nN\n)\n)\n\n\n{\\displaystyle O({N^{2}}/\\log \\log(N))', 'N\n\n\n{\\displaystyle N}\n and \n\n\n\nM', 'Gold and Sharir', 'O({N^{2}}/\\log \\log(N))']"
3457,dynamic time warping,Fast computation,"Fast techniques for computing DTW include PrunedDTW, SparseDTW, FastDTW, and the MultiscaleDTW.
A common task, retrieval of similar time series, can be accelerated by using lower bounds such as LB_Keogh or LB_Improved. In a survey, Wang et al. reported slightly better results with the LB_Improved lower bound than the LB_Keogh bound, and found that other techniques were inefficient.","Fast techniques for computing DTW include PrunedDTW, SparseDTW, FastDTW, and the MultiscaleDTW. A common task, retrieval of similar time series, can be accelerated by using lower bounds such as LB_Keogh or LB_Improved.","[' PrunedDTW, SparseDTW and FastDTW are examples of what?', ' What is a common task that can be accelerated by using lower bounds?', ' Lower bounds such as LB_Keogh can be used for what task?']","['Fast techniques', 'retrieval of similar time series', 'retrieval of similar time series']"
3458,dynamic time warping,Average sequence,"Averaging for dynamic time warping is the problem of finding an average sequence for a set of sequences. 
NLAAF is an exact method to average two sequences using DTW.
For more than two sequences, the problem is related to the one of the multiple alignment and requires heuristics.
DBA is currently a reference method to average a set of sequences consistently with DTW.
COMASA efficiently randomizes the search for the average sequence, using DBA as a local optimization process.
",Averaging for dynamic time warping is the problem of finding an average sequence for a set of sequences. NLAAF is an exact method to average two sequences using DTW.,"[' What is the problem of finding an average sequence for a set of sequences?', ' What is an exact method to average two sequences using DTW?']","['Averaging for dynamic time warping', 'NLAAF']"
3459,dynamic time warping,Alternative approaches,"In functional data analysis, time series are regarded as discretizations of smooth (differentiable) functions of time. By viewing the observed samples at smooth functions, one can utilize continuous mathematics for analyzing data. Smoothness and monotonicity of time warp functions may be obtained for instance by integrating a time-varying radial basis function, thus being a one-dimensional diffeomorphism. Optimal nonlinear time warping functions are computed by minimizing a measure of distance of the set of functions to their warped average. Roughness penalty terms for the warping functions may be added, e.g., by constraining the size of their curvature. The resultant warping functions are smooth, which facilitates further processing. This approach has been successfully applied to analyze patterns and variability of speech movements.","In functional data analysis, time series are regarded as discretizations of smooth (differentiable) functions of time. By viewing the observed samples at smooth functions, one can utilize continuous mathematics for analyzing data.","[' What are time series regarded as in functional data analysis?', ' What are smooth (differentiable) functions of time?']","['discretizations of smooth (differentiable) functions of time', 'time series']"
3460,dynamic time warping,Alternative approaches,"DTW and related warping methods are typically used as pre- or post-processing steps in data analyses. If the observed sequences contain both random variation in both their values, shape of observed sequences and random temporal misalignment, the warping may overfit to noise leading to biased results. A simultaneous model formulation with random variation in both values (vertical) and time-parametrization (horizontal) is an example of a nonlinear mixed-effects model. In human movement analysis, simultaneous nonlinear mixed-effects modeling has been shown to produce superior results compared to DTW.","DTW and related warping methods are typically used as pre- or post-processing steps in data analyses. If the observed sequences contain both random variation in both their values, shape of observed sequences and random temporal misalignment, the warping may overfit to noise leading to biased results.","[' DTW and related warping methods are typically used as what in data analyses?', ' If the observed sequences contain random variation in both their values, shape of observed sequence and random temporal misalignment, the warping may overfit to what?']","['pre- or post-processing steps', 'noise']"
3461,image classification,Summary,"Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to understand and automate tasks that the human visual system can do.","Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to understand and automate tasks that the human visual system can do.","[' Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from what?', ' From the perspective of engineering, what seeks to understand and automate tasks that the human visual system can do?']","['digital images or videos', 'Computer vision']"
3462,image classification,Summary,"Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions. Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that make sense to thought processes and can elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.","Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions.","[' Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images and extracting high-dimensional data from the real world?']",['numerical or symbolic information']
3463,image classification,Summary,"The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner, or medical scanning device. The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems.
","The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, multi-dimensional data from a 3D scanner, or medical scanning device.","[' What is the scientific discipline of computer vision concerned with?', ' What can image data take many forms?']","['the theory behind artificial systems that extract information from images', 'video sequences']"
3464,image classification,Definition,"Computer vision is an interdisciplinary field that deals with how computers and can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do. ""Computer vision is concerned with the automatic extraction, analysis and understanding of useful information from a single image or a sequence of images. It involves the development of a theoretical and algorithmic basis to achieve automatic visual understanding."" As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences, views from multiple cameras, or multi-dimensional data from a medical scanner. As a technological discipline, computer vision seeks to apply its theories and models for the construction of computer vision systems.
","Computer vision is an interdisciplinary field that deals with how computers and can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do.","[' Computer vision is an interdisciplinary field that deals with how computers can gain high-level understanding from what?', ' From the perspective of engineering, what does computer vision seek to automate?']","['digital images or videos', 'tasks that the human visual system can do']"
3465,image classification,History,"In the late 1960s, computer vision began at universities which were pioneering artificial intelligence. It was meant to mimic the human visual system, as a stepping stone to endowing robots with intelligent behavior. In 1966, it was believed that this could be achieved through a summer project, by attaching a camera to a computer and having it ""describe what it saw"".","In the late 1960s, computer vision began at universities which were pioneering artificial intelligence. It was meant to mimic the human visual system, as a stepping stone to endowing robots with intelligent behavior.","[' When did computer vision begin at universities?', ' What was computer vision meant to mimic?']","['late 1960s', 'the human visual system']"
3466,image classification,History,"What distinguished computer vision from the prevalent field of digital image processing at that time was a desire to extract three-dimensional structure from images with the goal of achieving full scene understanding. Studies in the 1970s formed the early foundations for many of the computer vision algorithms that exist today, including extraction of edges from images, labeling of lines, non-polyhedral and polyhedral modeling, representation of objects as interconnections of smaller structures, optical flow, and motion estimation.","What distinguished computer vision from the prevalent field of digital image processing at that time was a desire to extract three-dimensional structure from images with the goal of achieving full scene understanding. Studies in the 1970s formed the early foundations for many of the computer vision algorithms that exist today, including extraction of edges from images, labeling of lines, non-polyhedral and polyhedral modeling, representation of objects as interconnections of smaller structures, optical flow, and motion estimation.","[' What was the main difference between computer vision and digital image processing?', ' What did computer vision want to extract from images with the goal of achieving full scene understanding?', ' What are some of the computer vision algorithms that exist today?', ' How are edges extracted from images?', ' What are non-polyhedral modeling algorithms?']","['a desire to extract three-dimensional structure from images', 'three-dimensional structure', 'extraction of edges from images, labeling of lines, non-polyhedral and polyhedral modeling, representation of objects as interconnections of smaller structures, optical flow, and motion estimation', 'algorithms', 'polyhedral modeling']"
3467,image classification,History,"The next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes. Researchers also realized that many of these mathematical concepts could be treated within the same optimization framework as regularization and Markov random fields.
By the 1990s, some of the previous research topics became more active than the others. Research in projective 3-D reconstructions led to better understanding of camera calibration. With the advent of optimization methods for camera calibration, it was realized that a lot of the ideas were already explored in bundle adjustment theory from the field of photogrammetry. This led to methods for sparse 3-D reconstructions of scenes from multiple images. Progress was made on the dense stereo correspondence problem and further multi-view stereo techniques. At the same time, variations of graph cut were used to solve image segmentation. This decade also marked the first time statistical learning techniques were used in practice to recognize faces in images (see Eigenface). Toward the end of the 1990s, a significant change came about with the increased interaction between the fields of computer graphics and computer vision. This included image-based rendering, image morphing, view interpolation, panoramic image stitching and early light-field rendering.","The next decade saw studies based on more rigorous mathematical analysis and quantitative aspects of computer vision. These include the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes.","[' What did the next decade see studies based on more rigorous mathematical analysis and quantitative aspects of computer vision?', ' What is the inference of shape from various cues such as shading, texture and focus called?']","['the concept of scale-space, the inference of shape from various cues such as shading, texture and focus, and contour models known as snakes', 'snakes']"
3468,image classification,History,"Recent work has seen the resurgence of feature-based methods, used in conjunction with machine learning techniques and complex optimization frameworks. 
The advancement of Deep Learning techniques has brought further life to the field of computer vision. The accuracy of deep learning algorithms on several benchmark computer vision data sets for tasks ranging from classification, segmentation and optical flow has surpassed prior methods.","Recent work has seen the resurgence of feature-based methods, used in conjunction with machine learning techniques and complex optimization frameworks. The advancement of Deep Learning techniques has brought further life to the field of computer vision.","[' What have feature-based methods seen a resurgence of in recent work?', ' What has Deep Learning techniques brought further life to the field of computer vision?']","['machine learning techniques and complex optimization frameworks', 'The advancement']"
3469,image classification,Applications,"Applications range from tasks such as industrial machine vision systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them. The computer vision and machine vision fields have significant overlap. Computer vision covers the core technology of automated image analysis which is used in many fields. Machine vision usually refers to a process of combining automated image analysis with other methods and technologies to provide automated inspection and robot guidance in industrial applications. In many computer-vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common. Examples of applications of computer vision include systems for:
","Applications range from tasks such as industrial machine vision systems which, say, inspect bottles speeding by on a production line, to research into artificial intelligence and computers or robots that can comprehend the world around them. The computer vision and machine vision fields have significant overlap.","[' What do industrial machine vision systems inspect on a production line?', ' What do computer vision and machine vision fields overlap with?']","['bottles', 'research into artificial intelligence']"
3470,image classification,Typical tasks,"Each of the application areas described above employ a range of computer vision tasks; more or less well-defined measurement problems or processing problems, which can be solved using a variety of methods. Some examples of typical computer vision tasks are presented below.
","Each of the application areas described above employ a range of computer vision tasks; more or less well-defined measurement problems or processing problems, which can be solved using a variety of methods. Some examples of typical computer vision tasks are presented below.","[' What do each of the application areas described above employ?', ' What are some examples of typical computer vision tasks?']","['a range of computer vision tasks', 'presented below']"
3471,image classification,Typical tasks,"Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g., in the forms of decisions. Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that can interface with other thought processes and elicit appropriate action. This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry, physics, statistics, and learning theory.","Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g., in the forms of decisions. Understanding in this context means the transformation of visual images (the input of the retina) into descriptions of the world that can interface with other thought processes and elicit appropriate action.","[' What are computer vision tasks used for?', ' What is the purpose of computer vision?', ' In computer vision, what is the input of visual images?', ' What is the input of the retina?', ' What does the transformation of visual images into descriptions of the world do?']","['acquiring, processing, analyzing and understanding digital images, and extraction of high-dimensional data from the real world', 'to produce numerical or symbolic information', 'the retina', 'visual images', 'can interface with other thought processes and elicit appropriate action']"
3472,image classification,System methods,"The organization of a computer vision system is highly application-dependent. Some systems are stand-alone applications that solve a specific measurement or detection problem, while others constitute a sub-system of a larger design which, for example, also contains sub-systems for control of mechanical actuators, planning, information databases, man-machine interfaces, etc. The specific implementation of a computer vision system also depends on whether its functionality is pre-specified or if some part of it can be learned or modified during operation. Many functions are unique to the application. There are, however, typical functions that are found in many computer vision systems.
","The organization of a computer vision system is highly application-dependent. Some systems are stand-alone applications that solve a specific measurement or detection problem, while others constitute a sub-system of a larger design which, for example, also contains sub-systems for control of mechanical actuators, planning, information databases, man-machine interfaces, etc.","[' What is the organization of a computer vision system highly application-dependent?', ' Some systems are stand-alone applications that solve a specific measurement or detection problem.']","['Some systems are stand-alone applications that solve a specific measurement or detection problem', 'computer vision system']"
3473,image classification,Hardware,"There are many kinds of computer vision systems; however, all of them contain these basic elements: a power source, at least one image acquisition device (camera, ccd, etc.), a processor, and control and communication cables or some kind of wireless interconnection mechanism. In addition, a practical vision system contains software, as well as a display in order to monitor the system. Vision systems for inner spaces, as most industrial ones, contain an illumination system and may be placed in a controlled environment. Furthermore, a completed system includes many accessories such as camera supports, cables and connectors.
","There are many kinds of computer vision systems; however, all of them contain these basic elements: a power source, at least one image acquisition device (camera, ccd, etc. ), a processor, and control and communication cables or some kind of wireless interconnection mechanism.","[' How many types of computer vision systems are there?', ' What are the basic elements of a computer vision system?', ' A power source, an image acquisition device, a processor, and control and communication cables are what?']","['many', 'a power source, at least one image acquisition device (camera, ccd, etc. ), a processor, and control and communication cables or some kind of wireless interconnection mechanism', 'computer vision systems']"
3474,image classification,Hardware,"A few computer vision systems use image-acquisition hardware with active illumination or something other than visible light or both, such as structured-light 3D scanners, thermographic cameras, hyperspectral imagers, radar imaging, lidar scanners, magnetic resonance images, side-scan sonar, synthetic aperture sonar, etc. Such hardware captures ""images"" that are then processed often using the same computer vision algorithms used to process visible-light images.
","A few computer vision systems use image-acquisition hardware with active illumination or something other than visible light or both, such as structured-light 3D scanners, thermographic cameras, hyperspectral imagers, radar imaging, lidar scanners, magnetic resonance images, side-scan sonar, synthetic aperture sonar, etc. Such hardware captures ""images"" that are then processed often using the same computer vision algorithms used to process visible-light images.","[' What type of hardware does a few computer vision systems use?', ' What kind of hardware captures images that are then processed?', ' What does hardware capture?', ' What are the same computer vision algorithms used to process?']","['image-acquisition', 'image-acquisition', '""images""', 'visible-light images']"
3475,image classification,Hardware,"While traditional broadcast and consumer video systems operate at a rate of 30 frames per second, advances in digital signal processing and consumer graphics hardware has made high-speed image acquisition, processing, and display possible for real-time systems on the order of hundreds to thousands of frames per second. For applications in robotics, fast, real-time video systems are critically important and often can simplify the processing needed for certain algorithms. When combined with a high-speed projector, fast image acquisition allows 3D measurement and feature tracking to be realised.","While traditional broadcast and consumer video systems operate at a rate of 30 frames per second, advances in digital signal processing and consumer graphics hardware has made high-speed image acquisition, processing, and display possible for real-time systems on the order of hundreds to thousands of frames per second. For applications in robotics, fast, real-time video systems are critically important and often can simplify the processing needed for certain algorithms.","[' How many frames per second do traditional broadcast and consumer video systems operate?', ' Advances in digital signal processing and consumer graphics hardware have made what possible for real-time systems?', ' How many frames per second are needed for robotics applications?', ' What kind of video systems are critically important?', ' How can real-time video systems simplify processing?']","['30', 'high-speed image acquisition, processing, and display', 'hundreds to thousands', 'fast, real-time', 'certain algorithms']"
3476,homomorphic encryption,Summary,"Homomorphic encryption is a form of encryption that permits users to perform computations on its encrypted data without first decrypting it. These resulting computations are left in an encrypted form which, when decrypted, result in an identical output to that produced had the operations been performed on the unencrypted data.  Homomorphic encryption can be used for privacy-preserving outsourced storage and computation. This allows data to be encrypted and out-sourced to commercial cloud environments for processing, all while encrypted. 
","Homomorphic encryption is a form of encryption that permits users to perform computations on its encrypted data without first decrypting it. These resulting computations are left in an encrypted form which, when decrypted, result in an identical output to that produced had the operations been performed on the unencrypted data.","[' What is a form of encryption that permits users to perform computations on its encrypted data without first decrypting it?', ' What is left in an encrypted form which, when decrypted, result in an identical output to that produced had the operations been performed on the unencoded data?']","['Homomorphic encryption', 'computations']"
3477,homomorphic encryption,Summary,"For sensitive data, such as health care information, homomorphic encryption can be used to enable new services by removing privacy barriers inhibiting data sharing or increase security to existing services.  For example, predictive analytics in health care can be hard to apply via a third party service provider due to medical data privacy concerns, but if the predictive analytics service provider can operate on encrypted data instead, these privacy concerns are diminished. Moreover, even if the service provider's system is compromised, the data would remain secure. 
","For sensitive data, such as health care information, homomorphic encryption can be used to enable new services by removing privacy barriers inhibiting data sharing or increase security to existing services. For example, predictive analytics in health care can be hard to apply via a third party service provider due to medical data privacy concerns, but if the predictive analytics service provider can operate on encrypted data instead, these privacy concerns are diminished.","[' What type of encryption can be used to enable new services?', ' What can be difficult to apply via a third party service provider?', ' Why would a predictive analytics service provider apply via a third party service provider?', ' What is the reason for a medical data privacy concern?', ' If the predictive analytics provider can operate on encrypted data, what are the privacy concerns diminished?']","['homomorphic', 'predictive analytics', 'medical data privacy concerns', 'predictive analytics', 'medical data privacy concerns']"
3478,homomorphic encryption,Description,"Homomorphic encryption is a form of encryption with an additional evaluation capability for computing over encrypted data without access to the secret key. The result of such a computation remains encrypted. Homomorphic encryption can be viewed as an extension of public-key cryptography. Homomorphic refers to homomorphism in algebra: the encryption and decryption functions can be thought of as homomorphisms between plaintext and ciphertext spaces.
",Homomorphic encryption is a form of encryption with an additional evaluation capability for computing over encrypted data without access to the secret key. The result of such a computation remains encrypted.,"[' What is a form of encryption with an additional evaluation capability for computing over encrypted data without access to the secret key?', ' What remains encrypted?']","['Homomorphic encryption', 'The result of such a computation']"
3479,homomorphic encryption,Description,"Homomorphic encryption includes multiple types of encryption schemes that can perform different classes of computations over encrypted data. The computations are represented as either Boolean or arithmetic circuits.  Some common types of homomorphic encryption are partially homomorphic, somewhat homomorphic, leveled fully homomorphic, and fully homomorphic encryption: 
",Homomorphic encryption includes multiple types of encryption schemes that can perform different classes of computations over encrypted data. The computations are represented as either Boolean or arithmetic circuits.,"[' What type of encryption includes multiple types of encryption schemes that can perform different classes of computations over encrypted data?', ' The computations are represented as what?']","['Homomorphic', 'Boolean or arithmetic circuits']"
3480,homomorphic encryption,Description,"For the majority of homomorphic encryption schemes, the multiplicative depth of circuits is the main practical limitation in performing computations over encrypted data.  Homomorphic encryption schemes are inherently malleable. In terms of malleability, homomorphic encryption schemes have weaker security properties than non-homomorphic schemes.
","For the majority of homomorphic encryption schemes, the multiplicative depth of circuits is the main practical limitation in performing computations over encrypted data. Homomorphic encryption schemes are inherently malleable.","[' What is the main limitation of homomorphic encryption schemes?', ' What is inherently malleable?']","['multiplicative depth of circuits', 'Homomorphic encryption schemes']"
3481,homomorphic encryption,History,"Homomorphic encryption schemes have been developed using different approaches. Specifically, fully homomorphic encryption schemes are often grouped into generations corresponding to the underlying approach.","Homomorphic encryption schemes have been developed using different approaches. Specifically, fully homomorphic encryption schemes are often grouped into generations corresponding to the underlying approach.","[' Homomorphic encryption schemes have been developed using what approach?', ' What type of encryption schemes are often grouped into generations?']","['different approaches. Specifically, fully homomorphic encryption schemes are often grouped into generations corresponding to the underlying approach', 'fully homomorphic']"
3482,homomorphic encryption,Partially homomorphic cryptosystems,"If the RSA public key has modulus 



n


{\displaystyle n}
 and encryption exponent 



e


{\displaystyle e}
, then the encryption of a message 



m


{\displaystyle m}
 is given by 





E


(
m
)
=

m

e




mod




n


{\displaystyle {\mathcal {E}}(m)=m^{e}\;{\bmod {\;}}n}
. The homomorphic property is then
","If the RSA public key has modulus 



n


{\displaystyle n}
 and encryption exponent 



e


{\displaystyle e}
, then the encryption of a message 



m


{\displaystyle m}
 is given by 





E


(
m
)
=

m

e




mod




n


{\displaystyle {\mathcal {E}}(m)=m^{e}\;{\bmod {\;}}n}
. The homomorphic property is then","[' What is the modulus of the RSA public key?', ' What is given by E (m ) = m e mod n <unk>displaystyle?']","['n', '{\\mathcal {E}}(m)=m^{e}\\']"
3483,homomorphic encryption,Partially homomorphic cryptosystems,"In the ElGamal cryptosystem, in a cyclic group 



G


{\displaystyle G}
 of order 



q


{\displaystyle q}
 with generator 



g


{\displaystyle g}
, if the public key is 



(
G
,
q
,
g
,
h
)


{\displaystyle (G,q,g,h)}
, where 



h
=

g

x




{\displaystyle h=g^{x}}
, and 



x


{\displaystyle x}
 is the secret key, then the encryption of a message 



m


{\displaystyle m}
 is 





E


(
m
)
=
(

g

r


,
m
⋅

h

r


)


{\displaystyle {\mathcal {E}}(m)=(g^{r},m\cdot h^{r})}
, for some random 



r
∈
{
0
,
…
,
q
−
1
}


{\displaystyle r\in \{0,\ldots ,q-1\}}
. The homomorphic property is then
","In the ElGamal cryptosystem, in a cyclic group 



G


{\displaystyle G}
 of order 



q


{\displaystyle q}
 with generator 



g


{\displaystyle g}
, if the public key is 



(
G
,
q
,
g
,
h
)


{\displaystyle (G,q,g,h)}
, where 



h
=

g

x




{\displaystyle h=g^{x}}
, and 



x


{\displaystyle x}
 is the secret key, then the encryption of a message 



m


{\displaystyle m}
 is 





E


(
m
)
=
(

g

r


,
m
⋅

h

r


)


{\displaystyle {\mathcal {E}}(m)=(g^{r},m\cdot h^{r})}
, for some random 



r
∈
{
0
,
…
,
q
−
1
}


{\displaystyle r\in \{0,\ldots ,q-1\}}
. The homomorphic property is then","[' In the ElGamal cryptosystem, if the public key is (G,q,g,h)<unk> and x <unk>displaystyle x<unk> is the secret key, what is the encryption of a message?', ' What is the message m <unk>displaystyle m<unk> is E ( m) = ( g r, m and h r)?', ' The homomorphic property is then what?']","['x', '{\\mathcal {E}}(m)=(g^{r},m\\cdot h^{r', 'mathcal {E}}(m)=(g^{r},m\\cdot h^{r})']"
3484,homomorphic encryption,Partially homomorphic cryptosystems,"In the Goldwasser–Micali cryptosystem, if the public key is the modulus 



n


{\displaystyle n}
 and quadratic non-residue 



x


{\displaystyle x}
, then the encryption of a bit 



b


{\displaystyle b}
 is 





E


(
b
)
=

x

b



r

2




mod




n


{\displaystyle {\mathcal {E}}(b)=x^{b}r^{2}\;{\bmod {\;}}n}
, for some random 



r
∈
{
0
,
…
,
n
−
1
}


{\displaystyle r\in \{0,\ldots ,n-1\}}
. The homomorphic property is then
","In the Goldwasser–Micali cryptosystem, if the public key is the modulus 



n


{\displaystyle n}
 and quadratic non-residue 



x


{\displaystyle x}
, then the encryption of a bit 



b


{\displaystyle b}
 is 





E


(
b
)
=

x

b



r

2




mod




n


{\displaystyle {\mathcal {E}}(b)=x^{b}r^{2}\;{\bmod {\;}}n}
, for some random 



r
∈
{
0
,
…
,
n
−
1
}


{\displaystyle r\in \{0,\ldots ,n-1\}}
. The homomorphic property is then","[' In the Goldwasser-Micali cryptosystem, what is the public key?', ' What is the encryption of a bit b <unk>displaystyle b?']","['the modulus', '{\\mathcal {E}}(b)=x^{b}r^{2']"
3485,cnn,Summary,"The Cable News Network (CNN) is a multinational news-based pay television channel headquartered in Atlanta, United States. It is part of AT&T's WarnerMedia. It was founded in 1980 by American media proprietor Ted Turner and Reese Schonfeld as a 24-hour cable news channel. Upon its launch in 1980, CNN was the first television channel to provide 24-hour news coverage, and was the first all-news television channel in the United States.","The Cable News Network (CNN) is a multinational news-based pay television channel headquartered in Atlanta, United States. It is part of AT&T's WarnerMedia.","[' What is the name of the cable news network?', ' Where is the Cable News Network headquartered?']","['CNN', 'Atlanta, United States']"
3486,cnn,Summary,"As of September 2018, CNN had 90.1 million television households as subscribers (97.7% of households with cable) in the United States. According to Nielsen, in June 2021 CNN ranked third in viewership among cable news networks, behind Fox News and MSNBC, averaging 580,000 viewers throughout the day, down 49% from a year earlier, amid sharp declines in viewers across all cable news networks. While CNN ranked 14th among all basic cable networks in 2019, it was up to number 11 in 2021.","As of September 2018, CNN had 90.1 million television households as subscribers (97.7% of households with cable) in the United States. According to Nielsen, in June 2021 CNN ranked third in viewership among cable news networks, behind Fox News and MSNBC, averaging 580,000 viewers throughout the day, down 49% from a year earlier, amid sharp declines in viewers across all cable news networks.","[' How many television households were subscribed to CNN as of September 2018?', "" What was CNN's ranking in viewership among cable news networks in June 2021?"", ' How many viewers does CNN average throughout the day?', ' How many viewers does MSNBC average throughout the day?', ' How much did MSNBC lose from a year earlier?', ' What is the average MSNBC viewership?']","['90.1 million', 'third', '580,000', '580,000', '49%', '580,000']"
3487,cnn,Summary,"The network is known for its dramatic live coverage of breaking news, some of which has drawn criticism as overly sensationalistic. CNN claims to be ""The Most Trusted Name in News"", however its efforts to be nonpartisan have led to accusations of false balance.","The network is known for its dramatic live coverage of breaking news, some of which has drawn criticism as overly sensationalistic. CNN claims to be ""The Most Trusted Name in News"", however its efforts to be nonpartisan have led to accusations of false balance.","[' What network is known for its dramatic live coverage of breaking news?', ' What has been criticized as being overly sensationalist?']","['CNN', 'live coverage of breaking news']"
3488,cnn,Summary,"Globally, CNN programming has aired through CNN International, seen by viewers in over 212 countries and territories; since May 2019 however, the US domestic version has absorbed international news coverage in order to streamline programming expenses. The American version, sometimes referred to as CNN (US), is also available in Canada, some islands of the Caribbean and in Japan, where it was first broadcast on CNNj in 2003, with simultaneous translation in Japanese.","Globally, CNN programming has aired through CNN International, seen by viewers in over 212 countries and territories; since May 2019 however, the US domestic version has absorbed international news coverage in order to streamline programming expenses. The American version, sometimes referred to as CNN (US), is also available in Canada, some islands of the Caribbean and in Japan, where it was first broadcast on CNNj in 2003, with simultaneous translation in Japanese.","[' How many countries and territories has CNN International aired?', ' Since May 2019, what has the US domestic version of CNN absorbed?', ' What is the American version sometimes referred to as?', ' What is CNN also known as in Canada?', ' When was CNN first broadcast on CNNj?', ' What language was CNN originally broadcast in?']","['212', 'international news coverage', 'CNN (US', 'CNN (US', '2003', 'Japan']"
3489,cnn,Programming,"CNN's current weekday schedule consists mostly of rolling news programming during daytime hours, followed by in-depth news and information programs during the evening and prime time hours. The network's morning programming consists of Early Start, an early-morning news program hosted by Christine Romans and Laura Jarrett at 5–6 a.m. ET, which is followed by New Day, the network's morning show, hosted by John Berman and Brianna Keilar at 6–9 a.m. ET. Most of CNN's late-morning and early afternoon programming consists of CNN Newsroom, a rolling news program hosted by Jim Sciutto and Poppy Harlow in the morning and Ana Cabera, Victor Blackwell, and Alisyn Camerota in the afternoon. In between the editions of Newsroom, At This Hour with Kate Bolduan airs at 11 a.m. to noon Eastern, followed by Inside Politics with John King, hosted by John King at noon Eastern.","CNN's current weekday schedule consists mostly of rolling news programming during daytime hours, followed by in-depth news and information programs during the evening and prime time hours. The network's morning programming consists of Early Start, an early-morning news program hosted by Christine Romans and Laura Jarrett at 5–6 a.m.","["" What is CNN's morning news program called?"", ' When is Early Start broadcast?', ' Who hosts Early Start?']","['Early Start', '5–6 a.m', 'Christine Romans and Laura Jarrett']"
3490,cnn,Programming,"CNN's late afternoon and early evening lineup consists of The Lead with Jake Tapper, hosted by Jake Tapper at 4-6 p.m. Eastern and The Situation Room with Wolf Blitzer, hosted by Wolf Blitzer at 6 p.m. ET. The network's evening and primetime lineup shifts towards more in-depth programming, including Erin Burnett OutFront at 7 p.m. ET, and Anderson Cooper 360° at 8 p.m. ET, and Don Lemon Tonight at 10 p.m. Eastern.","CNN's late afternoon and early evening lineup consists of The Lead with Jake Tapper, hosted by Jake Tapper at 4-6 p.m. Eastern and The Situation Room with Wolf Blitzer, hosted by Wolf Blitzer at 6 p.m. ET.","[' Who hosts The Lead with Jake Tapper at 4-6 p.m. Eastern?', ' What host hosts The Situation Room with Wolf Blitzer at 6:15 pm ET?']","['CNN', 'Jake Tapper']"
3491,cnn,Programming,"Weekend primetime is dedicated mostly to factual programming, such as documentary specials and miniseries, and documentary-style reality series (such as Anthony Bourdain: Parts Unknown and United Shades of America), as well as acquired documentary films presented under the banner CNN Films. The network's weekend morning programming consists of CNN Newsroom (simulcast from CNN International) at 4–6 a.m. ET, which is followed by the weekend editions of New Day, hosted by Christi Paul and Boris Sanchez, which airs every Saturday at 6–9 a.m. ET and Sunday at 6–8 a.m. ET, and the network's Saturday program Smerconish with Michael Smerconish at 9 a.m. Eastern. Sunday morning lineup consists primarily of political talk shows, including Inside Politics Sunday, hosted by Abby Phillip at 8 a.m. Eastern and State of the Union, co-hosted by Jake Tapper and Dana Bash at 9 a.m. Eastern and replay at noon Eastern and the international affairs program Fareed Zakaria GPS, hosted by Fareed Zakaria at 10 a.m. Eastern and replay at 1 p.m. Eastern, and the media analysis program Reliable Sources, hosted by Brian Stelter at 11 a.m. Eastern. Weekend programming other than aforementioned slots is filled with CNN Newsroom by Fredricka Whitfield, Jim Acosta, Pamela Brown, and other rolling anchors.","Weekend primetime is dedicated mostly to factual programming, such as documentary specials and miniseries, and documentary-style reality series (such as Anthony Bourdain: Parts Unknown and United Shades of America), as well as acquired documentary films presented under the banner CNN Films. The network's weekend morning programming consists of CNN Newsroom (simulcast from CNN International) at 4–6 a.m.","[' What is the name of the reality series that airs on Sunday mornings on CNN?', ' What are two examples of reality series airing on Sundays on the network?', "" What is the name of CNN's morning programming?"", ' What is CNN Newsroom?', ' At what time of the day is CNN newsroom broadcast?']","['Anthony Bourdain: Parts Unknown and United Shades of America', 'Anthony Bourdain: Parts Unknown and United Shades of America', 'CNN Newsroom', 'simulcast from CNN International', '4–6 a.m']"
3492,cnn,Programming,"For the 2014–15 season, after cancelling Piers Morgan Tonight (which, itself, replaced the long-running Larry King Live), CNN experimented with running factual and reality-style programming during the 9:00 p.m. ET hour, such as John Walsh's The Hunt, This Is Life with Lisa Ling, and Mike Rowe's Somebody's Gotta Do It. Jeff Zucker explained that this new lineup was intended to shift CNN away from a reliance on pundit-oriented programs, and attract younger demographics to the network. Zucker stated that the 9:00 p.m. hour could be pre-empted during major news events for expanded coverage. These changes coincided with the introduction of a new imaging campaign for the network, featuring the slogan ""Go there"". In May 2014, CNN premiered The Sixties, a documentary miniseries produced by Tom Hanks, and Gary Goetzman which chronicled the United States in the 1960s. Owing to its success, CNN commissioned follow-ups focusing on other decades. Anderson Cooper 360° was expanded to run two hours long, from 8 PM to 10 PM.","For the 2014–15 season, after cancelling Piers Morgan Tonight (which, itself, replaced the long-running Larry King Live), CNN experimented with running factual and reality-style programming during the 9:00 p.m. ET hour, such as John Walsh's The Hunt, This Is Life with Lisa Ling, and Mike Rowe's Somebody's Gotta Do It.","[' When did CNN cancel Piers Morgan Tonight?', ' What was the name of the show that replaced Larry King Live?']","['2014–15 season', 'Piers Morgan Tonight']"
3493,cnn,Programming,"By 2019, CNN had produced at least 35 original series. Alongside the Hanks/Goetzman franchise (including the 2018 spin-off 1968), CNN has aired other documentary miniseries relating to news and U.S. policies, such as The Bush Years, and American Dynasties: The Kennedys—which saw the highest ratings of any CNN original series premiere to-date, with 1.7 million viewers. Parts Unknown concluded after the 2018 death by suicide of its host Anthony Bourdain; CNN announced several new miniseries and docuseries for 2019, including American Style (a miniseries produced by the digital media company Vox Media), The Redemption Project with Van Jones, Chasing Life with Sanjay Gupta, Tricky Dick (a miniseries chronicling Richard Nixon), The Movies (a spin-off of the Hanks/Goetzman decades miniseries), and Once in a Great City: Detroit 1962-64.","By 2019, CNN had produced at least 35 original series. Alongside the Hanks/Goetzman franchise (including the 2018 spin-off 1968), CNN has aired other documentary miniseries relating to news and U.S. policies, such as The Bush Years, and American Dynasties: The Kennedys—which saw the highest ratings of any CNN original series premiere to-date, with 1.7 million viewers.","[' How many original series had CNN produced by 2019?', ' What was the name of the 2018 spin-off of the Hanks/Goetzman series?', ' The Bush Years and American Dynasties: The Kennedys saw the highest ratings of any CNN original series.', ' How many viewers watched the first season of Kennedys?', ' What was the highest rated original series on CNN?']","['35', '1968', '—which saw the highest ratings of any CNN original series premiere to-date', '1.7 million', 'American Dynasties: The Kennedys']"
3494,cnn,Staff,"On July 27, 2012, CNN president Jim Walton announced he was resigning after 30 years at the network. Walton remained with CNN until the end of that year. In January 2013, former NBCUniversal President Jeff Zucker replaced Walton. In February 2022, Zucker was asked to resign by Jason Kilar, the chief executive of CNN's owner WarnerMedia, after Zucker's relationship with one of his lieutenants was discovered during the investigation into former CNN primetime host Chris Cuomo's efforts to control potentially damaging reporting regarding his brother Andrew Cuomo, governor of New York. Kilar announced that the interim co-heads would be executive vice presidents Michael Bass, Amy Entelis, and Ken Jautz.","On July 27, 2012, CNN president Jim Walton announced he was resigning after 30 years at the network. Walton remained with CNN until the end of that year.","[' When did Jim Walton resign as president of CNN?', ' How many years did Walton have at CNN before he resigned?', "" What was the name of Walton's position at CNN in 2012.""]","['July 27, 2012', '30', 'president']"
3495,cnn,Specialized channels,"Over the years, CNN has launched spin-off networks in the United States and other countries. Channels that currently operate as of 2021 include:
","Over the years, CNN has launched spin-off networks in the United States and other countries. Channels that currently operate as of 2021 include:","[' How many spin-off networks did CNN launch in the United States and other countries?', ' As of 2021, what are the channels that currently operate?', ' What is the name of the spin off networks that CNN launched?']","['2021', 'Channels', 'Channels']"
3496,cnn,Awards and honors,"2021: CNN won a George Polk Award for Foreign Reporting for their reporting on the coronavirus outbreak in Wuhan, China and later under quarantine in Beijing. CNN and Clarissa Ward were named finalists for the duPont-Columbia Award for their ""Russia’s Secret Influence Campaigns"" investigation.","2021: CNN won a George Polk Award for Foreign Reporting for their reporting on the coronavirus outbreak in Wuhan, China and later under quarantine in Beijing. CNN and Clarissa Ward were named finalists for the duPont-Columbia Award for their ""Russia’s Secret Influence Campaigns"" investigation.","[' Who won the George Polk Award for Foreign Reporting in 2021?', ' Where did CNN report on the coronavirus outbreak?', "" Who was named a finalists for the duPont-Columbia Award for their Russia's Secret Influence Campaigns investigation?""]","['CNN', 'Wuhan, China', 'CNN and Clarissa Ward']"
3497,cnn,Awards and honors,"2020: CNN's Ed Lavandera was awarded a Peabody for ""The Hidden Workforce: Undocumented in America"". CNN Films was awarded a Peabody for the documentary ""Apollo 11"".","2020: CNN's Ed Lavandera was awarded a Peabody for ""The Hidden Workforce: Undocumented in America"". CNN Films was awarded a Peabody for the documentary ""Apollo 11"".","[' Who was awarded a Peabody for ""The Hidden Workforce: Undocumented in America""?', ' What was the name of the documentary that CNN Films was awarded?']","['Ed Lavandera', 'Apollo 11']"
3498,cnn,Awards and honors,"2018: CNN won a network-record six news & documentary Emmy Awards. They are, Outstanding Breaking News Coverage, Outstanding Continuing Coverage of a News Story in a Newscast, Outstanding Live Interview, Outstanding Hard News Feature Story in a Newscast, Outstanding News Special, Outstanding Science, Medical and Environmental Report.","2018: CNN won a network-record six news & documentary Emmy Awards. They are, Outstanding Breaking News Coverage, Outstanding Continuing Coverage of a News Story in a Newscast, Outstanding Live Interview, Outstanding Hard News Feature Story in a Newscast, Outstanding News Special, Outstanding Science, Medical and Environmental Report.",[' How many Emmy Awards did CNN win in 2018?'],['six']
3499,cnn,Awards and honors,2018: CNN received the George Polk Award for Foreign Television Reporting for uncovering a hidden modern-day slave auction of African refugees in Libya. Reporting done by Nima Elbagir and Raja Razek.,2018: CNN received the George Polk Award for Foreign Television Reporting for uncovering a hidden modern-day slave auction of African refugees in Libya. Reporting done by Nima Elbagir and Raja Razek.,"[' What award did CNN receive for uncovering a hidden modern-day slave auction of African refugees in Libya?', ' Who did CNN report on?']","['George Polk Award for Foreign Television Reporting', 'African refugees']"
3500,cnn,Awards and honors,2018: CNN received the Overseas Press Club of America David Kaplan Award for best TV or video spot news reporting from abroad for reporting on the fall of ISIS. Reporting done by Nick Paton Walsh and Arwa Damon.,2018: CNN received the Overseas Press Club of America David Kaplan Award for best TV or video spot news reporting from abroad for reporting on the fall of ISIS. Reporting done by Nick Paton Walsh and Arwa Damon.,"[' Who won the Overseas Press Club of America David Kaplan Award for reporting on the fall of ISIS?', ' Who did CNN report on?']","['CNN', 'the fall of ISIS']"
3501,atomic proposition,Summary,"In logic and analytic philosophy, an atomic sentence is a type of declarative sentence which is either true or false (may also be referred to as a proposition, statement or truthbearer) and which cannot be broken down into other simpler sentences. For example, ""The dog ran"" is an atomic sentence in natural language, whereas ""The dog ran and the cat hid"" is a molecular sentence in natural language.
","In logic and analytic philosophy, an atomic sentence is a type of declarative sentence which is either true or false (may also be referred to as a proposition, statement or truthbearer) and which cannot be broken down into other simpler sentences. For example, ""The dog ran"" is an atomic sentence in natural language, whereas ""The dog ran and the cat hid"" is a molecular sentence in natural language.","[' What is an atomic sentence in logic and analytic philosophy?', ' What is a type of declarative sentence that is either true or false?', ' What is an atomic sentence in natural language?', ' What is a molecular sentence?']","['a type of declarative sentence which is either true or false', 'an atomic sentence', 'The dog ran', '""The dog ran and the cat hid']"
3502,atomic proposition,Summary,"From a logical analysis point of view, the truth or falsity of sentences in general is determined by only two things: the logical form of the sentence and the truth or falsity of its simple sentences. This is to say, for example, that the truth of the sentence ""John is Greek and John is happy"" is a function of the meaning of ""and"", and the truth values of the atomic sentences ""John is Greek"" and ""John is happy"". However, the truth or falsity of an atomic sentence is not a matter that is within the scope of logic itself, but rather whatever art or science the content of the atomic sentence happens to be talking about.","From a logical analysis point of view, the truth or falsity of sentences in general is determined by only two things: the logical form of the sentence and the truth or falsity of its simple sentences. This is to say, for example, that the truth of the sentence ""John is Greek and John is happy"" is a function of the meaning of ""and"", and the truth values of the atomic sentences ""John is Greek"" and ""John is happy"".","[' From a logical analysis point of view, what determines the truth or falsity of sentences in general?', ' The logical form of the sentence and the truth of its simple sentences determine what?', ' What is the truth of the sentence ""John is Greek and John is happy"" a function of?', ' What are the truth values of the atomic sentences?']","['the logical form of the sentence and the truth or falsity of its simple sentences', 'the truth or falsity', 'the meaning of ""and"",', '""John is Greek"" and ""John is happy"".']"
3503,atomic proposition,Summary,"Logic has developed artificial languages, for example sentential calculus and predicate calculus, partly with the purpose of revealing the underlying logic of natural-language statements, the surface grammar of which may conceal the underlying logical structure. In these artificial languages an atomic sentence is a string of symbols which can represent an elementary sentence in a natural language, and it can be defined as follows. In a formal language, a well-formed formula (or wff) is a string of symbols constituted in accordance with the rules of syntax of the language. A term is a variable, an individual constant or a n-place function letter followed by n terms. An atomic formula is a wff consisting of either a sentential letter or an n-place predicate letter followed by n terms. A sentence is a wff in which any variables are bound. An atomic sentence is an atomic formula containing no variables. It follows that an atomic sentence contains no logical connectives, variables or quantifiers. A sentence consisting of one or more sentences and a logical connective is a compound (or molecular) sentence.
","Logic has developed artificial languages, for example sentential calculus and predicate calculus, partly with the purpose of revealing the underlying logic of natural-language statements, the surface grammar of which may conceal the underlying logical structure. In these artificial languages an atomic sentence is a string of symbols which can represent an elementary sentence in a natural language, and it can be defined as follows.","[' Logic has developed artificial languages, such as what?', ' What are sentential calculus and predicate calculus?', ' What is an atomic sentence?', ' What can represent an elementary sentence in natural language?']","['sentential calculus and predicate calculus', 'artificial languages', 'a string of symbols', 'an atomic sentence']"
3504,atomic proposition,Interpretations,"A sentence is either true or false under an interpretation which assigns values to the logical variables. We might for example make the following assignments:
",A sentence is either true or false under an interpretation which assigns values to the logical variables. We might for example make the following assignments:,[' What is true or false under an interpretation that assigns values to the logical variables?'],['A sentence']
3505,atomic proposition,Translating sentences from a natural language into an artificial language,"Sentences in natural languages can be ambiguous, whereas the languages of the sentential logic and predicate logics are precise. Translation can reveal such ambiguities and express precisely the intended meaning.
","Sentences in natural languages can be ambiguous, whereas the languages of the sentential logic and predicate logics are precise. Translation can reveal such ambiguities and express precisely the intended meaning.","[' What can be ambiguous in natural languages?', ' What languages are precise?', ' Translation can reveal such ambiguities and express precisely the intended meaning?']","['Sentences', 'sentential logic and predicate logics', 'Sentences in natural languages']"
3506,atomic proposition,Translating sentences from a natural language into an artificial language,"For example, take the English sentence “Father Ted married Jack and Jill”. Does this mean Jack married Jill? In translating we might make the following assignments:
Individual Constants
","For example, take the English sentence “Father Ted married Jack and Jill”. Does this mean Jack married Jill?","[' What is the meaning of the English sentence ""Father Ted married Jack and Jill""?']",['Jack married Jill']
3507,atomic proposition,Philosophical significance,"No atomic sentence can be deduced from (is not entailed by) any other atomic sentence, no two atomic sentences are incompatible, and no sets of atomic sentences are self-contradictory. Wittgenstein made much of this in his Tractatus. If there are any atomic sentences then there must be ""atomic facts"" which correspond to those that are true, and the conjunction of all true atomic sentences would say all that was the case, i.e., ""the world"" since, according to Wittegenstein, ""The world is all that is the case"". (TLP:1). Similarly the set of all sets of atomic sentences corresponds to the set of all possible worlds (all that could be the case).
","No atomic sentence can be deduced from (is not entailed by) any other atomic sentence, no two atomic sentences are incompatible, and no sets of atomic sentences are self-contradictory. Wittgenstein made much of this in his Tractatus.","[' What can be deduced from (is not entailed by) any other atomic sentence?', ' What can no two atomic sentences be incompatible with?', ' Wittgenstein made much of this in what book?']","['No atomic sentence', 'self-contradictory', 'Tractatus']"
3508,unsupervised learning,Summary,"Unsupervised learning  is a type of algorithm that learns patterns from untagged data. The hope is that through mimicry, which is an important mode of learning in people, the machine is forced to build a compact internal representation of its world and then generate imaginative content from it. In contrast to supervised learning where data is tagged by an expert, e.g. as a ""ball"" or ""fish"", unsupervised methods exhibit self-organization that captures patterns as probability densities  or a combination of neural feature preferences. The other levels in the supervision spectrum are reinforcement learning where the machine is given only a numerical performance score as guidance, and semi-supervised learning where a smaller portion of the data is tagged. Two broad methods in Unsupervised Learning are Neural Networks and Probabilistic Methods.
","Unsupervised learning  is a type of algorithm that learns patterns from untagged data. The hope is that through mimicry, which is an important mode of learning in people, the machine is forced to build a compact internal representation of its world and then generate imaginative content from it.","[' What is a type of algorithm that learns patterns from untagged data?', ' What is an important mode of learning in people?']","['Unsupervised learning', 'mimicry']"
3509,unsupervised learning,Probabilistic methods,"Two of the main methods used in unsupervised learning are principal component and cluster analysis. Cluster analysis is used in unsupervised learning to group, or segment, datasets with shared attributes in order to extrapolate algorithmic relationships. Cluster analysis is a branch of machine learning that groups the data that has not been labelled, classified or categorized. Instead of responding to feedback, cluster analysis identifies commonalities in the data and reacts based on the presence or absence of such commonalities in each new piece of data. This approach helps detect anomalous data points that do not fit into either group.
","Two of the main methods used in unsupervised learning are principal component and cluster analysis. Cluster analysis is used in unsupervised learning to group, or segment, datasets with shared attributes in order to extrapolate algorithmic relationships.","[' What are the two main methods used in unsupervised learning?', ' What is used to group, or segment, datasets with shared attributes?']","['principal component and cluster analysis', 'Cluster analysis']"
3510,unsupervised learning,Probabilistic methods,"A central application of unsupervised learning is in the field of density estimation in statistics, though unsupervised learning encompasses many other domains involving summarizing and explaining data features. It can be contrasted with supervised learning by saying that whereas supervised learning intends to infer a conditional probability distribution  conditioned on the label  of input data; unsupervised learning intends to infer an a priori probability distribution .
","A central application of unsupervised learning is in the field of density estimation in statistics, though unsupervised learning encompasses many other domains involving summarizing and explaining data features. It can be contrasted with supervised learning by saying that whereas supervised learning intends to infer a conditional probability distribution  conditioned on the label  of input data; unsupervised learning intends to infer an a priori probability distribution .","[' What is a central application of unsupervised learning in the field of density estimation in statistics?', ' Unsupervised learning encompasses many other domains involving summarizing and explaining what?', ' What intends to infer a conditional probability distribution conditioned?', ' Unsupervised learning intends to infer an a priori probability distribution?']","['summarizing and explaining data features', 'data features', 'supervised learning', 'conditional probability distribution  conditioned on the label  of input data']"
3511,oblivious transfer,Summary,"The first form of oblivious transfer was introduced in 1981 by Michael O. Rabin.1  In this form, the sender sends a message to the receiver with probability 1/2, while the sender remains oblivious as to whether or not the receiver received the message.  Rabin's oblivious transfer scheme is based on the RSA cryptosystem.  A more useful form of oblivious transfer called 1–2 oblivious transfer or ""1 out of 2 oblivious transfer"", was developed later by Shimon Even, Oded Goldreich, and Abraham Lempel,2 in order to build protocols for secure multiparty computation. It is generalized to ""1 out of n oblivious transfer"" where the user gets exactly one database element without the server getting to know which element was queried, and without the user knowing anything about the other elements that were not retrieved. The latter notion of oblivious transfer is a strengthening of private information retrieval, in which the database is not kept private.
","The first form of oblivious transfer was introduced in 1981 by Michael O. Rabin.1  In this form, the sender sends a message to the receiver with probability 1/2, while the sender remains oblivious as to whether or not the receiver received the message. Rabin's oblivious transfer scheme is based on the RSA cryptosystem.","[' Who introduced the first form of oblivious transfer?', ' What is the probability of the sender sending a message to the receiver?', "" What is Rabin's oblivious transfer scheme based on?""]","['Michael O. Rabin', '1/2', 'RSA cryptosystem']"
3512,oblivious transfer,Summary,"Further work has revealed oblivious transfer to be a fundamental and important problem in cryptography.  It is considered one of the critical problems in the field, because of the importance of the applications that can be built based on it. In particular, it is complete for secure multiparty computation: that is, given an implementation of oblivious transfer it is possible to securely evaluate any polynomial time computable function without any additional primitive.4","Further work has revealed oblivious transfer to be a fundamental and important problem in cryptography. It is considered one of the critical problems in the field, because of the importance of the applications that can be built based on it.","[' What has been revealed to be a fundamental and important problem in cryptography?', ' Why is oblivious transfer considered one of the critical problems in the field?']","['oblivious transfer', 'the importance of the applications that can be built based on it']"
3513,oblivious transfer,Rabin's oblivious transfer protocol,"In Rabin's oblivious transfer protocol, the sender generates an RSA public modulus N=pq where p and q are large prime numbers, and an exponent e relatively prime to λ(N) = (p − 1)(q − 1).  The sender encrypts the message m as me mod N.
","In Rabin's oblivious transfer protocol, the sender generates an RSA public modulus N=pq where p and q are large prime numbers, and an exponent e relatively prime to λ(N) = (p − 1)(q − 1). The sender encrypts the message m as me mod N.","["" What is Rabin's oblivious transfer protocol?"", "" What is the sender's public modulus N=pq?"", ' Where p and q are large prime numbers?']","['the sender generates an RSA public modulus N=pq', 'RSA', 'RSA public modulus N=pq']"
3514,oblivious transfer,Rabin's oblivious transfer protocol,"If the receiver finds y is neither x nor −x modulo N, the receiver will be able to factor N and therefore decrypt me to recover m (see Rabin encryption for more details).  However, if y is x or −x mod N, the receiver will have no information about m beyond the encryption of it.  Since every quadratic residue modulo N has four square roots, the probability that the receiver learns m is 1/2.
","If the receiver finds y is neither x nor −x modulo N, the receiver will be able to factor N and therefore decrypt me to recover m (see Rabin encryption for more details). However, if y is x or −x mod N, the receiver will have no information about m beyond the encryption of it.","[' What happens if y is neither x nor <unk>x modulo N?', ' What will the receiver be able to factor N and thus recover?', ' The receiver will have no information about m beyond what?']","['the receiver will have no information about m beyond the encryption of it', 'm', 'the encryption of it']"
3515,oblivious transfer,1–2 oblivious transfer,"In a 1–2 oblivious transfer protocol, Alice the sender has two messages m0 and m1, and wants to ensure that the receiver only learns one. Bob, the receiver, has a bit b and wishes to receive mb without Alice learning b.
The protocol of Even, Goldreich, and Lempel (which the authors attribute partially to Silvio Micali), is general, but can be instantiated using RSA encryption as follows.
","In a 1–2 oblivious transfer protocol, Alice the sender has two messages m0 and m1, and wants to ensure that the receiver only learns one. Bob, the receiver, has a bit b and wishes to receive mb without Alice learning b.","[' How many messages does Alice send in a 1–2 oblivious transfer protocol?', ' What does Bob want to receive without Alice learning b?']","['two', 'mb']"
3516,oblivious transfer,1-out-of-<i>n</i> oblivious transfer and <i>k</i>-out-of-<i>n</i> oblivious transfer,"A 1-out-of-n oblivious transfer protocol can be defined as a natural generalization of a 1-out-of-2 oblivious transfer protocol. Specifically, a sender has n messages, and the receiver has an index i, and the receiver wishes to receive the i-th among the sender's messages, without the sender learning i, while the sender wants to ensure that the receiver receive only one of the n messages.
","A 1-out-of-n oblivious transfer protocol can be defined as a natural generalization of a 1-out-of-2 oblivious transfer protocol. Specifically, a sender has n messages, and the receiver has an index i, and the receiver wishes to receive the i-th among the sender's messages, without the sender learning i, while the sender wants to ensure that the receiver receive only one of the n messages.","[' What can be defined as a natural generalization of a 1-out-of-2 oblivious transfer protocol?', ' How many messages does a sender have?', "" What does the receiver wish to receive among the sender's messages?"", ' What does the sender want to ensure that the receiver receive?']","['1-out-of-n oblivious transfer protocol', 'n', 'the i-th', 'only one of the n messages']"
3517,oblivious transfer,1-out-of-<i>n</i> oblivious transfer and <i>k</i>-out-of-<i>n</i> oblivious transfer,"1-out-of-n oblivious transfer is incomparable to private information retrieval (PIR). 
On the one hand, 1-out-of-n oblivious transfer imposes an additional privacy requirement for the database: namely, that the receiver learn at most one of the database entries. On the other hand, PIR requires communication sublinear in n, whereas 1-out-of-n oblivious transfer has no such requirement. However, assuming single server PIR is a sufficient assumption in order to construct  1-out-of-2 Oblivious Transfer 14.
","1-out-of-n oblivious transfer is incomparable to private information retrieval (PIR). On the one hand, 1-out-of-n oblivious transfer imposes an additional privacy requirement for the database: namely, that the receiver learn at most one of the database entries.","[' What is the difference between 1-out-of-n oblivious transfer and private information retrieval (PIR)?', ' What is an additional privacy requirement for the database?']","['incomparable', 'that the receiver learn at most one of the database entries']"
3518,oblivious transfer,1-out-of-<i>n</i> oblivious transfer and <i>k</i>-out-of-<i>n</i> oblivious transfer,"1-out-of-n oblivious transfer protocol with sublinear communication was first constructed (as a generalization of single-server PIR) by Eyal Kushilevitz and Rafail Ostrovsky 15. More efficient constructions were proposed by Moni Naor and Benny Pinkas,10 William Aiello, Yuval Ishai and Omer Reingold,11 Sven Laur and Helger Lipmaa.12. In 2017, Kolesnikov et al.,13 proposed an efficient 1-n oblivious transfer protocol which requires roughly 4x the cost of 1-2 oblivious transfer in amortized setting.
","1-out-of-n oblivious transfer protocol with sublinear communication was first constructed (as a generalization of single-server PIR) by Eyal Kushilevitz and Rafail Ostrovsky 15. More efficient constructions were proposed by Moni Naor and Benny Pinkas,10 William Aiello, Yuval Ishai and Omer Reingold,11 Sven Laur and Helger Lipmaa.12.","[' Who first constructed 1-out-of-n oblivious transfer protocol with sublinear communication?', ' What was first constructed as a generalization of single-server PIR?', ' Who proposed more efficient constructions?']","['Eyal Kushilevitz and Rafail Ostrovsky', '1-out-of-n oblivious transfer protocol with sublinear communication', 'Moni Naor and Benny Pinkas']"
3519,oblivious transfer,1-out-of-<i>n</i> oblivious transfer and <i>k</i>-out-of-<i>n</i> oblivious transfer,"Brassard, Crépeau and Robert further generalized this notion to k-n oblivious transfer,5 wherein the receiver obtains a set of k messages from the n message collection.  The set of k messages may be received simultaneously (""non-adaptively""), or they may be requested consecutively, with each request based on previous messages received.6","Brassard, Crépeau and Robert further generalized this notion to k-n oblivious transfer,5 wherein the receiver obtains a set of k messages from the n message collection. The set of k messages may be received simultaneously (""non-adaptively""), or they may be requested consecutively, with each request based on previous messages received.6","[' Brassard, Crépeau and Robert generalized the concept of k-n oblivious transfer to what?', ' The receiver receives a set of what from the n message collection?']","['k-n oblivious transfer,5 wherein the receiver obtains a set of k messages from the n message collection', 'k messages']"
3520,oblivious transfer,Generalized oblivious transfer,"k-n Oblivious transfer is a special case of generalized oblivious transfer, which was presented by Ishai and Kushilevitz.7 In that setting, the sender has a set U of n messages, and the transfer constraints are specified by a collection A of permissible subsets of U.
The receiver may obtain any subset of the messages in U that appears in the collection A. The sender should remain oblivious of the selection made by the receiver, while the receiver cannot learn the value of the messages outside the subset of messages that he chose to obtain. The collection A is monotone decreasing, in the sense that it is closed under containment (i.e., if a given subset B is in the collection A, so are all of the subsets of B).
The solution proposed by Ishai and Kushilevitz uses the parallel invocations of 1-2 oblivious transfer while making use of a special model of private protocols. Later on, other solutions that are based on secret sharing were published – one by Bhavani Shankar, Kannan Srinathan, and C. Pandu Rangan,8 and another by Tamir Tassa.9","k-n Oblivious transfer is a special case of generalized oblivious transfer, which was presented by Ishai and Kushilevitz.7 In that setting, the sender has a set U of n messages, and the transfer constraints are specified by a collection A of permissible subsets of U. The receiver may obtain any subset of the messages in U that appears in the collection A.","[' What is a special case of generalized oblivious transfer?', ' Who presented k-n Oblivious Transfer?', "" What is the sender's set U of?"", ' The receiver may obtain any subset of the messages in U that appears in what collection?']","['k-n Oblivious transfer', 'Ishai and Kushilevitz', 'n messages', 'A']"
3521,oblivious transfer,Origins,"In the early seventies Stephen Wiesner introduced a primitive called multiplexing in his seminal paper ""Conjugate Coding"",
which was the starting point of quantum cryptography.[1] Unfortunately it took more than ten years to be published. Even though
this primitive was equivalent to what was later called 1–2 oblivious transfer, Wiesner did not see its application to cryptography.
","In the early seventies Stephen Wiesner introduced a primitive called multiplexing in his seminal paper ""Conjugate Coding"",
which was the starting point of quantum cryptography. [1] Unfortunately it took more than ten years to be published.","[' What did Stephen Wiesner introduce in the early seventies?', ' What was the starting point of quantum cryptography?', ' How long did it take for multiplexing to be published?']","['multiplexing', 'Conjugate Coding', 'more than ten years']"
3522,oblivious transfer,Quantum oblivious transfer,"Protocols for oblivious transfer can be implemented with quantum systems. In contrast to other tasks in quantum cryptography, like quantum key distribution, it has been shown that quantum oblivious transfer cannot be implemented with unconditional security, i.e. the security of quantum oblivious transfer protocols cannot be guaranteed only from the laws of quantum physics.","Protocols for oblivious transfer can be implemented with quantum systems. In contrast to other tasks in quantum cryptography, like quantum key distribution, it has been shown that quantum oblivious transfer cannot be implemented with unconditional security, i.e.","[' Protocols for oblivious transfer can be implemented with what kind of systems?', ' What is the opposite of other tasks in quantum cryptography?']","['quantum', 'quantum key distribution']"
