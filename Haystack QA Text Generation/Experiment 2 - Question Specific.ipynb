{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4222389d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nishu\\Anaconda3\\envs\\haystack\\lib\\site-packages\\ray\\autoscaler\\_private\\cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from haystack import *\n",
    "from haystack.document_stores import FAISSDocumentStore\n",
    "from haystack.schema import Document\n",
    "from haystack.nodes import DensePassageRetriever, FARMReader\n",
    "from haystack.pipelines import ExtractiveQAPipeline\n",
    "from haystack.utils import print_answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc8e231e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as urllib2\n",
    "from googlesearch import search\n",
    "from bs4.element import Comment\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fe04566",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_documents(urls):\n",
    "    \n",
    "    documents = []\n",
    "\n",
    "    for j, url in enumerate(urls):        \n",
    "        try:\n",
    "            hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "            req = urllib2.Request(url, headers = hdr)\n",
    "            page = urllib2.urlopen(req)\n",
    "            soup = BeautifulSoup(page, \"html.parser\")\n",
    "            \n",
    "            for i, paragraph in enumerate(soup.findAll(\"p\")):\n",
    "                p = paragraph.text.split()\n",
    "                if len(p) == 0:\n",
    "                    continue\n",
    "                documents.append(Document(content = \" \".join(p)))   \n",
    "        except:\n",
    "            \"crap\"\n",
    "        \n",
    "    return documents\n",
    "\n",
    "def create_docs(directory):\n",
    "    import os\n",
    "    \n",
    "    docs = []\n",
    "\n",
    "    document_names = os.listdir(f\"./{directory}\")\n",
    "    for doc in document_names:\n",
    "        text = open(f\"{directory}/{doc}\",  encoding = 'utf8').read()\n",
    "        docs.append(Document(content = text))\n",
    "        \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7f01c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\"What is Machine Learning?\", \"What is the History of Machine Learning?\", \"What is the Theory of Machine Learning?\", \"What are the Approaches of Machine Learning?\", \"What are the Types of Machine Learning Models?\", \"What are Real-world Applications of Machine Learning?\", \"What are the Limitations of Machine Learning?\", \"What Metrics do we use to Evaluate Machine Learning Models?\"]\n",
    "urls = [[url for url in search(query, tld = \"co.in\", num = 15, stop = 10, pause = 0.5)] for query in queries]\n",
    "urls = [item for sublist in urls for item in sublist]\n",
    "urls = [url for url in urls if \"html\" not in url and \"wikipedia\" not in url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d76d7e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n",
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    }
   ],
   "source": [
    "docs = create_documents(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5820c904",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = FAISSDocumentStore(vector_dim=128, faiss_index_factory_str=\"Flat\")\n",
    "document_store.delete_documents()\n",
    "document_store.write_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f6c6dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RetriBertModel were not initialized from the model checkpoint at yjernite/retribert-base-uncased and are newly initialized: ['bert_query.embeddings.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Updating Embedding:   0%|                                                                  | 0/3101 [00:00<?, ? docs/s]"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85ad29c5daf649fb979015744c8b4672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating Embeddings:   0%|          | 0/97 [00:00<?, ? Batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Documents Processed: 10000 docs [58:30,  2.85 docs/s]                                                                  \n"
     ]
    }
   ],
   "source": [
    "from haystack.retriever.dense import EmbeddingRetriever\n",
    "\n",
    "retriever = EmbeddingRetriever(document_store=document_store,\n",
    "                               embedding_model=\"yjernite/retribert-base-uncased\",\n",
    "                               model_format=\"retribert\")\n",
    "\n",
    "document_store.update_embeddings(retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b28b7845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.32, 0.25, 0.08, 0.35])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([0.45, 0.16, 0.15, 0.24]) @ np.array([[0, 1/3, 0, 2/3], [3/4, 0, 1/4, 0], [0, 2/3, 0, 1/3], [5/6, 0, 1/6, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92d40948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------Min Length: 0------------\n",
      "\n",
      "\n",
      " The limitations of machine learning aren't limited by the limitations of human learning. They are limited by how well we can teach machines how to learn.\n",
      "\n",
      "\n",
      "------------Min Length: 50------------\n",
      "\n",
      "\n",
      " The limitations of machine learning aren't limited by the limitations of human learning. They are limited by how well you can use machine learning to solve a problem. Machine learning is very good at solving problems, but it is very bad at solving *real* problems. For example, if you want to teach a computer how to recognize a dog, you can't just teach it to recognize dogs, you have to teach it how to distinguish a dog from a dog.\n",
      "\n",
      "\n",
      "------------Min Length: 100------------\n",
      "\n",
      "\n",
      " The limitations of machine learning aren't limited by the limitations of human learning. They are limited by how well you can use machine learning to solve a problem. Machine learning is very good at solving problems, but it is very bad at solving *real* problems. For example, if you want to teach a computer how to recognize a dog, you can't just teach it to recognize dogs, you have to teach it how to distinguish a dog from a dog that doesn't look like a dog. You have to train it to be able to tell the difference between a dog and a dog it doesn't recognize.\n",
      "\n",
      "\n",
      "------------Extracted Documents------------\n",
      "\n",
      "\n",
      "Classical, or \"non-deep\", machine learning is more dependent on human intervention to learn. Human experts determine the hierarchy of features to understand the differences between data inputs, usually requiring more structured data to learn. For example, let's say that I were to show you a series of images of different types of fast food, “pizza,” “burger,” or “taco.” The human expert on these images would determine the characteristics which distinguish each picture as the specific fast food type. For example, the bread of each food type might be a distinguishing feature across each picture. Alternatively, you might just use labels, such as “pizza,” “burger,” or “taco”, to streamline the learning process through supervised learning.\n",
      "\n",
      "\n",
      "Top machine learning approaches are categorized depending on the nature of their feedback mechanism for learning. Most of the machine learning problems may be addressed by adopting one of these approaches. Yet, we may still encounter complex machine learning solutions that do not fit into one of these approaches.\n",
      "\n",
      "\n",
      "Unless these models can be explained, these models can become powerless, and the process of human interpretation follows rules far beyond the technical strength. Therefore, interpretability is the highest quality that machine learning methods should achieve if they are to be applied in practice.\n",
      "\n",
      "\n",
      "Machine learning is stochastic, not deterministic.\n",
      "\n",
      "\n",
      "Compared with statistical modeling, the scope of machine learning analysis is inherently different-statistical modeling is confirmatory in nature, and machine learning is exploratory in nature.\n"
     ]
    }
   ],
   "source": [
    "from haystack.generator.transformers import Seq2SeqGenerator\n",
    "from haystack.pipeline import GenerativeQAPipeline\n",
    "\n",
    "generator0 = Seq2SeqGenerator(model_name_or_path=\"yjernite/bart_eli5\", min_length = 0)\n",
    "pipe0 = GenerativeQAPipeline(generator0, retriever)\n",
    "\n",
    "generator50 = Seq2SeqGenerator(model_name_or_path=\"yjernite/bart_eli5\", min_length = 50)\n",
    "pipe50 = GenerativeQAPipeline(generator50, retriever)\n",
    "\n",
    "generator100 = Seq2SeqGenerator(model_name_or_path=\"yjernite/bart_eli5\", min_length = 100)\n",
    "pipe100 = GenerativeQAPipeline(generator100, retriever)\n",
    "\n",
    "\n",
    "q = \"What are the limitations of machine learning?\"\n",
    "\n",
    "ret0 = pipe0.run(query = q, params = {\"Retriever\": {\"top_k\": 5}})\n",
    "ret50 = pipe50.run(query = q, params = {\"Retriever\": {\"top_k\": 5}})\n",
    "ret100 = pipe100.run(query = q, params = {\"Retriever\": {\"top_k\": 5}})\n",
    "\n",
    "print(\"\\n\\n------------Min Length: 0------------\\n\\n\")\n",
    "print(ret0[\"answers\"][0])\n",
    "\n",
    "print(\"\\n\\n------------Min Length: 50------------\\n\\n\")\n",
    "print(ret50[\"answers\"][0])\n",
    "\n",
    "print(\"\\n\\n------------Min Length: 100------------\\n\\n\")\n",
    "print(ret100[\"answers\"][0])\n",
    "\n",
    "\n",
    "print(\"\\n\\n------------Extracted Documents------------\\n\\n\")\n",
    "print(\"\\n\\n\\n\".join([c.content for c in ret0[\"documents\"]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
