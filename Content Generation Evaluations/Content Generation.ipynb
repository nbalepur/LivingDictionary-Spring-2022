{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e9c9844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "import urllib.request as urllib2\n",
    "import numpy as np\n",
    "from googlesearch import search\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ac00e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(topic, section):\n",
    "    query = f\"{topic} {section}\"\n",
    "    \n",
    "    try:\n",
    "        urls = [url for url in search(query, num_results = 15, lang = \"en\")]  \n",
    "        urls = [url for url in urls if \".org\" not in url and \".edu\" not in url and \"wikipedia\" not in url]\n",
    "    except HTTPError as my_exception:\n",
    "        print(my_exception.headers)\n",
    "        return []\n",
    "    \n",
    "    ret = []\n",
    "    itr = 0\n",
    "    \n",
    "    while len(ret) != 5:\n",
    "        \n",
    "        try:\n",
    "            hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "            req = urllib2.Request(urls[itr], headers = hdr)\n",
    "            page = urllib2.urlopen(req, timeout = 10)\n",
    "            soup = BeautifulSoup(page, \"html.parser\")\n",
    "\n",
    "            paragraphs_web = soup.findAll(\"p\")\n",
    "            paragraphs_web = [p.text for p in paragraphs_web]\n",
    "            \n",
    "            ret.append(urls[itr])\n",
    "        except:\n",
    "            a = 1\n",
    "            \n",
    "        itr += 1\n",
    "        \n",
    "    return ret\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6527985",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic, section = \"machine learning\", \"history\"\n",
    "urls = get_urls(topic, section)\n",
    "url = urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f269dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "req = urllib2.Request(url, headers = hdr)\n",
    "page = urllib2.urlopen(req, timeout = 1)\n",
    "soup = BeautifulSoup(page, \"html.parser\")\n",
    "\n",
    "paragraphs_web = soup.findAll(\"p\")\n",
    "paragraphs_web = [p.text for p in paragraphs_web]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f196a5e",
   "metadata": {},
   "source": [
    "## Our Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0eff0363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'[^A-Za-z0-9 ]+', '', text)\n",
    "    return cleaned_text.lower()\n",
    "\n",
    "def get_results_inst(paragraphs, M1, score):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for p in paragraphs:\n",
    "        sentences = [s for s in nltk.sent_tokenize(p)]\n",
    "        selected = [False for i in range(len(sentences))]\n",
    "        sentences_cleaned = [clean_text(s) for s in sentences]\n",
    "\n",
    "        pred = [(M1.predict(s)[0][0][len(\"__label__\"):][0], M1.predict(s)[1][0]) for s in sentences_cleaned]\n",
    "        pred = [(\"I\" if idx == \"I\" and s > score else \"N\") for idx, s in pred]\n",
    "        chunk_idxs = [(m.start(0), m.end(0)) for m in re.finditer(\"I*\", \"\".join(pred))]\n",
    "        \n",
    "        \n",
    "        chunks = []\n",
    "        for start, end in chunk_idxs:\n",
    "            if start == end:\n",
    "                continue\n",
    "            selected[start:end] = [True for i in range(end - start)]\n",
    "            chunks.append(sentences[start:end])\n",
    "        \n",
    "        chunk_text = [\" \".join(c) for c in chunks]\n",
    "        results.append((p, chunk_text, selected))\n",
    "        \n",
    "    return results\n",
    "\n",
    "def get_summary_inst(paragraphs_web, model, score):\n",
    "    \n",
    "    results_web = get_results_inst(paragraphs_web, model, score)\n",
    "    instances = [r[1] for r in results_web]\n",
    "    instances = [\" \".join(i) for i in instances]\n",
    "    instances = [i for i in instances if len(i) > 0]\n",
    "    \n",
    "    return instances\n",
    "\n",
    "def get_results_begend(paragraphs, M2_beg, M2_end, score):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for p in paragraphs:\n",
    "        sentences = [s for s in nltk.sent_tokenize(p)]\n",
    "        sentences_cleaned = [clean_text(s) for s in sentences]\n",
    "\n",
    "        selected = [False for i in range(len(sentences))]\n",
    "        \n",
    "        beg_pred = [(M2_beg.predict(s)[0][0][len(\"__label__\"):] == 'Beginning_Instance', M2_beg.predict(s)[1][0]) for s in sentences_cleaned]\n",
    "        end_pred = [(M2_end.predict(s)[0][0][len(\"__label__\"):] == 'Ending_Instance', M2_end.predict(s)[1][0]) for s in sentences_cleaned]\n",
    "\n",
    "        labels = []\n",
    "\n",
    "        for i in range(len(beg_pred)):\n",
    "            beg_lab, beg_score = beg_pred[i]\n",
    "            end_lab, end_score = end_pred[i]\n",
    "            \n",
    "            if max(beg_score, end_score) < score:\n",
    "                labels.append(\"N\")\n",
    "                continue\n",
    "            \n",
    "            if beg_lab and end_lab:\n",
    "                labels.append(\"B\" if beg_score >= end_score else \"E\")\n",
    "            elif beg_lab:\n",
    "                labels.append(\"B\")\n",
    "            elif end_lab:\n",
    "                labels.append(\"E\")\n",
    "            else:\n",
    "                labels.append(\"N\")\n",
    "        \n",
    "        chunks = []\n",
    "\n",
    "        for i in range(len(labels)):        \n",
    "            if labels[i] == \"B\":\n",
    "                new_chunk = [sentences[i]]\n",
    "                if i != len(labels) - 1 and labels[i + 1] == \"E\":\n",
    "                    new_chunk.append(sentences[i + 1]) \n",
    "                \n",
    "                chunks.append(new_chunk)\n",
    "                \n",
    "        chunk_text = [\" \".join(c) for c in chunks]\n",
    "\n",
    "        results.append((p, chunk_text, selected))\n",
    "\n",
    "    return results\n",
    "\n",
    "def get_summary_begend(paragraphs_web, model1, model2, score):\n",
    "\n",
    "    results_web = get_results_begend(paragraphs_web, model1, model2, score)\n",
    "    instances = [r[1] for r in results_web]\n",
    "    instances = [\" \".join(i) for i in instances]\n",
    "    instances = [i for i in instances if len(i) > 0]\n",
    "    \n",
    "    return instances\n",
    "\n",
    "def get_results_multi(paragraphs, M3):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for p in paragraphs:\n",
    "        sentences = [s for s in nltk.sent_tokenize(p)]\n",
    "        selected = [False for i in range(len(sentences))]\n",
    "        sentences_cleaned = [clean_text(s) for s in sentences]\n",
    "\n",
    "        pred = [M3.predict(s)[0][0][len(\"__label__\"):][0] for s in sentences_cleaned]\n",
    "        chunk_idxs = [(m.start(0), m.end(0)) for m in re.finditer(\"B*M*E*\", \"\".join(pred))]\n",
    "        \n",
    "        chunks = []\n",
    "        for start, end in chunk_idxs:\n",
    "            if start == end:\n",
    "                continue\n",
    "            selected[start:end] = [True for i in range(end - start)]\n",
    "            chunks.append(sentences[start:end])\n",
    "        \n",
    "        chunk_text = [\" \".join(c) for c in chunks]\n",
    "        results.append((p, chunk_text, selected))\n",
    "        \n",
    "    return results\n",
    "\n",
    "def get_summary_multi(paragraphs_web, model):\n",
    "        \n",
    "    results_web = get_results_multi(paragraphs_web, model)\n",
    "    instances = [r[1] for r in results_web]\n",
    "    instances = [\" \".join(i) for i in instances]\n",
    "    instances = [i for i in instances if len(i) > 0]\n",
    "    \n",
    "    return instances\n",
    "\n",
    "def get_urls(topic, section):\n",
    "    query = f\"{topic} {section}\"\n",
    "    urls = [url for url in search(query, num_results = 15, lang = \"en\")]  \n",
    "    urls = [url for url in urls if \".org\" not in url and \".edu\" not in url and \"wikipedia\" not in url]\n",
    "    return urls[:min(5, len(urls))]\n",
    "\n",
    "def create_summary_data(topic, section, paragraphs_web, score):\n",
    "\n",
    "    path = f\"../Instance Classification/models/{topic}/{section}\"\n",
    "\n",
    "    beg_model = fasttext.load_model(f\"{path}/beg_model.bin\")\n",
    "    end_model = fasttext.load_model(f\"{path}/end_model.bin\")\n",
    "\n",
    "    inst_model = fasttext.load_model(f\"{path}/inst_model.bin\")\n",
    "\n",
    "    multi_model = fasttext.load_model(f\"{path}/multi_model.bin\")\n",
    "\n",
    "    s1 = get_summary_begend(paragraphs_web, beg_model, end_model, score)\n",
    "    s2 = get_summary_inst(paragraphs_web, inst_model, score)\n",
    "    s3 = get_summary_multi(paragraphs_web, multi_model)\n",
    "\n",
    "    return s1, s2, s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f4599404",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Until the late 1970s, it was a part of AI’s evolution.\n",
      "\n",
      "Machine learning is a necessary aspect of modern business and research for many organizations today.\n",
      "\n",
      "The model was created in 1949 by Donald Hebb in a book titled The Organization of Behavior (PDF).\n",
      "\n",
      "Arthur Samuel of IBM developed a computer program for playing checkers in the 1950s. His design included a scoring function using the positions of the pieces on the board. The program chooses its next move using a minimax strategy, which eventually evolved into the minimax algorithm.\n",
      "\n",
      "In what Samuel called rote learning, his program recorded/remembered all positions it had already seen and combined this with the values of the reward function. Arthur Samuel first came up with the phrase “machine learning” in 1952.\n",
      "\n",
      "In 1957, Frank Rosenblatt – at the Cornell Aeronautical Laboratory – combined Donald Hebb’s model of brain cell interaction with Arthur Samuel’s machine learning efforts and created the perceptron. The perceptron was initially planned as a machine, not a program. The software, originally designed for the IBM 704, was installed in a custom-built machine called the Mark 1 perceptron, which had been constructed for image recognition. This made the software and the algorithms transferable and available for other machines.\n",
      "\n",
      "Described as the first successful neuro-computer, the Mark I perceptron developed some problems with broken expectations. Neural network/machine learning research struggled until a resurgence during the 1990s.\n",
      "\n",
      "In 1967, the nearest neighbor algorithm was conceived, which was the beginning of basic pattern recognition. This algorithm was used for mapping routes and was one of the earliest algorithms used in finding a solution to the traveling salesperson’s problem of finding the most efficient route. Marcello Pelillo has been given credit for inventing the “nearest neighbor rule.” He, in turn, credits the famous Cover and Hart paper of 1967 (PDF).\n",
      "\n",
      "In the 1960s, the discovery and use of multilayers opened a new path in neural network research. It was discovered that providing and using two or more layers in the perceptron offered significantly more processing power than a perceptron using one layer.\n",
      "\n",
      "In the late 1970s and early 1980s, artificial intelligence research had focused on using logical, knowledge-based approaches rather than algorithms. Until then, machine learning had been used as a training program for AI.\n",
      "\n",
      "The machine learning industry, which included a large number of researchers and technicians, was reorganized into a separate field and struggled for nearly a decade. During this time, the ML industry maintained its focus on neural networks and then flourished in the 1990s.\n",
      "\n",
      "“Boosting” was a necessary development for the evolution of machine learning. The concept of boosting was first presented in a 1990 paper titled “The Strength of Weak Learnability,” by Robert Schapire.\n",
      "\n",
      "This environment allows future weak learners to focus\n",
      "more extensively on previous weak learners that were misclassified.\n",
      "\n",
      "AdaBoost is a popular machine learning algorithm and historically significant, being the first algorithm capable of working with weak learners.\n",
      "\n",
      "Around the year 2007, long short-term memory started outperforming more traditional speech recognition programs. In 2015, the Google speech recognition program reportedly had a significant performance jump of 49 percent using a CTC-trained LSTM.\n",
      "\n",
      "Google is currently experimenting with machine learning using an approach called instruction fine-tuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s1, s2, s3 = create_summary_data(topic, section, paragraphs_web, 0.97)\n",
    "fasttext_summary = \"\\n\\n\".join(s2)\n",
    "print(fasttext_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "515c81b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 1408\n"
     ]
    }
   ],
   "source": [
    "num_sentences = 0\n",
    "length = 0\n",
    "for inst in s1:\n",
    "    length += len(inst)\n",
    "    sentences = nltk.sent_tokenize(inst)\n",
    "    num_sentences += len(sentences)\n",
    "            \n",
    "print(num_sentences, length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cfeb8e",
   "metadata": {},
   "source": [
    "### BERT Extractive Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc4ee165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nishant\\anaconda3\\envs\\haystack\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from summarizer import Summarizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbaffcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_bert = Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d22035c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = model_bert(\"\\n\".join(paragraphs_web), num_sentences = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27e2ca02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We suggest you try the following to help find what you're looking for:\n",
      "Build, test, and deploy applications by applying natural language processing—for free.\n",
      "\n",
      "These data sets are so voluminous that traditional data processing software just can’t manage them.\n",
      "\n",
      "A large part of the value they offer comes from their data, which they’re constantly analyzing to produce more efficiency and develop new products.\n",
      "\n",
      "The development of open-source frameworks, such as Hadoop (and more recently, Spark) was essential for the growth of big data because they make big data easier to work with and cheaper to store.\n",
      "\n",
      "The emergence of machine learning has produced still more data.\n",
      "\n",
      "Finally, big data technology is changing at a rapid pace.\n",
      "\n",
      "Getting started involves three key actions:\n",
      "1.\n",
      "\n",
      "Many people choose their storage solution according to where their data is currently residing.\n",
      "\n",
      "Analyze\r\n",
      "                        Your investment in big data pays off when you analyze and act on your data.\n",
      "\n",
      "Build data models with machine learning and artificial intelligence.\n",
      "\n",
      "For example, there is a difference in distinguishing all customer sentiment from that of only your best customers.\n",
      "\n",
      "Big data analytical capabilities include statistics, spatial analysis, semantics, interactive discovery, and visualization.\n",
      "\n",
      "Using analytical models, you can correlate different types and sources of data to make associations and meaningful discoveries.\n",
      "\n",
      "Sometimes we don’t even know what we’re looking for.\n"
     ]
    }
   ],
   "source": [
    "sent = nltk.sent_tokenize(summary)\n",
    "bert_summary = \"\\n\\n\".join(sent)\n",
    "print(bert_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9313a401",
   "metadata": {},
   "source": [
    "## Multi Document Paragraph Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac890990",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "69e70437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_lemmatize(text_):\n",
    "\n",
    "    ret = \"\"\n",
    "    cleaned_text = clean_text(text_)\n",
    "    \n",
    "    for word in nltk.word_tokenize(cleaned_text):\n",
    "        if word in stopwords:\n",
    "            continue\n",
    "        ret += f\"{lemmatizer.lemmatize(word)} \"    \n",
    "    return ret[:-1]\n",
    "\n",
    "all_text = []\n",
    "\n",
    "for u in urls:\n",
    "    hdr = {'User-Agent': 'Mozilla/5.0'}\n",
    "    req = urllib2.Request(u, headers = hdr)\n",
    "    page = urllib2.urlopen(req, timeout = 10)\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "\n",
    "    paragraphs_web = soup.findAll(\"p\")\n",
    "    paragraphs_web = [p.text for p in paragraphs_web]\n",
    "    all_text.extend(paragraphs_web)\n",
    "    \n",
    "all_text_clean = [clean_lemmatize(t) for t in all_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f53bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9cb7f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text_tfidf = tf_idf.fit_transform(all_text_clean)\n",
    "all_text_tfidf = tf_idf.transform(all_text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6d2f41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_tfidf = tf_idf.transform([clean_lemmatize(f\"{topic} {section}\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "086143b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(all_text[i], all_text_tfidf[i], len(nltk.sent_tokenize(all_text[i]))) for i in range(len(all_text))]\n",
    "data.sort(key = lambda item: cosine_similarity(item[1], topic_tfidf)[0][0], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50ddfa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_len = 0\n",
    "added = []\n",
    "itr = 0\n",
    "\n",
    "\n",
    "while total_len < num_sentences - 6:\n",
    "    curr_p, curr_vec, curr_num_sent = data[itr]\n",
    "    \n",
    "    can_be_added = True\n",
    "    total_sim = 0\n",
    "    for p, vec in added:\n",
    "        if cosine_similarity(vec, curr_vec)[0][0] > 0.5:\n",
    "            can_be_added = False\n",
    "            break\n",
    "            \n",
    "    if can_be_added:\n",
    "        added.append((curr_p, curr_vec))\n",
    "        total_len += len(nltk.sent_tokenize(curr_p))\n",
    "        \n",
    "    itr += 1\n",
    "    \n",
    "p_out = [a[0] for a in added]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "56a8d987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now, let’s learn Big Data definition\n",
      "\n",
      "What is Big Data?\n",
      "\n",
      "The definition of big data is data that contains greater variety, arriving in increasing volumes and with more velocity. This is also known as the three Vs.\n",
      "\n",
      "Big data refers to data that is so large, fast or complex that it’s difficult or impossible to process using traditional methods. The act of accessing and storing large amounts of information for analytics has been around for a long time. But the concept of big data gained momentum in the early 2000s when industry analyst Doug Laney articulated the now-mainstream definition of big data as the three V’s:\n",
      "\n",
      "Semi-structured data can contain both the forms of data. We can see semi-structured data as a structured in form but it is actually not defined with e.g. a table definition in relational DBMS. Example of semi-structured data is a data represented in an XML file.\n"
     ]
    }
   ],
   "source": [
    "paragraph_summary = \"\\n\\n\".join(p_out)\n",
    "print(paragraph_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62918918",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = f\"summaries/{topic}\"\n",
    "path2 = f\"summaries/{topic}/{section}\"\n",
    "\n",
    "if not os.path.exists(path1):\n",
    "    os.makedirs(path1)\n",
    "if not os.path.exists(path2):\n",
    "    os.makedirs(path2)\n",
    "\n",
    "fasttext_file = open(f\"{path2}/fasttext.txt\", \"w+\", encoding = \"utf-8\")\n",
    "bert_file = open(f\"{path2}/bert.txt\", \"w+\", encoding = \"utf-8\")\n",
    "paragraph_file = open(f\"{path2}/paragraph.txt\", \"w+\", encoding = \"utf-8\")\n",
    "\n",
    "fasttext_file.write(fasttext_summary)\n",
    "bert_file.write(bert_summary)\n",
    "paragraph_file.write(paragraph_summary)\n",
    "\n",
    "fasttext_file.close()\n",
    "bert_file.close()\n",
    "paragraph_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
